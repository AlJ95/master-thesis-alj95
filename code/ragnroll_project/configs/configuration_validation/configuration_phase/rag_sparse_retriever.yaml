components:
  llm:
    init_parameters:
      api_base_url: https://openrouter.ai/api/v1
      api_key:
        env_vars:
        - OPENROUTER_API_KEY
        strict: true
        type: env_var
      generation_kwargs: {}
      model: [deepseek/deepseek-chat-v3-0324, openai/gpt-4o-mini-2024-07-18]
      organization: null
      streaming_callback: null
      system_prompt: null
    type: haystack.components.generators.openai.OpenAIGenerator
  prompt_builder:
    init_parameters:
      required_variables: null
      template: '

        Given these documents, answer the question.

        Your answer is used within a pipeline and must follow the following format:
        "The answer is "valid"" or "The answer is "invalid"" with quotation marks. Also allowed are "yes" or "no", "true" or "false", "1" or "0".
        After stating your answer, explain your answer in detail.

        Example:
        Question: London is the capital of France.
        Answer: "false", because the capital of France is Paris, not London.

        Documents:

        {% for doc in documents %}\

        {{ doc.content }}

        {% endfor %}

        Question: {{query}}        

        Answer:

        '
      variables: null
    type: haystack.components.builders.prompt_builder.PromptBuilder
  retriever:
    init_parameters:
      document_store:
        init_parameters:
          bm25_algorithm: BM25L
          bm25_parameters: {}
          bm25_tokenization_regex: (?u)\b\w\w+\b
          index: documents
        type: haystack.document_stores.in_memory.document_store.InMemoryDocumentStore
      filter_policy: replace
      filters: null
      top_k: 10
    type: haystack.components.retrievers.in_memory.bm25_retriever.InMemoryBM25Retriever
  reranker:
    init_parameters:
      calibration_factor: 1.0
      device: null
      document_prefix: ''
      embedding_separator: '

        '
      meta_fields_to_embed: []
      model: cross-encoder/ms-marco-MiniLM-L-6-v2
      model_kwargs:
        device_map: cpu
      query_prefix: ''
      scale_score: true
      score_threshold: null
      token:
        env_vars:
        - HF_API_TOKEN
        - HF_TOKEN
        strict: false
        type: env_var
      tokenizer_kwargs: {}
      top_k: 10
    type: haystack.components.rankers.transformers_similarity.TransformersSimilarityRanker
  answer_builder:
    init_parameters:
      pattern: The answer is "(.*)".
      reference_pattern: null
    type: haystack.components.builders.answer_builder.AnswerBuilder
connections:
- receiver: llm.prompt
  sender: prompt_builder.prompt
- receiver: prompt_builder.documents
  sender: reranker.documents
- receiver: reranker.documents
  sender: embedding_retriever.documents
- receiver: embedding_retriever.query_embedding
  sender: embedder.embedding
- receiver: answer_builder.replies
  sender: llm.replies
max_runs_per_component: 10
metadata: 
  chunking:
    split: true
    chunk_size: 500
    chunk_overlap: 150
    chunk_separator: "\n\n"
  hypothesis: "Sparse RAGs have a higher retrieval quality than dense RAGs."
  reasoning: false