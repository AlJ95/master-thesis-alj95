id,content,source,source_type,metadata
1cc117f6d4c4097915063e92d2c062fff3b766700f6916e5a47a16a32f3ca303,"Docker Hub release notes
Here you can learn about the latest changes, new features, bug fixes, and known issues for each Docker Hub release.
2025-02-18
New
- You can delete images and image indexes using Image Management.
2024-12-12
New
- The AI Catalog in Docker Hub is available directly through Docker Desktop.
2024-03-23
New
- You can tag Docker Hub repositories with categories.
2023-12-11
The Advanced Image Management feature, along with the corresponding API endpoints, has been retired. See docker/roadmap#534.
The following API endpoints have been removed:
/namespaces/{namespace}/repositories/{repository}/images /namespaces/{namespace}/repositories/{repository}/images/{digest}/tags /namespaces/{namespace}/repositories/{repository}/images-summary /namespaces/{namespace}/delete-images
2023-08-28
- Organizations with SSO enabled can assign members to roles, organizations, and teams with SCIM role mapping.
2023-07-26
New
- Organizations can assign the editor role to members to grant additional permissions without full administrative access.
2023-05-09
New
- Docker Business subscribers can now create a company in Docker Hub to manage organizations and settings.
2023-03-07
New
- You can now automatically sync user updates with your Docker organizations and teams with Group Mapping for SSO and SCIM provisioning.
2022-12-12
New
- The new domain audit feature lets you audit your domains for users who aren't a member of your organization.
2022-09-26
New
- The new autobuild feature lets you view your in-progress logs every 30 seconds instead of when the build is complete.
2022-09-21
Bug fixes and enhancements
- In Docker Hub, you can now download a registry.json file or copy the commands to create a registry.json file to enforce sign-in for your organization.
2022-09-19
Bug fixes and enhancements
- You can now export a CSV file of members from organizations that you own.
2022-07-22
Bug fixes and enhancements
- You can now invite members to your organization with a CSV file containing their email addresses. The CSV file can either be a file you create for this specific purpose or one that’s extracted from another in-house system.
2022-07-19
Bug fixes and enhancements
- SCIM is now available for organizations with a Docker Business subscription using an Entra ID (formerly Azure AD) identity provider.
2022-06-23
New
- With SCIM, you can manage users within your Okta identity provider (IdP). In addition, you can enable SCIM on organizations that are part of the Docker Business subscription.
2022-05-24
New
- Registry Access Management is now available for all Docker Business subscriptions. When enabled, your users can access specific registries in Docker Hub.
2022-05-03
New
- Organization owners can invite new members to an organization via Docker ID or email address.
2021-11-15
New
- You can now purchase or upgrade to a Docker Business subscription using a credit card. To learn more, see Upgrade your subscription.
2021-08-31
New
Docker has announced updates and extensions to the product subscriptions to increase productivity, collaboration, and added security for our developers and businesses. Docker subscription tiers now include Personal, Pro, Team, and Business.
The updated Docker Subscription Service Agreement includes a change to the terms for Docker Desktop.
Docker Desktop remains free for small businesses (fewer than 250 employees AND less than $10 million in annual revenue), personal use, education, and non-commercial open source projects.
It requires a paid subscription (Pro, Team, or Business), for as little as $5 a month, for professional use in larger enterprises.
The effective date of these terms is August 31, 2021. There is a grace period until January 31, 2022 for those that will require a paid subscription to use Docker Desktop.
The Docker Pro and Docker Team subscriptions now include commercial use of Docker Desktop.
The existing Docker Free subscription has been renamed Docker Personal.
No changes to Docker Engine or any other upstream open source Docker or Moby project.
To understand how these changes affect you, read the FAQs. For more information, see Docker subscription overview.
2021-05-05
Enhancement
When managing the content of your repositories, you can now filter the results based on the currentness of the tags and more easily identify your untagged images.
For Docker Hub API documentation, see Docker Hub API Reference.
2021-04-13
Enhancement
The Billing Details page now shows any organizations you own, in addition to your personal account. This allows you to clearly identify the billing details of your chosen namespace, and enables you to switch between your personal and your organization accounts to view or update the details.
2021-04-09
Enhancement
You can now specify any email address to receive billing-related emails for your organization. The email address doesn't have to be associated with an organization owner account. You must be an owner of the organization to update any billing details.
To change the email address receiving billing-related emails, log into Docker Hub and navigate to the Billing tab of your organization. Select Payment Methods > Billing Information. Enter the new email address that you'd like to use in the Email field. Click Update for the changes to take effect.
For details on how to update your billing information, see Update billing information.
2021-03-22
New feature
Advanced Image Management dashboard
Docker introduces the Advanced Image Management dashboard that enables you to view and manage Docker images in your repositories.
2021-01-25
New feature
Docker introduces Audit logs, a new feature that allows team owners to view a list of activities that occur at organization and repository levels. This feature begins tracking the activities from the release date, that is, from 25 January 2021.
For more information about this feature and for instructions on how to use it, see Activity logs.
2020-11-10
New feature
The Repositories view now shows which images have gone stale because they haven't been pulled or pushed recently. For more information, see repository tags.
2020-10-07
New feature
Docker introduces Hub Vulnerability Scanning which enables you to automatically scan Docker images for vulnerabilities using Snyk. For more information, see Hub Vulnerability Scanning.
2020-05-14
New features
Docker has announced a new, per-seat pricing model to accelerate developer workflows for cloud-native development. The previous private repository/concurrent autobuild-based plans have been replaced with new Pro and Team plans that include unlimited private repositories. For more information, see Docker subscription.
Docker has enabled download rate limits for downloads and pull requests on Docker Hub. This caps the number of objects that users can download within a specified timeframe. For more information, see Usage and limits.
2019-11-04
Enhancements
- The
repositories page and all
related settings and tabs have been updated and moved from
cloud.docker.com
tohub.docker.com
. You can access the page at its new URL: https://hub.docker.com/repositories.
Known Issues
- Scan results don't appear for some official images.
2019-10-21
New features
Beta: Docker Hub now supports two-factor authentication (2FA). Enable it in your account settings, under the Security section.
If you lose both your 2FA authentication device and recovery code, you may not be able to recover your account.
Enhancements
- As a security measure, when two-factor authentication is enabled, the Docker CLI requires a personal access token instead of a password to log in.
Known Issues
- Scan results don't appear for some official images.
2019-10-02
Enhancements
- You can now manage teams and members straight from your
organization page.
Each organization page now breaks down into these tabs:
- New: Members - manage your members directly from this page (delete, add, or open their teams)
- New: Teams - search by team or username, and open up any team page to manage the team
- New: Invitees (conditional tab, only if an invite exists) - resend or remove invitations from this tab
- Repositories
- Settings
- Billing
Bug fixes
- Fixed an issue where Kinematic could not connect and log in to Docker Hub.
Known Issues
- Scan results don't appear for some official images.
2019-09-19
New features
- You can now create personal access tokens in Docker Hub and use them to authenticate from the Docker CLI. Find them in your account settings, under the new Security section.
Known Issues
- Scan results don't appear for some official images.
2019-09-16
Enhancements
- The billing page for personal accounts has been updated. You can access the page at its new URL: https://hub.docker.com/billing/plan.
Known Issues
- Scan results don't appear for some official images.
2019-09-05
Enhancements
- The
Tags
tab on an image page now provides additional information for each tag:- A list of digests associated with the tag
- The architecture it was built on
- The OS
- The user who most recently updated an image for a specific tag
- The security scan summary for Docker Official Images has been updated.
Known Issues
- Scan results don't appear for some official images.",,,
19fcf04484359c99b35d4b7ed4ac09ef3c72a8dc25b5456d4d245b3ff53d0c76,"Automated repository tests
Note
Automated builds require a Docker Pro, Team, or Business subscription.
Docker Hub can automatically test changes to your source code repositories
using containers. You can enable Autotest
on any Docker Hub repository
to run tests on each pull request to the source code repository to create a
continuous integration testing service.
Enabling Autotest
builds an image for testing purposes, but does not
automatically push the built image to the Docker repository. If you want to push
built images to your Docker Hub repository, enable
Automated Builds.
Set up automated test files
To set up your automated tests, create a docker-compose.test.yml
file which
defines a sut
service that lists the tests to be run.
The docker-compose.test.yml
file should be located in the same directory that
contains the Dockerfile used to build the image.
For example:
services:
sut:
build: .
command: run_tests.sh
The previous example builds the repository, and runs the run_tests.sh
file inside
a container using the built image.
You can define any number of linked services in this file. The only requirement
is that sut
is defined. Its return code determines if tests passed or not.
Tests pass if the sut
service returns 0
, and fail otherwise.
Note
Only the
sut
service and all other services listed independs_on
are started. If you have services that poll for changes in other services, be sure to include the polling services in thedepends_on
list to make sure all of your services start.
You can define more than one docker-compose.test.yml
file if needed. Any file
that ends in .test.yml
is used for testing, and the tests run sequentially.
You can also use
custom build hooks
to further customize your test behavior.
Note
If you enable automated builds, they also run any tests defined in the
test.yml
files.
Enable automated tests on a repository
To enable testing on a source code repository, you must first create an
associated build-repository in Docker Hub. Your Autotest
settings are
configured on the same page as
automated builds, however
you do not need to enable autobuilds to use autotest. Autobuild is enabled per
branch or tag, and you do not need to enable it at all.
Only branches that are configured to use autobuild push images to the Docker repository, regardless of the Autotest settings.
Sign in to Docker Hub and select Repositories.
Select the repository you want to enable
Autotest
on.From the repository view, select the Builds tab.
Select Configure automated builds.
Configure the automated build settings as explained in Automated builds.
At minimum you must configure:
- The source code repository
- The build location
- At least one build rule
Choose your Autotest option.
The following options are available:
Off
: No additional test builds. Tests only run if they're configured as part of an automated build.Internal pull requests
: Run a test build for any pull requests to branches that match a build rule, but only when the pull request comes from the same source repository.Internal and external pull requests
: Run a test build for any pull requests to branches that match a build rule, including when the pull request originated in an external source repository.
Important
For security purposes, autotest on external pull requests is limited on public repositories. Private images are not pulled and environment variables defined in Docker Hub are not available. Automated builds continue to work as usual.
Select Save to save the settings, or select Save and build to save and run an initial test.
Check your test results
From the repository's details page, select Timeline.
From this tab you can see any pending, in-progress, successful, and failed builds and test runs for the repository.
You can choose any timeline entry to view the logs for each test run.",,,
2653dd1174c1122115d1fd63f0ef6126244a89a49af05e0bf6952b750a0419bc,"Deactivate an organization
You can deactivate an account at any time. This section describes the prerequisites and steps to deactivate an organization account. For information on deactivating a user account, see Deactivate a user account.
Warning
All Docker products and services that use your Docker account or organization account will be inaccessible after deactivating your account.
Prerequisites
Before deactivating an organization, complete the following:
Download any images and tags you want to keep:
docker pull -a <image>:<tag>
.If you have an active Docker subscription, downgrade it to a free subscription.
Remove all other members within the organization.
Unlink your Github and Bitbucket accounts.
For Business organizations, remove your SSO connection.
Deactivate
Once you have completed all the previous steps, you can deactivate your organization.
Warning
This cannot be undone. Be sure you've gathered all the data you need from your organization before deactivating it.
- In Admin Console, choose the organization you want to deactivate.
- Under Organization settings, select Deactivate.
- Enter the organization name to confirm deactivation.
- Select Deactivate organization.
- On Docker Hub, select Organizations.
- Choose the organization you want to deactivate.
- In Settings, select the Deactivate Org tab and then Deactivate organization.",,,
e42857575f949db94908da87b9d55cf9d13935171e622b05de0c3b9f09b98700,"Local cache
The local
cache store is a simple cache option that stores your cache as files
in a directory on your filesystem, using an
OCI image layout
for the underlying directory structure. Local cache is a good choice if you're
just testing, or if you want the flexibility to self-manage a shared storage
solution.
Synopsis
$ docker buildx build --push -t <registry>/<image> \
--cache-to type=local,dest=path/to/local/dir[,parameters...] \
--cache-from type=local,src=path/to/local/dir .
The following table describes the available CSV parameters that you can pass to
--cache-to
and --cache-from
.
| Name | Option | Type | Default | Description |
|---|---|---|---|---|
src | cache-from | String | Path of the local directory where cache gets imported from. | |
digest | cache-from | String | Digest of manifest to import, see cache versioning. | |
dest | cache-to | String | Path of the local directory where cache gets exported to. | |
mode | cache-to | min ,max | min | Cache layers to export, see cache mode. |
oci-mediatypes | cache-to | true ,false | true | Use OCI media types in exported manifests, see OCI media types. |
image-manifest | cache-to | true ,false | false | When using OCI media types, generate an image manifest instead of an image index for the cache image, see OCI media types. |
compression | cache-to | gzip ,estargz ,zstd | gzip | Compression type, see cache compression. |
compression-level | cache-to | 0..22 | Compression level, see cache compression. | |
force-compression | cache-to | true ,false | false | Forcibly apply compression, see cache compression. |
ignore-error | cache-to | Boolean | false | Ignore errors caused by failed cache exports. |
If the src
cache doesn't exist, then the cache import step will fail, but the
build continues.
Cache versioning
This section describes how versioning works for caches on a local filesystem,
and how you can use the digest
parameter to use older versions of cache.
If you inspect the cache directory manually, you can see the resulting OCI image layout:
$ ls cache
blobs index.json ingest
$ cat cache/index.json | jq
{
""schemaVersion"": 2,
""manifests"": [
{
""mediaType"": ""application/vnd.oci.image.index.v1+json"",
""digest"": ""sha256:6982c70595cb91769f61cd1e064cf5f41d5357387bab6b18c0164c5f98c1f707"",
""size"": 1560,
""annotations"": {
""org.opencontainers.image.ref.name"": ""latest""
}
}
]
}
Like other cache types, local cache gets replaced on export, by replacing the
contents of the index.json
file. However, previous caches will still be
available in the blobs
directory. These old caches are addressable by digest,
and kept indefinitely. Therefore, the size of the local cache will continue to
grow (see
moby/buildkit#1896
for more information).
When importing cache using --cache-from
, you can specify the digest
parameter
to force loading an older version of the cache, for example:
$ docker buildx build --push -t <registry>/<image> \
--cache-to type=local,dest=path/to/local/dir \
--cache-from type=local,ref=path/to/local/dir,digest=sha256:6982c70595cb91769f61cd1e064cf5f41d5357387bab6b18c0164c5f98c1f707 .
Further reading
For an introduction to caching see Docker build cache.
For more information on the local
cache backend, see the
BuildKit README.",,,
78a8014e95e557a1352c3e219b7431fc9647ac1c5ba02d58e4dc5ea554a8249d,"Uninstall Docker Compose
Uninstalling Docker Compose depends on the method you have used to install Docker Compose. On this page you can find specific instructions to uninstall Docker Compose.
Uninstalling Docker Desktop
If you want to uninstall Docker Compose and you have installed Docker Desktop, see Uninstall Docker Desktop.
Note
Unless you have other Docker instances installed on that specific environment, you would be removing Docker altogether by uninstalling Docker Desktop.
Uninstalling the Docker Compose CLI plugin
To remove the Docker Compose CLI plugin, run:
Ubuntu, Debian:
$ sudo apt-get remove docker-compose-plugin
RPM-based distributions:
$ sudo yum remove docker-compose-plugin
Manually installed
If you used curl
to install Docker Compose CLI plugin, to uninstall it, run:
$ rm $DOCKER_CONFIG/cli-plugins/docker-compose
Remove for all users
Or, if you have installed Docker Compose for all users, run:
$ rm /usr/local/lib/docker/cli-plugins/docker-compose
Note
If you get a Permission denied error using either of the previous methods, you do not have the permissions needed to remove Docker Compose. To force the removal, prepend
sudo
to either of the previous instructions and run it again.
Inspect the location of the Compose CLI plugin
To check where Compose is installed, use:
$ docker info --format '{{range .ClientInfo.Plugins}}{{if eq .Name ""compose""}}{{.Path}}{{end}}{{end}}'",,,
2d250dc51744baaef3c23ede7608591f5a0ddcd5b6e82b5a4b00bf760a25b03e,"FAQs on SSO and managing users
How do I manage users when using SSO?
You can manage users through organizations in Docker Hub or Admin Console. When you configure SSO in Docker, you need to make sure an account exists for each user in your IdP account. When a user signs in to Docker for the first time using their domain email address, they will be automatically added to the organization after a successful authentication.
Do I need to manually add users to my organization?
No, you don’t need to manually add users to your organization in Docker or Admin Console. You just need to make sure an account for your users exists in your IdP. When users sign in to Docker, they're automatically assigned to the organization using their domain email address.
When a user signs in to Docker for the first time using their domain email address, they will be automatically added to the organization after a successful authentication.
Can users in my organization use different email addresses to authenticate through SSO?
During the SSO setup, you’ll have to specify the company email domains that are allowed to authenticate. All users in your organization must authenticate using the email domain specified during SSO setup. Some of your users may want to maintain a different account for their personal projects.
If SSO isn't enforced, users with an email address that doesn't match the verified email domain can sign in with username and password to join the organization as guests.
Can Docker organization and company owners approve users to join an organization and use a seat, rather than having them automatically added when SSO is enabled?
Organization owners and company owners can approve users by configuring their permissions through their IdP. If the user account is configured in the IdP, the user will be automatically added to the organization in Docker Hub as long as there’s an available seat.
How will users be made aware that they're being made a part of a Docker organization?
When SSO is enabled, users will be prompted to authenticate through SSO the next time they try to sign in to Docker Hub or Docker Desktop. The system will see the end-user has a domain email associated with the Docker ID they're trying to authenticate with, and prompts them to sign in with SSO email and credentials instead.
If users attempt to sign in through the CLI, they must authenticate using a personal access token (PAT).
Is it possible to force users of Docker Desktop to authenticate, and/or authenticate using their company’s domain?
Yes. Admins can
force users to authenticate with Docker Desktop using a registry key, .plist
file, or registry.json
file.
Once SSO enforcement is set up on their Docker Business organization or company on Hub, when the user is forced to authenticate with Docker Desktop, the SSO enforcement will also force users to authenticate through SSO with their IdP (instead of authenticating using their username and password).
Users may still be able to authenticate as a guest account using an email address that doesn't match the verified domain. However, they can only authenticate as guests if that non-domain email was invited.
Is it possible to convert existing users from non-SSO to SSO accounts?
Yes, you can convert existing users to an SSO account. To convert users from a non-SSO account:
- Ensure your users have a company domain email address and they have an account in your IdP.
- Verify that all users have Docker Desktop version 4.4.2 or later installed on their machines.
- Each user has created a PAT to replace their passwords to allow them to sign in through Docker CLI.
- Confirm that all CI/CD pipelines automation systems have replaced their passwords with PATs.
For detailed prerequisites and instructions on how to enable SSO, see Configure Single Sign-on.
What impact can users expect once we start onboarding them to SSO accounts?
When SSO is enabled and enforced, your users just have to sign in using the verified domain email address.
Is Docker SSO fully synced with the IdP?
Docker SSO provides Just-in-Time (JIT) provisioning by default, with an option to disable JIT. Users are provisioned when a user authenticates with SSO. If a user leaves the organization, administrators must sign in to Docker and manually remove the user from the organization.
SCIM is available to provide full synchronization with users and groups. When you auto-provision users with SCIM, the recommended configuration is to disable JIT so that all auto-provisioning is handled by SCIM.
Additionally, you can use the Docker Hub API to complete this process.
How does disabling Just-in-Time provisioning impact user sign-in?
The option to disable JIT is available when you use the Admin Console and enable SCIM. If a user attempts to sign in to Docker using an email address that is a verified domain for your SSO connection, they need to be a member of the organization to access it, or have a pending invitation to the organization. Users who don't meet these criteria will encounter an Access denied
error, and will need an administrator to invite them to the organization.
See SSO authentication with JIT provisioning disabled.
To auto-provision users without JIT provisioning, you can use SCIM.
What's the best way to provision the Docker subscription without SSO?
Company or organization owners can invite users through Docker Hub or Admin Console, by email address (for any user) or by Docker ID (assuming the user has an existing Docker account).
Can someone join an organization without an invitation? Is it possible to add specific users to an organization with existing email accounts?
Not without SSO. Joining requires an invite from an organization owner. When SSO is enforced, then the domains verified through SSO will let users automatically join the organization the next time they sign in as a user that has a domain email assigned.
When we send an invitation to the user, will the existing account be consolidated and retained?
Yes, the existing user account will join the organization with all assets retained.
How can I view, update, and remove multiple email addresses for my users?
We only support one email per user on the Docker platform.
How can I remove invitees to the organization who haven't signed in?
You can go to the Members page for your organization in Docker Hub or Admin Console, view pending invites, and remove invitees as needed.
Is the flow for service account authentication different from a UI user account?
No, we don't differentiate the two in product.
Is user information visible in Docker Hub?
All Docker accounts have a public profile associated with their namespace. If you don't want user information (for example, full name) to be visible, you can remove those attributes from your SSO and SCIM mappings. Alternatively, you can use a different identifier to replace a user's full name.
What happens to existing licensed users when SCIM is enabled?
Enabling SCIM does not immediately remove or modify existing licensed users in your Docker organization. They retain their current access and roles, but after enabling SCIM, you will manage them in your identity provider (IdP). If SCIM is later disabled, previously SCIM-managed users remain in Docker but are no longer automatically updated or removed based on your IdP.",,,
7e749337e3655bf615145d48da48a151379a26ae2c8208a3585867df64e33cdc,"Use WSL
The following section describes how to start developing your applications using Docker and WSL 2. We recommend that you have your code in your default Linux distribution for the best development experience using Docker and WSL 2. After you have turned on the WSL 2 feature on Docker Desktop, you can start working with your code inside the Linux distribution and ideally with your IDE still in Windows. This workflow is straightforward if you are using VS Code.
Develop with Docker and WSL 2
Open VS Code and install the Remote - WSL extension. This extension lets you work with a remote server in the Linux distribution and your IDE client still on Windows.
Open your terminal and type:
$ wsl
Navigate to your project directory and then type:
$ code .
This opens a new VS Code window connected remotely to your default Linux distribution which you can check in the bottom corner of the screen.
Alternatively, you can open your default Linux distribution from the Start menu, navigate to your project directory, and then run code .",,,
def108166de83972d41539ad206e1ae38f9cf59a4264e186b8da0b14b2c63eb6,"General FAQs for Docker accounts
What is a Docker ID?
A Docker ID is a username for your Docker account that lets you access Docker products. To create a Docker ID, you need an email address or you can sign up with your social or GitHub accounts. Your Docker ID must be between 4 and 30 characters long, and can only contain numbers and lowercase letters. You can't use any special characters or spaces.
For more information, see Docker ID. If your administrator enforces single sign-on (SSO), this provisions a Docker ID for new users.
Developers may have multiple Docker IDs in order to separate their Docker IDs associated with an organization with a Docker Business or Team subscription, and their personal use Docker IDs.
What if my Docker ID is taken?
All Docker IDs are first-come, first-served except for companies that have a US Trademark on a username. If you have a trademark for your namespace, Docker Support can retrieve the Docker ID for you.
What’s an organization?
An organization in Docker is a collection of teams and repositories that are managed together. Docker users become members of an organization once they're associated with that organization by an organization owner. An organization owner is a user with administrative access to the organization. For more information on creating organizations, see Create your organization.
What's an organization name or namespace?
The organization name, sometimes referred to as the organization namespace or the organization ID, is the unique identifier of a Docker organization. The organization name can't be the same as an existing Docker ID.
What are roles?
A role is a collection of permissions granted to members. Roles define access to perform actions in Docker Hub like creating repositories, managing tags, or viewing teams. See Roles and permissions.
What’s a team?
A team is a group of Docker users that belong to an organization. An organization can have multiple teams. An organization owner can then create new teams and add members to an existing team using Docker IDs or email address and by selecting a team the user should be part of. See Create and manage a team.
What's a company?
A company is a management layer that centralizes administration of multiple organizations. Administrators can add organizations with a Docker Business subscription to a company and configure settings for all organizations under the company. See Set up your company.
Who is an organization owner?
An organization owner is an administrator who has permissions to manage repositories, add members, and manage member roles. They have full access to private repositories, all teams, billing information, and organization settings. An organization owner can also specify repository permissions for each team in the organization. Only an organization owner can enable SSO for the organization. When SSO is enabled for your organization, the organization owner can also manage users.
Docker can auto-provision Docker IDs for new end-users or users who'd like to have a separate Docker ID for company use through SSO enforcement.
The organization owner can also add additional owners to help them manage users, teams, and repositories in the organization.
Can I configure multiple SSO identity providers (IdPs) to authenticate users to a single org?
Docker SSO allows only one IdP configuration per organization. For more information, see Configure SSO and SSO FAQs.
What is a service account?
Important
As of December 10, 2024, service accounts are no longer available. Existing Service Account agreements will be honored until their current term expires, but new purchases or renewals of service accounts no longer available and customers must renew under a new subscription plan. It is recommended to transition to Organization Access Tokens (OATs), which can provide similar functionality. For more information, see Organization access tokens (Beta).
A service account is a Docker ID used for automated management of container images or containerized applications. Service accounts are typically used in automated workflows, and don't share Docker IDs with the members in the Team or Business plan. Common use cases for service accounts include mirroring content on Docker Hub, or tying in image pulls from your CI/CD process.
Can I delete or deactivate a Docker account for another user?
Only someone with access to the Docker account can deactivate the account. For more details, see Deactivating an account.
If the user is a member of your organization, you can remove the user from your organization. For more details, see Remove a member or invitee.
How do I manage settings for a user account?
You can manage your account settings anytime when you sign in to your Docker account. In Docker Home, select your avatar in the top-right navigation, then select My Account. You can also access this menu from any Docker web applications when you're signed in to your account. See Manage your Docker account. If your account is associated with an organization that uses SSO, you may have limited access to the settings that you can control.
How do I add an avatar to my Docker account?
To add an avatar to your Docker account, create a Gravatar account and create your avatar. Next, add your Gravatar email to your Docker account settings.
Note, that it may take some time for your avatar to update in Docker.",,,
3c58c78634d5fb052091eacb19f14a6f65eaa4072e0917808cdeb30c31d5e7f2,"Join nodes to a swarm
When you first create a swarm, you place a single Docker Engine into Swarm mode. To take full advantage of Swarm mode you can add nodes to the swarm:
- Adding worker nodes increases capacity. When you deploy a service to a swarm, the engine schedules tasks on available nodes whether they are worker nodes or manager nodes. When you add workers to your swarm, you increase the scale of the swarm to handle tasks without affecting the manager raft consensus.
- Manager nodes increase fault-tolerance. Manager nodes perform the orchestration and cluster management functions for the swarm. Among manager nodes, a single leader node conducts orchestration tasks. If a leader node goes down, the remaining manager nodes elect a new leader and resume orchestration and maintenance of the swarm state. By default, manager nodes also run tasks.
Docker Engine joins the swarm depending on the join-token you provide to
the docker swarm join
command. The node only uses the token at join time. If
you subsequently rotate the token, it doesn't affect existing swarm nodes. Refer
to
Run Docker Engine in swarm mode.
Join as a worker node
To retrieve the join command including the join token for worker nodes, run the following command on a manager node:
$ docker swarm join-token worker
To add a worker to this swarm, run the following command:
docker swarm join \
--token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
192.168.99.100:2377
Run the command from the output on the worker to join the swarm:
$ docker swarm join \
--token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
192.168.99.100:2377
This node joined a swarm as a worker.
The docker swarm join
command does the following:
- Switches Docker Engine on the current node into Swarm mode.
- Requests a TLS certificate from the manager.
- Names the node with the machine hostname.
- Joins the current node to the swarm at the manager listen address based upon the swarm token.
- Sets the current node to
Active
availability, meaning it can receive tasks from the scheduler. - Extends the
ingress
overlay network to the current node.
Join as a manager node
When you run docker swarm join
and pass the manager token, Docker Engine
switches into Swarm mode the same as for workers. Manager nodes also participate
in the raft consensus. The new nodes should be Reachable
, but the existing
manager remains the swarm Leader
.
Docker recommends three or five manager nodes per cluster to implement high availability. Because Swarm-mode manager nodes share data using Raft, there must be an odd number of managers. The swarm can continue to function after as long as a quorum of more than half of the manager nodes are available.
For more detail about swarm managers and administering a swarm, see Administer and maintain a swarm of Docker Engines.
To retrieve the join command including the join token for manager nodes, run the following command on a manager node:
$ docker swarm join-token manager
To add a manager to this swarm, run the following command:
docker swarm join \
--token SWMTKN-1-61ztec5kyafptydic6jfc1i33t37flcl4nuipzcusor96k7kby-5vy9t8u35tuqm7vh67lrz9xp6 \
192.168.99.100:2377
Run the command from the output on the new manager node to join it to the swarm:
$ docker swarm join \
--token SWMTKN-1-61ztec5kyafptydic6jfc1i33t37flcl4nuipzcusor96k7kby-5vy9t8u35tuqm7vh67lrz9xp6 \
192.168.99.100:2377
This node joined a swarm as a manager.
Learn More
swarm join
command line reference- Swarm mode tutorial",,,
4e244640216e866353e45cc80cc942f42f52df9b50e2d010dbb8d1234d563644,"Scale the service in the swarm
Once you have deployed a service to a swarm, you are ready to use the Docker CLI to scale the number of containers in the service. Containers running in a service are called tasks.
If you haven't already, open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named
manager1
.Run the following command to change the desired state of the service running in the swarm:
$ docker service scale <SERVICE-ID>=<NUMBER-OF-TASKS>
For example:
$ docker service scale helloworld=5 helloworld scaled to 5
Run
docker service ps <SERVICE-ID>
to see the updated task list:$ docker service ps helloworld NAME IMAGE NODE DESIRED STATE CURRENT STATE helloworld.1.8p1vev3fq5zm0mi8g0as41w35 alpine worker2 Running Running 7 minutes helloworld.2.c7a7tcdq5s0uk3qr88mf8xco6 alpine worker1 Running Running 24 seconds helloworld.3.6crl09vdcalvtfehfh69ogfb1 alpine worker1 Running Running 24 seconds helloworld.4.auky6trawmdlcne8ad8phb0f1 alpine manager1 Running Running 24 seconds helloworld.5.ba19kca06l18zujfwxyc5lkyn alpine worker2 Running Running 24 seconds
You can see that swarm has created 4 new tasks to scale to a total of 5 running instances of Alpine Linux. The tasks are distributed between the three nodes of the swarm. One is running on
manager1
.Run
docker ps
to see the containers running on the node where you're connected. The following example shows the tasks running onmanager1
:$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 528d68040f95 alpine:latest ""ping docker.com"" About a minute ago Up About a minute helloworld.4.auky6trawmdlcne8ad8phb0f1
If you want to see the containers running on other nodes, ssh into those nodes and run the
docker ps
command.
Next steps
At this point in the tutorial, you're finished with the helloworld
service. Next, you'll delete the service",,,
c6044e4de3d2a9cb839157c5a6ebdf87954e16d36200bd4f39720224270d5d54,"OverlayFS storage driver
OverlayFS is a union filesystem.
This page refers to the Linux kernel driver as OverlayFS
and to the Docker
storage driver as overlay2
.
Note
For
fuse-overlayfs
driver, check Rootless mode documentation.
Prerequisites
OverlayFS is the recommended storage driver, and supported if you meet the following prerequisites:
Version 4.0 or higher of the Linux kernel, or RHEL or CentOS using version 3.10.0-514 of the kernel or higher.
The
overlay2
driver is supported onxfs
backing filesystems, but only withd_type=true
enabled.Use
xfs_info
to verify that theftype
option is set to1
. To format anxfs
filesystem correctly, use the flag-n ftype=1
.Changing the storage driver makes existing containers and images inaccessible on the local system. Use
docker save
to save any images you have built or push them to Docker Hub or a private registry before changing the storage driver, so that you don't need to re-create them later.
Configure Docker with the overlay2
storage driver
Before following this procedure, you must first meet all the prerequisites.
The following steps outline how to configure the overlay2
storage driver.
Stop Docker.
$ sudo systemctl stop docker
Copy the contents of
/var/lib/docker
to a temporary location.$ cp -au /var/lib/docker /var/lib/docker.bk
If you want to use a separate backing filesystem from the one used by
/var/lib/
, format the filesystem and mount it into/var/lib/docker
. Make sure to add this mount to/etc/fstab
to make it permanent.Edit
/etc/docker/daemon.json
. If it doesn't yet exist, create it. Assuming that the file was empty, add the following contents.{ ""storage-driver"": ""overlay2"" }
Docker doesn't start if the
daemon.json
file contains invalid JSON.Start Docker.
$ sudo systemctl start docker
Verify that the daemon is using the
overlay2
storage driver. Use thedocker info
command and look forStorage Driver
andBacking filesystem
.$ docker info Containers: 0 Images: 0 Storage Driver: overlay2 Backing Filesystem: xfs Supports d_type: true Native Overlay Diff: true <...>
Docker is now using the overlay2
storage driver and has automatically
created the overlay mount with the required lowerdir
, upperdir
, merged
,
and workdir
constructs.
Continue reading for details about how OverlayFS works within your Docker containers, as well as performance advice and information about limitations of its compatibility with different backing filesystems.
How the overlay2
driver works
OverlayFS layers two directories on a single Linux host and presents them as
a single directory. These directories are called layers, and the unification
process is referred to as a union mount. OverlayFS refers to the lower directory
as lowerdir
and the upper directory as upperdir
. The unified view is exposed
through its own directory called merged
.
The overlay2
driver natively supports up to 128 lower OverlayFS layers. This
capability provides better performance for layer-related Docker commands such
as docker build
and docker commit
, and consumes fewer inodes on the backing
filesystem.
Image and container layers on-disk
After downloading a five-layer image using docker pull ubuntu
, you can see
six directories under /var/lib/docker/overlay2
.
Warning
Don't directly manipulate any files or directories within
/var/lib/docker/
. These files and directories are managed by Docker.
$ ls -l /var/lib/docker/overlay2
total 24
drwx------ 5 root root 4096 Jun 20 07:36 223c2864175491657d238e2664251df13b63adb8d050924fd1bfcdb278b866f7
drwx------ 3 root root 4096 Jun 20 07:36 3a36935c9df35472229c57f4a27105a136f5e4dbef0f87905b2e506e494e348b
drwx------ 5 root root 4096 Jun 20 07:36 4e9fa83caff3e8f4cc83693fa407a4a9fac9573deaf481506c102d484dd1e6a1
drwx------ 5 root root 4096 Jun 20 07:36 e8876a226237217ec61c4baf238a32992291d059fdac95ed6303bdff3f59cff5
drwx------ 5 root root 4096 Jun 20 07:36 eca1e4e1694283e001f200a667bb3cb40853cf2d1b12c29feda7422fed78afed
drwx------ 2 root root 4096 Jun 20 07:36 l
The new l
(lowercase L
) directory contains shortened layer identifiers as
symbolic links. These identifiers are used to avoid hitting the page size
limitation on arguments to the mount
command.
$ ls -l /var/lib/docker/overlay2/l
total 20
lrwxrwxrwx 1 root root 72 Jun 20 07:36 6Y5IM2XC7TSNIJZZFLJCS6I4I4 -> ../3a36935c9df35472229c57f4a27105a136f5e4dbef0f87905b2e506e494e348b/diff
lrwxrwxrwx 1 root root 72 Jun 20 07:36 B3WWEFKBG3PLLV737KZFIASSW7 -> ../4e9fa83caff3e8f4cc83693fa407a4a9fac9573deaf481506c102d484dd1e6a1/diff
lrwxrwxrwx 1 root root 72 Jun 20 07:36 JEYMODZYFCZFYSDABYXD5MF6YO -> ../eca1e4e1694283e001f200a667bb3cb40853cf2d1b12c29feda7422fed78afed/diff
lrwxrwxrwx 1 root root 72 Jun 20 07:36 NFYKDW6APBCCUCTOUSYDH4DXAT -> ../223c2864175491657d238e2664251df13b63adb8d050924fd1bfcdb278b866f7/diff
lrwxrwxrwx 1 root root 72 Jun 20 07:36 UL2MW33MSE3Q5VYIKBRN4ZAGQP -> ../e8876a226237217ec61c4baf238a32992291d059fdac95ed6303bdff3f59cff5/diff
The lowest layer contains a file called link
, which contains the name of the
shortened identifier, and a directory called diff
which contains the
layer's contents.
$ ls /var/lib/docker/overlay2/3a36935c9df35472229c57f4a27105a136f5e4dbef0f87905b2e506e494e348b/
diff link
$ cat /var/lib/docker/overlay2/3a36935c9df35472229c57f4a27105a136f5e4dbef0f87905b2e506e494e348b/link
6Y5IM2XC7TSNIJZZFLJCS6I4I4
$ ls /var/lib/docker/overlay2/3a36935c9df35472229c57f4a27105a136f5e4dbef0f87905b2e506e494e348b/diff
bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var
The second-lowest layer, and each higher layer, contain a file called lower
,
which denotes its parent, and a directory called diff
which contains its
contents. It also contains a merged
directory, which contains the unified
contents of its parent layer and itself, and a work
directory which is used
internally by OverlayFS.
$ ls /var/lib/docker/overlay2/223c2864175491657d238e2664251df13b63adb8d050924fd1bfcdb278b866f7
diff link lower merged work
$ cat /var/lib/docker/overlay2/223c2864175491657d238e2664251df13b63adb8d050924fd1bfcdb278b866f7/lower
l/6Y5IM2XC7TSNIJZZFLJCS6I4I4
$ ls /var/lib/docker/overlay2/223c2864175491657d238e2664251df13b63adb8d050924fd1bfcdb278b866f7/diff/
etc sbin usr var
To view the mounts which exist when you use the overlay
storage driver with
Docker, use the mount
command. The output below is truncated for readability.
$ mount | grep overlay
overlay on /var/lib/docker/overlay2/9186877cdf386d0a3b016149cf30c208f326dca307529e646afce5b3f83f5304/merged
type overlay (rw,relatime,
lowerdir=l/DJA75GUWHWG7EWICFYX54FIOVT:l/B3WWEFKBG3PLLV737KZFIASSW7:l/JEYMODZYFCZFYSDABYXD5MF6YO:l/UL2MW33MSE3Q5VYIKBRN4ZAGQP:l/NFYKDW6APBCCUCTOUSYDH4DXAT:l/6Y5IM2XC7TSNIJZZFLJCS6I4I4,
upperdir=9186877cdf386d0a3b016149cf30c208f326dca307529e646afce5b3f83f5304/diff,
workdir=9186877cdf386d0a3b016149cf30c208f326dca307529e646afce5b3f83f5304/work)
The rw
on the second line shows that the overlay
mount is read-write.
The following diagram shows how a Docker image and a Docker container are
layered. The image layer is the lowerdir
and the container layer is the
upperdir
. If the image has multiple layers, multiple lowerdir
directories
are used. The unified view is exposed through a directory called merged
which
is effectively the containers mount point.
Where the image layer and the container layer contain the same files, the
container layer (upperdir
) takes precedence and obscures the existence of the
same files in the image layer.
To create a container, the overlay2
driver combines the directory representing
the image's top layer plus a new directory for the container. The image's
layers are the lowerdirs
in the overlay and are read-only. The new directory for
the container is the upperdir
and is writable.
Image and container layers on-disk
The following docker pull
command shows a Docker host downloading a Docker
image comprising five layers.
$ docker pull ubuntu
Using default tag: latest
latest: Pulling from library/ubuntu
5ba4f30e5bea: Pull complete
9d7d19c9dc56: Pull complete
ac6ad7efd0f9: Pull complete
e7491a747824: Pull complete
a3ed95caeb02: Pull complete
Digest: sha256:46fb5d001b88ad904c5c732b086b596b92cfb4a4840a3abd0e35dbb6870585e4
Status: Downloaded newer image for ubuntu:latest
The image layers
Each image layer has its own directory within /var/lib/docker/overlay/
, which
contains its contents, as shown in the following example. The image layer IDs
don't correspond to the directory IDs.
Warning
Don't directly manipulate any files or directories within
/var/lib/docker/
. These files and directories are managed by Docker.
$ ls -l /var/lib/docker/overlay/
total 20
drwx------ 3 root root 4096 Jun 20 16:11 38f3ed2eac129654acef11c32670b534670c3a06e483fce313d72e3e0a15baa8
drwx------ 3 root root 4096 Jun 20 16:11 55f1e14c361b90570df46371b20ce6d480c434981cbda5fd68c6ff61aa0a5358
drwx------ 3 root root 4096 Jun 20 16:11 824c8a961a4f5e8fe4f4243dab57c5be798e7fd195f6d88ab06aea92ba931654
drwx------ 3 root root 4096 Jun 20 16:11 ad0fe55125ebf599da124da175174a4b8c1878afe6907bf7c78570341f308461
drwx------ 3 root root 4096 Jun 20 16:11 edab9b5e5bf73f2997524eebeac1de4cf9c8b904fa8ad3ec43b3504196aa3801
The image layer directories contain the files unique to that layer as well as hard links to the data shared with lower layers. This allows for efficient use of disk space.
$ ls -i /var/lib/docker/overlay2/38f3ed2eac129654acef11c32670b534670c3a06e483fce313d72e3e0a15baa8/root/bin/ls
19793696 /var/lib/docker/overlay2/38f3ed2eac129654acef11c32670b534670c3a06e483fce313d72e3e0a15baa8/root/bin/ls
$ ls -i /var/lib/docker/overlay2/55f1e14c361b90570df46371b20ce6d480c434981cbda5fd68c6ff61aa0a5358/root/bin/ls
19793696 /var/lib/docker/overlay2/55f1e14c361b90570df46371b20ce6d480c434981cbda5fd68c6ff61aa0a5358/root/bin/ls
The container layer
Containers also exist on-disk in the Docker host's filesystem under
/var/lib/docker/overlay/
. If you list a running container's subdirectory
using the ls -l
command, three directories and one file exist:
$ ls -l /var/lib/docker/overlay2/<directory-of-running-container>
total 16
-rw-r--r-- 1 root root 64 Jun 20 16:39 lower-id
drwxr-xr-x 1 root root 4096 Jun 20 16:39 merged
drwxr-xr-x 4 root root 4096 Jun 20 16:39 upper
drwx------ 3 root root 4096 Jun 20 16:39 work
The lower-id
file contains the ID of the top layer of the image the container
is based on, which is the OverlayFS lowerdir
.
$ cat /var/lib/docker/overlay2/ec444863a55a9f1ca2df72223d459c5d940a721b2288ff86a3f27be28b53be6c/lower-id
55f1e14c361b90570df46371b20ce6d480c434981cbda5fd68c6ff61aa0a5358
The upper
directory contains the contents of the container's read-write layer,
which corresponds to the OverlayFS upperdir
.
The merged
directory is the union mount of the lowerdir
and upperdirs
, which
comprises the view of the filesystem from within the running container.
The work
directory is internal to OverlayFS.
To view the mounts which exist when you use the overlay2
storage driver with
Docker, use the mount
command. The following output is truncated for
readability.
$ mount | grep overlay
overlay on /var/lib/docker/overlay2/l/ec444863a55a.../merged
type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/55f1e14c361b.../root,
upperdir=/var/lib/docker/overlay2/l/ec444863a55a.../upper,
workdir=/var/lib/docker/overlay2/l/ec444863a55a.../work)
The rw
on the second line shows that the overlay
mount is read-write.
How container reads and writes work with overlay2
Reading files
Consider three scenarios where a container opens a file for read access with overlay.
The file does not exist in the container layer
If a container opens a file for read access and the file does not already exist
in the container (upperdir
) it is read from the image (lowerdir
). This
incurs very little performance overhead.
The file only exists in the container layer
If a container opens a file for read access and the file exists in the
container (upperdir
) and not in the image (lowerdir
), it's read directly
from the container.
The file exists in both the container layer and the image layer
If a container opens a file for read access and the file exists in the image
layer and the container layer, the file's version in the container layer is
read. Files in the container layer (upperdir
) obscure files with the same
name in the image layer (lowerdir
).
Modifying files or directories
Consider some scenarios where files in a container are modified.
Writing to a file for the first time
The first time a container writes to an existing file, that file does not
exist in the container (upperdir
). The overlay2
driver performs a
copy_up
operation to copy the file from the image (lowerdir
) to the
container (upperdir
). The container then writes the changes to the new copy
of the file in the container layer.
However, OverlayFS works at the file level rather than the block level. This
means that all OverlayFS copy_up
operations copy the entire file, even if
the file is large and only a small part of it's being modified. This can have
a noticeable impact on container write performance. However, two things are
worth noting:
The
copy_up
operation only occurs the first time a given file is written to. Subsequent writes to the same file operate against the copy of the file already copied up to the container.OverlayFS works with multiple layers. This means that performance can be impacted when searching for files in images with many layers.
Deleting files and directories
When a file is deleted within a container, a whiteout file is created in the container (
upperdir
). The version of the file in the image layer (lowerdir
) is not deleted (because thelowerdir
is read-only). However, the whiteout file prevents it from being available to the container.When a directory is deleted within a container, an opaque directory is created within the container (
upperdir
). This works in the same way as a whiteout file and effectively prevents the directory from being accessed, even though it still exists in the image (lowerdir
).
Renaming directories
Calling rename(2)
for a directory is allowed only when both the source and
the destination path are on the top layer. Otherwise, it returns EXDEV
error
(""cross-device link not permitted""). Your application needs to be designed to
handle EXDEV
and fall back to a ""copy and unlink"" strategy.
OverlayFS and Docker Performance
overlay2
may perform better than btrfs
. However, be aware of the following details:
Page caching
OverlayFS supports page cache sharing. Multiple containers accessing the same
file share a single page cache entry for that file. This makes the overlay2
drivers efficient with memory and a good option for high-density use cases such
as PaaS.
Copyup
As with other copy-on-write filesystems, OverlayFS performs copy-up operations whenever a container writes to a file for the first time. This can add latency into the write operation, especially for large files. However, once the file has been copied up, all subsequent writes to that file occur in the upper layer, without the need for further copy-up operations.
Performance best practices
The following generic performance best practices apply to OverlayFS.
Use fast storage
Solid-state drives (SSDs) provide faster reads and writes than spinning disks.
Use volumes for write-heavy workloads
Volumes provide the best and most predictable performance for write-heavy workloads. This is because they bypass the storage driver and don't incur any of the potential overheads introduced by thin provisioning and copy-on-write. Volumes have other benefits, such as allowing you to share data among containers and persisting your data even if no running container is using them.
Limitations on OverlayFS compatibility
To summarize the OverlayFS's aspect which is incompatible with other filesystems:
open(2)
- OverlayFS only implements a subset of the POSIX standards.
This can result in certain OverlayFS operations breaking POSIX standards. One
such operation is the copy-up operation. Suppose that your application calls
fd1=open(""foo"", O_RDONLY)
and thenfd2=open(""foo"", O_RDWR)
. In this case, your application expectsfd1
andfd2
to refer to the same file. However, due to a copy-up operation that occurs after the second calling toopen(2)
, the descriptors refer to different files. Thefd1
continues to reference the file in the image (lowerdir
) and thefd2
references the file in the container (upperdir
). A workaround for this is totouch
the files which causes the copy-up operation to happen. All subsequentopen(2)
operations regardless of read-only or read-write access mode reference the file in the container (upperdir
).yum
is known to be affected unless theyum-plugin-ovl
package is installed. If theyum-plugin-ovl
package is not available in your distribution such as RHEL/CentOS prior to 6.8 or 7.2, you may need to runtouch /var/lib/rpm/*
before runningyum install
. This package implements thetouch
workaround referenced above foryum
. rename(2)
- OverlayFS does not fully support the
rename(2)
system call. Your application needs to detect its failure and fall back to a ""copy and unlink"" strategy.",,,
590e9316b696e756f47c2b4c1de17ad2555bc0f03939ab48b232f9167f208143,"FAQS on SSO and domains
Can I add sub-domains?
Yes, you can add sub-domains to your SSO connection, however all email addresses should also be on that domain. Verify that your DNS provider supports multiple TXT records for the same domain.
Can the DNS provider configure it once for one-time verification and remove it later or will it be needed permanently?
You can do it one time to add the domain to a connection. If your organization ever changes IdPs and has to set up SSO again, your DNS provider will need to verify again.
Is adding domain required to configure SSO? What domains should I be adding? And how do I add it?
Adding and verifying a domain is required to enable and enforce SSO. See
Configure single sign-on for more information. This should include all email domains users will use to access Docker. Public domains, for example gmail.com
or outlook.com
, are not permitted. Also, the email domain should be set as the primary email.
Is IdP-initiated authentication supported?
IdP-initiated authentication isn't supported by Docker SSO. Users must initiate sign-in through Docker Desktop or Hub.
Can I verify the same domain on multiple organizations?
You can't verify the same domain for multiple orgnaizations at the organization level. If you want to verify one domain for multiple organizations, you must have a Docker Business subscription, and create a company. A company enables centralized management of organizations and allows domain verification at the company level.",,,
6a3b6943b81ce6c9bb8135ee02d57164a0dfc9bbccb00c4f5d803c3d4c088950,"Docker Hub catalogs
Docker Hub catalogs are your go-to collections of trusted, ready-to-use container images and resources, tailored to meet specific development needs. They make it easier to find high-quality, pre-verified content so you can quickly build, deploy, and manage your applications with confidence. Catalogs in Docker Hub:
- Simplify content discovery: Organized and curated content makes it easy to discover tools and resources tailored to your specific domain or technology.
- Reduce complexity: Trusted resources, vetted by Docker and its partners, ensure security, reliability, and adherence to best practices.
- Accelerate development: Quickly integrate advanced capabilities into your applications without the hassle of extensive research or setup.
The generative AI catalog is the first catalog in Docker Hub, offering specialized content for AI development.
Generative AI catalog
The generative AI catalog makes it easy to explore and add AI capabilities to your applications. With trusted, ready-to-use content and comprehensive documentation, you can skip the hassle of sorting through countless tools and configurations. Instead, focus your time and energy on creating innovative AI-powered applications.
The generative AI catalog provides a wide range of trusted content, organized into key areas to support diverse AI development needs:
- Demos: Ready-to-deploy examples showcasing generative AI capabilities. These demos provide a hands-on way to explore AI tools and frameworks, making it easier to understand how they can be integrated into real-world applications.
- Model Context Protocol (MCP) servers: MCP servers provide reusable toolsets that can be used across clients, like Claude Desktop.
- Models: Pre-trained AI models for tasks like text generation, Natural Language Processing (NLP), and conversational AI. These models provide a foundation for AI applications without requiring developers to train models from scratch.
- Applications and end-to-end platforms: Comprehensive platforms and tools that simplify AI application development, including low-code solutions and frameworks for building multi-agent and Retrieval-Augmented Generation (RAG) applications.
- Model deployment and serving: Tools and frameworks that enable developers to efficiently deploy and serve AI models in production environments. These resources include pre-configured stacks for GPUs and other specialized hardware, ensuring performance at scale.
- Orchestration: Solutions for managing complex AI workflows, such as workflow engines, Large Language Model (LLM) application frameworks, and lifecycle management tools, to help streamline development and operations.
- Machine learning frameworks: Popular frameworks like TensorFlow and PyTorch that provide the building blocks for creating, training, and fine-tuning machine learning models.
- Databases: Databases optimized for AI workloads, including vector databases for similarity search, time-series databases for analytics, and NoSQL solutions for handling unstructured data.
Note
For publishers, contact us to join the generative AI catalog.",,,
2d12bdfad645609b74a29d61001a5f11279efba1a98321647edcfe3e8e005c89,"Domain audit
Domain audit identifies uncaptured users in an organization. Uncaptured users are Docker users who have authenticated to Docker using an email address associated with one of your verified domains, but they're not a member of your organization in Docker. You can audit domains on organizations that are part of the Docker Business subscription. To upgrade your existing account to a Docker Business subscription, see Upgrade your subscription.
Uncaptured users who access Docker Desktop in your environment may pose a security risk because your organization's security settings, like Image Access Management and Registry Access Management, aren't applied to a user's session. In addition, you won't have visibility into the activity of uncaptured users. You can add uncaptured users to your organization to gain visibility into their activity and apply your organization's security settings.
Domain audit can't identify the following Docker users in your environment:
- Users who access Docker Desktop without authenticating
- Users who authenticate using an account that doesn't have an email address associated with one of your verified domains
Although domain audit can't identify all Docker users in your environment, you can enforce sign-in to prevent unidentifiable users from accessing Docker Desktop in your environment. For more details about enforcing sign-in, see Configure registry.json to enforce sign-in.
Tip
You can use endpoint management (MDM) software to identify the number of Docker Desktop instances and their versions within your environment. This can provide accurate license reporting, help ensure your machines use the latest version of Docker Desktop, and enable you to enforce sign-in.
Prerequisites
Before you audit your domains, review the following required prerequisites:
- Your organization must be part of a Docker Business subscription. To upgrade your existing account to a Docker Business subscription, see Upgrade your subscription.
- You must add and verify your domains.
Important
Domain audit is not supported for companies or organizations within a company.
Audit your domains for uncaptured users
To audit your domains:
Sign in to Docker Hub.
Select Organizations, your organization, Settings, and then Security.
In Domain Audit, select Export Users to export a CSV file of uncaptured users with the following columns:
- Name: The name of the user.
- Username: The Docker ID of the user.
- Email: The email address of the user.
You can invite all the uncaptured users to your organization using the exported CSV file. For more details, see Invite members. Optionally, enforce single sign-on or enable SCIM to add users to your organization automatically. For more details, see SSO or SCIM.
Note
Domain audit may identify accounts of users who are no longer a part of your organization. If you don't want to add a user to your organization and you don't want the user to appear in future domain audits, you must deactivate the account or update the associated email address.
Only someone with access to the Docker account can deactivate the account or update the associated email address. For more details, see Deactivating an account.
To audit your domains:
Sign in to the Admin Console.
Select your organization on the Choose profile page, and then select Domain management.
In Domain Audit, select Export Users to export a CSV file of uncaptured users with the following columns:
- Name: The name of the user.
- Username: The Docker ID of the user.
- Email: The email address of the user.
You can invite all the uncaptured users to your organization using the exported CSV file. For more details, see Invite members. Optionally, enforce single sign-on or enable SCIM to add users to your organization automatically. For more details, see SSO or SCIM.
Note
Domain audit may identify accounts of users who are no longer a part of your organization. If you don't want to add a user to your organization and you don't want the user to appear in future domain audits, you must deactivate the account or update the associated email address.
Only someone with access to the Docker account can deactivate the account or update the associated email address. For more details, see Deactivating an account.",,,
d624cc459c6eaff8d430a17794b4cc434d4b43808afcc5b069cf57f0a4551cf1,"Docker Engine
Docker Engine is an open source containerization technology for building and containerizing your applications. Docker Engine acts as a client-server application with:
- A server with a long-running daemon process
dockerd
. - APIs which specify interfaces that programs can use to talk to and instruct the Docker daemon.
- A command line interface (CLI) client
docker
.
The CLI uses Docker APIs to control or interact with the Docker daemon through scripting or direct CLI commands. Many other Docker applications use the underlying API and CLI. The daemon creates and manages Docker objects, such as images, containers, networks, and volumes.
For more details, see Docker Architecture.
Licensing
The Docker Engine is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.
However, for commercial use of Docker Engine obtained via Docker Desktop within larger enterprises (exceeding 250 employees OR with annual revenue surpassing $10 million USD), a paid subscription is required.",,,
ded4fa888e21c084b72751995f10dfeca4749c1f45ab55599527493fd8ce60db,"Install Docker Engine on Ubuntu
To get started with Docker Engine on Ubuntu, make sure you meet the prerequisites, and then follow the installation steps.
Prerequisites
Firewall limitations
Warning
Before you install Docker, make sure you consider the following security implications and firewall incompatibilities.
- If you use ufw or firewalld to manage firewall settings, be aware that when you expose container ports using Docker, these ports bypass your firewall rules. For more information, refer to Docker and ufw.
- Docker is only compatible with
iptables-nft
andiptables-legacy
. Firewall rules created withnft
are not supported on a system with Docker installed. Make sure that any firewall rulesets you use are created withiptables
orip6tables
, and that you add them to theDOCKER-USER
chain, see Packet filtering and firewalls.
OS requirements
To install Docker Engine, you need the 64-bit version of one of these Ubuntu versions:
- Ubuntu Oracular 24.10
- Ubuntu Noble 24.04 (LTS)
- Ubuntu Jammy 22.04 (LTS)
- Ubuntu Focal 20.04 (LTS)
Docker Engine for Ubuntu is compatible with x86_64 (or amd64), armhf, arm64, s390x, and ppc64le (ppc64el) architectures.
Note
Installation on Ubuntu derivative distributions, such as Linux Mint, is not officially supported (though it may work).
Uninstall old versions
Before you can install Docker Engine, you need to uninstall any conflicting packages.
Your Linux distribution may provide unofficial Docker packages, which may conflict with the official packages provided by Docker. You must uninstall these packages before you install the official version of Docker Engine.
The unofficial packages to uninstall are:
docker.io
docker-compose
docker-compose-v2
docker-doc
podman-docker
Moreover, Docker Engine depends on containerd
and runc
. Docker Engine
bundles these dependencies as one bundle: containerd.io
. If you have
installed the containerd
or runc
previously, uninstall them to avoid
conflicts with the versions bundled with Docker Engine.
Run the following command to uninstall all conflicting packages:
$ for pkg in docker.io docker-doc docker-compose docker-compose-v2 podman-docker containerd runc; do sudo apt-get remove $pkg; done
apt-get
might report that you have none of these packages installed.
Images, containers, volumes, and networks stored in /var/lib/docker/
aren't
automatically removed when you uninstall Docker. If you want to start with a
clean installation, and prefer to clean up any existing data, read the
uninstall Docker Engine section.
Installation methods
You can install Docker Engine in different ways, depending on your needs:
Docker Engine comes bundled with Docker Desktop for Linux. This is the easiest and quickest way to get started.
Set up and install Docker Engine from Docker's
apt
repository.Install it manually and manage upgrades manually.
Use a convenience script. Only recommended for testing and development environments.
Install using the apt
repository
Before you install Docker Engine for the first time on a new host machine, you
need to set up the Docker apt
repository. Afterward, you can install and update
Docker from the repository.
Set up Docker's
apt
repository.# Add Docker's official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \ ""deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \ $(. /etc/os-release && echo ""${UBUNTU_CODENAME:-$VERSION_CODENAME}"") stable"" | \ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update
Install the Docker packages.
To install the latest version, run:
$ sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
To install a specific version of Docker Engine, start by listing the available versions in the repository:
# List the available versions: $ apt-cache madison docker-ce | awk '{ print $3 }' 5:28.0.1-1~ubuntu.24.04~noble 5:28.0.0-1~ubuntu.24.04~noble ...
Select the desired version and install:
$ VERSION_STRING=5:28.0.1-1~ubuntu.24.04~noble $ sudo apt-get install docker-ce=$VERSION_STRING docker-ce-cli=$VERSION_STRING containerd.io docker-buildx-plugin docker-compose-plugin
Verify that the installation is successful by running the
hello-world
image:$ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Upgrade Docker Engine
To upgrade Docker Engine, follow step 2 of the installation instructions, choosing the new version you want to install.
Install from a package
If you can't use Docker's apt
repository to install Docker Engine, you can
download the deb
file for your release and install it manually. You need to
download a new file each time you want to upgrade Docker Engine.
Select your Ubuntu version in the list.
Go to
pool/stable/
and select the applicable architecture (amd64
,armhf
,arm64
, ors390x
).Download the following
deb
files for the Docker Engine, CLI, containerd, and Docker Compose packages:containerd.io_<version>_<arch>.deb
docker-ce_<version>_<arch>.deb
docker-ce-cli_<version>_<arch>.deb
docker-buildx-plugin_<version>_<arch>.deb
docker-compose-plugin_<version>_<arch>.deb
Install the
.deb
packages. Update the paths in the following example to where you downloaded the Docker packages.$ sudo dpkg -i ./containerd.io_<version>_<arch>.deb \ ./docker-ce_<version>_<arch>.deb \ ./docker-ce-cli_<version>_<arch>.deb \ ./docker-buildx-plugin_<version>_<arch>.deb \ ./docker-compose-plugin_<version>_<arch>.deb
The Docker daemon starts automatically.
Verify that the installation is successful by running the
hello-world
image:$ sudo service docker start $ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Upgrade Docker Engine
To upgrade Docker Engine, download the newer package files and repeat the installation procedure, pointing to the new files.
Install using the convenience script
Docker provides a convenience script at
https://get.docker.com/ to install Docker into
development environments non-interactively. The convenience script isn't
recommended for production environments, but it's useful for creating a
provisioning script tailored to your needs. Also refer to the
install using the repository steps to learn
about installation steps to install using the package repository. The source code
for the script is open source, and you can find it in the
docker-install
repository on GitHub.
Always examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script:
- The script requires
root
orsudo
privileges to run. - The script attempts to detect your Linux distribution and version and configure your package management system for you.
- The script doesn't allow you to customize most installation parameters.
- The script installs dependencies and recommendations without asking for confirmation. This may install a large number of packages, depending on the current configuration of your host machine.
- By default, the script installs the latest stable release of Docker, containerd, and runc. When using this script to provision a machine, this may result in unexpected major version upgrades of Docker. Always test upgrades in a test environment before deploying to your production systems.
- The script isn't designed to upgrade an existing Docker installation. When using the script to update an existing installation, dependencies may not be updated to the expected version, resulting in outdated versions.
Tip
Preview script steps before running. You can run the script with the
--dry-run
option to learn what steps the script will run when invoked:$ curl -fsSL https://get.docker.com -o get-docker.sh $ sudo sh ./get-docker.sh --dry-run
This example downloads the script from https://get.docker.com/ and runs it to install the latest stable release of Docker on Linux:
$ curl -fsSL https://get.docker.com -o get-docker.sh
$ sudo sh get-docker.sh
Executing docker install script, commit: 7cae5f8b0decc17d6571f9f52eb840fbc13b2737
<...>
You have now successfully installed and started Docker Engine. The docker
service starts automatically on Debian based distributions. On RPM
based
distributions, such as CentOS, Fedora, RHEL or SLES, you need to start it
manually using the appropriate systemctl
or service
command. As the message
indicates, non-root users can't run Docker commands by default.
Use Docker as a non-privileged user, or install in rootless mode?
The installation script requires
root
orsudo
privileges to install and use Docker. If you want to grant non-root users access to Docker, refer to the post-installation steps for Linux. You can also install Docker withoutroot
privileges, or configured to run in rootless mode. For instructions on running Docker in rootless mode, refer to run the Docker daemon as a non-root user (rootless mode).
Install pre-releases
Docker also provides a convenience script at
https://test.docker.com/ to install pre-releases of
Docker on Linux. This script is equal to the script at get.docker.com
, but
configures your package manager to use the test channel of the Docker package
repository. The test channel includes both stable and pre-releases (beta
versions, release-candidates) of Docker. Use this script to get early access to
new releases, and to evaluate them in a testing environment before they're
released as stable.
To install the latest version of Docker on Linux from the test channel, run:
$ curl -fsSL https://test.docker.com -o test-docker.sh
$ sudo sh test-docker.sh
Upgrade Docker after using the convenience script
If you installed Docker using the convenience script, you should upgrade Docker using your package manager directly. There's no advantage to re-running the convenience script. Re-running it can cause issues if it attempts to re-install repositories which already exist on the host machine.
Uninstall Docker Engine
Uninstall the Docker Engine, CLI, containerd, and Docker Compose packages:
$ sudo apt-get purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras
Images, containers, volumes, or custom configuration files on your host aren't automatically removed. To delete all images, containers, and volumes:
$ sudo rm -rf /var/lib/docker $ sudo rm -rf /var/lib/containerd
Remove source list and keyrings
$ sudo rm /etc/apt/sources.list.d/docker.list $ sudo rm /etc/apt/keyrings/docker.asc
You have to delete any edited configuration files manually.
Next steps
- Continue to Post-installation steps for Linux.",,,
1cc4fcae940da8e4b2d43317c36511ae9e74e9ac45e4816a69cee604d6c4db5a,"Swarm mode
Note
Swarm mode is an advanced feature for managing a cluster of Docker daemons.
Use Swarm mode if you intend to use Swarm as a production runtime environment.
If you're not planning on deploying with Swarm, use Docker Compose instead. If you're developing for a Kubernetes deployment, consider using the integrated Kubernetes feature in Docker Desktop.
Current versions of Docker include Swarm mode for natively managing a cluster of Docker Engines called a swarm. Use the Docker CLI to create a swarm, deploy application services to a swarm, and manage swarm behavior.
Docker Swarm mode is built into the Docker Engine. Do not confuse Docker Swarm mode with Docker Classic Swarm which is no longer actively developed.
Feature highlights
Cluster management integrated with Docker Engine
Use the Docker Engine CLI to create a swarm of Docker Engines where you can deploy application services. You don't need additional orchestration software to create or manage a swarm.
Decentralized design
Instead of handling differentiation between node roles at deployment time, the Docker Engine handles any specialization at runtime. You can deploy both kinds of nodes, managers and workers, using the Docker Engine. This means you can build an entire swarm from a single disk image.
Declarative service model
Docker Engine uses a declarative approach to let you define the desired state of the various services in your application stack. For example, you might describe an application comprised of a web front end service with message queueing services and a database backend.
Scaling
For each service, you can declare the number of tasks you want to run. When you scale up or down, the swarm manager automatically adapts by adding or removing tasks to maintain the desired state.
Desired state reconciliation
The swarm manager node constantly monitors the cluster state and reconciles any differences between the actual state and your expressed desired state. For example, if you set up a service to run 10 replicas of a container, and a worker machine hosting two of those replicas crashes, the manager creates two new replicas to replace the replicas that crashed. The swarm manager assigns the new replicas to workers that are running and available.
Multi-host networking
You can specify an overlay network for your services. The swarm manager automatically assigns addresses to the containers on the overlay network when it initializes or updates the application.
Service discovery
Swarm manager nodes assign each service in the swarm a unique DNS name and load balance running containers. You can query every container running in the swarm through a DNS server embedded in the swarm.
Load balancing
You can expose the ports for services to an external load balancer. Internally, the swarm lets you specify how to distribute service containers between nodes.
Secure by default
Each node in the swarm enforces TLS mutual authentication and encryption to secure communications between itself and all other nodes. You have the option to use self-signed root certificates or certificates from a custom root CA.
Rolling updates
At rollout time you can apply service updates to nodes incrementally. The swarm manager lets you control the delay between service deployment to different sets of nodes. If anything goes wrong, you can roll back to a previous version of the service.
What's next?
- Learn Swarm mode key concepts.
- Get started with the Swarm mode tutorial.
- Explore Swarm mode CLI commands",,,
77d14a3b69cd55ed78305b287b22cf8064a02e556e9b74361110373ec2a8424b,"Contribute to Docker's docs
Table of contents
We value documentation contributions from the Docker community. We'd like to make it as easy as possible for you to contribute to the Docker documentation.
Find the contribution guidelines in
CONTRIBUTING.md in
the docker/docs
GitHub repository. Use the following links to review our
style guide and instructions on how to use our page templates and components.
Additional resources
See also:
- A section of useful components you can add to your documentation.
- Information on Docker's tone and voice.
- A writing checklist to help you when you're contributing to Docker's documentation.",,,
cacca7c5b1ef4171d144ef05d405f4f906c3ea6186eb3f4e37ef44284dfc3fc5,"Use the docker dev CLI plugin
Table of contents
Important
Dev Environments is no longer under active development.
While the current functionality remains available, it may take us longer to respond to support requests.
Use the new docker dev
CLI plugin to get the full Dev Environments experience from the terminal in addition to the Dashboard.
It is available with Docker Desktop 4.13.0 and later.
Usage
docker dev [OPTIONS] COMMAND
Commands
| Command | Description |
|---|---|
check | Check Dev Environments |
create | Create a new dev environment |
list | Lists all dev environments |
logs | Traces logs from a dev environment |
open | Open Dev Environment with the IDE |
rm | Removes a dev environment |
start | Starts a dev environment |
stop | Stops a dev environment |
version | Shows the Docker Dev version information |
docker dev check
Usage
docker dev check [OPTIONS]
Options
| Name, shorthand | Description |
|---|---|
--format ,-f | Format the output. |
docker dev create
Usage
docker dev create [OPTIONS] REPOSITORY_URL
Options
| Name, shorthand | Description |
|---|---|
--detach ,-d | Detach creates a Dev Env without attaching to it's logs. |
--open ,-o | Open IDE after a successful creation |
docker dev list
Usage
docker dev list [OPTIONS]
Options
| Name, shorthand | Description |
|---|---|
--format ,-f | Format the output |
--quiet ,-q | Only show dev environments names |
docker dev logs
Usage
docker dev logs [OPTIONS] DEV_ENV_NAME
docker dev open
Usage
docker dev open DEV_ENV_NAME CONTAINER_REF [OPTIONS]
Options
| Name, shorthand | Description |
|---|---|
--editor ,-e | Editor. |
docker dev rm
Usage
docker dev rm DEV_ENV_NAME
docker dev start
Usage
docker dev start DEV_ENV_NAME
docker dev stop
Usage
docker dev stop DEV_ENV_NAME
docker dev version
Usage
docker dev version [OPTIONS]
Options
| Name, shorthand | Description |
|---|---|
--format ,-f | Format the output. |
--short ,-s | Shows only Docker Dev's version number. |",,,
0f68f51204fff9b23b04a1d27d273b472581774b1ae7816481fde9ba07bdab7a,"Install Docker Desktop on Arch-based distributions
Docker Desktop terms
Commercial use of Docker Desktop in larger enterprises (more than 250 employees OR more than $10 million USD in annual revenue) requires a paid subscription.
This page contains information on how to install, launch and upgrade Docker Desktop on an Arch-based distribution.
Prerequisites
To install Docker Desktop successfully, you must meet the general system requirements.
Install Docker Desktop
Install the Docker client binary on Linux. Static binaries for the Docker client are available for Linux as
docker
. You can use:$ wget https://download.docker.com/linux/static/stable/x86_64/docker-28.0.1.tgz -qO- | tar xvfz - docker/docker --strip-components=1 $ mv ./docker /usr/local/bin
Download the latest Arch package from the Release notes.
Install the package:
$ sudo pacman -U ./docker-desktop-x86_64.pkg.tar.zst
By default, Docker Desktop is installed at
/opt/docker-desktop
.
Launch Docker Desktop
To start Docker Desktop for Linux:
Navigate to the Docker Desktop application in your Gnome/KDE Desktop.
Select Docker Desktop to start Docker.
The Docker Subscription Service Agreement displays.
Select Accept to continue. Docker Desktop starts after you accept the terms.
Note that Docker Desktop won't run if you do not agree to the terms. You can choose to accept the terms at a later date by opening Docker Desktop.
For more information, see Docker Desktop Subscription Service Agreement. It is recommended that you also read the FAQs.
Alternatively, open a terminal and run:
$ systemctl --user start docker-desktop
When Docker Desktop starts, it creates a dedicated context that the Docker CLI can use as a target and sets it as the current context in use. This is to avoid a clash with a local Docker Engine that may be running on the Linux host and using the default context. On shutdown, Docker Desktop resets the current context to the previous one.
The Docker Desktop installer updates Docker Compose and the Docker CLI binaries
on the host. It installs Docker Compose V2 and gives users the choice to
link it as docker-compose from the Settings panel. Docker Desktop installs
the new Docker CLI binary that includes cloud-integration capabilities in /usr/local/bin/com.docker.cli
and creates a symlink to the classic Docker CLI at /usr/local/bin
.
After you’ve successfully installed Docker Desktop, you can check the versions of these binaries by running the following commands:
$ docker compose version
Docker Compose version v2.29.1
$ docker --version
Docker version 27.1.1, build 6312585
$ docker version
Client:
Version: 23.0.5
API version: 1.42
Go version: go1.21.12
<...>
To enable Docker Desktop to start on sign in, from the Docker menu, select Settings > General > Start Docker Desktop when you sign in to your computer.
Alternatively, open a terminal and run:
$ systemctl --user enable docker-desktop
To stop Docker Desktop, select the Docker menu icon to open the Docker menu and select Quit Docker Desktop.
Alternatively, open a terminal and run:
$ systemctl --user stop docker-desktop
Next steps
- Explore Docker's subscriptions to see what Docker can offer you.
- Take a look at the Docker workshop to learn how to build an image and run it as a containerized application.
- Explore Docker Desktop and all its features.
- Troubleshooting describes common problems, workarounds, how to run and submit diagnostics, and submit issues.
- FAQs provide answers to frequently asked questions.
- Release notes lists component updates, new features, and improvements associated with Docker Desktop releases.
- Back up and restore data provides instructions on backing up and restoring data related to Docker.",,,
fc7fbc62d6cb712603fd69efba4bda6c42c692976c45f442f0bf91bbcf101e26,"Create your organization
This section describes how to create an organization. Before you begin:
- You need a Docker ID
- Review the Docker subscriptions and features to determine what plan to choose for your organization
Create an organization
There are multiple ways to create an organization. You can either:
- Create a new organization using the Create Organization option in Docker Hub
- Convert an existing user account to an organization
The following section contains instructions on how to create a new organization. For prerequisites and detailed instructions on converting an existing user account to an organization, see Convert an account into an organization.
Sign in to Docker Hub using your Docker ID, your email address, or your social provider.
Select Organizations and then Create Organization to create a new organization.
Choose a plan for your organization, a billing cycle, and specify how many seats you need. See Docker Pricing for details on the features offered in the Team and Business plan.
Select Continue to profile.
Enter an Organization namespace. This is the official, unique name for your organization in Docker Hub. It's not possible to change the name of the organization after you've created it.
Note
You can't use the same name for the organization and your Docker ID. If you want to use your Docker ID as the organization name, then you must first convert your account into an organization.
Enter your Company name. This is the full name of your company. Docker displays the company name on your organization page and in the details of any public images you publish. You can update the company name anytime by navigating to your organization's Settings page.
Select Continue to billing to continue.
Enter your organization's billing information and select Continue to payment to continue to the billing portal.
Provide your card details and select Purchase.
You've now created an organization.
To create an organization:
Sign in to Docker Home.
Under Settings and administration, select Go to Admin Console.
Select the Organization drop-down in the left-hand navigation and then Create Organization.
Choose a plan for your organization, a billing cycle, and specify how many seats you need. See Docker Pricing for details on the features offered in the Team and Business plan.
Select Continue to profile.
Enter an Organization namespace. This is the official, unique name for your organization in Docker Hub. It's not possible to change the name of the organization after you've created it.
Note
You can't use the same name for the organization and your Docker ID. If you want to use your Docker ID as the organization name, then you must first convert your account into an organization.
Enter your Company name. This is the full name of your company. Docker displays the company name on your organization page and in the details of any public images you publish. You can update the company name anytime by navigating to your organization's Settings page.
Select Continue to billing to continue.
Enter your organization's billing information and select Continue to payment to continue to the billing portal.
Provide your card details and select Purchase.
You've now created an organization.
View an organization
To view an organization:
Sign in to Docker Hub with a user account that is a member of any team in the organization.
Note
You can't directly sign in to an organization. This is especially important to note if you create an organization by converting a user account, as conversion means you lose the ability to log into that ""account"", since it no longer exists. To view the organization you need to sign in with the new owner account assigned during the conversion or another account that was added as a member. If you don't see the organization after logging in, then you are neither a member or an owner of it. An organization administrator needs to add you as a member of the organization.
Select Organizations in the top navigation bar, then choose your organization from the list.
The organization landing page displays various options that let you to configure your organization.
Members: Displays a list of team members. You can invite new members using the Invite members button. See Manage members for details.
Teams: Displays a list of existing teams and the number of members in each team. See Create a team for details.
Repositories: Displays a list of repositories associated with the organization. See Repositories for detailed information about working with repositories.
Activity Displays the audit logs, a chronological list of activities that occur at organization and repository levels. It provides the org owners a report of all their team member activities. See Audit logs for details.
Settings: Displays information about your organization, and you to view and change your repository privacy settings, configure org permissions such as Image Access Management, configure notification settings, and deactivate You can also update your organization name and company name that appear on your organization landing page. You must be an owner to access the organization's Settings page.
Billing: Displays information about your existing Docker subscription (plan), including the number of seats and next payment due date. For how to access the billing history and payment methods for your organization, see View billing history.
To view an organization in the Admin Console:
- Sign in to Docker Home.
- Under Settings and administration, select Go to Admin Console.
- Select your organization from the Organization drop-down in the left-hand navigation.
The Admin Console displays various options that let you to configure your organization.
Members: Displays a list of team members. You can invite new members using the Invite members button. See Manage members for details.
Teams: Displays a list of existing teams and the number of members in each team. See Create a team for details.
Activity Displays the audit logs, a chronological list of activities that occur at organization and repository levels. It provides the org owners a report of all their team member activities. See Audit logs for details.
Security and access: Manage security settings. For more information, see Security.
Organization settings: Update general settings, manage your company settings, or deactivate your organization.
Merge organizations
Warning
If you are merging organizations, it is recommended to do so at the end of your billing cycle. When you merge an organization and downgrade another, you will lose seats on your downgraded organization. Docker does not offer refunds for downgrades.
If you have multiple organizations that you want to merge into one, complete the following:
- Based on the number of seats from the secondary organization, purchase additional seats for the primary organization account that you want to keep.
- Manually add users to the primary organization and remove existing users from the secondary organization.
- Manually move over your data, including all repositories.
- Once you're done moving all of your users and data, downgrade the secondary account to a free subscription. Note that Docker does not offer refunds for downgrading organizations mid-billing cycle.
Tip
If your organization has a Docker Business subscription with a purchase order, contact Support or your Account Manager at Docker.",,,
c73a0538aa77a4cfe1deef951f7b973cd121cdc4306c20ea2e1f1ca5aadfa2f4,"MCP
What is MCP?
Anthropic recently announced the Model Context Protocol (MCP) specification, an open protocol that standardises how applications provide context to large language models. MCP functions as a client-server protocol, where the client (e.g., an application like Gordon) sends requests, and the server processes those requests to deliver the necessary context to the AI.
Gordon, along with other MCP clients like Claude Desktop, can interact with MCP servers running as containers. Docker has partnered with Anthropic to build container images for the reference implementations of MCP servers, available on Docker Hub under the mcp namespace.
Simple MCP server usage with Gordon
When you run the docker ai
command in your terminal to ask a question, Gordon looks for a gordon-mcp.yml
file in your working directory for a list of MCP servers that should be used when in that context. The gordon-mcp.yml
file is a Docker Compose file that configures MCP servers as Compose services for Gordon to access.
The following minimal example shows how you can use the mcp-time server to provide temporal capabilities to Gordon. For more information, you can check out the source code and documentation.
Create the
gordon-mcp.yml
file and add the time server:services: time: image: mcp/time
With this file you can now ask Gordon to tell you the time in another timezone:
$ docker ai 'what time is it now in kiribati?' • Calling get_current_time The current time in Kiribati (Tarawa) is 9:38 PM on January 7, 2025.
As you can see, Gordon found the MCP time server and called its tool when needed.
Advanced usage
Some MCP servers need access to your filesystem or system environment variables. Docker Compose can help with this. Since gordon-mcp.yml
is a Compose file you can add bind mounts using the regular Docker Compose syntax, which makes your filesystem resources available to the container:
services:
fs:
image: mcp/filesystem
command:
- /rootfs
volumes:
- .:/rootfs
The gordon-mcp.yml
file adds filesystem access capabilities to Gordon and since everything runs inside a container Gordon only has access to the directories you specify.
Gordon can handle any number of MCP servers. For example, if you give Gordon access to the internet with the mcp/fetch
server:
services:
fetch:
image: mcp/fetch
fs:
image: mcp/filesystem
command:
- /rootfs
volumes:
- .:/rootfs
You can now ask things like:
$ docker ai can you fetch rumpl.dev and write the summary to a file test.txt
• Calling fetch ✔️
• Calling write_file ✔️
The summary of the website rumpl.dev has been successfully written to the file test.txt in the allowed directory. Let me know if you need further assistance!
$ cat test.txt
The website rumpl.dev features a variety of blog posts and articles authored by the site owner. Here's a summary of the content:
1. **Wasmio 2023 (March 25, 2023)**: A recap of the WasmIO 2023 conference held in Barcelona. The author shares their experience as a speaker and praises the organizers for a successful event.
2. **Writing a Window Manager in Rust - Part 2 (January 3, 2023)**: The second part of a series on creating a window manager in Rust. This installment focuses on enhancing the functionality to manage windows effectively.
3. **2022 in Review (December 29, 2022)**: A personal and professional recap of the year 2022. The author reflects on the highs and lows of the year, emphasizing professional achievements.
4. **Writing a Window Manager in Rust - Part 1 (December 28, 2022)**: The first part of the series on building a window manager in Rust. The author discusses setting up a Linux machine and the challenges of working with X11 and Rust.
5. **Add docker/docker to your dependencies (May 10, 2020)**: A guide for Go developers on how to use the Docker client library in their projects. The post includes a code snippet demonstrating the integration.
6. **First (October 11, 2019)**: The inaugural post on the blog, featuring a simple ""Hello World"" program in Go.%
What’s next?
Now that you’ve learned how to use MCP servers with Gordon, here are a few ways you can get started:
- Experiment: Try integrating one or more of the tested MCP servers into your
gordon-mcp.yml
file and explore their capabilities.
- Explore the ecosystem: Check out the reference implementations on GitHub or browse the Docker Hub MCP namespace for additional servers that might suit your needs.
- Build your own: If none of the existing servers meet your needs, or you’re curious about exploring how they work in more detail, consider developing a custom MCP server. Use the MCP specification as a guide.
- Share your feedback: If you discover new servers that work well with Gordon or encounter issues with existing ones, share your findings to help improve the ecosystem.
With MCP support, Gordon offers powerful extensibility and flexibility to meet your specific use cases whether you’re adding temporal awareness, file management, or internet access.
List of known working MCP Servers
These are the MCP servers that have been tested successfully with Gordon:
mcp/time
mcp/fetch
mcp/filesystem
mcp/postgres
mcp/git
mcp/sqlite
mcp/github
List of untested MCP servers
These are the MCP servers that were not tested but should work if given the appropriate API tokens:
mcp/brave-search
mcp/gdrive
mcp/slack
mcp/google-maps
mcp/gitlab
mcp/everything
mcp/aws-kb-retrieval-server
mcp/sentry
List of MCP servers that don’t work with Gordon
These are the MCP servers that are currently unsupported:
mcp/sequentialthinking
- The tool description is too longmcp/puppeteer
- Puppeteer sends back images and Gordon doesn’t know how to handle them, it only handles text responses from toolsmcp/everart
- Everart sends back images and Gordon doesn’t know how to handle them, it only handles text responses from toolsmcp/memory
- There is no way to configure the server to use a custom path for its knowledge base",,,
2b29ca32c5acaf42a2be9c7587a44929e32b2bdcf75168d099aeccd24ec496aa,"Docker Engine 17.07 release notes
Table of contents
17.07.0-ce
2017-08-29
API & Client
- Add support for proxy configuration in config.json docker/cli#93
- Enable pprof/debug endpoints by default moby/moby#32453
- Passwords can now be passed using
STDIN
using the new--password-stdin
flag ondocker login
docker/cli#271
- Add
--detach
to docker scale docker/cli#243
- Prevent
docker logs --no-stream
from hanging due to non-existing containers moby/moby#34004
- Fix
docker stack ps
printing error tostdout
instead ofstderr
docker/cli#298
- Fix progress bar being stuck on
docker service create
if an error occurs during deploy docker/cli#259 - Improve presentation of progress bars in interactive mode docker/cli#260 docker/cli#237
- Print a warning if
docker login --password
is used, and recommend--password-stdin
docker/cli#270 - Make API version negotiation more robust moby/moby#33827
- Hide
--detach
when connected to daemons older than Docker 17.05 docker/cli#219
- Add
scope
filter inGET /networks/(id or name)
moby/moby#33630
Builder
- Implement long running interactive session and sending build context incrementally moby/moby#32677 docker/cli#231 moby/moby#33859
- Warn on empty continuation lines moby/moby#33719
- Fix
.dockerignore
entries with a leading/
not matching anything moby/moby#32088
Logging
- Fix wrong filemode for rotate log files moby/moby#33926
- Fix stderr logging for journald and syslog moby/moby#33832
Runtime
- Allow stopping of paused container moby/moby#34027
- Add quota support for the overlay2 storage driver moby/moby#32977
- Remove container locks on
docker ps
moby/moby#31273 - Store container names in memdb moby/moby#33886
- Fix race condition between
docker exec
anddocker pause
moby/moby#32881 - Devicemapper: Rework logging and add
--storage-opt dm.libdm_log_level
moby/moby#33845 - Devicemapper: Prevent ""device in use"" errors if deferred removal is enabled, but not deferred deletion moby/moby#33877
- Devicemapper: Use KeepAlive to prevent tasks being garbage-collected while still in use moby/moby#33376
- Report intermediate prune results if prune is cancelled moby/moby#33979
- Fix run
docker rename <container-id> new_name
concurrently resulting in the having multiple names moby/moby#33940
- Fix file-descriptor leak and error handling moby/moby#33713
- Fix SIGSEGV when running containers docker/cli#303
- Prevent a goroutine leak when healthcheck gets stopped moby/moby#33781
- Image: Improve store locking moby/moby#33755
- Fix Btrfs quota groups not being removed when container is destroyed moby/moby#29427
- Libcontainerd: fix defunct containerd processes not being properly reaped moby/moby#33419
- Preparations for Linux Containers on Windows
- LCOW: Dedicated scratch space for service VM utilities moby/moby#33809
- LCOW: Support most operations excluding remote filesystem moby/moby#33241 moby/moby#33826
- LCOW: Change directory from lcow to ""Linux Containers"" moby/moby#33835
- LCOW: pass command arguments without extra quoting moby/moby#33815
- LCOW: Updates necessary due to platform schema change moby/moby#33785
Swarm mode
- Initial support for plugable secret backends moby/moby#34157 moby/moby#34123
- Sort swarm stacks and nodes using natural sorting docker/cli#315
- Make engine support cluster config event moby/moby#34032
- Only pass a join address when in the process of joining a cluster moby/moby#33361
- Fix error during service creation if a network with the same name exists both as ""local"" and ""swarm"" scoped network docker/cli#184
- (experimental) Add support for plugins on swarm moby/moby#33575",,,
d97133c8c1200b29055b1f7d9e83b54299076fc87a026b250d555b9eec06ee2c,"Docker Hub search
The Docker Hub search interface lets you explore millions of resources. To help you find exactly what you need, it offers a variety of filters that let you narrow your results or discover different types of content.
Filters
The search functionality includes filters to narrow down results based on your requirements, such as products, categories, and trusted content. This ensures that you can quickly find and access the resources best suited to your project.
Products
Docker Hub's content library features three products, each designed to meet specific needs of developers and organizations. These products include images, plugins, and extensions.
Images
Docker Hub hosts millions of container images, making it the go-to repository for containerized applications and solutions. These images include:
- Operating system images: Foundational images for Linux distributions like Ubuntu, Debian, and Alpine, or Windows Server images.
- Database and storage images: Pre-configured databases such as MySQL, PostgreSQL, and MongoDB to simplify application development.
- Languages and frameworks-based images: Popular images for Java, Python, Node.js, Ruby, .NET, and more, offering pre-built environments for faster development.
Images in Docker Hub simplify the development process by providing pre-built, reusable building blocks, reducing the need to start from scratch. Whether you're a beginner building your first container or an enterprise managing complex architectures, Docker Hub images provide a reliable foundation.
Plugins
Plugins in Docker Hub let you extend and customize Docker Engine to suit specialized requirements. Plugins integrate directly with the Docker Engine and provide capabilities such as:
- Network plugins: Enhance networking functionality, enabling integration with complex network infrastructures.
- Volume plugins: Provide advanced storage options, supporting persistent and distributed storage across various backends.
- Authorization plugins: Offer fine-grained access control to secure Docker environments.
By leveraging Docker plugins, teams can tailor Docker Engine to meet their specific operational needs, ensuring compatibility with existing infrastructures and workflows.
To learn more about plugins, see Docker Engine managed plugin system.
Extensions
Docker Hub offers extensions for Docker Desktop, which enhance its core functionality. These extensions are purpose-built to streamline the software development lifecycle. Extensions provide tools for:
- System optimization and monitoring: Manage resources and optimize Docker Desktop’s performance.
- Container management: Simplify container deployment and monitoring.
- Database management: Facilitate efficient database operations within containers.
- Kubernetes and cloud integration: Bridge local environments with cloud-native and Kubernetes workflows.
- Visualization tools: Gain insights into container resource usage through graphical representations.
Extensions help developers and teams create a more efficient and unified workflow by reducing context switching and bringing essential tools into Docker Desktop's interface.
To learn more about extensions, see Docker Extensions.
Trusted content
Docker Hub's trusted content provides a curated selection of high-quality, secure images designed to give developers confidence in the reliability and security of the resources they use. These images are stable, regularly updated, and adhere to industry best practices, making them a strong foundation for building and deploying applications. Docker Hub's trusted content includes, Docker Official Images, Verified Publisher images, and Docker-Sponsored Open Source Software images.
For more details, see Trusted content.
Categories
Docker Hub makes it easy to find and explore container images with categories. Categories group images based on their primary use case, helping you quickly locate the tools and resources you need to build, deploy, and run your applications.
The categories include:
- API Management: Tools for creating, publishing, analyzing, and securing APIs.
- Content Management System: Software applications to create and manage digital content through templates, procedures, and standard formats.
- Data Science: Tools and software to support analyzing data and generating actionable insights.
- Databases & Storage: Systems for storing, retrieving, and managing data.
- Languages & Frameworks: Programming language runtimes and frameworks.
- Integrations & Delivery: Tools for Continuous Integration (CI) and Continuous Delivery (CD).
- Internet of Things: Tools supporting Internet of Things (IoT) applications.
- Machine Learning & AI: Tools and frameworks optimized for artificial intelligence and machine learning projects, such as pre-installed libraries and frameworks for data analysis, model training, and deployment.
- Message Queues: Message queuing systems optimized for reliable, scalable, and efficient message handling.
- Monitoring & Observability: Tools to track software and system performance through metrics, logs, and traces, as well as observability to explore the system’s state and diagnose issues.
- Networking: Repositories that support data exchange and connecting computers and other devices to share resources.
- Operating Systems: Software that manages all other programs on a computer and serves as an intermediary between users and the computer hardware, while overseeing applications and system resources.
- Security: Tools to protect a computer system or network from theft, unauthorized access, or damage to their hardware, software, or electronic data, as well as from service disruption.
- Web Servers: Software to serve web pages, HTML files, and other assets to users or other systems.
- Web Analytics: Tools to collect, measure, analyze, and report on web data and website visitor engagement.
Operating systems
The Operating systems filter lets you narrow your search to container images compatible with specific host operating systems. This filter ensures that the images you use align with your target environment, whether you're developing for Linux-based systems, Windows, or both.
- Linux: Access a wide range of images tailored for Linux environments. These images provide foundational environments for building and running Linux-based applications in containers.
- Windows: Explore Windows container images.
Note
The Operating systems filter is only available for images. If you select the Extensions or Plugins filter, then the Operating systems filter isn't available.
Architectures
The Architectures filter lets you find images built to support specific CPU architectures. This ensures compatibility with your hardware environment, from development machines to production servers.
- ARM: Select images compatible with ARM processors, commonly used in IoT devices and embedded systems.
- ARM 64: Locate 64-bit ARM-compatible images for modern ARM processors, such as those in AWS Graviton or Apple Silicon.
- IBM POWER: Find images optimized for IBM Power Systems, offering performance and reliability for enterprise workloads.
- PowerPC 64 LE: Access images designed for the little-endian PowerPC 64-bit architecture.
- IBM Z: Discover images tailored for IBM Z mainframes, ensuring compatibility with enterprise-grade hardware.
- x86: Choose images compatible with 32-bit x86 architectures, suitable for older systems or lightweight environments.
- x86-64: Filter images for modern 64-bit x86 systems, widely used in desktops, servers, and cloud infrastructures.
Note
The Architectures filter is only available for images. If you select the Extensions or Plugins filter, then the Architectures filter isn't available.
Reviewed by Docker
The Reviewed by Docker filter provides an extra layer of assurance when selecting extensions. This filter helps you identify whether a Docker Desktop extension has been reviewed by Docker for quality and reliability.
- Reviewed: Extensions that have undergone Docker's review process, ensuring they meet high standards.
- Not Reviewed: Extensions that have not been reviewed by Docker.
Note
The Reviewed by Docker filter is only available for extensions. To make the filter available, you must select only the Extensions filter in Products.",,,
2454dc53125278529584337e98aba8a332ab040c3a697a766a67f7e39b73a2ad,"Extension metadata
The metadata.json file
The metadata.json
file is the entry point for your extension. It contains the metadata for your extension, such as the
name, version, and description. It also contains the information needed to build and run your extension. The image for
a Docker extension must include a metadata.json
file at the root of its filesystem.
The format of the metadata.json
file must be:
{
""icon"": ""extension-icon.svg"",
""ui"": ...
""vm"": ...
""host"": ...
}
The ui
, vm
, and host
sections are optional and depend on what a given extension provides. They describe the extension content to be installed.
UI section
The ui
section defines a new tab that's added to the dashboard in Docker Desktop. It follows the form:
""ui"":{
""dashboard-tab"":
{
""title"":""MyTitle"",
""root"":""/ui"",
""src"":""index.html""
}
}
root
specifies the folder where the UI code is within the extension image filesystem.
src
specifies the entrypoint that should be loaded in the extension tab.
Other UI extension points will be available in the future.
VM section
The vm
section defines a backend service that runs inside the Desktop VM. It must define either an image
or a
compose.yaml
file that specifies what service to run in the Desktop VM.
""vm"": {
""image"":""${DESKTOP_PLUGIN_IMAGE}""
},
When you use image
, a default compose file is generated for the extension.
${DESKTOP_PLUGIN_IMAGE}
is a specific keyword that allows an easy way to refer to the image packaging the extension. It is also possible to specify any other full image name here. However, in many cases using the same image makes things easier for extension development.
""vm"": {
""composefile"": ""compose.yaml""
},
The Compose file, with a volume definition for example, would look like:
services:
myExtension:
image: ${DESKTOP_PLUGIN_IMAGE}
volumes:
- /host/path:/container/path
Host section
The host
section defines executables that Docker Desktop copies on the host.
""host"": {
""binaries"": [
{
""darwin"": [
{
""path"": ""/darwin/myBinary""
},
],
""windows"": [
{
""path"": ""/windows/myBinary.exe""
},
],
""linux"": [
{
""path"": ""/linux/myBinary""
},
]
}
]
}
binaries
defines a list of binaries Docker Desktop copies from the extension image to the host.
path
specifies the binary path in the image filesystem. Docker Desktop is responsible for copying these files in its own location, and the JavaScript API allows invokes these binaries.
Learn how to invoke executables.",,,
1251adb69db0a075dfde1e8d14c1b7e1567bce1b84f9d155ebd8ebedd6a03ac1,"Docker security announcements
Docker Desktop 4.34.2 Security Update: CVE-2024-8695 and CVE-2024-8696
Last updated September 13, 2024
Two remote code execution (RCE) vulnerabilities in Docker Desktop related to Docker Extensions were reported by Cure53 and were fixed on September 12 in the 4.34.2 release.
- CVE-2024-8695: A remote code execution (RCE) vulnerability via crafted extension description/changelog could be abused by a malicious extension in Docker Desktop before 4.34.2. [Critical]
- CVE-2024-8696: A remote code execution (RCE) vulnerability via crafted extension publisher-url/additional-urls could be abused by a malicious extension in Docker Desktop before 4.34.2. [High]
No existing extensions exploiting the vulnerabilities were found in the Extensions Marketplace. The Docker team will be closely monitoring and diligently reviewing any requests for publishing new extensions.
We strongly encourage you to update to Docker Desktop 4.34.2. If you are unable to update promptly, you can disable Docker Extensions as a workaround.
Deprecation of password logins on CLI when SSO enforced
Last updated July, 2024
When SSO enforcement was first introduced, Docker provided a grace period to continue to let passwords be used on the Docker CLI when authenticating to Docker Hub. This was allowed so organizations could more easily use SSO enforcement. It is recommended that administrators configuring SSO encourage users using the CLI to switch over to Personal Access Tokens in anticipation of this grace period ending.
On September 16, 2024 the grace period will end and passwords will no longer be able to authenticate to Docker Hub via the Docker CLI when SSO is enforced. Affected users are required to switch over to using PATs to continue signing in.
At Docker, we want the experience to be the most secure for our developers and organizations and this deprecation is an essential step in that direction.
SOC 2 Type 2 attestation and ISO 27001 certification
Last updated June, 2024
Docker is pleased to announce that we have received our SOC 2 Type 2 attestation and ISO 27001 certification with no exceptions or major non-conformities.
Security is a fundamental pillar to Docker’s operations, which is embedded into our overall mission and company strategy. Docker’s products are core to our user community and our SOC 2 Type 2 attestation and ISO 27001 certification demonstrate Docker’s ongoing commitment to security to our user base.
For more information, see the Blog announcement.
Docker Security Advisory: Multiple Vulnerabilities in runc, BuildKit, and Moby
Last updated February 2, 2024
We at Docker prioritize the security and integrity of our software and the trust of our users. Security researchers at Snyk Labs identified and reported four security vulnerabilities in the container ecosystem. One of the vulnerabilities, CVE-2024-21626, concerns the runc container runtime, and the other three affect BuildKit ( CVE-2024-23651, CVE-2024-23652, and CVE-2024-23653). We want to assure our community that our team, in collaboration with the reporters and open source maintainers, has been diligently working on coordinating and implementing necessary remediations.
We are committed to maintaining the highest security standards. We have published patched versions of runc, BuildKit, and Moby on January 31 and released an update for Docker Desktop on February 1 to address these vulnerabilities. Additionally, our latest BuildKit and Moby releases included fixes for CVE-2024-23650 and CVE-2024-24557, discovered respectively by an independent researcher and through Docker’s internal research initiatives.
| Versions Impacted | |
|---|---|
runc | <= 1.1.11 |
BuildKit | <= 0.12.4 |
Moby (Docker Engine) | <= 25.0.1 and <= 24.0.8 |
Docker Desktop | <= 4.27.0 |
What should I do if I’m on an affected version?
If you are using affected versions of runc, BuildKit, Moby, or Docker Desktop, make sure to update to the latest versions, linked in the following table:
| Patched Versions | |
|---|---|
runc | >= 1.1.12 |
BuildKit | >= 0.12.5 |
Moby (Docker Engine) | >= 25.0.2 and >= 24.0.9 |
Docker Desktop | >= 4.27.1 |
If you are unable to update to an unaffected version promptly, follow these best practices to mitigate risk:
- Only use trusted Docker images (such as Docker Official Images).
- Don’t build Docker images from untrusted sources or untrusted Dockerfiles.
- If you are a Docker Business customer using Docker Desktop and unable to update to v4.27.1, make sure to enable
Hardened Docker Desktop features such as:
- Enhanced Container Isolation, which mitigates the impact of CVE-2024-21626 in the case of running containers from malicious images.
- Image Access Management, and Registry Access Management, which give organizations control over which images and repositories their users can access.
- For CVE-2024-23650, CVE-2024-23651, CVE-2024-23652, and CVE-2024-23653, avoid using BuildKit frontend from an untrusted source. A frontend image is usually specified as the #syntax line on your Dockerfile, or with
--frontend
flag when using thebuildctl build
command. - To mitigate CVE-2024-24557, make sure to either use BuildKit or disable caching when building images. From the CLI this can be done via the
DOCKER_BUILDKIT=1
environment variable (default for Moby >= v23.0 if the buildx plugin is installed) or the--no-cache flag
. If you are using the HTTP API directly or through a client, the same can be done by settingnocache
totrue
orversion
to2
for the /build API endpoint.
Technical details and impact
CVE-2024-21626 (High)
In runc v1.1.11 and earlier, due to certain leaked file descriptors, an attacker can gain access to the host filesystem by causing a newly-spawned container process (from runc exec
) to have a working directory in the host filesystem namespace, or by tricking a user to run a malicious image and allow a container process to gain access to the host filesystem through runc run
. The attacks can also be adapted to overwrite semi-arbitrary host binaries, allowing for complete container escapes. Note that when using higher-level runtimes (such as Docker or Kubernetes), this vulnerability can be exploited by running a malicious container image without additional configuration or by passing specific workdir options when starting a container. The vulnerability can also be exploited from within Dockerfiles in the case of Docker.
The issue has been fixed in runc v1.1.12.
CVE-2024-23651 (High)
In BuildKit <= v0.12.4, two malicious build steps running in parallel sharing the same cache mounts with subpaths could cause a race condition, leading to files from the host system being accessible to the build container. This will only occur if a user is trying to build a Dockerfile of a malicious project.
The issue has been fixed in BuildKit v0.12.5.
CVE-2024-23652 (High)
In BuildKit <= v0.12.4, a malicious BuildKit frontend or Dockerfile using RUN --mount
could trick the feature that removes empty files created for the mountpoints into removing a file outside the container from the host system. This will only occur if a user is using a malicious Dockerfile.
The issue has been fixed in BuildKit v0.12.5.
CVE-2024-23653 (High)
In addition to running containers as build steps, BuildKit also provides APIs for running interactive containers based on built images. In BuildKit <= v0.12.4, it is possible to use these APIs to ask BuildKit to run a container with elevated privileges. Normally, running such containers is only allowed if special security.insecure
entitlement is enabled both by buildkitd configuration and allowed by the user initializing the build request.
The issue has been fixed in BuildKit v0.12.5.
CVE-2024-23650 (Medium)
In BuildKit <= v0.12.4, a malicious BuildKit client or frontend could craft a request that could lead to BuildKit daemon crashing with a panic.
The issue has been fixed in BuildKit v0.12.5.
CVE-2024-24557 (Medium)
In Moby <= v25.0.1 and <= v24.0.8, the classic builder cache system is prone to cache poisoning if the image is built FROM scratch. Also, changes to some instructions (most important being HEALTHCHECK
and ONBUILD
) would not cause a cache miss. An attacker with knowledge of the Dockerfile someone is using could poison their cache by making them pull a specially crafted image that would be considered a valid cache candidate for some build steps.
The issue has been fixed in Moby >= v25.0.2 and >= v24.0.9.
How are Docker products affected?
Docker Desktop
Docker Desktop v4.27.0 and earlier are affected. Docker Desktop v4.27.1 was released on February 1 and includes runc, BuildKit, and dockerd binaries patches. In addition to updating to this new version, we encourage all Docker users to diligently use Docker images and Dockerfiles and ensure you only use trusted content in your builds.
As always, you should check Docker Desktop system requirements for your operating system ( Windows, Linux, Mac) before updating to ensure full compatibility.
Docker Build Cloud
Any new Docker Build Cloud builder instances will be provisioned with the latest Docker Engine and BuildKit versions and will, therefore, be unaffected by these CVEs. Updates have also been rolled out to existing Docker Build Cloud builders.
No other Docker products are affected by these vulnerabilities.
Advisory links
- Runc
- BuildKit
- Moby
Text4Shell CVE-2022-42889
Last updated October 2022
CVE-2022-42889 has been discovered in the popular Apache Commons Text library. Versions of this library up to but not including 1.10.0 are affected by this vulnerability.
We strongly encourage you to update to the latest version of Apache Commons Text.
Scan images on Docker Hub
Docker Hub security scans triggered after 1200 UTC 21 October 2021 are now correctly identifying the Text4Shell CVE. Scans before this date do not currently reflect the status of this vulnerability. Therefore, we recommend that you trigger scans by pushing new images to Docker Hub to view the status of the Text4Shell CVE in the vulnerability report. For detailed instructions, see Scan images on Docker Hub.
Docker Official Images impacted by CVE-2022-42889
A number of Docker Official Images contain the vulnerable versions of Apache Commons Text. The following lists Docker Official Images that may contain the vulnerable versions of Apache Commons Text:
We have updated Apache Commons Text in these images to the latest version. Some of these images may not be vulnerable for other reasons. We recommend that you also review the guidelines published on the upstream websites.
Log4j 2 CVE-2021-44228
Last updated December 2021
The Log4j 2 CVE-2021-44228 vulnerability in Log4j 2, a very common Java logging library, allows remote code execution, often from a context that is easily available to an attacker. For example, it was found in Minecraft servers which allowed the commands to be typed into chat logs as these were then sent to the logger. This makes it a very serious vulnerability, as the logging library is used so widely and it may be simple to exploit. Many open source maintainers are working hard with fixes and updates to the software ecosystem.
The vulnerable versions of Log4j 2 are versions 2.0 to version 2.14.1 inclusive. The first fixed version is 2.15.0. We strongly encourage you to update to the latest version if you can. If you are using a version before 2.0, you are also not vulnerable.
You may not be vulnerable if you are using these versions, as your configuration may already mitigate this, or the things you log may not include any user input. This may be difficult to validate however without understanding all the code paths that may log in detail, and where they may get input from. So you probably will want to upgrade all code using vulnerable versions.
CVE-2021-45046
As an update to CVE-2021-44228, the fix made in version 2.15.0 was incomplete. Additional issues have been identified and are tracked with CVE-2021-45046 and CVE-2021-45105. For a more complete fix to this vulnerability, we recommended that you update to 2.17.0 where possible.
Scan images on Docker Hub
Docker Hub security scans triggered after 1700 UTC 13 December 2021 are now correctly identifying the Log4j 2 CVEs. Scans before this date do not currently reflect the status of this vulnerability. Therefore, we recommend that you trigger scans by pushing new images to Docker Hub to view the status of Log4j 2 CVE in the vulnerability report. For detailed instructions, see Scan images on Docker Hub.
Docker Official Images impacted by Log4j 2 CVE
Last updated December 2021
A number of Docker Official Images contain the vulnerable versions of Log4j 2 CVE-2021-44228. The following table lists Docker Official Images that may contained the vulnerable versions of Log4j 2. We updated Log4j 2 in these images to the latest version. Some of these images may not be vulnerable for other reasons. We recommend that you also review the guidelines published on the upstream websites.
| Repository | Patched version | Additional documentation |
|---|---|---|
| couchbase | 7.0.3 | Couchbase blog |
| Elasticsearch | 6.8.22, 7.16.2 | Elasticsearch announcement |
| Flink | 1.11.6, 1.12.7, 1.13.5, 1.14.2 | Flink advice on Log4j CVE |
| Geonetwork | 3.10.10 | Geonetwork GitHub discussion |
| lightstreamer | Awaiting info | Awaiting info |
| logstash | 6.8.22, 7.16.2 | Elasticsearch announcement |
| neo4j | 4.4.2 | Neo4j announcement |
| solr | 8.11.1 | Solr security news |
| sonarqube | 8.9.5, 9.2.2 | SonarQube announcement |
| storm | Awaiting info | Awaiting info |
Note
Although xwiki images may be detected as vulnerable by some scanners, the authors believe the images are not vulnerable by Log4j 2 CVE as the API jars do not contain the vulnerability. The Nuxeo image is deprecated and will not be updated.",,,
d5a47edf80ce291f2641968ae0f4308c7aa7220c2c87c4d5a23a1506278a82c2,"Docker Engine 17.03 release notes
Table of contents
17.03.3-ce
2018-08-30
Runtime
- Update go-connections to d217f8e #28
17.03.2-ce
2017-05-29
Networking
- Fix a concurrency issue preventing network creation #33273
Runtime
- Relabel secrets path to avoid a Permission Denied on selinux enabled systems #33236 (ref #32529
- Fix cases where local volume were not properly relabeled if needed #33236 (ref #29428)
- Fix an issue while upgrading if a plugin rootfs was still mounted #33236 (ref #32525)
- Fix an issue where volume wouldn't default to the
rprivate
propagation mode #33236 (ref #32851) - Fix a panic that could occur when a volume driver could not be retrieved #33236 (ref #32347)
- Add a warning in
docker info
when theoverlay
oroverlay2
graphdriver is used on a filesystem withoutd_type
support #33236 (ref #31290)
- Fix an issue with backporting mount spec to older volumes #33207
- Fix issue where a failed unmount can lead to data loss on local volume remove #33120
Swarm Mode
- Fix a case where tasks could get killed unexpectedly #33118
- Fix an issue preventing to deploy services if the registry cannot be reached despite the needed images being locally present #33117
17.03.1-ce
2017-03-27
Remote API (v1.27) & Client
- Fix autoremove on older api #31692
- Fix default network customization for a stack #31258
- Correct CPU usage calculation in presence of offline CPUs and newer Linux #31802
- Fix issue where service healthcheck is
{}
in remote API #30197
Runtime
- Update runc to 54296cf40ad8143b62dbcaa1d90e520a2136ddfe #31666
- Ignore cgroup2 mountpoints opencontainers/runc#1266
- Update containerd to 4ab9917febca54791c5f071a9d1f404867857fcc #31662 #31852
- Register healtcheck service before calling restore() docker/containerd#609
- Fix
docker exec
not working after unattended upgrades that reload apparmor profiles #31773 - Fix unmounting layer without merge dir with Overlay2 #31069
- Do not ignore ""volume in use"" errors when force-delete #31450
Swarm Mode
- Update swarmkit to 17756457ad6dc4d8a639a1f0b7a85d1b65a617bb #31807
- Scheduler now correctly considers tasks which have been assigned to a node but aren't yet running docker/swarmkit#1980
- Allow removal of a network when only dead tasks reference it docker/swarmkit#2018
- Retry failed network allocations less aggressively docker/swarmkit#2021
- Avoid network allocation for tasks that are no longer running docker/swarmkit#2017
- Bookkeeping fixes inside network allocator allocator docker/swarmkit#2019 docker/swarmkit#2020
Windows
- Cleanup HCS on restore #31503
17.03.0-ce
2017-03-01
Important
Starting with this release, Docker is on a monthly release cycle and uses a new YY.MM versioning scheme to reflect this. Two channels are available: monthly and quarterly. Any given monthly release will only receive security and bugfixes until the next monthly release is available. Quarterly releases receive security and bugfixes for 4 months after initial release. This release includes bugfixes for 1.13.1 but there are no major feature additions and the API version stays the same. Upgrading from Docker 1.13.1 to 17.03.0 is expected to be simple and low-risk.
Client
- Fix panic in
docker stats --format
#30776
Contrib
- Update various
bash
andzsh
completion scripts #30823, #30945 and more... - Block obsolete socket families in default seccomp profile - mitigates unpatched kernels' CVE-2017-6074 #29076
Networking
- Fix bug on overlay encryption keys rotation in cross-datacenter swarm #30727
- Fix side effect panic in overlay encryption and network control plane communication failure (""No installed keys could decrypt the message"") on frequent swarm leader re-election #25608
- Several fixes around system responsiveness and datapath programming when using overlay network with external kv-store docker/libnetwork#1639, docker/libnetwork#1632 and more...
- Discard incoming plain vxlan packets for encrypted overlay network #31170
- Release the network attachment on allocation failure #31073
- Fix port allocation when multiple published ports map to the same target port docker/swarmkit#1835
Runtime
- Fix a deadlock in docker logs #30223
- Fix CPU spin waiting for log write events #31070
- Fix a possible crash when using journald #31231 #31263
- Fix a panic on close of nil channel #31274
- Fix duplicate mount point for
--volumes-from
indocker run
#29563 - Fix
--cache-from
does not cache last step #31189
Swarm Mode
- Shutdown leaks an error when the container was never started #31279
- Fix possibility of tasks getting stuck in the ""NEW"" state during a leader failover docker/swarmkit#1938
- Fix extraneous task creations for global services that led to confusing replica counts in
docker service ls
docker/swarmkit#1957 - Fix problem that made rolling updates slow when
task-history-limit
was set to 1 docker/swarmkit#1948 - Restart tasks elsewhere, if appropriate, when they are shut down as a result of nodes no longer satisfying constraints docker/swarmkit#1958
- (experimental)",,,
87d52b501c777faad4320ff557e886d26786f3fadd91e069f7501547dd342745,"Manage subscription seats
You can add seats at anytime to your existing subscription.
When you add seats to your subscription in the middle of your billing cycle, you are charged a prorated amount for the additional seats.
Important
Starting July 1, 2024, Docker will begin collecting sales tax on subscription fees in compliance with state regulations for customers in the United States. For our global customers subject to VAT, the implementation will start rolling out on July 1, 2024. Note that while the roll out begins on this date, VAT charges may not apply to all applicable subscriptions immediately.
To ensure that tax assessments are correct, make sure that your billing information and VAT/Tax ID, if applicable, are updated. If you're exempt from sales tax, see Register a tax certificate.
Add seats
Important
If you have a sales-assisted Docker Business subscription, contact your account manager to add seats to your subscription.
To add seats to your subscription:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- Select your account from the drop-down menu in the top-left.
- Select Add seats.
- Follow the on-screen instructions to complete adding seats.
You can now add more members to your organization. For more information, see Manage organization members.
Important
If you have a sales-assisted Docker Business subscription, contact your account manager to add seats to your subscription.
Add seats to Legacy Docker plan
- Sign in to Docker Hub.
- Select your avatar in the top-left, and select Billing from the drop-down menu.
- On the Billing page, select Add seats.
- Select the number of seats you want to add, then select Purchase.
Add seats to Docker Build Cloud
- Sign in to Docker Build Cloud.
- Select Account settings, then Add seats.
- Select the number of seats you want to add, then select Add seats.
Volume pricing
Docker offers volume pricing for Docker Business subscriptions starting at 25 seats. Contact the Docker Sales Team for more information.
Remove seats
You can remove seats from your Team or Business subscription at anytime.
If you remove seats in the middle of the billing cycle, changes apply in the next billing cycle. Any unused portion of the subscription for removed seats isn't refundable or creditable.
For example, if you receive your billing on the 8th of every month for 10 seats and you want to remove 2 seats on the 15th of the month, the 2 seats will be removed from your subscription the next month. Your payment for 8 seats begins on the next billing cycle. If you're on the annual subscription, the 2 seats are still available until the next year, and your payment for the 8 seats begins on the next billing cycle.
Important
If you have a sales-assisted Docker Business subscription, contact your account manager to remove seats from your subscription.
To remove seats:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- Select your account from the drop-down menu in the top-left.
- Select the action icon and then select Remove seats.
- Follow the on-screen instructions to complete removing seats.
You can cancel the removal of seats before your next billing cycle. To do so, select Cancel change.
Important
If you have a sales-assisted Docker Business subscription, contact your account manager to remove seats from your subscription.
Remove seats from Legacy Docker plan
- Sign in to Docker Hub.
- Select your avatar in the top-left, and select Billing from the drop-down menu.
- On the Billing page, select Remove seats.
- Follow the on-screen instructions to complete removing seats.
Remove seats from Docker Build Cloud
- Sign in to Docker Build Cloud.
- Select Account settings, then Remove seats.
- Follow the on-screen instructions to complete removing seats.",,,
b62cb71a011ee1c076e4ba4fa761c0a7ec890d59c0495358b09ee691cf6d79ef,"Install Docker Engine
This section describes how to install Docker Engine on Linux, also known as Docker CE. Docker Engine is also available for Windows, macOS, and Linux, through Docker Desktop. For instructions on how to install Docker Desktop, see: Overview of Docker Desktop.
Supported platforms
| Platform | x86_64 / amd64 | arm64 / aarch64 | arm (32-bit) | ppc64le | s390x |
|---|---|---|---|---|---|
| CentOS | ✅ | ✅ | ✅ | ||
| Debian | ✅ | ✅ | ✅ | ✅ | |
| Fedora | ✅ | ✅ | ✅ | ||
| Raspberry Pi OS (32-bit) | ✅ | ||||
| RHEL | ✅ | ✅ | ✅ | ||
| SLES | ✅ | ||||
| Ubuntu | ✅ | ✅ | ✅ | ✅ | ✅ |
| Binaries | ✅ | ✅ | ✅ |
Other Linux distributions
Note
While the following instructions may work, Docker doesn't test or verify installation on distribution derivatives.
- If you use Debian derivatives such as ""BunsenLabs Linux"", ""Kali Linux"" or ""LMDE"" (Debian-based Mint) should follow the installation instructions for Debian, substitute the version of your distribution for the corresponding Debian release. Refer to the documentation of your distribution to find which Debian release corresponds with your derivative version.
- Likewise, if you use Ubuntu derivatives such as ""Kubuntu"", ""Lubuntu"" or ""Xubuntu"" you should follow the installation instructions for Ubuntu, substituting the version of your distribution for the corresponding Ubuntu release. Refer to the documentation of your distribution to find which Ubuntu release corresponds with your derivative version.
- Some Linux distributions provide a package of Docker Engine through their package repositories. These packages are built and maintained by the Linux distribution's package maintainers and may have differences in configuration or are built from modified source code. Docker isn't involved in releasing these packages and you should report any bugs or issues involving these packages to your Linux distribution's issue tracker.
Docker provides binaries for manual installation of Docker Engine. These binaries are statically linked and you can use them on any Linux distribution.
Release channels
Docker Engine has two types of update channels, stable and test:
- The stable channel gives you the latest versions released for general availability.
- The test channel gives you pre-release versions that are ready for testing before general availability.
Use the test channel with caution. Pre-release versions include experimental and early-access features that are subject to breaking changes.
Support
Docker Engine is an open source project, supported by the Moby project maintainers and community members. Docker doesn't provide support for Docker Engine. Docker provides support for Docker products, including Docker Desktop, which uses Docker Engine as one of its components.
For information about the open source project, refer to the Moby project website.
Upgrade path
Patch releases are always backward compatible with its major and minor version.
Licensing
Docker Engine is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.
Reporting security issues
If you discover a security issue, we request that you bring it to our attention immediately.
DO NOT file a public issue. Instead, submit your report privately to security@docker.com.
Security reports are greatly appreciated, and Docker will publicly thank you for it.
Get started
After setting up Docker, you can learn the basics with Getting started with Docker.",,,
50ada2673eaa514e515558809863e25d5b6221c484dbe7e3f343843a5ed09965,"Integrate Docker Scout with Slack
You can integrate Docker Scout with Slack by creating a Slack webhook and adding it to the Docker Scout Dashboard. Docker Scout will notify you about when a new vulnerability is disclosed, and it affects one or more of your images.
How it works
After configuring the integration, Docker Scout sends notifications about changes to policy compliance and vulnerability exposure for your repositories, to the Slack channels associated with the webhook.
Note
Notifications are only triggered for the last pushed image tags for each repository. ""Last pushed"" refers to the image tag that was most recently pushed to the registry and analyzed by Docker Scout. If the last pushed image is not by a newly disclosed CVE, then no notification will be triggered.
For more information about Docker Scout notifications, see Notification settings
Setup
To add a Slack integration:
Create a webhook, see Slack documentation.
Go to the Slack integration page in the Docker Scout Dashboard.
In the How to integrate section, enter a Configuration name. Docker Scout uses this label as a display name for the integration, so you might want to change the default name into something more meaningful. For example the
#channel-name
, or the name of the team that this configuration belongs to.Paste the webhook you just created in the Slack webhook field.
Select the Test webhook button if you wish to verify the connection. Docker Scout will send a test message to the specified webhook.
Select whether you want to enable notifications for all your Scout-enabled image repositories, or enter the names of the repositories that you want to send notifications for.
When you're ready to enable the integration, select Create.
After creating the webhook, Docker Scout begins to send notifications updates to the Slack channels associated with the webhook.
Remove a Slack integration
To remove a Slack integration:
- Go to the Slack integration page in the Docker Scout Dashboard.
- Select the Remove icon for the integration that you want to remove.
- Confirm by selecting Remove again in the confirmation dialog.",,,
9e3015863cb32727275784f91bbedbec752a8368437744e3d1c5b4fb5adc4089,"Reference documentation
This section includes the reference documentation for the Docker platform's various APIs, CLIs, drivers and specifications, and file formats.
This section includes the reference documentation for the Docker platform's various APIs, CLIs, drivers and specifications, and file formats.",,,
31b6fa31dcd9599f12f644ba82042ebc84fe6323055fc4e216fc489f86f49bd9,"Networking using a macvlan network
This series of tutorials deals with networking standalone containers which
connect to macvlan
networks. In this type of network, the Docker host accepts
requests for multiple MAC addresses at its IP address, and routes those requests
to the appropriate container. For other networking topics, see the
overview.
Goal
The goal of these tutorials is to set up a bridged macvlan
network and attach
a container to it, then set up an 802.1Q trunked macvlan
network and attach a
container to it.
Prerequisites
Most cloud providers block
macvlan
networking. You may need physical access to your networking equipment.The
macvlan
networking driver only works on Linux hosts, and is not supported on Docker Desktop or Docker Engine on Windows.You need at least version 3.9 of the Linux kernel, and version 4.0 or higher is recommended.
The examples assume your ethernet interface is
eth0
. If your device has a different name, use that instead.The
macvlan
driver is not supported in rootless mode.
Bridge example
In the simple bridge example, your traffic flows through eth0
and Docker
routes traffic to your container using its MAC address. To network devices
on your network, your container appears to be physically attached to the network.
Create a
macvlan
network calledmy-macvlan-net
. Modify thesubnet
,gateway
, andparent
values to values that make sense in your environment.$ docker network create -d macvlan \ --subnet=172.16.86.0/24 \ --gateway=172.16.86.1 \ -o parent=eth0 \ my-macvlan-net
You can use
docker network ls
anddocker network inspect my-macvlan-net
commands to verify that the network exists and is amacvlan
network.Start an
alpine
container and attach it to themy-macvlan-net
network. The-dit
flags start the container in the background but allow you to attach to it. The--rm
flag means the container is removed when it is stopped.$ docker run --rm -dit \ --network my-macvlan-net \ --name my-macvlan-alpine \ alpine:latest \ ash
Inspect the
my-macvlan-alpine
container and notice theMacAddress
key within theNetworks
key:$ docker container inspect my-macvlan-alpine ...truncated... ""Networks"": { ""my-macvlan-net"": { ""IPAMConfig"": null, ""Links"": null, ""Aliases"": [ ""bec64291cd4c"" ], ""NetworkID"": ""5e3ec79625d388dbcc03dcf4a6dc4548644eb99d58864cf8eee2252dcfc0cc9f"", ""EndpointID"": ""8caf93c862b22f379b60515975acf96f7b54b7cf0ba0fb4a33cf18ae9e5c1d89"", ""Gateway"": ""172.16.86.1"", ""IPAddress"": ""172.16.86.2"", ""IPPrefixLen"": 24, ""IPv6Gateway"": """", ""GlobalIPv6Address"": """", ""GlobalIPv6PrefixLen"": 0, ""MacAddress"": ""02:42:ac:10:56:02"", ""DriverOpts"": null } } ...truncated
Check out how the container sees its own network interfaces by running a couple of
docker exec
commands.$ docker exec my-macvlan-alpine ip addr show eth0 9: eth0@tunl0: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:10:56:02 brd ff:ff:ff:ff:ff:ff inet 172.16.86.2/24 brd 172.16.86.255 scope global eth0 valid_lft forever preferred_lft forever
$ docker exec my-macvlan-alpine ip route default via 172.16.86.1 dev eth0 172.16.86.0/24 dev eth0 scope link src 172.16.86.2
Stop the container (Docker removes it because of the
--rm
flag), and remove the network.$ docker container stop my-macvlan-alpine $ docker network rm my-macvlan-net
802.1Q trunked bridge example
In the 802.1Q trunked bridge example, your traffic flows through a sub-interface
of eth0
(called eth0.10
) and Docker routes traffic to your container using
its MAC address. To network devices on your network, your container appears to
be physically attached to the network.
Create a
macvlan
network calledmy-8021q-macvlan-net
. Modify thesubnet
,gateway
, andparent
values to values that make sense in your environment.$ docker network create -d macvlan \ --subnet=172.16.86.0/24 \ --gateway=172.16.86.1 \ -o parent=eth0.10 \ my-8021q-macvlan-net
You can use
docker network ls
anddocker network inspect my-8021q-macvlan-net
commands to verify that the network exists, is amacvlan
network, and has parenteth0.10
. You can useip addr show
on the Docker host to verify that the interfaceeth0.10
exists and has a separate IP addressStart an
alpine
container and attach it to themy-8021q-macvlan-net
network. The-dit
flags start the container in the background but allow you to attach to it. The--rm
flag means the container is removed when it is stopped.$ docker run --rm -itd \ --network my-8021q-macvlan-net \ --name my-second-macvlan-alpine \ alpine:latest \ ash
Inspect the
my-second-macvlan-alpine
container and notice theMacAddress
key within theNetworks
key:$ docker container inspect my-second-macvlan-alpine ...truncated... ""Networks"": { ""my-8021q-macvlan-net"": { ""IPAMConfig"": null, ""Links"": null, ""Aliases"": [ ""12f5c3c9ba5c"" ], ""NetworkID"": ""c6203997842e654dd5086abb1133b7e6df627784fec063afcbee5893b2bb64db"", ""EndpointID"": ""aa08d9aa2353c68e8d2ae0bf0e11ed426ea31ed0dd71c868d22ed0dcf9fc8ae6"", ""Gateway"": ""172.16.86.1"", ""IPAddress"": ""172.16.86.2"", ""IPPrefixLen"": 24, ""IPv6Gateway"": """", ""GlobalIPv6Address"": """", ""GlobalIPv6PrefixLen"": 0, ""MacAddress"": ""02:42:ac:10:56:02"", ""DriverOpts"": null } } ...truncated
Check out how the container sees its own network interfaces by running a couple of
docker exec
commands.$ docker exec my-second-macvlan-alpine ip addr show eth0 11: eth0@if10: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:10:56:02 brd ff:ff:ff:ff:ff:ff inet 172.16.86.2/24 brd 172.16.86.255 scope global eth0 valid_lft forever preferred_lft forever
$ docker exec my-second-macvlan-alpine ip route default via 172.16.86.1 dev eth0 172.16.86.0/24 dev eth0 scope link src 172.16.86.2
Stop the container (Docker removes it because of the
--rm
flag), and remove the network.$ docker container stop my-second-macvlan-alpine $ docker network rm my-8021q-macvlan-net",,,
5d8f71af1a73b1a4e1d06bab1a789830c72f73db77b5aa67579b8c9aca4cbfbc,"dockerd
daemon
Usage: dockerd [OPTIONS]
A self-sufficient runtime for containers.
Options:
--add-runtime runtime Register an additional OCI compatible runtime (default [])
--authorization-plugin list Authorization plugins to load
--bip string IPv4 address for the default bridge
--bip6 string IPv6 address for the default bridge
-b, --bridge string Attach containers to a network bridge
--cdi-spec-dir list CDI specification directories to use
--cgroup-parent string Set parent cgroup for all containers
--config-file string Daemon configuration file (default ""/etc/docker/daemon.json"")
--containerd string containerd grpc address
--containerd-namespace string Containerd namespace to use (default ""moby"")
--containerd-plugins-namespace string Containerd namespace to use for plugins (default ""plugins.moby"")
--cpu-rt-period int Limit the CPU real-time period in microseconds for the
parent cgroup for all containers (not supported with cgroups v2)
--cpu-rt-runtime int Limit the CPU real-time runtime in microseconds for the
parent cgroup for all containers (not supported with cgroups v2)
--cri-containerd start containerd with cri
--data-root string Root directory of persistent Docker state (default ""/var/lib/docker"")
-D, --debug Enable debug mode
--default-address-pool pool-options Default address pools for node specific local networks
--default-cgroupns-mode string Default mode for containers cgroup namespace (""host"" | ""private"") (default ""private"")
--default-gateway ip Default gateway IPv4 address for the default bridge network
--default-gateway-v6 ip Default gateway IPv6 address for the default bridge network
--default-ipc-mode string Default mode for containers ipc (""shareable"" | ""private"") (default ""private"")
--default-network-opt mapmap Default network options (default map[])
--default-runtime string Default OCI runtime for containers (default ""runc"")
--default-shm-size bytes Default shm size for containers (default 64MiB)
--default-ulimit ulimit Default ulimits for containers (default [])
--dns list DNS server to use
--dns-opt list DNS options to use
--dns-search list DNS search domains to use
--exec-opt list Runtime execution options
--exec-root string Root directory for execution state files (default ""/var/run/docker"")
--experimental Enable experimental features
--feature map Enable feature in the daemon
--fixed-cidr string IPv4 subnet for the default bridge network
--fixed-cidr-v6 string IPv6 subnet for the default bridge network
-G, --group string Group for the unix socket (default ""docker"")
--help Print usage
-H, --host list Daemon socket(s) to connect to
--host-gateway-ip list IP addresses that the special 'host-gateway' string in --add-host resolves to.
Defaults to the IP addresses of the default bridge
--http-proxy string HTTP proxy URL to use for outgoing traffic
--https-proxy string HTTPS proxy URL to use for outgoing traffic
--icc Enable inter-container communication for the default bridge network (default true)
--init Run an init in the container to forward signals and reap processes
--init-path string Path to the docker-init binary
--insecure-registry list Enable insecure registry communication
--ip ip Host IP for port publishing from the default bridge network (default 0.0.0.0)
--ip-forward Enable IP forwarding in system configuration (default true)
--ip-forward-no-drop Do not set the filter-FORWARD policy to DROP when enabling IP forwarding
--ip-masq Enable IP masquerading for the default bridge network (default true)
--ip6tables Enable addition of ip6tables rules (default true)
--iptables Enable addition of iptables rules (default true)
--ipv6 Enable IPv6 networking for the default bridge network
--label list Set key=value labels to the daemon
--live-restore Enable live restore of docker when containers are still running
--log-driver string Default driver for container logs (default ""json-file"")
--log-format string Set the logging format (""text""|""json"") (default ""text"")
-l, --log-level string Set the logging level (""debug""|""info""|""warn""|""error""|""fatal"") (default ""info"")
--log-opt map Default log driver options for containers (default map[])
--max-concurrent-downloads int Set the max concurrent downloads (default 3)
--max-concurrent-uploads int Set the max concurrent uploads (default 5)
--max-download-attempts int Set the max download attempts for each pull (default 5)
--metrics-addr string Set default address and port to serve the metrics api on
--mtu int Set the MTU for the default ""bridge"" network (default 1500)
--network-control-plane-mtu int Network Control plane MTU (default 1500)
--no-new-privileges Set no-new-privileges by default for new containers
--no-proxy string Comma-separated list of hosts or IP addresses for which the proxy is skipped
--node-generic-resource list Advertise user-defined resource
-p, --pidfile string Path to use for daemon PID file (default ""/var/run/docker.pid"")
--raw-logs Full timestamps without ANSI coloring
--registry-mirror list Preferred registry mirror
--rootless Enable rootless mode; typically used with RootlessKit
--seccomp-profile string Path to seccomp profile. Set to ""unconfined"" to disable the default seccomp profile (default ""builtin"")
--selinux-enabled Enable selinux support
--shutdown-timeout int Set the default shutdown timeout (default 15)
-s, --storage-driver string Storage driver to use
--storage-opt list Storage driver options
--swarm-default-advertise-addr string Set default address or interface for swarm advertised address
--tls Use TLS; implied by --tlsverify
--tlscacert string Trust certs signed only by this CA (default ""~/.docker/ca.pem"")
--tlscert string Path to TLS certificate file (default ""~/.docker/cert.pem"")
--tlskey string Path to TLS key file (default ""~/.docker/key.pem"")
--tlsverify Use TLS and verify the remote
--userland-proxy Use userland proxy for loopback traffic (default true)
--userland-proxy-path string Path to the userland proxy binary
--userns-remap string User/Group setting for user namespaces
--validate Validate daemon configuration and exit
-v, --version Print version information and quit
Options with [] may be specified multiple times.
Description
dockerd
is the persistent process that manages containers. Docker
uses different binaries for the daemon and client. To run the daemon you
type dockerd
.
To run the daemon with debug output, use dockerd --debug
or add ""debug"": true
to
the daemon.json
file.
Note
Enabling experimental features
Enable experimental features by starting
dockerd
with the--experimental
flag or adding""experimental"": true
to thedaemon.json
file.
Environment variables
The following list of environment variables are supported by the dockerd
daemon.
Some of these environment variables are supported both by the Docker Daemon and
the docker
CLI. Refer to
Environment variables
to learn about environment variables supported by the docker
CLI.
| Variable | Description |
|---|---|
DOCKER_CERT_PATH | Location of your authentication keys. This variable is used both by the
docker CLI and the dockerd daemon. |
DOCKER_DRIVER | The storage driver to use. |
DOCKER_RAMDISK | If set this disables pivot_root . |
DOCKER_TLS_VERIFY | When set Docker uses TLS and verifies the remote. This variable is used both by the
docker CLI and the dockerd daemon. |
DOCKER_TMPDIR | Location for temporary files created by the daemon. |
HTTP_PROXY | Proxy URL for HTTP requests unless overridden by NoProxy. See the Go specification for details. |
HTTPS_PROXY | Proxy URL for HTTPS requests unless overridden by NoProxy. See the Go specification for details. |
MOBY_DISABLE_PIGZ | Disables the use of
unpigz to decompress layers in parallel when pulling images, even if it is installed. |
NO_PROXY | Comma-separated values specifying hosts that should be excluded from proxying. See the Go specification for details. |
Examples
Proxy configuration
Note
Refer to the Docker Desktop manual if you are running Docker Desktop.
If you are behind an HTTP proxy server, for example in corporate settings, you may have to configure the Docker daemon to use the proxy server for operations such as pulling and pushing images. The daemon can be configured in three ways:
- Using environment variables (
HTTP_PROXY
,HTTPS_PROXY
, andNO_PROXY
). - Using the
http-proxy
,https-proxy
, andno-proxy
fields in the daemon configuration file (Docker Engine version 23.0 or later). - Using the
--http-proxy
,--https-proxy
, and--no-proxy
command-line options. (Docker Engine version 23.0 or later).
The command-line and configuration file options take precedence over environment
variables. Refer to
control and configure Docker with systemd
to set these environment variables on a host using systemd
.
Daemon socket option
The Docker daemon can listen for
Docker Engine API
requests via three different types of Socket: unix
, tcp
, and fd
.
By default, a unix
domain socket (or IPC socket) is created at
/var/run/docker.sock
, requiring either root
permission, or docker
group
membership.
If you need to access the Docker daemon remotely, you need to enable the tcp
Socket. When using a TCP socket, the Docker daemon provides un-encrypted and
un-authenticated direct access to the Docker daemon by default. You should secure
the daemon either using the
built in HTTPS encrypted socket,
or by putting a secure web proxy in front of it. You can listen on port 2375
on all
network interfaces with -H tcp://0.0.0.0:2375
, or on a particular network
interface using its IP address: -H tcp://192.168.59.103:2375
. It is
conventional to use port 2375
for un-encrypted, and port 2376
for encrypted
communication with the daemon.
Note
If you're using an HTTPS encrypted socket, keep in mind that only TLS version 1.0 and higher is supported. Protocols SSLv3 and below are not supported for security reasons.
On systemd based systems, you can communicate with the daemon via
systemd socket activation,
with dockerd -H fd://
. Using fd://
works for most setups, but
you can also specify individual sockets: dockerd -H fd://3
. If the
specified socket activated files aren't found, the daemon exits. You can
find examples of using systemd socket activation with Docker and systemd in the
Docker source tree.
You can configure the Docker daemon to listen to multiple sockets at the same
time using multiple -H
options:
The example below runs the daemon listening on the default Unix socket, and on 2 specific IP addresses on this host:
$ sudo dockerd -H unix:///var/run/docker.sock -H tcp://192.168.59.106 -H tcp://10.10.10.2
The Docker client honors the DOCKER_HOST
environment variable to set the
-H
flag for the client. Use one of the following commands:
$ docker -H tcp://0.0.0.0:2375 ps
$ export DOCKER_HOST=""tcp://0.0.0.0:2375""
$ docker ps
Setting the DOCKER_TLS_VERIFY
environment variable to any value other than
the empty string is equivalent to setting the --tlsverify
flag. The following
are equivalent:
$ docker --tlsverify ps
# or
$ export DOCKER_TLS_VERIFY=1
$ docker ps
The Docker client honors the HTTP_PROXY
, HTTPS_PROXY
, and NO_PROXY
environment variables (or the lowercase versions thereof). HTTPS_PROXY
takes
precedence over HTTP_PROXY
.
The Docker client supports connecting to a remote daemon via SSH:
$ docker -H ssh://me@example.com:22/var/run/docker.sock ps
$ docker -H ssh://me@example.com:22 ps
$ docker -H ssh://me@example.com ps
$ docker -H ssh://example.com ps
To use SSH connection, you need to set up ssh
so that it can reach the
remote host with public key authentication. Password authentication is not
supported. If your key is protected with passphrase, you need to set up
ssh-agent
.
Bind Docker to another host/port or a Unix socket
Warning
Changing the default
docker
daemon binding to a TCP port or Unixdocker
user group introduces security risks, as it may allow non-root users to gain root access on the host. Make sure you control access todocker
. If you are binding to a TCP port, anyone with access to that port has full Docker access; so it's not advisable on an open network.
With -H
it's possible to make the Docker daemon to listen on a specific IP
and port. By default, it listens on unix:///var/run/docker.sock
to allow
only local connections by the root user. You could set it to 0.0.0.0:2375
or
a specific host IP to give access to everybody, but that isn't recommended
because someone could gain root access to the host where the daemon is running.
Similarly, the Docker client can use -H
to connect to a custom port.
The Docker client defaults to connecting to unix:///var/run/docker.sock
on Linux, and tcp://127.0.0.1:2376
on Windows.
-H
accepts host and port assignment in the following format:
tcp://[host]:[port][path] or unix://path
For example:
tcp://
-> TCP connection to127.0.0.1
on either port2376
when TLS encryption is on, or port2375
when communication is in plain text.tcp://host:2375
-> TCP connection on host:2375tcp://host:2375/path
-> TCP connection on host:2375 and prepend path to all requestsunix://path/to/socket
-> Unix socket located atpath/to/socket
-H
, when empty, defaults to the same value as
when no -H
was passed in.
-H
also accepts short form for TCP bindings: host:
or host:port
or :port
Run Docker in daemon mode:
$ sudo <path to>/dockerd -H 0.0.0.0:5555 &
Download an ubuntu
image:
$ docker -H :5555 pull ubuntu
You can use multiple -H
, for example, if you want to listen on both
TCP and a Unix socket
$ sudo dockerd -H tcp://127.0.0.1:2375 -H unix:///var/run/docker.sock &
# Download an ubuntu image, use default Unix socket
$ docker pull ubuntu
# OR use the TCP port
$ docker -H tcp://127.0.0.1:2375 pull ubuntu
Daemon storage-driver
On Linux, the Docker daemon has support for several different image layer storage
drivers: overlay2
, fuse-overlayfs
, btrfs
, and zfs
.
overlay2
is the preferred storage driver for all currently supported Linux distributions,
and is selected by default. Unless users have a strong reason to prefer another storage driver,
overlay2
should be used.
You can find out more about storage drivers and how to select one in Select a storage driver.
On Windows, the Docker daemon only supports the windowsfilter
storage driver.
Options per storage driver
Particular storage-driver can be configured with options specified with
--storage-opt
flags. Options for zfs
start with zfs
, and options for
btrfs
start with btrfs
.
ZFS options
zfs.fsname
Specifies the ZFS filesystem that the daemon should use to create its datasets.
By default, the ZFS filesystem in /var/lib/docker
is used.
Example
$ sudo dockerd -s zfs --storage-opt zfs.fsname=zroot/docker
Btrfs options
btrfs.min_space
Specifies the minimum size to use when creating the subvolume which is used for containers. If user uses disk quota for btrfs when creating or running a container with --storage-opt size option, Docker should ensure the size can't be smaller than btrfs.min_space.
Example
$ sudo dockerd -s btrfs --storage-opt btrfs.min_space=10G
Overlay2 options
overlay2.size
Sets the default max size of the container. It is supported only when the
backing filesystem is xfs
and mounted with pquota
mount option. Under these
conditions the user can pass any size less than the backing filesystem size.
Example
$ sudo dockerd -s overlay2 --storage-opt overlay2.size=1G
Windowsfilter options
size
Specifies the size to use when creating the sandbox which is used for containers. Defaults to 20G.
Example
C:\> dockerd --storage-opt size=40G
Runtime options
The Docker daemon relies on a
OCI compliant runtime
(invoked via the containerd
daemon) as its interface to the Linux
kernel namespaces
, cgroups
, and SELinux
.
Configure container runtimes
By default, the Docker daemon uses runc as a container runtime. You can configure the daemon to add additional runtimes.
containerd shims installed on PATH
can be used directly, without the need
to edit the daemon's configuration. For example, if you install the Kata
Containers shim (containerd-shim-kata-v2
) on PATH
, then you can select that
runtime with docker run
without having to edit the daemon's configuration:
$ docker run --runtime io.containerd.kata.v2
Container runtimes that don't implement containerd shims, or containerd shims
installed outside of PATH
, must be registered with the daemon, either via the
configuration file or using the --add-runtime
command line flag.
For examples on how to use other container runtimes, see Alternative container runtimes
Configure runtimes using daemon.json
To register and configure container runtimes using the daemon's configuration
file, add the runtimes as entries under runtimes
:
{
""runtimes"": {
""<runtime>"": {}
}
}
The key of the entry (<runtime>
in the previous example) represents the name
of the runtime. This is the name that you reference when you run a container,
using docker run --runtime <runtime>
.
The runtime entry contains an object specifying the configuration for your runtime. The properties of the object depends on what kind of runtime you're looking to register:
If the runtime implements its own containerd shim, the object shall contain a
runtimeType
field and an optionaloptions
field.{ ""runtimes"": { ""<runtime>"": { ""runtimeType"": ""<name-or-path>"", ""options"": {} } } }
See Configure shims.
If the runtime is designed to be a drop-in replacement for runc, the object contains a
path
field, and an optionalruntimeArgs
field.{ ""runtimes"": { ""<runtime>"": { ""path"": ""/path/to/bin"", ""runtimeArgs"": [""...args""] } } }
After changing the runtimes configuration in the configuration file, you must reload or restart the daemon for changes to take effect:
$ sudo systemctl reload dockerd
Configure containerd shims
If the runtime that you want to register implements a containerd shim, or if you want to register a runtime which uses the runc shim, use the following format for the runtime entry:
{
""runtimes"": {
""<runtime>"": {
""runtimeType"": ""<name-or-path>"",
""options"": {}
}
}
}
runtimeType
refers to either:
A fully qualified name of a containerd shim.
The fully qualified name of a shim is the same as the
runtime_type
used to register the runtime in containerd's CRI configuration. For example,io.containerd.runsc.v1
.The path of a containerd shim binary.
This option is useful if you installed the containerd shim binary outside of
PATH
.
options
is optional. It lets you specify the runtime configuration that you
want to use for the shim. The configuration parameters that you can specify in
options
depends on the runtime you're registering. For most shims,
the supported configuration options are TypeUrl
and ConfigPath
.
For example:
{
""runtimes"": {
""gvisor"": {
""runtimeType"": ""io.containerd.runsc.v1"",
""options"": {
""TypeUrl"": ""io.containerd.runsc.v1.options"",
""ConfigPath"": ""/etc/containerd/runsc.toml""
}
}
}
}
You can configure multiple runtimes using the same runtimeType. For example:
{
""runtimes"": {
""gvisor-foo"": {
""runtimeType"": ""io.containerd.runsc.v1"",
""options"": {
""TypeUrl"": ""io.containerd.runsc.v1.options"",
""ConfigPath"": ""/etc/containerd/runsc-foo.toml""
}
},
""gvisor-bar"": {
""runtimeType"": ""io.containerd.runsc.v1"",
""options"": {
""TypeUrl"": ""io.containerd.runsc.v1.options"",
""ConfigPath"": ""/etc/containerd/runsc-bar.toml""
}
}
}
}
The options
field takes a special set of configuration parameters when used
with ""runtimeType"": ""io.containerd.runc.v2""
. For more information about runc
parameters, refer to the runc configuration section in
CRI Plugin Config Guide.
Configure runc drop-in replacements
If the runtime that you want to register can act as a drop-in replacement for
runc, you can register the runtime either using the daemon configuration file,
or using the --add-runtime
flag for the dockerd
cli.
When you use the configuration file, the entry uses the following format:
{
""runtimes"": {
""<runtime>"": {
""path"": ""/path/to/binary"",
""runtimeArgs"": [""...args""]
}
}
}
Where path
is either the absolute path to the runtime executable, or the name
of an executable installed on PATH
:
{
""runtimes"": {
""runc"": {
""path"": ""runc""
}
}
}
And runtimeArgs
lets you optionally pass additional arguments to the runtime.
Entries with this format use the containerd runc shim to invoke a custom
runtime binary.
When you use the --add-runtime
CLI flag, use the following format:
$ sudo dockerd --add-runtime <runtime>=<path>
Defining runtime arguments via the command line is not supported.
For an example configuration for a runc drop-in replacment, see Alternative container runtimes > youki
Configure the default container runtime
You can specify either the name of a fully qualified containerd runtime shim,
or the name of a registered runtime. You can specify the default runtime either
using the daemon configuration file, or using the --default-runtime
flag for
the dockerd
cli.
When you use the configuration file, the entry uses the following format:
{
""default-runtime"": ""io.containerd.runsc.v1""
}
When you use the --default-runtime
CLI flag, use the following format:
$ dockerd --default-runtime io.containerd.runsc.v1
Run containerd standalone
By default, the Docker daemon automatically starts containerd
. If you want to
control containerd
startup, manually start containerd
and pass the path to
the containerd
socket using the --containerd
flag. For example:
$ sudo dockerd --containerd /run/containerd/containerd.sock
Configure cgroup driver
You can configure how the runtime should manage container cgroups, using the
--exec-opt native.cgroupdriver
CLI flag.
You can only specify cgroupfs
or systemd
. If you specify
systemd
and it is not available, the system errors out. If you omit the
native.cgroupdriver
option, cgroupfs
is used on cgroup v1 hosts, systemd
is used on cgroup v2 hosts with systemd available.
This example sets the cgroupdriver
to systemd
:
$ sudo dockerd --exec-opt native.cgroupdriver=systemd
Setting this option applies to all containers the daemon launches.
Configure container isolation technology (Windows)
For Windows containers, you can specify the default container isolation
technology to use, using the --exec-opt isolation
flag.
The following example makes hyperv
the default isolation technology:
> dockerd --exec-opt isolation=hyperv
If no isolation value is specified on daemon start, on Windows client,
the default is hyperv
, and on Windows server, the default is process
.
Daemon DNS options
To set the DNS server for all Docker containers, use:
$ sudo dockerd --dns 8.8.8.8
To set the DNS search domain for all Docker containers, use:
$ sudo dockerd --dns-search example.com
Insecure registries
In this section, ""registry"" refers to a private registry, and myregistry:5000
is a placeholder example of a private registry.
Docker considers a private registry either secure or insecure.
A secure registry uses TLS and a copy of its CA certificate is placed on the
Docker host at /etc/docker/certs.d/myregistry:5000/ca.crt
. An insecure
registry is either not using TLS (i.e., listening on plain text HTTP), or is
using TLS with a CA certificate not known by the Docker daemon. The latter can
happen when the certificate wasn't found under
/etc/docker/certs.d/myregistry:5000/
, or if the certificate verification
failed (i.e., wrong CA).
By default, Docker assumes all registries to be secure, except for local registries.
Communicating with an insecure registry isn't possible
if Docker assumes that registry is secure. In order to communicate with an
insecure registry, the Docker daemon requires --insecure-registry
in one of
the following two forms:
--insecure-registry myregistry:5000
tells the Docker daemon that myregistry:5000 should be considered insecure.--insecure-registry 10.1.0.0/16
tells the Docker daemon that all registries whose domain resolve to an IP address is part of the subnet described by the CIDR syntax, should be considered insecure.
The flag can be used multiple times to allow multiple registries to be marked as insecure.
If an insecure registry isn't marked as insecure, docker pull
,
docker push
, and docker search
result in error messages, prompting
the user to either secure or pass the --insecure-registry
flag to the Docker
daemon as described above.
Local registries, whose IP address falls in the 127.0.0.0/8 range, are automatically marked as insecure as of Docker 1.3.2. It isn't recommended to rely on this, as it may change in the future.
Enabling --insecure-registry
, i.e., allowing un-encrypted and/or untrusted
communication, can be useful when running a local registry. However,
because its use creates security vulnerabilities it should only be enabled for
testing purposes. For increased security, users should add their CA to their
system's list of trusted CAs instead of enabling --insecure-registry
.
Legacy Registries
Operations against registries supporting only the legacy v1 protocol are no longer
supported. Specifically, the daemon doesn't attempt to push, pull or sign in
to v1 registries. The exception to this is search
which can still be performed
on v1 registries.
Running a Docker daemon behind an HTTPS_PROXY
When running inside a LAN that uses an HTTPS
proxy, the proxy's certificates
replace Docker Hub's certificates. These certificates must be added to your
Docker host's configuration:
- Install the
ca-certificates
package for your distribution - Ask your network admin for the proxy's CA certificate and append them to
/etc/pki/tls/certs/ca-bundle.crt
- Then start your Docker daemon with
HTTPS_PROXY=http://username:password@proxy:port/ dockerd
. Theusername:
andpassword@
are optional - and are only needed if your proxy is set up to require authentication.
This only adds the proxy and authentication to the Docker daemon's requests. To use the proxy when building images and running containers, see Configure Docker to use a proxy server
Default ulimit
settings
The --default-ulimit
flag lets you set the default ulimit
options to use for
all containers. It takes the same options as --ulimit
for docker run
. If
these defaults aren't set, ulimit
settings are inherited from the Docker daemon.
Any --ulimit
options passed to docker run
override the daemon defaults.
Be careful setting nproc
with the ulimit
flag, as nproc
is designed by Linux to
set the maximum number of processes available to a user, not to a container.
For details, see
docker run
reference.
Access authorization
Docker's access authorization can be extended by authorization plugins that your
organization can purchase or build themselves. You can install one or more
authorization plugins when you start the Docker daemon
using the
--authorization-plugin=PLUGIN_ID
option.
$ sudo dockerd --authorization-plugin=plugin1 --authorization-plugin=plugin2,...
The PLUGIN_ID
value is either the plugin's name or a path to its specification
file. The plugin's implementation determines whether you can specify a name or
path. Consult with your Docker administrator to get information about the
plugins available to you.
Once a plugin is installed, requests made to the daemon
through the
command line or Docker's Engine API are allowed or denied by the plugin.
If you have multiple plugins installed, each plugin, in order, must
allow the request for it to complete.
For information about how to create an authorization plugin, refer to the authorization plugin section.
Daemon user namespace options
The Linux kernel
user namespace support
provides additional security by enabling a process, and therefore a container,
to have a unique range of user and group IDs which are outside the traditional
user and group range utilized by the host system. One of the most important
security improvements is that, by default, container processes running as the
root
user have expected administrative privileges it expects (with some restrictions)
inside the container, but are effectively mapped to an unprivileged uid
on
the host.
For details about how to use this feature, as well as limitations, see Isolate containers with a user namespace.
Configure host gateway IP
The Docker daemon supports a special host-gateway
value for the --add-host
flag for the docker run
and docker build
commands. This value resolves to
addresses on the host, so that containers can connect to services running on the
host.
By default, host-gateway
resolves to the IPv4 address of the default bridge,
and its IPv6 address if it has one.
You can configure this to resolve to a different IP using the --host-gateway-ip
flag for the dockerd command line interface, or the host-gateway-ip
key in
the daemon configuration file.
To supply both IPv4 and IPv6 addresses on the command line, use two
--host-gateway-ip
options.
To supply addresses in the daemon configuration file, use ""host-gateway-ips""
with a JSON array, as shown below. For compatibility with older versions of the
daemon, a single IP address can also be specified as a JSON string in option
""host-gateway-ip""
.
$ cat > /etc/docker/daemon.json
{ ""host-gateway-ips"": [""192.0.2.1"", ""2001:db8::1111""]}
$ sudo systemctl restart docker
$ docker run -it --add-host host.docker.internal:host-gateway \
busybox ping host.docker.internal
PING host.docker.internal (192.0.2.1): 56 data bytes
$ docker run -it --add-host host.docker.internal:host-gateway \
busybox ping -6 host.docker.internal
PING host.docker.internal (2001:db8::1111): 56 data bytes
Enable CDI devices
Note
This is experimental feature and as such doesn't represent a stable API.
This feature isn't enabled by default. To this feature, set
features.cdi
totrue
in thedaemon.json
configuration file.
Container Device Interface (CDI) is a standardized mechanism for container runtimes to create containers which are able to interact with third party devices.
The Docker daemon supports running containers with CDI devices if the requested device specifications are available on the filesystem of the daemon.
The default specification directors are:
/etc/cdi/
for static CDI Specs/var/run/cdi
for generated CDI Specs
Alternatively, you can set custom locations for CDI specifications using the
cdi-spec-dirs
option in the daemon.json
configuration file, or the
--cdi-spec-dir
flag for the dockerd
CLI.
{
""features"": {
""cdi"": true
},
""cdi-spec-dirs"": [""/etc/cdi/"", ""/var/run/cdi""]
}
When CDI is enabled for a daemon, you can view the configured CDI specification
directories using the docker info
command.
Daemon logging format
The --log-format
option or ""log-format"" option in the
daemon configuration file
lets you set the format for logs produced by the daemon. The logging format should
only be configured either through the --log-format
command line option or
through the ""log-format"" field in the configuration file; using both
the command-line option and the ""log-format"" field in the configuration
file produces an error. If this option is not set, the default is ""text"".
The following example configures the daemon through the --log-format
command
line option to use json
formatted logs;
$ dockerd --log-format=json
# ...
{""level"":""info"",""msg"":""API listen on /var/run/docker.sock"",""time"":""2024-09-16T11:06:08.558145428Z""}
The following example shows a daemon.json
configuration file with the
""log-format"" set;
{
""log-format"": ""json""
}
Miscellaneous options
IP masquerading uses address translation to allow containers without a public
IP to talk to other machines on the internet. This may interfere with some
network topologies, and can be disabled with --ip-masq=false
.
Docker supports soft links for the Docker data directory (/var/lib/docker
) and
for /var/lib/docker/tmp
. The DOCKER_TMPDIR
and the data directory can be
set like this:
$ export DOCKER_TMPDIR=/mnt/disk2/tmp
$ sudo -E dockerd --data-root /var/lib/docker -H unix://
Default cgroup parent
The --cgroup-parent
option lets you set the default cgroup parent
for containers. If this option isn't set, it defaults to /docker
for
the cgroupfs driver, and system.slice
for the systemd cgroup driver.
If the cgroup has a leading forward slash (/
), the cgroup is created
under the root cgroup, otherwise the cgroup is created under the daemon
cgroup.
Assuming the daemon is running in cgroup daemoncgroup
,
--cgroup-parent=/foobar
creates a cgroup in
/sys/fs/cgroup/memory/foobar
, whereas using --cgroup-parent=foobar
creates the cgroup in /sys/fs/cgroup/memory/daemoncgroup/foobar
The systemd cgroup driver has different rules for --cgroup-parent
. systemd
represents hierarchy by slice and the name of the slice encodes the location in
the tree. So --cgroup-parent
for systemd cgroups should be a slice name. A
name can consist of a dash-separated series of names, which describes the path
to the slice from the root slice. For example, --cgroup-parent=user-a-b.slice
means the memory cgroup for the container is created in
/sys/fs/cgroup/memory/user.slice/user-a.slice/user-a-b.slice/docker-<id>.scope
.
This setting can also be set per container, using the --cgroup-parent
option on docker create
and docker run
, and takes precedence over
the --cgroup-parent
option on the daemon.
Daemon metrics
The --metrics-addr
option takes a TCP address to serve the metrics API.
This feature is still experimental, therefore, the daemon must be running in experimental
mode for this feature to work.
To serve the metrics API on localhost:9323
you would specify --metrics-addr 127.0.0.1:9323
,
allowing you to make requests on the API at 127.0.0.1:9323/metrics
to receive metrics in the
prometheus format.
Port 9323
is the
default port associated with Docker
metrics
to avoid collisions with other Prometheus exporters and services.
If you are running a Prometheus server you can add this address to your scrape configs to have Prometheus collect metrics on Docker. For more information, see Collect Docker metrics with Prometheus.
Node generic resources
The --node-generic-resources
option takes a list of key-value
pair (key=value
) that allows you to advertise user defined resources
in a Swarm cluster.
The current expected use case is to advertise NVIDIA GPUs so that services
requesting NVIDIA-GPU=[0-16]
can land on a node that has enough GPUs for
the task to run.
Example of usage:
{
""node-generic-resources"": [
""NVIDIA-GPU=UUID1"",
""NVIDIA-GPU=UUID2""
]
}
Enable feature in the daemon (--feature)
The --feature
option lets you enable or disable a feature in the daemon.
This option corresponds with the ""features"" field in the
daemon.json configuration file.
Features should only be configured either through the --feature
command line
option or through the ""features"" field in the configuration file; using both
the command-line option and the ""features"" field in the configuration
file produces an error. The feature option can be specified multiple times
to configure multiple features. The --feature
option accepts a name and
optional boolean value. When omitting the value, the default is true
.
The following example runs the daemon with the cdi
and containerd-snapshotter
features enabled. The cdi
option is provided with a value;
$ dockerd --feature cdi=true --feature containerd-snapshotter
The following example is the equivalent using the daemon.json
configuration
file;
{
""features"": {
""cdi"": true,
""containerd-snapshotter"": true
}
}
Daemon configuration file
The --config-file
option allows you to set any configuration option
for the daemon in a JSON format. This file uses the same flag names as keys,
except for flags that allow several entries, where it uses the plural
of the flag name, e.g., labels
for the label
flag.
The options set in the configuration file must not conflict with options set
using flags. The Docker daemon fails to start if an option is duplicated between
the file and the flags, regardless of their value. This is intentional, and avoids
silently ignore changes introduced in configuration reloads.
For example, the daemon fails to start if you set daemon labels
in the configuration file and also set daemon labels via the --label
flag.
Options that are not present in the file are ignored when the daemon starts.
The --validate
option allows to validate a configuration file without
starting the Docker daemon. A non-zero exit code is returned for invalid
configuration files.
$ dockerd --validate --config-file=/tmp/valid-config.json
configuration OK
$ echo $?
0
$ dockerd --validate --config-file /tmp/invalid-config.json
unable to configure the Docker daemon with file /tmp/invalid-config.json: the following directives don't match any configuration option: unknown-option
$ echo $?
1
On Linux
The default location of the configuration file on Linux is
/etc/docker/daemon.json
. Use the --config-file
flag to specify a
non-default location.
The following is a full example of the allowed configuration options on Linux:
{
""authorization-plugins"": [],
""bip"": """",
""bip6"": """",
""bridge"": """",
""builder"": {
""gc"": {
""enabled"": true,
""defaultKeepStorage"": ""10GB"",
""policy"": [
{ ""keepStorage"": ""10GB"", ""filter"": [""unused-for=2200h""] },
{ ""keepStorage"": ""50GB"", ""filter"": [""unused-for=3300h""] },
{ ""keepStorage"": ""100GB"", ""all"": true }
]
}
},
""cgroup-parent"": """",
""containerd"": ""/run/containerd/containerd.sock"",
""containerd-namespace"": ""docker"",
""containerd-plugins-namespace"": ""docker-plugins"",
""data-root"": """",
""debug"": true,
""default-address-pools"": [
{
""base"": ""172.30.0.0/16"",
""size"": 24
},
{
""base"": ""172.31.0.0/16"",
""size"": 24
}
],
""default-cgroupns-mode"": ""private"",
""default-gateway"": """",
""default-gateway-v6"": """",
""default-network-opts"": {},
""default-runtime"": ""runc"",
""default-shm-size"": ""64M"",
""default-ulimits"": {
""nofile"": {
""Hard"": 64000,
""Name"": ""nofile"",
""Soft"": 64000
}
},
""dns"": [],
""dns-opts"": [],
""dns-search"": [],
""exec-opts"": [],
""exec-root"": """",
""experimental"": false,
""features"": {
""cdi"": true,
""containerd-snapshotter"": true
},
""fixed-cidr"": """",
""fixed-cidr-v6"": """",
""group"": """",
""host-gateway-ip"": """",
""hosts"": [],
""proxies"": {
""http-proxy"": ""http://proxy.example.com:80"",
""https-proxy"": ""https://proxy.example.com:443"",
""no-proxy"": ""*.test.example.com,.example.org""
},
""icc"": false,
""init"": false,
""init-path"": ""/usr/libexec/docker-init"",
""insecure-registries"": [],
""ip"": ""0.0.0.0"",
""ip-forward"": false,
""ip-masq"": false,
""iptables"": false,
""ip6tables"": false,
""ipv6"": false,
""labels"": [],
""live-restore"": true,
""log-driver"": ""json-file"",
""log-format"": ""text"",
""log-level"": """",
""log-opts"": {
""cache-disabled"": ""false"",
""cache-max-file"": ""5"",
""cache-max-size"": ""20m"",
""cache-compress"": ""true"",
""env"": ""os,customer"",
""labels"": ""somelabel"",
""max-file"": ""5"",
""max-size"": ""10m""
},
""max-concurrent-downloads"": 3,
""max-concurrent-uploads"": 5,
""max-download-attempts"": 5,
""mtu"": 0,
""no-new-privileges"": false,
""node-generic-resources"": [
""NVIDIA-GPU=UUID1"",
""NVIDIA-GPU=UUID2""
],
""pidfile"": """",
""raw-logs"": false,
""registry-mirrors"": [],
""runtimes"": {
""cc-runtime"": {
""path"": ""/usr/bin/cc-runtime""
},
""custom"": {
""path"": ""/usr/local/bin/my-runc-replacement"",
""runtimeArgs"": [
""--debug""
]
}
},
""seccomp-profile"": """",
""selinux-enabled"": false,
""shutdown-timeout"": 15,
""storage-driver"": """",
""storage-opts"": [],
""swarm-default-advertise-addr"": """",
""tls"": true,
""tlscacert"": """",
""tlscert"": """",
""tlskey"": """",
""tlsverify"": true,
""userland-proxy"": false,
""userland-proxy-path"": ""/usr/libexec/docker-proxy"",
""userns-remap"": """"
}
Note
You can't set options in
daemon.json
that have already been set on daemon startup as a flag. On systems that use systemd to start the Docker daemon,-H
is already set, so you can't use thehosts
key indaemon.json
to add listening addresses. See custom Docker daemon options for an example on how to configure the daemon using systemd drop-in files.
On Windows
The default location of the configuration file on Windows is
%programdata%\docker\config\daemon.json
. Use the --config-file
flag
to specify a non-default location.
The following is a full example of the allowed configuration options on Windows:
{
""authorization-plugins"": [],
""bridge"": """",
""containerd"": ""\\\\.\\pipe\\containerd-containerd"",
""containerd-namespace"": ""docker"",
""containerd-plugins-namespace"": ""docker-plugins"",
""data-root"": """",
""debug"": true,
""default-network-opts"": {},
""default-runtime"": """",
""default-ulimits"": {},
""dns"": [],
""dns-opts"": [],
""dns-search"": [],
""exec-opts"": [],
""experimental"": false,
""features"": {},
""fixed-cidr"": """",
""group"": """",
""host-gateway-ip"": """",
""hosts"": [],
""insecure-registries"": [],
""labels"": [],
""log-driver"": """",
""log-format"": ""text"",
""log-level"": """",
""max-concurrent-downloads"": 3,
""max-concurrent-uploads"": 5,
""max-download-attempts"": 5,
""mtu"": 0,
""pidfile"": """",
""raw-logs"": false,
""registry-mirrors"": [],
""shutdown-timeout"": 15,
""storage-driver"": """",
""storage-opts"": [],
""swarm-default-advertise-addr"": """",
""tlscacert"": """",
""tlscert"": """",
""tlskey"": """",
""tlsverify"": true
}
The default-runtime
option is by default unset, in which case dockerd automatically detects the runtime.
This detection is based on if the containerd
flag is set.
Accepted values:
com.docker.hcsshim.v1
- This is the built-in runtime that Docker has used since Windows supported was first added and uses the v1 HCS API's in Windows.io.containerd.runhcs.v1
- This is uses the containerdrunhcs
shim to run the container and uses the v2 HCS API's in Windows.
Feature options
The optional field features
in daemon.json
lets you enable or disable specific
daemon features.
{
""features"": {
""some-feature"": true,
""some-disabled-feature-enabled-by-default"": false
}
}
The list of feature options include:
containerd-snapshotter
: when set totrue
, the daemon uses containerd snapshotters instead of the classic storage drivers for storing image and container data. For more information, see containerd storage.windows-dns-proxy
: when set totrue
, the daemon's internal DNS resolver will forward requests to external servers. Without this, most applications running in the container will still be able to use secondary DNS servers configured in the container itself, butnslookup
won't be able to resolve external names. The current default isfalse
, it will change totrue
in a future release. This option is only allowed on Windows.Warning
The
windows-dns-proxy
feature flag will be removed in a future release.
Configuration reload behavior
Some options can be reconfigured when the daemon is running without requiring
to restart the process. The daemon uses the SIGHUP
signal in Linux to reload,
and a global event in Windows with the key Global\docker-daemon-config-$PID
.
You can modify the options in the configuration file, but the daemon still
checks for conflicting settings with the specified CLI flags. The daemon fails
to reconfigure itself if there are conflicts, but it won't stop execution.
The list of currently supported options that can be reconfigured is this:
| Option | Description |
|---|---|
debug | Toggles debug mode of the daemon. |
labels | Replaces the daemon labels with a new set of labels. |
live-restore | Toggles live restore. |
max-concurrent-downloads | Configures the max concurrent downloads for each pull. |
max-concurrent-uploads | Configures the max concurrent uploads for each push. |
max-download-attempts | Configures the max download attempts for each pull. |
default-runtime | Configures the runtime to be used if not is specified at container creation. |
runtimes | Configures the list of available OCI runtimes that can be used to run containers. |
authorization-plugin | Specifies the authorization plugins to use. |
insecure-registries | Specifies a list of registries that the daemon should consider insecure. |
registry-mirrors | Specifies a list of registry mirrors. |
shutdown-timeout | Configures the daemon's existing configuration timeout with a new timeout for shutting down all containers. |
features | Enables or disables specific features. |
Run multiple daemons
Note
Running multiple daemons on a single host is considered experimental. You may encounter unsolved problems, and things may not work as expected in some cases.
This section describes how to run multiple Docker daemons on a single host. To run multiple daemons, you must configure each daemon so that it doesn't conflict with other daemons on the same host. You can set these options either by providing them as flags, or by using a daemon configuration file.
The following daemon options must be configured for each daemon:
-b, --bridge= Attach containers to a network bridge
--exec-root=/var/run/docker Root of the Docker execdriver
--data-root=/var/lib/docker Root of persisted Docker data
-p, --pidfile=/var/run/docker.pid Path to use for daemon PID file
-H, --host=[] Daemon socket(s) to connect to
--iptables=true Enable addition of iptables rules
--config-file=/etc/docker/daemon.json Daemon configuration file
--tlscacert=""~/.docker/ca.pem"" Trust certs signed only by this CA
--tlscert=""~/.docker/cert.pem"" Path to TLS certificate file
--tlskey=""~/.docker/key.pem"" Path to TLS key file
When your daemons use different values for these flags, you can run them on the same host without any problems. It is important that you understand the meaning of these options and to use them correctly.
- The
-b, --bridge=
flag is set todocker0
as default bridge network. It is created automatically when you install Docker. If you aren't using the default, you must create and configure the bridge manually, or set it to 'none':--bridge=none
--exec-root
is the path where the container state is stored. The default value is/var/run/docker
. Specify the path for your running daemon here.--data-root
is the path where persisted data such as images, volumes, and cluster state are stored. The default value is/var/lib/docker
. To avoid any conflict with other daemons, set this parameter separately for each daemon.-p, --pidfile=/var/run/docker.pid
is the path where the process ID of the daemon is stored. Specify the path for your PID file here.--host=[]
specifies where the Docker daemon listens for client connections. If unspecified, it defaults to/var/run/docker.sock
.--iptables=false
prevents the Docker daemon from adding iptables rules. If multiple daemons manage iptables rules, they may overwrite rules set by another daemon. Be aware that disabling this option requires you to manually add iptables rules to expose container ports. If you prevent Docker from adding iptables rules, Docker also doesn't add IP masquerading rules, even if you set--ip-masq
totrue
. Without IP masquerading rules, Docker containers can't connect to external hosts or the internet when using network other than default bridge.--config-file=/etc/docker/daemon.json
is the path where configuration file is stored. You can use it instead of daemon flags. Specify the path for each daemon.--tls*
Docker daemon supports--tlsverify
mode that enforces encrypted and authenticated remote connections. The--tls*
options enable use of specific certificates for individual daemons.
Example script for a separate “bootstrap” instance of the Docker daemon without network:
$ sudo dockerd \
-H unix:///var/run/docker-bootstrap.sock \
-p /var/run/docker-bootstrap.pid \
--iptables=false \
--ip-masq=false \
--bridge=none \
--data-root=/var/lib/docker-bootstrap \
--exec-root=/var/run/docker-bootstrap
Default network options
The default-network-opts
key in the daemon.json
configuration file, and the
equivalent --default-network-opt
CLI flag, let you specify default values for
driver network driver options for new networks.
The following example shows how to configure options for the bridge
driver
using the daemon.json
file.
{
""default-network-opts"": {
""bridge"": {
""com.docker.network.bridge.host_binding_ipv4"": ""127.0.0.1"",
""com.docker.network.driver.mtu"": ""1234""
}
}
}
This example uses the bridge
network driver. Refer to the
bridge network driver page
for an overview of available driver options.
After changing the configuration and restarting the daemon, new networks that you create use these option configurations as defaults.
$ docker network create mynet
$ docker network inspect mynet --format ""{{json .Options}}""
{""com.docker.network.bridge.host_binding_ipv4"":""127.0.0.1"",""com.docker.network.driver.mtu"":""1234""}
Note that changing this daemon configuration doesn't affect pre-existing networks.
Using the --default-network-opt
CLI flag is useful for testing and debugging
purposes, but you should prefer using the daemon.json
file for persistent
daemon configuration. The CLI flag expects a value with the following format:
driver=opt=value
, for example:
$ sudo dockerd \
--default-network-opt bridge=com.docker.network.bridge.host_binding_ipv4=127.0.0.1 \
--default-network-opt bridge=com.docker.network.driver.mtu=1234",,,
caf7aed06a391124dcdc65865b3ffaeb39991b3e5b63f895d5887531360db9fe,"Start containers automatically
Docker provides restart policies to control whether your containers start automatically when they exit, or when Docker restarts. Restart policies start linked containers in the correct order. Docker recommends that you use restart policies, and avoid using process managers to start containers.
Restart policies are different from the --live-restore
flag of the dockerd
command. Using --live-restore
lets you to keep your containers running during
a Docker upgrade, though networking and user input are interrupted.
Use a restart policy
To configure the restart policy for a container, use the --restart
flag
when using the docker run
command. The value of the --restart
flag can be
any of the following:
| Flag | Description |
|---|---|
no | Don't automatically restart the container. (Default) |
on-failure[:max-retries] | Restart the container if it exits due to an error, which manifests as a non-zero exit code. Optionally, limit the number of times the Docker daemon attempts to restart the container using the :max-retries option. The on-failure policy only prompts a restart if the container exits with a failure. It doesn't restart the container if the daemon restarts. |
always | Always restart the container if it stops. If it's manually stopped, it's restarted only when Docker daemon restarts or the container itself is manually restarted. (See the second bullet listed in restart policy details) |
unless-stopped | Similar to always , except that when the container is stopped (manually or otherwise), it isn't restarted even after Docker daemon restarts. |
The following command starts a Redis container and configures it to always restart, unless the container is explicitly stopped, or the daemon restarts.
$ docker run -d --restart unless-stopped redis
The following command changes the restart policy for an already running
container named redis
.
$ docker update --restart unless-stopped redis
The following command ensures all running containers restart.
$ docker update --restart unless-stopped $(docker ps -q)
Restart policy details
Keep the following in mind when using restart policies:
A restart policy only takes effect after a container starts successfully. In this case, starting successfully means that the container is up for at least 10 seconds and Docker has started monitoring it. This prevents a container which doesn't start at all from going into a restart loop.
If you manually stop a container, the restart policy is ignored until the Docker daemon restarts or the container is manually restarted. This prevents a restart loop.
Restart policies only apply to containers. To configure restart policies for Swarm services, see flags related to service restart.
Restarting foreground containers
When you run a container in the foreground, stopping a container causes the attached CLI to exit as well, regardless of the restart policy of the container. This behavior is illustrated in the following example.
Create a Dockerfile that prints the numbers 1 to 5 and then exits.
FROM busybox:latest COPY --chmod=755 <<""EOF"" /start.sh echo ""Starting..."" for i in $(seq 1 5); do echo ""$i"" sleep 1 done echo ""Exiting..."" exit 1 EOF ENTRYPOINT /start.sh
Build an image from the Dockerfile.
$ docker build -t startstop .
Run a container from the image, specifying
always
for its restart policy.The container prints the numbers 1..5 to stdout, and then exits. This causes the attached CLI to exit as well.
$ docker run --restart always startstop Starting... 1 2 3 4 5 Exiting... $
Running
docker ps
shows that is still running or restarting, thanks to the restart policy. The CLI session has already exited, however. It doesn't survive the initial container exit.$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 081991b35afe startstop ""/bin/sh -c /start.sh"" 9 seconds ago Up 4 seconds gallant_easley
You can re-attach your terminal to the container between restarts, using the
docker container attach
command. It's detached again the next time the container exits.$ docker container attach 081991b35afe 4 5 Exiting... $
Use a process manager
If restart policies don't suit your needs, such as when processes outside Docker depend on Docker containers, you can use a process manager such as systemd or supervisor instead.
Warning
Don't combine Docker restart policies with host-level process managers, as this creates conflicts.
To use a process manager, configure it to start your container or service using
the same docker start
or docker service
command you would normally use to
start the container manually. Consult the documentation for the specific
process manager for more details.
Using a process manager inside containers
Process managers can also run within the container to check whether a process is running and starts/restart it if not.
Warning
These aren't Docker-aware, and only monitor operating system processes within the container. Docker doesn't recommend this approach, because it's platform-dependent and may differ between versions of a given Linux distribution.",,,
4e5a00020a176ee4a47b24939ece735a50ab07f0e15367c84cce32ecea43facf,"Bind mounts
When you use a bind mount, a file or directory on the host machine is mounted from the host into a container. By contrast, when you use a volume, a new directory is created within Docker's storage directory on the host machine, and Docker manages that directory's contents.
When to use bind mounts
Bind mounts are appropriate for the following types of use case:
Sharing source code or build artifacts between a development environment on the Docker host and a container.
When you want to create or generate files in a container and persist the files onto the host's filesystem.
Sharing configuration files from the host machine to containers. This is how Docker provides DNS resolution to containers by default, by mounting
/etc/resolv.conf
from the host machine into each container.
Bind mounts are also available for builds: you can bind mount source code from the host into the build container to test, lint, or compile a project.
Bind-mounting over existing data
If you bind mount file or directory into a directory in the container in which
files or directories exist, the pre-existing files are obscured by the mount.
This is similar to if you were to save files into /mnt
on a Linux host, and
then mounted a USB drive into /mnt
. The contents of /mnt
would be obscured
by the contents of the USB drive until the USB drive was unmounted.
With containers, there's no straightforward way of removing a mount to reveal the obscured files again. Your best option is to recreate the container without the mount.
Considerations and constraints
Bind mounts have write access to files on the host by default.
One side effect of using bind mounts is that you can change the host filesystem via processes running in a container, including creating, modifying, or deleting important system files or directories. This capability can have security implications. For example, it may affect non-Docker processes on the host system.
You can use the
readonly
orro
option to prevent the container from writing to the mount.Bind mounts are created to the Docker daemon host, not the client.
If you're using a remote Docker daemon, you can't create a bind mount to access files on the client machine in a container.
For Docker Desktop, the daemon runs inside a Linux VM, not directly on the native host. Docker Desktop has built-in mechanisms that transparently handle bind mounts, allowing you to share native host filesystem paths with containers running in the virtual machine.
Containers with bind mounts are strongly tied to the host.
Bind mounts rely on the host machine's filesystem having a specific directory structure available. This reliance means that containers with bind mounts may fail if run on a different host without the same directory structure.
Syntax
To create a bind mount, you can use either the --mount
or --volume
flag.
$ docker run --mount type=bind,src=<host-path>,dst=<container-path>
$ docker run --volume <host-path>:<container-path>
In general, --mount
is preferred. The main difference is that the --mount
flag is more explicit and supports all the available options.
If you use --volume
to bind-mount a file or directory that does not yet
exist on the Docker host, Docker automatically creates the directory on the
host for you. It's always created as a directory.
--mount
does not automatically create a directory if the specified mount
path does not exist on the host. Instead, it produces an error:
$ docker run --mount type=bind,src=/dev/noexist,dst=/mnt/foo alpine
docker: Error response from daemon: invalid mount config for type ""bind"": bind source path does not exist: /dev/noexist.
Options for --mount
The --mount
flag consists of multiple key-value pairs, separated by commas
and each consisting of a <key>=<value>
tuple. The order of the keys isn't
significant.
$ docker run --mount type=bind,src=<host-path>,dst=<container-path>[,<key>=<value>...]
Valid options for --mount type=bind
include:
| Option | Description |
|---|---|
source , src | The location of the file or directory on the host. This can be an absolute or relative path. |
destination , dst , target | The path where the file or directory is mounted in the container. Must be an absolute path. |
readonly , ro | If present, causes the bind mount to be mounted into the container as read-only. |
bind-propagation | If present, changes the bind propagation. |
$ docker run --mount type=bind,src=.,dst=/project,ro,bind-propagation=rshared
Options for --volume
The --volume
or -v
flag consists of three fields, separated by colon
characters (:
). The fields must be in the correct order.
$ docker run -v <host-path>:<container-path>[:opts]
The first field is the path on the host to bind mount into the container. The second field is the path where the file or directory is mounted in the container.
The third field is optional, and is a comma-separated list of options. Valid
options for --volume
with a bind mount include:
| Option | Description |
|---|---|
readonly , ro | If present, causes the bind mount to be mounted into the container as read-only. |
z , Z | Configures SELinux labeling. See Configure the SELinux label |
rprivate (default) | Sets bind propagation to rprivate for this mount. See
Configure bind propagation. |
private | Sets bind propagation to private for this mount. See
Configure bind propagation. |
rshared | Sets bind propagation to rshared for this mount. See
Configure bind propagation. |
shared | Sets bind propagation to shared for this mount. See
Configure bind propagation. |
rslave | Sets bind propagation to rslave for this mount. See
Configure bind propagation. |
slave | Sets bind propagation to slave for this mount. See
Configure bind propagation. |
$ docker run -v .:/project:ro,rshared
Start a container with a bind mount
Consider a case where you have a directory source
and that when you build the
source code, the artifacts are saved into another directory, source/target/
.
You want the artifacts to be available to the container at /app/
, and you
want the container to get access to a new build each time you build the source
on your development host. Use the following command to bind-mount the target/
directory into your container at /app/
. Run the command from within the
source
directory. The $(pwd)
sub-command expands to the current working
directory on Linux or macOS hosts.
If you're on Windows, see also
Path conversions on Windows.
The following --mount
and -v
examples produce the same result. You can't
run them both unless you remove the devtest
container after running the first
one.
$ docker run -d \
-it \
--name devtest \
--mount type=bind,source=""$(pwd)""/target,target=/app \
nginx:latest
$ docker run -d \
-it \
--name devtest \
-v ""$(pwd)""/target:/app \
nginx:latest
Use docker inspect devtest
to verify that the bind mount was created
correctly. Look for the Mounts
section:
""Mounts"": [
{
""Type"": ""bind"",
""Source"": ""/tmp/source/target"",
""Destination"": ""/app"",
""Mode"": """",
""RW"": true,
""Propagation"": ""rprivate""
}
],
This shows that the mount is a bind
mount, it shows the correct source and
destination, it shows that the mount is read-write, and that the propagation is
set to rprivate
.
Stop and remove the container:
$ docker container rm -fv devtest
Mount into a non-empty directory on the container
If you bind-mount a directory into a non-empty directory on the container, the directory's existing contents are obscured by the bind mount. This can be beneficial, such as when you want to test a new version of your application without building a new image. However, it can also be surprising and this behavior differs from that of volumes.
This example is contrived to be extreme, but replaces the contents of the
container's /usr/
directory with the /tmp/
directory on the host machine. In
most cases, this would result in a non-functioning container.
The --mount
and -v
examples have the same end result.
$ docker run -d \
-it \
--name broken-container \
--mount type=bind,source=/tmp,target=/usr \
nginx:latest
docker: Error response from daemon: oci runtime error: container_linux.go:262:
starting container process caused ""exec: \""nginx\"": executable file not found in $PATH"".
$ docker run -d \
-it \
--name broken-container \
-v /tmp:/usr \
nginx:latest
docker: Error response from daemon: oci runtime error: container_linux.go:262:
starting container process caused ""exec: \""nginx\"": executable file not found in $PATH"".
The container is created but does not start. Remove it:
$ docker container rm broken-container
Use a read-only bind mount
For some development applications, the container needs to write into the bind mount, so changes are propagated back to the Docker host. At other times, the container only needs read access.
This example modifies the previous one, but mounts the directory as a read-only
bind mount, by adding ro
to the (empty by default) list of options, after the
mount point within the container. Where multiple options are present, separate
them by commas.
The --mount
and -v
examples have the same result.
$ docker run -d \
-it \
--name devtest \
--mount type=bind,source=""$(pwd)""/target,target=/app,readonly \
nginx:latest
$ docker run -d \
-it \
--name devtest \
-v ""$(pwd)""/target:/app:ro \
nginx:latest
Use docker inspect devtest
to verify that the bind mount was created
correctly. Look for the Mounts
section:
""Mounts"": [
{
""Type"": ""bind"",
""Source"": ""/tmp/source/target"",
""Destination"": ""/app"",
""Mode"": ""ro"",
""RW"": false,
""Propagation"": ""rprivate""
}
],
Stop and remove the container:
$ docker container rm -fv devtest
Recursive mounts
When you bind mount a path that itself contains mounts, those submounts are
also included in the bind mount by default. This behavior is configurable,
using the bind-recursive
option for --mount
. This option is only supported
with the --mount
flag, not with -v
or --volume
.
If the bind mount is read-only, the Docker Engine makes a best-effort attempt
at making the submounts read-only as well. This is referred to as recursive
read-only mounts. Recursive read-only mounts require Linux kernel version 5.12
or later. If you're running an older kernel version, submounts are
automatically mounted as read-write by default. Attempting to set submounts to
be read-only on a kernel version earlier than 5.12, using the
bind-recursive=readonly
option, results in an error.
Supported values for the bind-recursive
option are:
| Value | Description |
|---|---|
enabled (default) | Read-only mounts are made recursively read-only if kernel is v5.12 or later. Otherwise, submounts are read-write. |
disabled | Submounts are ignored (not included in the bind mount). |
writable | Submounts are read-write. |
readonly | Submounts are read-only. Requires kernel v5.12 or later. |
Configure bind propagation
Bind propagation defaults to rprivate
for both bind mounts and volumes. It is
only configurable for bind mounts, and only on Linux host machines. Bind
propagation is an advanced topic and many users never need to configure it.
Bind propagation refers to whether or not mounts created within a given
bind-mount can be propagated to replicas of that mount. Consider
a mount point /mnt
, which is also mounted on /tmp
. The propagation settings
control whether a mount on /tmp/a
would also be available on /mnt/a
. Each
propagation setting has a recursive counterpoint. In the case of recursion,
consider that /tmp/a
is also mounted as /foo
. The propagation settings
control whether /mnt/a
and/or /tmp/a
would exist.
Note
Mount propagation doesn't work with Docker Desktop.
| Propagation setting | Description |
|---|---|
shared | Sub-mounts of the original mount are exposed to replica mounts, and sub-mounts of replica mounts are also propagated to the original mount. |
slave | similar to a shared mount, but only in one direction. If the original mount exposes a sub-mount, the replica mount can see it. However, if the replica mount exposes a sub-mount, the original mount cannot see it. |
private | The mount is private. Sub-mounts within it are not exposed to replica mounts, and sub-mounts of replica mounts are not exposed to the original mount. |
rshared | The same as shared, but the propagation also extends to and from mount points nested within any of the original or replica mount points. |
rslave | The same as slave, but the propagation also extends to and from mount points nested within any of the original or replica mount points. |
rprivate | The default. The same as private, meaning that no mount points anywhere within the original or replica mount points propagate in either direction. |
Before you can set bind propagation on a mount point, the host filesystem needs to already support bind propagation.
For more information about bind propagation, see the Linux kernel documentation for shared subtree.
The following example mounts the target/
directory into the container twice,
and the second mount sets both the ro
option and the rslave
bind propagation
option.
The --mount
and -v
examples have the same result.
$ docker run -d \
-it \
--name devtest \
--mount type=bind,source=""$(pwd)""/target,target=/app \
--mount type=bind,source=""$(pwd)""/target,target=/app2,readonly,bind-propagation=rslave \
nginx:latest
$ docker run -d \
-it \
--name devtest \
-v ""$(pwd)""/target:/app \
-v ""$(pwd)""/target:/app2:ro,rslave \
nginx:latest
Now if you create /app/foo/
, /app2/foo/
also exists.
Configure the SELinux label
If you use SELinux, you can add the z
or Z
options to modify the SELinux
label of the host file or directory being mounted into the container. This
affects the file or directory on the host machine itself and can have
consequences outside of the scope of Docker.
- The
z
option indicates that the bind mount content is shared among multiple containers. - The
Z
option indicates that the bind mount content is private and unshared.
Use extreme caution with these options. Bind-mounting a system directory
such as /home
or /usr
with the Z
option renders your host machine
inoperable and you may need to relabel the host machine files by hand.
Important
When using bind mounts with services, SELinux labels (
:Z
and:z
), as well as:ro
are ignored. See moby/moby #32579 for details.
This example sets the z
option to specify that multiple containers can share
the bind mount's contents:
It is not possible to modify the SELinux label using the --mount
flag.
$ docker run -d \
-it \
--name devtest \
-v ""$(pwd)""/target:/app:z \
nginx:latest
Use a bind mount with Docker Compose
A single Docker Compose service with a bind mount looks like this:
services:
frontend:
image: node:lts
volumes:
- type: bind
source: ./static
target: /opt/app/static
volumes:
myapp:
For more information about using volumes of the bind
type with Compose, see
Compose reference on volumes.
and
Compose reference on volume configuration.
Next steps
- Learn about volumes.
- Learn about tmpfs mounts.
- Learn about storage drivers.",,,
a4a2f980413b00f9c9ce8ca3a6e37ac6d40e4609c97ed61e9d156e4c4ff9661f,"Continuous Integration (CI)
In order to help validate your extension and ensure it's functional, the Extension SDK provides tools to help you setup continuous integration for your extension.
Important
The Docker Desktop Action and the extension-test-helper library are both experimental.
Setup CI environment with GitHub Actions
You need Docker Desktop to be able to install and validate your extension. You can start Docker Desktop in GitHub Actions using the Docker Desktop Action, by adding the following to a workflow file:
steps:
- id: start_desktop
uses: docker/desktop-action/start@v0.1.0
Note
This action supports only Github Action macOS runners at the moment. You need to specify
runs-on: macOS-latest
for your end to end tests.
Once the step has executed, the next steps use Docker Desktop and the Docker CLI to install and test the extension.
Validating your extension with Puppeteer
Once Docker Desktop starts in CI, you can build, install, and validate your extension with Jest and Puppeteer.
First, build and install the extension from your test:
import { DesktopUI } from ""@docker/extension-test-helper"";
import { exec as originalExec } from ""child_process"";
import * as util from ""util"";
export const exec = util.promisify(originalExec);
// keep a handle on the app to stop it at the end of tests
let dashboard: DesktopUI;
beforeAll(async () => {
await exec(`docker build -t my/extension:latest .`, {
cwd: ""my-extension-src-root"",
});
await exec(`docker extension install -f my/extension:latest`);
});
Then open the Docker Desktop Dashboard and run some tests in your extension's UI:
describe(""Test my extension"", () => {
test(""should be functional"", async () => {
dashboard = await DesktopUI.start();
const eFrame = await dashboard.navigateToExtension(""my/extension"");
// use puppeteer APIs to manipulate the UI, click on buttons, expect visual display and validate your extension
await eFrame.waitForSelector(""#someElementId"");
});
});
Finally, close the Docker Desktop Dashboard and uninstall your extension:
afterAll(async () => {
dashboard?.stop();
await exec(`docker extension uninstall my/extension`);
});
What's next
- Build an advanced frontend extension.
- Learn more about extensions architecture.
- Learn how to publish your extension.",,,
b937a09bc62992984890d3350b29b5215a8f5ef60a47ab1f9e9a19518363ed24,"Docker Build GitHub Actions
GitHub Actions is a popular CI/CD platform for automating your build, test, and deployment pipeline. Docker provides a set of official GitHub Actions for you to use in your workflows. These official actions are reusable, easy-to-use components for building, annotating, and pushing images.
The following GitHub Actions are available:
- Build and push Docker images: build and push Docker images with BuildKit.
- Docker Login: sign in to a Docker registry.
- Docker Setup Buildx: initiates a BuildKit builder.
- Docker Metadata action: extracts metadata from Git reference and GitHub events.
- Docker Setup QEMU: installs QEMU static binaries for multi-arch builds.
- Docker Buildx Bake: enables using high-level builds with Bake.
- Docker Scout: analyze Docker images for security vulnerabilities.
Using Docker's actions provides an easy-to-use interface, while still allowing flexibility for customizing build parameters.
Examples
If you're looking for examples on how to use the Docker GitHub Actions, refer to the following sections:
Get started with GitHub Actions
The Introduction to GitHub Actions with Docker guide walks you through the process of setting up and using Docker GitHub Actions for building Docker images, and pushing images to Docker Hub.",,,
bd744e319921242a87991ef79bcd594ea3fd9cdcd4563c213f68ae1e786e9aff,"Swarm task states
Docker lets you create services, which can start tasks. A service is a description of a desired state, and a task does the work. Work is scheduled on swarm nodes in this sequence:
- Create a service by using
docker service create
. - The request goes to a Docker manager node.
- The Docker manager node schedules the service to run on particular nodes.
- Each service can start multiple tasks.
- Each task has a life cycle, with states like
NEW
,PENDING
, andCOMPLETE
.
Tasks are execution units that run once to completion. When a task stops, it isn't executed again, but a new task may take its place.
Tasks advance through a number of states until they complete or fail. Tasks are
initialized in the NEW
state. The task progresses forward through a number of
states, and its state doesn't go backward. For example, a task never goes from
COMPLETE
to RUNNING
.
Tasks go through the states in the following order:
| Task state | Description |
|---|---|
NEW | The task was initialized. |
PENDING | Resources for the task were allocated. |
ASSIGNED | Docker assigned the task to nodes. |
ACCEPTED | The task was accepted by a worker node. If a worker node rejects the task, the state changes to REJECTED . |
READY | The worker node is ready to start the task |
PREPARING | Docker is preparing the task. |
STARTING | Docker is starting the task. |
RUNNING | The task is executing. |
COMPLETE | The task exited without an error code. |
FAILED | The task exited with an error code. |
SHUTDOWN | Docker requested the task to shut down. |
REJECTED | The worker node rejected the task. |
ORPHANED | The node was down for too long. |
REMOVE | The task is not terminal but the associated service was removed or scaled down. |
View task state
Run docker service ps <service-name>
to get the state of a task. The
CURRENT STATE
field shows the task's state and how long it's been
there.
$ docker service ps webserver
ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS
owsz0yp6z375 webserver.1 nginx UbuntuVM Running Running 44 seconds ago
j91iahr8s74p \_ webserver.1 nginx UbuntuVM Shutdown Failed 50 seconds ago ""No such container: webserver.…""
7dyaszg13mw2 \_ webserver.1 nginx UbuntuVM Shutdown Failed 5 hours ago ""No such container: webserver.…""",,,
34f465ffa8d97977a8d3c87ea0c2bef17e0972af91969510337ef2a27ec4aa6c,"Copy image between registries with GitHub Actions
Multi-platform images built using Buildx can
be copied from one registry to another using the
buildx imagetools create
command:
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Login to GitHub Container Registry
uses: docker/login-action@v3
with:
registry: ghcr.io
username: ${{ github.repository_owner }}
password: ${{ secrets.GITHUB_TOKEN }}
- name: Set up QEMU
uses: docker/setup-qemu-action@v3
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build and push
uses: docker/build-push-action@v6
with:
platforms: linux/amd64,linux/arm64
push: true
tags: |
user/app:latest
user/app:1.0.0
- name: Push image to GHCR
run: |
docker buildx imagetools create \
--tag ghcr.io/user/app:latest \
--tag ghcr.io/user/app:1.0.0 \
user/app:latest",,,
a618b54ff27257597e018e5dc1834d435eae94e407bf7e5f9bb618ecd3c35158,"Change your Docker Desktop settings
To navigate to Settings either:
- Select the Docker menu and then Settings
- Select the Settings icon from the Docker Desktop Dashboard.
You can also locate the settings-store.json
file (or settings.json
for Docker Desktop versions 4.34 and earlier) at:
- Mac:
~/Library/Group\ Containers/group.com.docker/settings-store.json
- Windows:
C:\Users\[USERNAME]\AppData\Roaming\Docker\settings-store.json
- Linux:
~/.docker/desktop/settings-store.json
General
On the General tab, you can configure when to start Docker and specify other settings:
Start Docker Desktop when you sign in to your computer. Select to automatically start Docker Desktop when you sign in to your machine.
Open Docker Dashboard when Docker Desktop starts. Select to automatically open the dashboard when starting Docker Desktop.
Choose theme for Docker Desktop. Choose whether you want to apply a Light or Dark theme to Docker Desktop. Alternatively you can set Docker Desktop to Use system settings.
Choose container terminal. Determines which terminal is launched when opening the terminal from a container. If you choose the integrated terminal, you can run commands in a running container straight from the Docker Desktop Dashboard. For more information, see Explore containers.
Enable Docker terminal. Interact with your host machine and execute commands directly from Docker Desktop.
Enable Docker Debug by default. Check this option to use Docker Debug by default when accessing the integrated terminal. For more information, see Explore containers.
Mac only Include VM in Time Machine backups. Select to back up the Docker Desktop virtual machine. This option is turned off by default.
Windows only Expose daemon on tcp://localhost:2375 without TLS. Check this option to enable legacy clients to connect to the Docker daemon. You must use this option with caution as exposing the daemon without TLS can result in remote code execution attacks.
Windows only Use the WSL 2 based engine. WSL 2 provides better performance than the Hyper-V backend. For more information, see Docker Desktop WSL 2 backend.
Windows only Add the
*.docker.internal
names to the host's/etc/hosts
file (Password required). Lets you resolve*.docker.internal
DNS names from both the host and your containers.Use containerd for pulling and storing images. Turns on the containerd image store. This brings new features like faster container startup performance by lazy-pulling images, and the ability to run Wasm applications with Docker. For more information, see containerd image store.
Mac only Choose Virtual Machine Manager (VMM). Choose the Virtual Machine Manager for creating and managing the Docker Desktop Linux VM.
- Select Docker VMM for the latest and most performant Hypervisor/Virtual Machine Manager. This option is available only on Apple Silicon Macs running macOS 12.5 or later and is currently in Beta.
Tip
Turn this setting on to make Docker Desktop run faster.
- Alternatively, you can choose Apple Virtualization framework, QEMU (for Apple Silicon), or HyperKit (for Intel Macs). For macOS 12.5 and later, Apple Virtualization framework is the default setting.
For more information, see Virtual Machine Manager.
- Select Docker VMM for the latest and most performant Hypervisor/Virtual Machine Manager. This option is available only on Apple Silicon Macs running macOS 12.5 or later and is currently in Beta.
Mac only Choose file sharing implementation for your containers. Choose whether you want to share files using VirtioFS, gRPC FUSE, or osxfs (Legacy). VirtioFS is only available for macOS 12.5 and later, and is turned on by default.
Tip
Use VirtioFS for speedy file sharing. VirtioFS has reduced the time taken to complete filesystem operations by up to 98%. It is the only file sharing implementation supported by Docker VMM.
Mac only Use Rosetta for x86_64/amd64 emulation on Apple Silicon. Turns on Rosetta to accelerate x86/AMD64 binary emulation on Apple Silicon. This option is only available if you have selected Apple Virtualization framework as the Virtual Machine Manager. You must also be on macOS 13 or later.
Send usage statistics. Select so Docker Desktop sends diagnostics, crash reports, and usage data. This information helps Docker improve and troubleshoot the application. Clear the checkbox to opt out. Docker may periodically prompt you for more information.
Use Enhanced Container Isolation. Select to enhance security by preventing containers from breaching the Linux VM. For more information, see Enhanced Container Isolation.
Note
This setting is only available if you are signed in to Docker Desktop and have a Docker Business subscription.
Show CLI hints. Displays CLI hints and tips when running Docker commands in the CLI. This is turned on by default. To turn CLI hints on or off from the CLI, set
DOCKER_CLI_HINTS
totrue
orfalse
respectively.SBOM Indexing. When this option is enabled, inspecting an image in Docker Desktop shows a Start analysis button that, when selected, analyzes the image with Docker Scout.
Enable background SBOM indexing. When this option is enabled, Docker Scout automatically analyzes images that you build or pull.
Mac only Automatically check configuration. Regularly checks your configuration to ensure no unexpected changes have been made by another application.
Docker Desktop checks if your setup, configured during installation, has been altered by external apps like Orbstack. Docker Desktop checks:
- The symlinks of Docker binaries to
/usr/local/bin
. - The symlink of the default Docker socket.
Additionally, Docker Desktop ensures that the context is switched to
desktop-linux
on startup.
You are notified if changes are found and are able to restore the configuration directly from the notification. For more information, see the FAQs.
- The symlinks of Docker binaries to
Resources
The Resources tab allows you to configure CPU, memory, disk, proxies, network, and other resources.
Advanced
Note
On Windows, the Resource allocation options in the Advanced tab are only available in Hyper-V mode, because Windows manages the resources in WSL 2 mode and Windows container mode. In WSL 2 mode, you can configure limits on the memory, CPU, and swap size allocated to the WSL 2 utility VM.
On the Advanced tab, you can limit resources available to the Docker Linux VM.
Advanced settings are:
CPU limit. Specify the maximum number of CPUs to be used by Docker Desktop. By default, Docker Desktop is set to use all the processors available on the host machine.
Memory limit. By default, Docker Desktop is set to use up to 50% of your host's memory. To increase the RAM, set this to a higher number; to decrease it, lower the number.
Swap. Configure swap file size as needed. The default is 1 GB.
Virtual disk limit. Specify the maximum size of the disk image.
Disk image location. Specify the location of the Linux volume where containers and images are stored.
You can also move the disk image to a different location. If you attempt to move a disk image to a location that already has one, you are asked if you want to use the existing image or replace it.
Tip
If you feel Docker Desktop starting to get slow or you're running multi-container workloads, increase the memory and disk image space allocation
Resource Saver. Enable or disable Resource Saver mode, which significantly reduces CPU and memory utilization on the host by automatically turning off the Linux VM when Docker Desktop is idle (i.e., no containers are running).
You can also configure the Resource Saver timeout which indicates how long should Docker Desktop be idle before Resource Saver mode kicks in. Default is 5 minutes.
Note
Exit from Resource Saver mode occurs automatically when containers run. Exit may take a few seconds (~3 to 10 secs) as Docker Desktop restarts the Linux VM.
File sharing
Note
On Windows, the File sharing tab is only available in Hyper-V mode because the files are automatically shared in WSL 2 mode and Windows container mode.
Use File sharing to allow local directories on your machine to be shared with Linux containers. This is especially useful for editing source code in an IDE on the host while running and testing the code in a container.
Synchronized file shares
Synchronized file shares is an alternative file sharing mechanism that provides fast and flexible host-to-VM file sharing, enhancing bind mount performance through the use of synchronized filesystem caches. Available with Pro, Team, and Business subscriptions.
To learn more, see Synchronized file share.
Virtual file shares
By default the /Users
, /Volumes
, /private
, /tmp
and /var/folders
directory are shared.
If your project is outside this directory then it must be added to the list,
otherwise you may get Mounts denied
or cannot start service
errors at runtime.
File share settings are:
Add a Directory. Select
+
and navigate to the directory you want to add.Remove a Directory. Select
-
next to the directory you want to removeApply & Restart makes the directory available to containers using Docker's bind mount (
-v
) feature.
Tip
- Share only the directories that you need with the container. File sharing introduces overhead as any changes to the files on the host need to be notified to the Linux VM. Sharing too many files can lead to high CPU load and slow filesystem performance.
- Shared folders are designed to allow application code to be edited on the host while being executed in containers. For non-code items such as cache directories or databases, the performance will be much better if they are stored in the Linux VM, using a data volume (named volume) or data container.
- If you share the whole of your home directory into a container, MacOS may prompt you to give Docker access to personal areas of your home directory such as your Reminders or Downloads.
- By default, Mac file systems are case-insensitive while Linux is case-sensitive. On Linux, it is possible to create two separate files:
test
andTest
, while on Mac these filenames would actually refer to the same underlying file. This can lead to problems where an app works correctly on a developer's machine (where the file contents are shared) but fails when run in Linux in production (where the file contents are distinct). To avoid this, Docker Desktop insists that all shared files are accessed as their original case. Therefore, if a file is created calledtest
, it must be opened astest
. Attempts to openTest
will fail with the error ""No such file or directory"". Similarly, once a file calledtest
is created, attempts to create a second file calledTest
will fail.For more information, see Volume mounting requires file sharing for any project directories outside of
/Users
Shared folders on demand
On Windows, you can share a folder ""on demand"" the first time a particular folder is used by a container.
If you run a Docker command from a shell with a volume mount (as shown in the example below) or kick off a Compose file that includes volume mounts, you get a popup asking if you want to share the specified folder.
You can select to Share it, in which case it is added to your Docker Desktop Shared Folders list and available to containers. Alternatively, you can opt not to share it by selecting Cancel.
Proxies
Docker Desktop supports the use of HTTP/HTTPS and SOCKS5 proxies.
HTTP/HTTPS proxies can be used when:
- Signing in to Docker
- Pulling or pushing images
- Fetching artifacts during image builds
- Containers interact with the external network
- Scanning images
If the host uses a HTTP/HTTPS proxy configuration (static or via Proxy Auto-Configuration (PAC)), Docker Desktop reads
this configuration
and automatically uses these settings for signing in to Docker, for pulling and pushing images, and for
container Internet access. If the proxy requires authorization then Docker Desktop dynamically asks
the developer for a username and password. All passwords are stored securely in the OS credential store.
Note that only the Basic
proxy authentication method is supported so we recommend using an https://
URL for your HTTP/HTTPS proxies to protect passwords while in transit on the network. Docker Desktop
supports TLS 1.3 when communicating with proxies.
To set a different proxy for Docker Desktop, turn on Manual proxy configuration and enter a single
upstream proxy URL of the form http://proxy:port
or https://proxy:port
.
To prevent developers from accidentally changing the proxy settings, see Settings Management.
The HTTPS proxy settings used for scanning images are set using the HTTPS_PROXY
environment variable.
Note
If you are using a PAC file hosted on a web server, make sure to add the MIME type
application/x-ns-proxy-autoconfig
for the.pac
file extension on the server or website. Without this configuration, the PAC file may not be parsed correctly.
Important
You cannot configure the proxy settings using the Docker daemon configuration file (
daemon.json
), and we recommend you do not configure the proxy settings via the Docker CLI configuration file (config.json
).To manage proxy configurations for Docker Desktop, configure the settings in the Docker Desktop app or use Settings Management.
Proxy authentication
Basic authentication
If your proxy uses Basic authentication, Docker Desktop prompts developers for a username and password and caches the credentials. All passwords are stored securely in the OS credential store. It will request re-authentication if that cache is removed.
It's recommended that you use an https://
URL for HTTP/HTTPS proxies to protect passwords during network transit. Docker Desktop also supports TLS 1.3 for communication with proxies.
Kerberos and NTLM authentication
Note
Available for Docker Business subscribers with Docker Desktop for Windows version 4.30 and later.
Developers are no longer interrupted by prompts for proxy credentials as authentication is centralized. This also reduces the risk of account lockouts due to incorrect sign in attempts.
If your proxy offers multiple authentication schemes in 407 (Proxy Authentication Required) response, Docker Desktop by default selects the Basic authentication scheme.
For Docker Desktop version 4.30 to 4.31:
To enable Kerberos or NTLM proxy authentication, no additional configuration is needed beyond specifying the proxy IP address and port.
For Docker Desktop version 4.32 and later:
To enable Kerberos or NTLM proxy authentication you must pass the --proxy-enable-kerberosntlm
installer flag during installation via the command line, and ensure your proxy server is properly configured for Kerberos or NTLM authentication.
Network
Note
On Windows, the Network tab isn't available in the Windows container mode because Windows manages networking.
Docker Desktop uses a private IPv4 network for internal services such as a DNS server and an HTTP proxy. In case Docker Desktop's choice of subnet clashes with IPs in your environment, you can specify a custom subnet using the Network setting.
On Mac, you can also select the Use kernel networking for UDP setting. This lets you use a more efficient kernel networking path for UDP. This may not be compatible with your VPN software.
WSL Integration
On Windows in WSL 2 mode, you can configure which WSL 2 distributions will have the Docker WSL integration.
By default, the integration is enabled on your default WSL distribution.
To change your default WSL distribution, run wsl --set-default <distribution name>
. (For example,
to set Ubuntu as your default WSL distribution, run wsl --set-default ubuntu
).
You can also select any additional distributions you would like to enable the WSL 2 integration on.
For more details on configuring Docker Desktop to use WSL 2, see Docker Desktop WSL 2 backend.
Docker Engine
The Docker Engine tab allows you to configure the Docker daemon used to run containers with Docker Desktop.
You configure the daemon using a JSON configuration file. Here's what the file might look like:
{
""builder"": {
""gc"": {
""defaultKeepStorage"": ""20GB"",
""enabled"": true
}
},
""experimental"": false
}
You can find this file at $HOME/.docker/daemon.json
. To change the configuration, either
edit the JSON configuration directly from the dashboard in Docker Desktop, or open and
edit the file using your favorite text editor.
To see the full list of possible configuration options, see the dockerd command reference.
Select Apply & Restart to save your settings and restart Docker Desktop.
Builders
If you have turned on the Docker Desktop Builds view, you can use the Builders tab to inspect and manage builders in the Docker Desktop settings.
Inspect
To inspect builders, find the builder that you want to inspect and select the expand icon. You can only inspect active builders.
Inspecting an active builder shows:
- BuildKit version
- Status
- Driver type
- Supported capabilities and platforms
- Disk usage
- Endpoint address
Select a different builder
The Selected builder section displays the selected builder. To select a different builder:
- Find the builder that you want to use under Available builders
- Open the drop-down menu next to the builder's name.
- Select Use to switch to this builder.
Your build commands now use the selected builder by default.
Create a builder
To create a builder, use the Docker CLI. See Create a new builder
Remove a builder
You can remove a builder if:
The builder isn't your selected builder
The builder isn't associated with a Docker context.
To remove builders associated with a Docker context, remove the context using the
docker context rm
command.
To remove a builder:
- Find the builder that you want to remove under Available builders
- Open the drop-down menu.
- Select Remove to remove this builder.
If the builder uses the docker-container
or kubernetes
driver,
the build cache is also removed, along with the builder.
Stop and start a builder
Builders that use the
docker-container
driver
run the BuildKit daemon in a container.
You can start and stop the BuildKit container using the drop-down menu.
Running a build automatically starts the container if it's stopped.
You can only start and stop builders using the docker-container
driver.
Kubernetes
Note
On Windows the Kubernetes tab is not available in Windows container mode.
Docker Desktop includes a standalone Kubernetes server, so that you can test deploying your Docker workloads on Kubernetes. To turn on Kubernetes support and install a standalone instance of Kubernetes running as a Docker container, select Enable Kubernetes.
With Docker Desktop version 4.38 and later, you can choose your cluster provisioning method:
- Kubeadm creates a single-node cluster and the version is set by Docker Desktop.
- kind creates a multi-node cluster and you can set the version and number of nodes.
Docker Desktop version 4.38 and later also lets you install the Kubernetes Dashboard within an existing Kubernetes cluster with the Deploy the Kubernetes Dashboard into cluster setting. It provides real-time visibility into workloads and nodes and helps you manage and monitor your Kubernetes clusters and applications easily.
Select Show system containers (advanced) to view internal containers when using Docker commands.
Select Reset Kubernetes cluster to delete all stacks and Kubernetes resources.
For more information about using the Kubernetes integration with Docker Desktop, see Deploy on Kubernetes.
Software Updates
The Software Updates tab notifies you of any updates available to Docker Desktop. When there's a new update, you can choose to download the update right away, or select the Release Notes option to learn what's included in the updated version.
Turn off the check for updates by clearing the Automatically check for updates check box. This disables notifications in the Docker menu and the notification badge that appears on the Docker Desktop Dashboard. To check for updates manually, select the Check for updates option in the Docker menu.
To allow Docker Desktop to automatically download new updates in the background, select Always download updates. This downloads newer versions of Docker Desktop when an update becomes available. After downloading the update, select Apply and Restart to install the update. You can do this either through the Docker menu or in the Updates section in the Docker Desktop Dashboard.
Tip
With Docker Desktop version 4.38 and later, components of Docker Desktop, such as Docker Compose, Docker Scout, and the Docker CLI, can be updated independently without the need for a full restart. This feature is still in Beta.
Extensions
Use the Extensions tab to:
- Enable Docker Extensions
- Allow only extensions distributed through the Docker Marketplace
- Show Docker Extensions system containers
For more information about Docker extensions, see Extensions.
Features in development
On the Feature control tab you can control your settings for Beta features and Experimental features.
You can also sign up to the Developer Preview program from the Features in development tab.
Beta features
Beta features provide access to future product functionality. These features are intended for testing and feedback only as they may change between releases without warning or remove them entirely from a future release. Beta features must not be used in production environments. Docker doesn't offer support for beta features.
Experimental features
Experimental features provide early access to future product functionality. These features are intended for testing and feedback only as they may change between releases without warning or can be removed entirely from a future release. Experimental features must not be used in production environments. Docker does not offer support for experimental features.
For a list of current experimental features in the Docker CLI, see Docker CLI Experimental features.
Notifications
Use the Notifications tab to turn on or turn off notifications for the following events:
- Status updates on tasks and processes
- Docker announcements
- Docker surveys
By default, all notifications are turned on. You'll always receive error notifications and notifications about new Docker Desktop releases and updates.
Notifications momentarily appear in the lower-right of the Docker Desktop Dashboard and then move to the Notifications drawer. To open the Notifications drawer, select .
Advanced
On Mac, you can reconfigure your initial installation settings on the Advanced tab:
Choose how to configure the installation of Docker's CLI tools.
- System: Docker CLI tools are installed in the system directory under
/usr/local/bin
- User: Docker CLI tools are installed in the user directory under
$HOME/.docker/bin
. You must then add$HOME/.docker/bin
to your PATH. To add$HOME/.docker/bin
to your path:- Open your shell configuration file. This is
~/.bashrc
if you're using a bash shell, or~/.zshrc
if you're using a zsh shell. - Copy and paste the following:
$ export PATH=$PATH:~/.docker/bin
- Save and the close the file. Restart your shell to apply the changes to the PATH variable.
- Open your shell configuration file. This is
- System: Docker CLI tools are installed in the system directory under
Enable default Docker socket (Requires password). Creates
/var/run/docker.sock
which some third party clients may use to communicate with Docker Desktop. For more information, see permission requirements for macOS.Enable privileged port mapping (Requires password). Starts the privileged helper process which binds the ports that are between 1 and 1024. For more information, see permission requirements for macOS.
For more information on each configuration and use case, see Permission requirements.",,,
3a8020ea0b8927f0f4dc4755087bad4e8d34c6ae153cfa88bddbb2c6fddfc48c,"Push images to a repository
To add content to a repository on Docker Hub, you need to tag your Docker image and then push it to your repository. This process lets you share your images with others or use them in different environments.
Tag your Docker image.
The
docker tag
command assigns a tag to your Docker image, which includes your Docker Hub namespace and the repository name. The general syntax is:$ docker tag [SOURCE_IMAGE[:TAG]] [NAMESPACE/REPOSITORY[:TAG]]
Example:
If your local image is called
my-app
and you want to tag it for the repositorymy-namespace/my-repo
with the tagv1.0
, run:$ docker tag my-app my-namespace/my-repo:v1.0
Push the image to Docker Hub.
Use the
docker push
command to upload your tagged image to the specified repository on Docker Hub.Example:
$ docker push my-namespace/my-repo:v1.0
This command pushes the image tagged
v1.0
to themy-namespace/my-repo
repository.Verify the image on Docker Hub.",,,
bedb4421e82ddcc904464c33769d8ca2b61f28152f2cc0a6b95b07f0b257beeb,"Share your extension
Once your extension image is accessible on Docker Hub, anyone with access to the image can install the extension.
People can install your extension by typing docker extension install my/awesome-extension:latest
in to the terminal.
However, this option doesn't provide a preview of the extension before it's installed.
Create a share URL
Docker lets you share your extensions using a URL.
When people navigate to this URL, it opens Docker Desktop and displays a preview of your extension in the same way as an extension in the Marketplace. From the preview, users can then select Install.
To generate this link you can either:
Run the following command:
$ docker extension share my/awesome-extension:0.0.1
Once you have installed your extension locally, navigate to the Manage tab and select Share.
Note
Previews of the extension description or screenshots, for example, are created using extension labels.",,,
1ea380fb7933dfd16d7beb46a924eb08c0776c4a7a86234452cbb1f6913c741a,"Wasm workloads (Beta)
Wasm (short for WebAssembly) is a fast, light alternative to the Linux and Windows containers you’re using in Docker today (with some tradeoffs).
This page provides information about the new ability to run Wasm applications alongside your Linux containers in Docker.
Turn on Wasm workloads
Wasm workloads require the containerd image store feature to be turned on. If you’re not already using the containerd image store, then pre-existing images and containers will be inaccessible.
- Navigate to Settings in Docker Desktop.
- In the General tab, check Use containerd for pulling and storing images.
- Go to Features in development and check the Enable Wasm option.
- Select Apply & restart to save the settings.
- In the confirmation dialog, select Install to install the Wasm runtimes.
Docker Desktop downloads and installs the following runtimes that you can use to run Wasm workloads:
io.containerd.slight.v1
io.containerd.spin.v2
io.containerd.wasmedge.v1
io.containerd.wasmtime.v1
io.containerd.lunatic.v1
io.containerd.wws.v1
io.containerd.wasmer.v1
Usage examples
Running a Wasm application with docker run
The following docker run
command starts a Wasm container on your system:
$ docker run \
--runtime=io.containerd.wasmedge.v1 \
--platform=wasi/wasm \
secondstate/rust-example-hello
After running this command, you can visit http://localhost:8080/ to see the ""Hello world"" output from this example module.
If you are receiving an error message, see the troubleshooting section for help.
Note the --runtime
and --platform
flags used in this command:
--runtime=io.containerd.wasmedge.v1
: Informs the Docker engine that you want to use the Wasm containerd shim instead of the standard Linux container runtime--platform=wasi/wasm
: Specifies the architecture of the image you want to use. By leveraging a Wasm architecture, you don’t need to build separate images for the different machine architectures. The Wasm runtime takes care of the final step of converting the Wasm binary to machine instructions.
Running a Wasm application with Docker Compose
The same application can be run using the following Docker Compose file:
services:
app:
image: secondstate/rust-example-hello
platform: wasi/wasm
runtime: io.containerd.wasmedge.v1
Start the application using the normal Docker Compose commands:
$ docker compose up
Running a multi-service application with Wasm
Networking works the same as you expect with Linux containers, giving you the flexibility to combine Wasm applications with other containerized workloads, such as a database, in a single application stack.
In the following example, the Wasm application leverages a MariaDB database running in a container.
Clone the repository.
$ git clone https://github.com/second-state/microservice-rust-mysql.git Cloning into 'microservice-rust-mysql'... remote: Enumerating objects: 75, done. remote: Counting objects: 100% (75/75), done. remote: Compressing objects: 100% (42/42), done. remote: Total 75 (delta 29), reused 48 (delta 14), pack-reused 0 Receiving objects: 100% (75/75), 19.09 KiB | 1.74 MiB/s, done. Resolving deltas: 100% (29/29), done.
Navigate into the cloned project and start the project using Docker Compose.
$ cd microservice-rust-mysql $ docker compose up [+] Running 0/1 ⠿ server Warning 0.4s [+] Building 4.8s (13/15) ... microservice-rust-mysql-db-1 | 2022-10-19 19:54:45 0 [Note] mariadbd: ready for connections. microservice-rust-mysql-db-1 | Version: '10.9.3-MariaDB-1:10.9.3+maria~ubu2204' socket: '/run/mysqld/mysqld.sock' port: 3306 mariadb.org binary distribution
If you run
docker image ls
from another terminal window, you can see the Wasm image in your image store.$ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE server latest 2c798ddecfa1 2 minutes ago 3MB
Inspecting the image shows the image has a
wasi/wasm
platform, a combination of OS and architecture:$ docker image inspect server | grep -A 3 ""Architecture"" ""Architecture"": ""wasm"", ""Os"": ""wasi"", ""Size"": 3001146, ""VirtualSize"": 3001146,
Open the URL
http://localhost:8090
in a browser and create a few sample orders. All of these are interacting with the Wasm server.When you're all done, tear everything down by hitting
Ctrl+C
in the terminal you launched the application.
Building and pushing a Wasm module
Create a Dockerfile that builds your Wasm application.
Exactly how to do this varies depending on the programming language you use.
In a separate stage in your
Dockerfile
, extract the module and set it as theENTRYPOINT
.# syntax=docker/dockerfile:1 FROM scratch COPY --from=build /build/hello_world.wasm /hello_world.wasm ENTRYPOINT [ ""/hello_world.wasm"" ]
Build and push the image specifying the
wasi/wasm
architecture. Buildx makes this easy to do in a single command.$ docker buildx build --platform wasi/wasm -t username/hello-world . ... => exporting to image 0.0s => => exporting layers 0.0s => => exporting manifest sha256:2ca02b5be86607511da8dc688234a5a00ab4d58294ab9f6beaba48ab3ba8de56 0.0s => => exporting config sha256:a45b465c3b6760a1a9fd2eda9112bc7e3169c9722bf9e77cf8c20b37295f954b 0.0s => => naming to docker.io/username/hello-world:latest 0.0s => => unpacking to docker.io/username/hello-world:latest 0.0s $ docker push username/hello-world
Troubleshooting
This section contains instructions on how to resolve common issues.
Unknown runtime specified
If you try to run a Wasm container without the containerd image store, an error similar to the following displays:
docker: Error response from daemon: Unknown runtime specified io.containerd.wasmedge.v1.
Turn on the containerd feature in Docker Desktop settings and try again.
Failed to start shim: failed to resolve runtime path
If you use an older version of Docker Desktop that doesn't support running Wasm workloads, you will see an error message similar to the following:
docker: Error response from daemon: failed to start shim: failed to resolve runtime path: runtime ""io.containerd.wasmedge.v1"" binary not installed ""containerd-shim-wasmedge-v1"": file does not exist: unknown.
Update your Docker Desktop to the latest version and try again.
Known issues
- Docker Compose may not exit cleanly when interrupted
- Workaround: Clean up
docker-compose
processes by sending them a SIGKILL (killall -9 docker-compose
).
- Workaround: Clean up
- Pushes to Hub might give an error stating
server message: insufficient_scope: authorization failed
, even after logging in using Docker Desktop- Workaround: Run
docker login
in the CLI
- Workaround: Run
Feedback
Thanks for trying out Wasm workloads with Docker. Give feedback or report any bugs you may find through the issues tracker on the public roadmap item.",,,
058eb6e4697a5ccf849eba691040671e9083f0bd779c42da17483dd43188cbfa,"Docker volume plugins
Docker Engine volume plugins enable Engine deployments to be integrated with external storage systems such as Amazon EBS, and enable data volumes to persist beyond the lifetime of a single Docker host. See the plugin documentation for more information.
Changelog
1.13.0
- If used as part of the v2 plugin architecture, mountpoints that are part of
paths returned by the plugin must be mounted under the directory specified by
PropagatedMount
in the plugin configuration ( #26398)
1.12.0
- Add
Status
field toVolumeDriver.Get
response ( #21006) - Add
VolumeDriver.Capabilities
to get capabilities of the volume driver ( #22077)
1.10.0
- Add
VolumeDriver.Get
which gets the details about the volume ( #16534) - Add
VolumeDriver.List
which lists all volumes owned by the driver ( #16534)
1.8.0
- Initial support for volume driver plugins ( #14659)
Command-line changes
To give a container access to a volume, use the --volume
and --volume-driver
flags on the docker container run
command. The --volume
(or -v
) flag
accepts a volume name and path on the host, and the --volume-driver
flag
accepts a driver type.
$ docker volume create --driver=flocker volumename
$ docker container run -it --volume volumename:/data busybox sh
--volume
The --volume
(or -v
) flag takes a value that is in the format
<volume_name>:<mountpoint>
. The two parts of the value are
separated by a colon (:
) character.
- The volume name is a human-readable name for the volume, and cannot begin with
a
/
character. It is referred to asvolume_name
in the rest of this topic. - The
Mountpoint
is the path on the host (v1) or in the plugin (v2) where the volume has been made available.
volumedriver
Specifying a volumedriver
in conjunction with a volumename
allows you to
use plugins such as
Flocker to manage
volumes external to a single host, such as those on EBS.
Create a VolumeDriver
The container creation endpoint (/containers/create
) accepts a VolumeDriver
field of type string
allowing to specify the name of the driver. If not
specified, it defaults to ""local""
(the default driver for local volumes).
Volume plugin protocol
If a plugin registers itself as a VolumeDriver
when activated, it must
provide the Docker Daemon with writeable paths on the host filesystem. The Docker
daemon provides these paths to containers to consume. The Docker daemon makes
the volumes available by bind-mounting the provided paths into the containers.
Note
Volume plugins should not write data to the
/var/lib/docker/
directory, including/var/lib/docker/volumes
. The/var/lib/docker/
directory is reserved for Docker.
/VolumeDriver.Create
Request:
{
""Name"": ""volume_name"",
""Opts"": {}
}
Instruct the plugin that the user wants to create a volume, given a user
specified volume name. The plugin does not need to actually manifest the
volume on the filesystem yet (until Mount
is called).
Opts
is a map of driver specific options passed through from the user request.
Response:
{
""Err"": """"
}
Respond with a string error if an error occurred.
/VolumeDriver.Remove
Request:
{
""Name"": ""volume_name""
}
Delete the specified volume from disk. This request is issued when a user
invokes docker rm -v
to remove volumes associated with a container.
Response:
{
""Err"": """"
}
Respond with a string error if an error occurred.
/VolumeDriver.Mount
Request:
{
""Name"": ""volume_name"",
""ID"": ""b87d7442095999a92b65b3d9691e697b61713829cc0ffd1bb72e4ccd51aa4d6c""
}
Docker requires the plugin to provide a volume, given a user specified volume
name. Mount
is called once per container start. If the same volume_name
is requested
more than once, the plugin may need to keep track of each new mount request and provision
at the first mount request and deprovision at the last corresponding unmount request.
ID
is a unique ID for the caller that is requesting the mount.
Response:
v1
{ ""Mountpoint"": ""/path/to/directory/on/host"", ""Err"": """" }
v2
{ ""Mountpoint"": ""/path/under/PropagatedMount"", ""Err"": """" }
Mountpoint
is the path on the host (v1) or in the plugin (v2) where the volume
has been made available.
Err
is either empty or contains an error string.
/VolumeDriver.Path
Request:
{
""Name"": ""volume_name""
}
Request the path to the volume with the given volume_name
.
Response:
v1
{ ""Mountpoint"": ""/path/to/directory/on/host"", ""Err"": """" }
v2
{ ""Mountpoint"": ""/path/under/PropagatedMount"", ""Err"": """" }
Respond with the path on the host (v1) or inside the plugin (v2) where the volume has been made available, and/or a string error if an error occurred.
Mountpoint
is optional. However, the plugin may be queried again later if one
is not provided.
/VolumeDriver.Unmount
Request:
{
""Name"": ""volume_name"",
""ID"": ""b87d7442095999a92b65b3d9691e697b61713829cc0ffd1bb72e4ccd51aa4d6c""
}
Docker is no longer using the named volume. Unmount
is called once per
container stop. Plugin may deduce that it is safe to deprovision the volume at
this point.
ID
is a unique ID for the caller that is requesting the mount.
Response:
{
""Err"": """"
}
Respond with a string error if an error occurred.
/VolumeDriver.Get
Request:
{
""Name"": ""volume_name""
}
Get info about volume_name
.
Response:
v1
{ ""Volume"": { ""Name"": ""volume_name"", ""Mountpoint"": ""/path/to/directory/on/host"", ""Status"": {} }, ""Err"": """" }
v2
{ ""Volume"": { ""Name"": ""volume_name"", ""Mountpoint"": ""/path/under/PropagatedMount"", ""Status"": {} }, ""Err"": """" }
Respond with a string error if an error occurred. Mountpoint
and Status
are
optional.
/VolumeDriver.List
Request:
{}
Get the list of volumes registered with the plugin.
Response:
v1
{ ""Volumes"": [ { ""Name"": ""volume_name"", ""Mountpoint"": ""/path/to/directory/on/host"" } ], ""Err"": """" }
v2
{ ""Volumes"": [ { ""Name"": ""volume_name"", ""Mountpoint"": ""/path/under/PropagatedMount"" } ], ""Err"": """" }
Respond with a string error if an error occurred. Mountpoint
is optional.
/VolumeDriver.Capabilities
Request:
{}
Get the list of capabilities the driver supports.
The driver is not required to implement Capabilities
. If it is not
implemented, the default values are used.
Response:
{
""Capabilities"": {
""Scope"": ""global""
}
}
Supported scopes are global
and local
. Any other value in Scope
will be
ignored, and local
is used. Scope
allows cluster managers to handle the
volume in different ways. For instance, a scope of global
, signals to the
cluster manager that it only needs to create the volume once instead of on each
Docker host. More capabilities may be added in the future.",,,
0186e4f326852d70928c61f225a8a0ccbc637a5f11e50805122f92d2217e6244,"Overriding configurations
Bake supports loading build definitions from files, but sometimes you need even more flexibility to configure these definitions. For example, you might want to override an attribute when building in a particular environment or for a specific target.
The following list of attributes can be overridden:
args
attest
cache-from
cache-to
context
contexts
dockerfile
entitlements
labels
network
no-cache
output
platform
pull
secrets
ssh
tags
target
To override these attributes, you can use the following methods:
File overrides
You can load multiple Bake files that define build configurations for your targets. This is useful when you want to separate configurations into different files for better organization, or to conditionally override configurations based on which files are loaded.
Default file lookup
You can use the --file
or -f
flag to specify which files to load.
If you don't specify any files, Bake will use the following lookup order:
compose.yaml
compose.yml
docker-compose.yml
docker-compose.yaml
docker-bake.json
docker-bake.hcl
docker-bake.override.json
docker-bake.override.hcl
If more than one Bake file is found, all files are loaded and merged into a single definition. Files are merged according to the lookup order.
$ docker buildx bake --print
[+] Building 0.0s (1/1) FINISHED
=> [internal] load local bake definitions 0.0s
=> => reading compose.yaml 45B / 45B 0.0s
=> => reading docker-bake.hcl 113B / 113B 0.0s
=> => reading docker-bake.override.hcl 65B / 65B
If merged files contain duplicate attribute definitions, those definitions are either merged or overridden by the last occurrence, depending on the attribute.
Bake will attempt to load all of the files in the order they are found. If multiple files define the same target, attributes are either merged or overridden. In the case of overrides, the last one loaded takes precedence.
For example, given the following files:
variable ""TAG"" {
default = ""foo""
}
target ""default"" {
tags = [""username/my-app:${TAG}""]
}
variable ""TAG"" {
default = ""bar""
}
Since docker-bake.override.hcl
is loaded last in the default lookup order,
the TAG
variable is overridden with the value bar
.
$ docker buildx bake --print
{
""target"": {
""default"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""tags"": [""username/my-app:bar""]
}
}
}
Manual file overrides
You can use the --file
flag to explicitly specify which files to load,
and use this as a way to conditionally apply override files.
For example, you can create a file that defines a set of configurations for a
specific environment, and load it only when building for that environment. The
following example shows how to load an override.hcl
file that sets the TAG
variable to bar
. The TAG
variable is then used in the default
target.
variable ""TAG"" {
default = ""foo""
}
target ""default"" {
tags = [""username/my-app:${TAG}""]
}
variable ""TAG"" {
default = ""bar""
}
Printing the build configuration without the --file
flag shows the TAG
variable is set to the default value foo
.
$ docker buildx bake --print
{
""target"": {
""default"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""tags"": [
""username/my-app:foo""
]
}
}
}
Using the --file
flag to load the overrides.hcl
file overrides the TAG
variable with the value bar
.
$ docker buildx bake -f docker-bake.hcl -f overrides.hcl --print
{
""target"": {
""default"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""tags"": [
""username/my-app:bar""
]
}
}
}
Command line
You can also override target configurations from the command line with the
--set
flag:
# docker-bake.hcl
target ""app"" {
args = {
mybuildarg = ""foo""
}
}
$ docker buildx bake --set app.args.mybuildarg=bar --set app.platform=linux/arm64 app --print
{
""group"": {
""default"": {
""targets"": [""app""]
}
},
""target"": {
""app"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""args"": {
""mybuildarg"": ""bar""
},
""platforms"": [""linux/arm64""]
}
}
}
Pattern matching syntax defined in https://golang.org/pkg/path/#Match is also supported:
$ docker buildx bake --set foo*.args.mybuildarg=value # overrides build arg for all targets starting with ""foo""
$ docker buildx bake --set *.platform=linux/arm64 # overrides platform for all targets
$ docker buildx bake --set foo*.no-cache # bypass caching only for targets starting with ""foo""
Complete list of attributes that can be overridden with --set
are:
args
attest
cache-from
cache-to
context
contexts
dockerfile
entitlements
labels
network
no-cache
output
platform
pull
secrets
ssh
tags
target
Environment variables
You can also use environment variables to override configurations.
Bake lets you use environment variables to override the value of a variable
block. Only variable
blocks can be overridden with environment variables.
This means you need to define the variables in the bake file and then set the
environment variable with the same name to override it.
The following example shows how you can define a TAG
variable with a default
value in the Bake file, and override it with an environment variable.
variable ""TAG"" {
default = ""latest""
}
target ""default"" {
context = "".""
dockerfile = ""Dockerfile""
tags = [""docker.io/username/webapp:${TAG}""]
}
$ export TAG=$(git rev-parse --short HEAD)
$ docker buildx bake --print webapp
The TAG
variable is overridden with the value of the environment variable,
which is the short commit hash generated by git rev-parse --short HEAD
.
{
""group"": {
""default"": {
""targets"": [""webapp""]
}
},
""target"": {
""webapp"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""tags"": [""docker.io/username/webapp:985e9e9""]
}
}
}
Type coercion
Overriding non-string variables with environment variables is supported. Values passed as environment variables are coerced into suitable types first.
The following example defines a PORT
variable. The backend
target uses the
PORT
variable as-is, and the frontend
target uses the value of PORT
incremented by one.
variable ""PORT"" {
default = 3000
}
group ""default"" {
targets = [""backend"", ""frontend""]
}
target ""backend"" {
args = {
PORT = PORT
}
}
target ""frontend"" {
args = {
PORT = add(PORT, 1)
}
}
Overriding PORT
using an environment variable will first coerce the value
into the expected type, an integer, before the expression in the frontend
target runs.
$ PORT=7070 docker buildx bake --print
{
""group"": {
""default"": {
""targets"": [
""backend"",
""frontend""
]
}
},
""target"": {
""backend"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""args"": {
""PORT"": ""7070""
}
},
""frontend"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""args"": {
""PORT"": ""7071""
}
}
}
}",,,
75e433023cc8a56820691e2cffcbe1a04d5ada3b2b63304ab3f112aa8ed27f31,"Play in a content trust sandbox
This page explains how to set up and use a sandbox for experimenting with trust. The sandbox allows you to configure and try trust operations locally without impacting your production images.
Before working through this sandbox, you should have read through the trust overview.
Prerequisites
These instructions assume you are running in Linux or macOS. You can run this sandbox on a local machine or on a virtual machine. You need to have privileges to run docker commands on your local machine or in the VM.
This sandbox requires you to install two Docker tools: Docker Engine >= 1.10.0 and Docker Compose >= 1.6.0. To install the Docker Engine, choose from the list of supported platforms. To install Docker Compose, see the detailed instructions here.
What is in the sandbox?
If you are just using trust out-of-the-box you only need your Docker Engine client and access to the Docker Hub. The sandbox mimics a production trust environment, and sets up these additional components.
| Container | Description |
|---|---|
| trustsandbox | A container with the latest version of Docker Engine and with some preconfigured certificates. This is your sandbox where you can use the docker client to test trust operations. |
| Registry server | A local registry service. |
| Notary server | The service that does all the heavy-lifting of managing trust |
This means you run your own content trust (Notary) server and registry. If you work exclusively with the Docker Hub, you would not need these components. They are built into the Docker Hub for you. For the sandbox, however, you build your own entire, mock production environment.
Within the trustsandbox
container, you interact with your local registry rather
than the Docker Hub. This means your everyday image repositories are not used.
They are protected while you play.
When you play in the sandbox, you also create root and repository keys. The
sandbox is configured to store all the keys and files inside the trustsandbox
container. Since the keys you create in the sandbox are for play only,
destroying the container destroys them as well.
By using a docker-in-docker image for the trustsandbox
container, you also
don't pollute your real Docker daemon cache with any images you push and pull.
The images are stored in an anonymous volume attached to this container,
and can be destroyed after you destroy the container.
Build the sandbox
In this section, you use Docker Compose to specify how to set up and link together
the trustsandbox
container, the Notary server, and the Registry server.
Create a new
trustsandbox
directory and change into it.$ mkdir trustsandbox $ cd trustsandbox
Create a file called
compose.yaml
with your favorite editor. For example, using vim:$ touch compose.yaml $ vim compose.yaml
Add the following to the new file.
version: ""2"" services: notaryserver: image: dockersecurity/notary_autobuilds:server-v0.5.1 volumes: - notarycerts:/var/lib/notary/fixtures networks: - sandbox environment: - NOTARY_SERVER_STORAGE_TYPE=memory - NOTARY_SERVER_TRUST_SERVICE_TYPE=local sandboxregistry: image: registry:2.4.1 networks: - sandbox container_name: sandboxregistry trustsandbox: image: docker:dind networks: - sandbox volumes: - notarycerts:/notarycerts privileged: true container_name: trustsandbox entrypoint: """" command: |- sh -c ' cp /notarycerts/root-ca.crt /usr/local/share/ca-certificates/root-ca.crt && update-ca-certificates && dockerd-entrypoint.sh --insecure-registry sandboxregistry:5000' volumes: notarycerts: external: false networks: sandbox: external: false
Save and close the file.
Run the containers on your local system.
$ docker compose up -d
The first time you run this, the docker-in-docker, Notary server, and registry images are downloaded from Docker Hub.
Play in the sandbox
Now that everything is setup, you can go into your trustsandbox
container and
start testing Docker content trust. From your host machine, obtain a shell
in the trustsandbox
container.
$ docker container exec -it trustsandbox sh
/ #
Test some trust operations
Now, pull some images from within the trustsandbox
container.
Download a
docker
image to test with./ # docker pull docker/trusttest docker pull docker/trusttest Using default tag: latest latest: Pulling from docker/trusttest b3dbab3810fc: Pull complete a9539b34a6ab: Pull complete Digest: sha256:d149ab53f8718e987c3a3024bb8aa0e2caadf6c0328f1d9d850b2a2a67f2819a Status: Downloaded newer image for docker/trusttest:latest
Tag it to be pushed to our sandbox registry:
/ # docker tag docker/trusttest sandboxregistry:5000/test/trusttest:latest
Enable content trust.
/ # export DOCKER_CONTENT_TRUST=1
Identify the trust server.
/ # export DOCKER_CONTENT_TRUST_SERVER=https://notaryserver:4443
This step is only necessary because the sandbox is using its own server. Normally, if you are using the Docker Public Hub this step isn't necessary.
Pull the test image.
/ # docker pull sandboxregistry:5000/test/trusttest Using default tag: latest Error: remote trust data does not exist for sandboxregistry:5000/test/trusttest: notaryserver:4443 does not have trust data for sandboxregistry:5000/test/trusttest
You see an error, because this content doesn't exist on the
notaryserver
yet.Push and sign the trusted image.
/ # docker push sandboxregistry:5000/test/trusttest:latest The push refers to a repository [sandboxregistry:5000/test/trusttest] 5f70bf18a086: Pushed c22f7bc058a9: Pushed latest: digest: sha256:ebf59c538accdf160ef435f1a19938ab8c0d6bd96aef8d4ddd1b379edf15a926 size: 734 Signing and pushing trust metadata You are about to create a new root signing key passphrase. This passphrase will be used to protect the most sensitive key in your signing system. Please choose a long, complex passphrase and be careful to keep the password and the key file itself secure and backed up. It is highly recommended that you use a password manager to generate the passphrase and keep it safe. There will be no way to recover this key. You can find the key in your config directory. Enter passphrase for new root key with ID 27ec255: Repeat passphrase for new root key with ID 27ec255: Enter passphrase for new repository key with ID 58233f9 (sandboxregistry:5000/test/trusttest): Repeat passphrase for new repository key with ID 58233f9 (sandboxregistry:5000/test/trusttest): Finished initializing ""sandboxregistry:5000/test/trusttest"" Successfully signed ""sandboxregistry:5000/test/trusttest"":latest
Because you are pushing this repository for the first time, Docker creates new root and repository keys and asks you for passphrases with which to encrypt them. If you push again after this, it only asks you for repository passphrase so it can decrypt the key and sign again.
Try pulling the image you just pushed:
/ # docker pull sandboxregistry:5000/test/trusttest Using default tag: latest Pull (1 of 1): sandboxregistry:5000/test/trusttest:latest@sha256:ebf59c538accdf160ef435f1a19938ab8c0d6bd96aef8d4ddd1b379edf15a926 sha256:ebf59c538accdf160ef435f1a19938ab8c0d6bd96aef8d4ddd1b379edf15a926: Pulling from test/trusttest Digest: sha256:ebf59c538accdf160ef435f1a19938ab8c0d6bd96aef8d4ddd1b379edf15a926 Status: Downloaded newer image for sandboxregistry:5000/test/trusttest@sha256:ebf59c538accdf160ef435f1a19938ab8c0d6bd96aef8d4ddd1b379edf15a926 Tagging sandboxregistry:5000/test/trusttest@sha256:ebf59c538accdf160ef435f1a19938ab8c0d6bd96aef8d4ddd1b379edf15a926 as sandboxregistry:5000/test/trusttest:latest
Test with malicious images
What happens when data is corrupted and you try to pull it when trust is
enabled? In this section, you go into the sandboxregistry
and tamper with some
data. Then, you try and pull it.
Leave the
trustsandbox
shell and container running.Open a new interactive terminal from your host, and obtain a shell into the
sandboxregistry
container.$ docker container exec -it sandboxregistry bash root@65084fc6f047:/#
List the layers for the
test/trusttest
image you pushed:root@65084fc6f047:/# ls -l /var/lib/registry/docker/registry/v2/repositories/test/trusttest/_layers/sha256 total 12 drwxr-xr-x 2 root root 4096 Jun 10 17:26 a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 drwxr-xr-x 2 root root 4096 Jun 10 17:26 aac0c133338db2b18ff054943cee3267fe50c75cdee969aed88b1992539ed042 drwxr-xr-x 2 root root 4096 Jun 10 17:26 cc7629d1331a7362b5e5126beb5bf15ca0bf67eb41eab994c719a45de53255cd
Change into the registry storage for one of those layers (this is in a different directory):
root@65084fc6f047:/# cd /var/lib/registry/docker/registry/v2/blobs/sha256/aa/aac0c133338db2b18ff054943cee3267fe50c75cdee969aed88b1992539ed042
Add malicious data to one of the
trusttest
layers:root@65084fc6f047:/# echo ""Malicious data"" > data
Go back to your
trustsandbox
terminal.List the
trusttest
image./ # docker image ls | grep trusttest REPOSITORY TAG IMAGE ID CREATED SIZE docker/trusttest latest cc7629d1331a 11 months ago 5.025 MB sandboxregistry:5000/test/trusttest latest cc7629d1331a 11 months ago 5.025 MB sandboxregistry:5000/test/trusttest <none> cc7629d1331a 11 months ago 5.025 MB
Remove the
trusttest:latest
image from our local cache./ # docker image rm -f cc7629d1331a Untagged: docker/trusttest:latest Untagged: sandboxregistry:5000/test/trusttest:latest Untagged: sandboxregistry:5000/test/trusttest@sha256:ebf59c538accdf160ef435f1a19938ab8c0d6bd96aef8d4ddd1b379edf15a926 Deleted: sha256:cc7629d1331a7362b5e5126beb5bf15ca0bf67eb41eab994c719a45de53255cd Deleted: sha256:2a1f6535dc6816ffadcdbe20590045e6cbf048d63fd4cc753a684c9bc01abeea Deleted: sha256:c22f7bc058a9a8ffeb32989b5d3338787e73855bf224af7aa162823da015d44c
Docker does not re-download images that it already has cached, but we want Docker to attempt to download the tampered image from the registry and reject it because it is invalid.
Pull the image again. This downloads the image from the registry, because we don't have it cached.
/ # docker pull sandboxregistry:5000/test/trusttest Using default tag: latest Pull (1 of 1): sandboxregistry:5000/test/trusttest:latest@sha256:35d5bc26fd358da8320c137784fe590d8fcf9417263ef261653e8e1c7f15672e sha256:35d5bc26fd358da8320c137784fe590d8fcf9417263ef261653e8e1c7f15672e: Pulling from test/trusttest aac0c133338d: Retrying in 5 seconds a3ed95caeb02: Download complete error pulling image configuration: unexpected EOF
The pull did not complete because the trust system couldn't verify the image.
More play in the sandbox
Now, you have a full Docker content trust sandbox on your local system, feel free to play with it and see how it behaves. If you find any security issues with Docker, feel free to send us an email at security@docker.com.
Clean up your sandbox
When you are done, and want to clean up all the services you've started and any anonymous volumes that have been created, just run the following command in the directory where you've created your Docker Compose file:
$ docker compose down -v",,,
5bb489d69d118515edeb054c5e67c132a0f62ee055ddebccbf0bef8b0d8a93f2,"Manage single sign-on
Manage organizations
Note
You must have a company to manage more than one organization.
Connect an organization
- Sign in to the Admin Console.
- Select your company from the Choose profile page, and then select SSO and SCIM.
- In the SSO connections table, select the Action icon and then Edit connection.
- Select Next to navigate to the section where connected organizations are listed.
- In the Organizations drop-down, select the organization to add to the connection.
- Select Next to confirm or change the default organization and team provisioning.
- Review the Connection Summary and select Update connection.
Remove an organization
- Sign in to the Admin Console.
- Select your company from the Choose profile page, and then select SSO and SCIM.
- In the SSO connections table, select the Action icon and then Edit connection.
- Select Next to navigate to the section where connected organizations are listed.
- In the Organizations drop-down, select Remove to remove the connection.
- Select Next to confirm or change the default organization and team provisioning.
- Review the Connection Summary and select Update connection.
Manage domains
Remove a domain from an SSO connection
- Sign in to the Admin Console.
- Select your organization or company from the Choose profile page, and then select SSO and SCIM.
- In the SSO connections table, select the Action icon and then Edit connection.
- Select Next to navigate to the section where the connected domains are listed.
- In the Domain drop-down, select the x icon next to the domain that you want to remove.
- Select Next to confirm or change the connected organization(s).
- Select Next to confirm or change the default organization and team provisioning selections.
- Review the Connection Summary and select Update connection.
Note
If you want to re-add the domain, a new TXT record value is assigned. You must then complete the verification steps with the new TXT record value.
Remove a domain from an SSO connection
- Sign in to Docker Hub.
- Navigate to the SSO settings page for your organization. Select Organizations, your organization, Settings, and then Security.
- In the SSO connections table, select the Action icon and then Edit connection.
- Select Next to navigate to the section where the connected domains are listed.
- In the Domain drop-down, select the x icon next to the domain that you want to remove.
- Select Next to confirm or change the connected organization(s).
- Select Next to confirm or change the default organization and team provisioning selections.
- Review the Connection Summary and select Update connection.
Note
If you want to re-add the domain, a new TXT record value is assigned. You must then complete the verification steps with the new TXT record value.
Manage SSO connections
Edit a connection
- Sign in to the Admin Console.
- Select your organization or company from the Choose proifle page, and then select SSO and SCIM. Note that when an organization is part of a company, you must select the company and configure SSO for that organization at the company level. Each organization can have its own SSO configuration and domain, but it must be configured at the company level.
- In the SSO connections table, select the Action icon.
- Select Edit connection.
- Follow the on-screen instructions to edit the connection.
Delete a connection
- Sign in to the Admin Console.
- Select your organization or company from the Choose proifle page, and then select SSO and SCIM. Note that when an organization is part of a company, you must select the company and configure SSO for that organization at the company level. Each organization can have its own SSO configuration and domain, but it must be configured at the company level.
- In the SSO connections table, select the Action icon.
- Select Delete connection.
- Follow the on-screen instructions to delete a connection.
Deleting SSO
When you disable SSO, you can delete the connection to remove the configuration settings and the added domains. Once you delete this connection, it can't be undone. If an SSO connection is deleted, Docker users must authenticate with their Docker ID and password.
Edit a connection
- Sign in to Docker Hub.
- Navigate to the SSO settings page for your organization. Select Organizations, your organization, Settings, and then Security.
- In the SSO connections table, select the Action icon.
- Select Edit connection.
- Follow the on-screen instructions to edit the connection.
Delete a connection
- Sign in to Docker Hub.
- Navigate to the SSO settings page for your organization. Select Organizations, your organization, Settings, and then Security.
- In the SSO connections table, select the Action icon.
- Select Delete connection.
- Follow the on-screen instructions to delete a connection.
Deleting SSO
When you disable SSO, you can delete the connection to remove the configuration settings and the added domains. Once you delete this connection, it can't be undone. If an SSO connection is deleted, Docker users must authenticate with their Docker ID and password.
Manage users
Important
SSO has Just-In-Time (JIT) Provisioning enabled by default unless you have disabled it. This means your users are auto-provisioned to your organization.
You can change this on a per-app basis. To prevent auto-provisioning users, you can create a security group in your IdP and configure the SSO app to authenticate and authorize only those users that are in the security group. Follow the instructions provided by your IdP:
Alternatively, see the Provisioning overview guide.
Add guest users when SSO is enabled
To add a guest that isn't verified through your IdP:
- Sign in to the Admin Console.
- Select your organization or company from the Choose profile page, then select Members.
- Select Invite.
- Follow the on-screen instructions to invite the user.
Remove users from the SSO company
To remove a user:
- Sign in to Admin Console.
- Select your organization or company from the Choose profile page, then select Members.
- Select the action icon next to a user’s name, and then select Remove member, if you're an organization, or Remove user, if you're a company.
- Follow the on-screen instructions to remove the user.
Manage provisioning
Users are provisioned with Just-in-Time (JIT) provisioning by default. If you enable SCIM, you can disable JIT. For more information, see the Provisioning overview guide.",,,
5868437c8a8979956411a7fad1faf6a1e8991439cbf99dd5da8b278218497e8a,"Sign in to Docker Desktop
Docker recommends that you authenticate using the Sign in option in the top-right corner of the Docker Dashboard.
In large enterprises where admin access is restricted, administrators can enforce sign-in.
Tip
Explore Docker's core subscriptions to see what else Docker can offer you.
Benefits of signing in
You can access your Docker Hub repositories directly from Docker Desktop.
Authenticated users also get a higher pull rate limit compared to anonymous users. For more information, see Usage and limits.
Improve your organization’s security posture for containerized development by taking advantage of Hardened Desktop.
Note
Docker Desktop automatically signs you out after 90 days, or after 30 days of inactivity.
Signing in with Docker Desktop for Linux
Docker Desktop for Linux relies on
pass
to store credentials in gpg2-encrypted files.
Before signing in to Docker Desktop with your
Docker ID, you must initialize pass
.
Docker Desktop displays a warning if you've not initialized pass
.
You can initialize pass by using a gpg key. To generate a gpg key, run:
$ gpg --generate-key
The following is an example similar to what you see once you run the previous command:
...
GnuPG needs to construct a user ID to identify your key.
Real name: Molly
Email address: molly@example.com
You selected this USER-ID:
""Molly <molly@example.com>""
Change (N)ame, (E)mail, or (O)kay/(Q)uit? O
...
pubrsa3072 2022-03-31 [SC] [expires: 2024-03-30]
<generated gpg-id public key>
uid Molly <molly@example.com>
subrsa3072 2022-03-31 [E] [expires: 2024-03-30]
To initialize pass
, run the following command using the public key generated from the previous command:
$ pass init <your_generated_gpg-id_public_key>
The following is an example similar to what you see once you run the previous command:
mkdir: created directory '/home/molly/.password-store/'
Password store initialized for <generated_gpg-id_public_key>
Once you initialize pass
, you can sign in and pull your private images.
When Docker CLI or Docker Desktop use credentials, a user prompt may pop up for the password you set during the gpg key generation.
$ docker pull molly/privateimage
Using default tag: latest
latest: Pulling from molly/privateimage
3b9cc81c3203: Pull complete
Digest: sha256:3c6b73ce467f04d4897d7a7439782721fd28ec9bf62ea2ad9e81a5fb7fb3ff96
Status: Downloaded newer image for molly/privateimage:latest
docker.io/molly/privateimage:latest
What's next?
- Explore Docker Desktop and its features.
- Change your Docker Desktop settings
- Browse common FAQs",,,
73d9802c385b613903ee5bf35dc764e318ead167ea410fbcf0edd9498469d8be,"Extension Backend
The ddClient.extension.vm
object can be used to communicate with the backend defined in the
vm section of the extension metadata.
get
▸ get(url
): Promise
<unknown
>
Performs an HTTP GET request to a backend service.
ddClient.extension.vm.service
.get(""/some/service"")
.then((value: any) => console.log(value)
See Service API Reference for other methods such as POST, UPDATE, and DELETE.
Deprecated extension backend communication
The methods below that use
window.ddClient.backend
are deprecated and will be removed in a future version. Use the methods specified above.
The window.ddClient.backend
object can be used to communicate with the backend
defined in the
vm section of the
extension metadata. The client is already connected to the backend.
Example usages:
window.ddClient.backend
.get(""/some/service"")
.then((value: any) => console.log(value));
window.ddClient.backend
.post(""/some/service"", { ... })
.then((value: any) => console.log(value));
window.ddClient.backend
.put(""/some/service"", { ... })
.then((value: any) => console.log(value));
window.ddClient.backend
.patch(""/some/service"", { ... })
.then((value: any) => console.log(value));
window.ddClient.backend
.delete(""/some/service"")
.then((value: any) => console.log(value));
window.ddClient.backend
.head(""/some/service"")
.then((value: any) => console.log(value));
window.ddClient.backend
.request({ url: ""/url"", method: ""GET"", headers: { 'header-key': 'header-value' }, data: { ... }})
.then((value: any) => console.log(value));
Run a command in the extension backend container
For example, execute the command ls -l
inside the backend container:
await ddClient.extension.vm.cli.exec(""ls"", [""-l""]);
Stream the output of the command executed in the backend container. For example, spawn the command ls -l
inside the backend container:
await ddClient.extension.vm.cli.exec(""ls"", [""-l""], {
stream: {
onOutput(data) {
if (data.stdout) {
console.error(data.stdout);
} else {
console.log(data.stderr);
}
},
onError(error) {
console.error(error);
},
onClose(exitCode) {
console.log(""onClose with exit code "" + exitCode);
},
},
});
For more details, refer to the Extension VM API Reference
Deprecated extension backend command execution
This method is deprecated and will be removed in a future version. Use the specified method above.
If your extension ships with additional binaries that should be run inside the
backend container, you can use the execInVMExtension
function:
const output = await window.ddClient.backend.execInVMExtension(
`cliShippedInTheVm xxx`
);
console.log(output);
Invoke an extension binary on the host
You can run binaries defined in the host section of the extension metadata.
For example, execute the shipped binary kubectl -h
command in the host:
await ddClient.extension.host.cli.exec(""kubectl"", [""-h""]);
As long as the kubectl
binary is shipped as part of your extension, you can spawn the kubectl -h
command in the host and get the output stream:
await ddClient.extension.host.cli.exec(""kubectl"", [""-h""], {
stream: {
onOutput(data: { stdout: string } | { stderr: string }): void {
if (data.stdout) {
console.error(data.stdout);
} else {
console.log(data.stderr);
}
},
onError(error: any): void {
console.error(error);
},
onClose(exitCode: number): void {
console.log(""onClose with exit code "" + exitCode);
},
},
});
You can stream the output of the command executed in the backend container or in the host.
For more details, refer to the Extension Host API Reference
Deprecated invocation of extension binary
This method is deprecated and will be removed in a future version. Use the method specified above.
To execute a command in the host:
window.ddClient.execHostCmd(`cliShippedOnHost xxx`).then((cmdResult: any) => {
console.log(cmdResult);
});
To stream the output of the command executed in the backend container or in the host:
window.ddClient.spawnHostCmd(
`cliShippedOnHost`,
[`arg1`, `arg2`],
(data: any, err: any) => {
console.log(data.stdout, data.stderr);
// Once the command exits we get the status code
if (data.code) {
console.log(data.code);
}
}
);
Note
You cannot use this to chain commands in a single
exec()
invocation (likecmd1 $(cmd2)
or using pipe between commands).You need to invoke
exec()
for each command and parse results to pass parameters to the next command if needed.",,,
9b7b12a71f8706500b744182da1c4560f6a81c578f170ef9bcbb1662556834af,"Enforce sign-in for Docker Desktop
By default, members of your organization can use Docker Desktop without signing in. When users don’t sign in as a member of your organization, they don’t receive the benefits of your organization’s subscription and they can circumvent Docker’s security features for your organization.
There are multiple methods for enforcing sign-in, depending on your companies' set up and preferences:
- Registry key method (Windows only) New
- Configuration profiles method (Mac only) New
.plist
method (Mac only) Newregistry.json
method (All)
How is sign-in enforced?
When Docker Desktop starts and it detects a registry key, .plist
file, or registry.json
file, the following occurs:
- A Sign in required! prompt appears requiring the user to sign in as a member of your organization to use Docker Desktop.
- When a user signs in to an account that isn’t a member of your organization, they are automatically signed out and can’t use Docker Desktop. The user can select Sign in and try again.
- When a user signs in to an account that is a member of your organization, they can use Docker Desktop.
- When a user signs out, the Sign in required! prompt appears and they can no longer use Docker Desktop.
Note
Enforcing sign-in for Docker Desktop does not impact accessing the Docker CLI. CLI access is only impacted for organizations that enforce single sign-on.
Enforcing sign-in versus enforcing single sign-on (SSO)
Enforcing SSO and enforcing sign-in are different features. The following table provides a description and benefits when using each feature.
| Enforcement | Description | Benefits |
|---|---|---|
| Enforce sign-in only | Users must sign in before using Docker Desktop. | Ensures users receive the benefits of your subscription and ensures security features are applied. In addition, you gain insights into users’ activity. |
| Enforce single sign-on (SSO) only | If users sign in, they must sign in using SSO. | Centralizes authentication and enforces unified policies set by the identity provider. |
| Enforce both | Users must sign in using SSO before using Docker Desktop. | Ensures users receive the benefits of your subscription and ensures security features are applied. In addition, you gain insights into users’ activity. Finally, it centralizes authentication and enforces unified policies set by the identity provider. |
| Enforce neither | If users sign in, they can use SSO or their Docker credentials. | Lets users access Docker Desktop without barriers, but at the cost of reduced security and insights. |
What's next?
- To enforce sign-in, review the Methods guide.
- To enforce SSO, review the Enforce SSO steps.",,,
47d4aa3a8cc7074ef4d8107c5f411103eebba96f841deaf72d4b7ea69e1a6f7a,"Running containers
Docker runs processes in isolated containers. A container is a process
which runs on a host. The host may be local or remote. When you
execute docker run
, the container process that runs is isolated in
that it has its own file system, its own networking, and its own
isolated process tree separate from the host.
This page details how to use the docker run
command to run containers.
General form
A docker run
command takes the following form:
$ docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]
The docker run
command must specify an
image reference
to create the container from.
Image references
The image reference is the name and version of the image. You can use the image reference to create or run a container based on an image.
docker run IMAGE[:TAG][@DIGEST]
docker create IMAGE[:TAG][@DIGEST]
An image tag is the image version, which defaults to latest
when omitted. Use
the tag to run a container from specific version of an image. For example, to
run version 24.04
of the ubuntu
image: docker run ubuntu:24.04
.
Image digests
Images using the v2 or later image format have a content-addressable identifier called a digest. As long as the input used to generate the image is unchanged, the digest value is predictable.
The following example runs a container from the alpine
image with the
sha256:9cacb71397b640eca97488cf08582ae4e4068513101088e9f96c9814bfda95e0
digest:
$ docker run alpine@sha256:9cacb71397b640eca97488cf08582ae4e4068513101088e9f96c9814bfda95e0 date
Options
[OPTIONS]
let you configure options for the container. For example, you can
give the container a name (--name
), or run it as a background process (-d
).
You can also set options to control things like resource constraints and
networking.
Commands and arguments
You can use the [COMMAND]
and [ARG...]
positional arguments to specify
commands and arguments for the container to run when it starts up. For example,
you can specify sh
as the [COMMAND]
, combined with the -i
and -t
flags,
to start an interactive shell in the container (if the image you select has an
sh
executable on PATH
).
$ docker run -it IMAGE sh
Note
Depending on your Docker system configuration, you may be required to preface the
docker run
command withsudo
. To avoid having to usesudo
with thedocker
command, your system administrator can create a Unix group calleddocker
and add users to it. For more information about this configuration, refer to the Docker installation documentation for your operating system.
Foreground and background
When you start a container, the container runs in the foreground by default.
If you want to run the container in the background instead, you can use the
--detach
(or -d
) flag. This starts the container without occupying your
terminal window.
$ docker run -d <IMAGE>
While the container runs in the background, you can interact with the container
using other CLI commands. For example, docker logs
lets you view the logs for
the container, and docker attach
brings it to the foreground.
$ docker run -d nginx
0246aa4d1448a401cabd2ce8f242192b6e7af721527e48a810463366c7ff54f1
$ docker ps
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
0246aa4d1448 nginx ""/docker-entrypoint.…"" 2 seconds ago Up 1 second 80/tcp pedantic_liskov
$ docker logs -n 5 0246aa4d1448
2023/11/06 15:58:23 [notice] 1#1: start worker process 33
2023/11/06 15:58:23 [notice] 1#1: start worker process 34
2023/11/06 15:58:23 [notice] 1#1: start worker process 35
2023/11/06 15:58:23 [notice] 1#1: start worker process 36
2023/11/06 15:58:23 [notice] 1#1: start worker process 37
$ docker attach 0246aa4d1448
^C
2023/11/06 15:58:40 [notice] 1#1: signal 2 (SIGINT) received, exiting
...
For more information about docker run
flags related to foreground and
background modes, see:
docker run --detach
: run container in backgrounddocker run --attach
: attach tostdin
,stdout
, andstderr
docker run --tty
: allocate a pseudo-ttydocker run --interactive
: keepstdin
open even if not attached
For more information about re-attaching to a background container, see
docker attach
.
Container identification
You can identify a container in three ways:
| Identifier type | Example value |
|---|---|
| UUID long identifier | f78375b1c487e03c9438c729345e54db9d20cfa2ac1fc3494b6eb60872e74778 |
| UUID short identifier | f78375b1c487 |
| Name | evil_ptolemy |
The UUID identifier is a random ID assigned to the container by the daemon.
The daemon generates a random string name for containers automatically. You can
also define a custom name using
the --name
flag.
Defining a name
can be a handy way to add meaning to a container. If you
specify a name
, you can use it when referring to the container in a
user-defined network. This works for both background and foreground Docker
containers.
A container identifier is not the same thing as an image reference. The image
reference specifies which image to use when you run a container. You can't run
docker exec nginx:alpine sh
to open a shell in a container based on the
nginx:alpine
image, because docker exec
expects a container identifier
(name or ID), not an image.
While the image used by a container is not an identifier for the container, you
find out the IDs of containers using an image by using the --filter
flag. For
example, the following docker ps
command gets the IDs of all running
containers based on the nginx:alpine
image:
$ docker ps -q --filter ancestor=nginx:alpine
For more information about using filters, see Filtering.
Container networking
Containers have networking enabled by default, and they can make outgoing connections. If you're running multiple containers that need to communicate with each other, you can create a custom network and attach the containers to the network.
When multiple containers are attached to the same custom network, they can
communicate with each other using the container names as a DNS hostname. The
following example creates a custom network named my-net
, and runs two
containers that attach to the network.
$ docker network create my-net
$ docker run -d --name web --network my-net nginx:alpine
$ docker run --rm -it --network my-net busybox
/ # ping web
PING web (172.18.0.2): 56 data bytes
64 bytes from 172.18.0.2: seq=0 ttl=64 time=0.326 ms
64 bytes from 172.18.0.2: seq=1 ttl=64 time=0.257 ms
64 bytes from 172.18.0.2: seq=2 ttl=64 time=0.281 ms
^C
--- web ping statistics ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 0.257/0.288/0.326 ms
For more information about container networking, see Networking overview
Filesystem mounts
By default, the data in a container is stored in an ephemeral, writable container layer. Removing the container also removes its data. If you want to use persistent data with containers, you can use filesystem mounts to store the data persistently on the host system. Filesystem mounts can also let you share data between containers and the host.
Docker supports two main categories of mounts:
- Volume mounts
- Bind mounts
Volume mounts are great for persistently storing data for containers, and for sharing data between containers. Bind mounts, on the other hand, are for sharing data between a container and the host.
You can add a filesystem mount to a container using the --mount
flag for the
docker run
command.
The following sections show basic examples of how to create volumes and bind mounts. For more in-depth examples and descriptions, refer to the section of the storage section in the documentation.
Volume mounts
To create a volume mount:
$ docker run --mount source=<VOLUME_NAME>,target=[PATH] [IMAGE] [COMMAND...]
The --mount
flag takes two parameters in this case: source
and target
.
The value for the source
parameter is the name of the volume. The value of
target
is the mount location of the volume inside the container. Once you've
created the volume, any data you write to the volume is persisted, even if you
stop or remove the container:
$ docker run --rm --mount source=my_volume,target=/foo busybox \
echo ""hello, volume!"" > /foo/hello.txt
$ docker run --mount source=my_volume,target=/bar busybox
cat /bar/hello.txt
hello, volume!
The target
must always be an absolute path, such as /src/docs
. An absolute
path starts with a /
(forward slash). Volume names must start with an
alphanumeric character, followed by a-z0-9
, _
(underscore), .
(period) or
-
(hyphen).
Bind mounts
To create a bind mount:
$ docker run -it --mount type=bind,source=[PATH],target=[PATH] busybox
In this case, the --mount
flag takes three parameters. A type (bind
), and
two paths. The source
path is a the location on the host that you want to
bind mount into the container. The target
path is the mount destination
inside the container.
Bind mounts are read-write by default, meaning that you can both read and write files to and from the mounted location from the container. Changes that you make, such as adding or editing files, are reflected on the host filesystem:
$ docker run -it --mount type=bind,source=.,target=/foo busybox
/ # echo ""hello from container"" > /foo/hello.txt
/ # exit
$ cat hello.txt
hello from container
Exit status
The exit code from docker run
gives information about why the container
failed to run or why it exited. The following sections describe the meanings of
different container exit codes values.
125
Exit code 125
indicates that the error is with Docker daemon itself.
$ docker run --foo busybox; echo $?
flag provided but not defined: --foo
See 'docker run --help'.
125
126
Exit code 126
indicates that the specified contained command can't be invoked.
The container command in the following example is: /etc
.
$ docker run busybox /etc; echo $?
docker: Error response from daemon: Container command '/etc' could not be invoked.
126
127
Exit code 127
indicates that the contained command can't be found.
$ docker run busybox foo; echo $?
docker: Error response from daemon: Container command 'foo' not found or does not exist.
127
Other exit codes
Any exit code other than 125
, 126
, and 127
represent the exit code of the
provided container command.
$ docker run busybox /bin/sh -c 'exit 3'
$ echo $?
3
Runtime constraints on resources
The operator can also adjust the performance parameters of the container:
| Option | Description |
|---|---|
-m , --memory="""" | Memory limit (format: <number>[<unit>] ). Number is a positive integer. Unit can be one of b , k , m , or g . Minimum is 6M. |
--memory-swap="""" | Total memory limit (memory + swap, format: <number>[<unit>] ). Number is a positive integer. Unit can be one of b , k , m , or g . |
--memory-reservation="""" | Memory soft limit (format: <number>[<unit>] ). Number is a positive integer. Unit can be one of b , k , m , or g . |
--kernel-memory="""" | Kernel memory limit (format: <number>[<unit>] ). Number is a positive integer. Unit can be one of b , k , m , or g . Minimum is 4M. |
-c , --cpu-shares=0 | CPU shares (relative weight) |
--cpus=0.000 | Number of CPUs. Number is a fractional number. 0.000 means no limit. |
--cpu-period=0 | Limit the CPU CFS (Completely Fair Scheduler) period |
--cpuset-cpus="""" | CPUs in which to allow execution (0-3, 0,1) |
--cpuset-mems="""" | Memory nodes (MEMs) in which to allow execution (0-3, 0,1). Only effective on NUMA systems. |
--cpu-quota=0 | Limit the CPU CFS (Completely Fair Scheduler) quota |
--cpu-rt-period=0 | Limit the CPU real-time period. In microseconds. Requires parent cgroups be set and cannot be higher than parent. Also check rtprio ulimits. |
--cpu-rt-runtime=0 | Limit the CPU real-time runtime. In microseconds. Requires parent cgroups be set and cannot be higher than parent. Also check rtprio ulimits. |
--blkio-weight=0 | Block IO weight (relative weight) accepts a weight value between 10 and 1000. |
--blkio-weight-device="""" | Block IO weight (relative device weight, format: DEVICE_NAME:WEIGHT ) |
--device-read-bps="""" | Limit read rate from a device (format: <device-path>:<number>[<unit>] ). Number is a positive integer. Unit can be one of kb , mb , or gb . |
--device-write-bps="""" | Limit write rate to a device (format: <device-path>:<number>[<unit>] ). Number is a positive integer. Unit can be one of kb , mb , or gb . |
--device-read-iops="""" | Limit read rate (IO per second) from a device (format: <device-path>:<number> ). Number is a positive integer. |
--device-write-iops="""" | Limit write rate (IO per second) to a device (format: <device-path>:<number> ). Number is a positive integer. |
--oom-kill-disable=false | Whether to disable OOM Killer for the container or not. |
--oom-score-adj=0 | Tune container's OOM preferences (-1000 to 1000) |
--memory-swappiness="""" | Tune a container's memory swappiness behavior. Accepts an integer between 0 and 100. |
--shm-size="""" | Size of /dev/shm . The format is <number><unit> . number must be greater than 0 . Unit is optional and can be b (bytes), k (kilobytes), m (megabytes), or g (gigabytes). If you omit the unit, the system uses bytes. If you omit the size entirely, the system uses 64m . |
User memory constraints
We have four ways to set user memory usage:
| Option | Result |
|---|---|
| memory=inf, memory-swap=inf (default) | There is no memory limit for the container. The container can use as much memory as needed. |
| memory=L<inf, memory-swap=inf | (specify memory and set memory-swap as -1 ) The container is
not allowed to use more than L bytes of memory, but can use as much swap
as is needed (if the host supports swap memory). |
| memory=L<inf, memory-swap=2*L | (specify memory without memory-swap) The container is not allowed to use more than L bytes of memory, swap plus memory usage is double of that. |
| memory=L<inf, memory-swap=S<inf, L<=S | (specify both memory and memory-swap) The container is not allowed to use more than L bytes of memory, swap plus memory usage is limited by S. |
Examples:
$ docker run -it ubuntu:24.04 /bin/bash
We set nothing about memory, this means the processes in the container can use as much memory and swap memory as they need.
$ docker run -it -m 300M --memory-swap -1 ubuntu:24.04 /bin/bash
We set memory limit and disabled swap memory limit, this means the processes in the container can use 300M memory and as much swap memory as they need (if the host supports swap memory).
$ docker run -it -m 300M ubuntu:24.04 /bin/bash
We set memory limit only, this means the processes in the container can use 300M memory and 300M swap memory, by default, the total virtual memory size (--memory-swap) will be set as double of memory, in this case, memory + swap would be 2*300M, so processes can use 300M swap memory as well.
$ docker run -it -m 300M --memory-swap 1G ubuntu:24.04 /bin/bash
We set both memory and swap memory, so the processes in the container can use 300M memory and 700M swap memory.
Memory reservation is a kind of memory soft limit that allows for greater
sharing of memory. Under normal circumstances, containers can use as much of
the memory as needed and are constrained only by the hard limits set with the
-m
/--memory
option. When memory reservation is set, Docker detects memory
contention or low memory and forces containers to restrict their consumption to
a reservation limit.
Always set the memory reservation value below the hard limit, otherwise the hard limit takes precedence. A reservation of 0 is the same as setting no reservation. By default (without reservation set), memory reservation is the same as the hard memory limit.
Memory reservation is a soft-limit feature and does not guarantee the limit won't be exceeded. Instead, the feature attempts to ensure that, when memory is heavily contended for, memory is allocated based on the reservation hints/setup.
The following example limits the memory (-m
) to 500M and sets the memory
reservation to 200M.
$ docker run -it -m 500M --memory-reservation 200M ubuntu:24.04 /bin/bash
Under this configuration, when the container consumes memory more than 200M and less than 500M, the next system memory reclaim attempts to shrink container memory below 200M.
The following example set memory reservation to 1G without a hard memory limit.
$ docker run -it --memory-reservation 1G ubuntu:24.04 /bin/bash
The container can use as much memory as it needs. The memory reservation setting ensures the container doesn't consume too much memory for long time, because every memory reclaim shrinks the container's consumption to the reservation.
By default, kernel kills processes in a container if an out-of-memory (OOM)
error occurs. To change this behaviour, use the --oom-kill-disable
option.
Only disable the OOM killer on containers where you have also set the
-m/--memory
option. If the -m
flag is not set, this can result in the host
running out of memory and require killing the host's system processes to free
memory.
The following example limits the memory to 100M and disables the OOM killer for this container:
$ docker run -it -m 100M --oom-kill-disable ubuntu:24.04 /bin/bash
The following example, illustrates a dangerous way to use the flag:
$ docker run -it --oom-kill-disable ubuntu:24.04 /bin/bash
The container has unlimited memory which can cause the host to run out memory
and require killing system processes to free memory. The --oom-score-adj
parameter can be changed to select the priority of which containers will
be killed when the system is out of memory, with negative scores making them
less likely to be killed, and positive scores more likely.
Kernel memory constraints
Kernel memory is fundamentally different than user memory as kernel memory can't be swapped out. The inability to swap makes it possible for the container to block system services by consuming too much kernel memory. Kernel memory includes：
- stack pages
- slab pages
- sockets memory pressure
- tcp memory pressure
You can setup kernel memory limit to constrain these kinds of memory. For example, every process consumes some stack pages. By limiting kernel memory, you can prevent new processes from being created when the kernel memory usage is too high.
Kernel memory is never completely independent of user memory. Instead, you limit kernel memory in the context of the user memory limit. Assume ""U"" is the user memory limit and ""K"" the kernel limit. There are three possible ways to set limits:
| Option | Result |
|---|---|
| U != 0, K = inf (default) | This is the standard memory limitation mechanism already present before using kernel memory. Kernel memory is completely ignored. |
| U != 0, K < U | Kernel memory is a subset of the user memory. This setup is useful in deployments where the total amount of memory per-cgroup is overcommitted. Overcommitting kernel memory limits is definitely not recommended, since the box can still run out of non-reclaimable memory. In this case, you can configure K so that the sum of all groups is never greater than the total memory. Then, freely set U at the expense of the system's service quality. |
| U != 0, K > U | Since kernel memory charges are also fed to the user counter and reclamation is triggered for the container for both kinds of memory. This configuration gives the admin a unified view of memory. It is also useful for people who just want to track kernel memory usage. |
Examples:
$ docker run -it -m 500M --kernel-memory 50M ubuntu:24.04 /bin/bash
We set memory and kernel memory, so the processes in the container can use 500M memory in total, in this 500M memory, it can be 50M kernel memory tops.
$ docker run -it --kernel-memory 50M ubuntu:24.04 /bin/bash
We set kernel memory without -m, so the processes in the container can use as much memory as they want, but they can only use 50M kernel memory.
Swappiness constraint
By default, a container's kernel can swap out a percentage of anonymous pages.
To set this percentage for a container, specify a --memory-swappiness
value
between 0 and 100. A value of 0 turns off anonymous page swapping. A value of
100 sets all anonymous pages as swappable. By default, if you are not using
--memory-swappiness
, memory swappiness value will be inherited from the parent.
For example, you can set:
$ docker run -it --memory-swappiness=0 ubuntu:24.04 /bin/bash
Setting the --memory-swappiness
option is helpful when you want to retain the
container's working set and to avoid swapping performance penalties.
CPU share constraint
By default, all containers get the same proportion of CPU cycles. This proportion can be modified by changing the container's CPU share weighting relative to the weighting of all other running containers.
To modify the proportion from the default of 1024, use the -c
or --cpu-shares
flag to set the weighting to 2 or higher. If 0 is set, the system will ignore the
value and use the default of 1024.
The proportion will only apply when CPU-intensive processes are running. When tasks in one container are idle, other containers can use the left-over CPU time. The actual amount of CPU time will vary depending on the number of containers running on the system.
For example, consider three containers, one has a cpu-share of 1024 and two others have a cpu-share setting of 512. When processes in all three containers attempt to use 100% of CPU, the first container would receive 50% of the total CPU time. If you add a fourth container with a cpu-share of 1024, the first container only gets 33% of the CPU. The remaining containers receive 16.5%, 16.5% and 33% of the CPU.
On a multi-core system, the shares of CPU time are distributed over all CPU cores. Even if a container is limited to less than 100% of CPU time, it can use 100% of each individual CPU core.
For example, consider a system with more than three cores. If you start one
container {C0}
with -c=512
running one process, and another container
{C1}
with -c=1024
running two processes, this can result in the following
division of CPU shares:
PID container CPU CPU share
100 {C0} 0 100% of CPU0
101 {C1} 1 100% of CPU1
102 {C1} 2 100% of CPU2
CPU period constraint
The default CPU CFS (Completely Fair Scheduler) period is 100ms. We can use
--cpu-period
to set the period of CPUs to limit the container's CPU usage.
And usually --cpu-period
should work with --cpu-quota
.
Examples:
$ docker run -it --cpu-period=50000 --cpu-quota=25000 ubuntu:24.04 /bin/bash
If there is 1 CPU, this means the container can get 50% CPU worth of run-time every 50ms.
In addition to use --cpu-period
and --cpu-quota
for setting CPU period constraints,
it is possible to specify --cpus
with a float number to achieve the same purpose.
For example, if there is 1 CPU, then --cpus=0.5
will achieve the same result as
setting --cpu-period=50000
and --cpu-quota=25000
(50% CPU).
The default value for --cpus
is 0.000
, which means there is no limit.
For more information, see the CFS documentation on bandwidth limiting.
Cpuset constraint
We can set cpus in which to allow execution for containers.
Examples:
$ docker run -it --cpuset-cpus=""1,3"" ubuntu:24.04 /bin/bash
This means processes in container can be executed on cpu 1 and cpu 3.
$ docker run -it --cpuset-cpus=""0-2"" ubuntu:24.04 /bin/bash
This means processes in container can be executed on cpu 0, cpu 1 and cpu 2.
We can set mems in which to allow execution for containers. Only effective on NUMA systems.
Examples:
$ docker run -it --cpuset-mems=""1,3"" ubuntu:24.04 /bin/bash
This example restricts the processes in the container to only use memory from memory nodes 1 and 3.
$ docker run -it --cpuset-mems=""0-2"" ubuntu:24.04 /bin/bash
This example restricts the processes in the container to only use memory from memory nodes 0, 1 and 2.
CPU quota constraint
The --cpu-quota
flag limits the container's CPU usage. The default 0 value
allows the container to take 100% of a CPU resource (1 CPU). The CFS (Completely Fair
Scheduler) handles resource allocation for executing processes and is default
Linux Scheduler used by the kernel. Set this value to 50000 to limit the container
to 50% of a CPU resource. For multiple CPUs, adjust the --cpu-quota
as necessary.
For more information, see the
CFS documentation on bandwidth limiting.
Block IO bandwidth (Blkio) constraint
By default, all containers get the same proportion of block IO bandwidth
(blkio). This proportion is 500. To modify this proportion, change the
container's blkio weight relative to the weighting of all other running
containers using the --blkio-weight
flag.
Note
The blkio weight setting is only available for direct IO. Buffered IO is not currently supported.
The --blkio-weight
flag can set the weighting to a value between 10 to 1000.
For example, the commands below create two containers with different blkio
weight:
$ docker run -it --name c1 --blkio-weight 300 ubuntu:24.04 /bin/bash
$ docker run -it --name c2 --blkio-weight 600 ubuntu:24.04 /bin/bash
If you do block IO in the two containers at the same time, by, for example:
$ time dd if=/mnt/zerofile of=test.out bs=1M count=1024 oflag=direct
You'll find that the proportion of time is the same as the proportion of blkio weights of the two containers.
The --blkio-weight-device=""DEVICE_NAME:WEIGHT""
flag sets a specific device weight.
The DEVICE_NAME:WEIGHT
is a string containing a colon-separated device name and weight.
For example, to set /dev/sda
device weight to 200
:
$ docker run -it \
--blkio-weight-device ""/dev/sda:200"" \
ubuntu
If you specify both the --blkio-weight
and --blkio-weight-device
, Docker
uses the --blkio-weight
as the default weight and uses --blkio-weight-device
to override this default with a new value on a specific device.
The following example uses a default weight of 300
and overrides this default
on /dev/sda
setting that weight to 200
:
$ docker run -it \
--blkio-weight 300 \
--blkio-weight-device ""/dev/sda:200"" \
ubuntu
The --device-read-bps
flag limits the read rate (bytes per second) from a device.
For example, this command creates a container and limits the read rate to 1mb
per second from /dev/sda
:
$ docker run -it --device-read-bps /dev/sda:1mb ubuntu
The --device-write-bps
flag limits the write rate (bytes per second) to a device.
For example, this command creates a container and limits the write rate to 1mb
per second for /dev/sda
:
$ docker run -it --device-write-bps /dev/sda:1mb ubuntu
Both flags take limits in the <device-path>:<limit>[unit]
format. Both read
and write rates must be a positive integer. You can specify the rate in kb
(kilobytes), mb
(megabytes), or gb
(gigabytes).
The --device-read-iops
flag limits read rate (IO per second) from a device.
For example, this command creates a container and limits the read rate to
1000
IO per second from /dev/sda
:
$ docker run -it --device-read-iops /dev/sda:1000 ubuntu
The --device-write-iops
flag limits write rate (IO per second) to a device.
For example, this command creates a container and limits the write rate to
1000
IO per second to /dev/sda
:
$ docker run -it --device-write-iops /dev/sda:1000 ubuntu
Both flags take limits in the <device-path>:<limit>
format. Both read and
write rates must be a positive integer.
Additional groups
--group-add: Add additional groups to run as
By default, the docker container process runs with the supplementary groups looked up for the specified user. If one wants to add more to that list of groups, then one can use this flag:
$ docker run --rm --group-add audio --group-add nogroup --group-add 777 busybox id
uid=0(root) gid=0(root) groups=10(wheel),29(audio),99(nogroup),777
Runtime privilege and Linux capabilities
| Option | Description |
|---|---|
--cap-add | Add Linux capabilities |
--cap-drop | Drop Linux capabilities |
--privileged | Give extended privileges to this container |
--device=[] | Allows you to run devices inside the container without the --privileged flag. |
By default, Docker containers are ""unprivileged"" and cannot, for example, run a Docker daemon inside a Docker container. This is because by default a container is not allowed to access any devices, but a ""privileged"" container is given access to all devices (see the documentation on cgroups devices).
The --privileged
flag gives all capabilities to the container. When the operator
executes docker run --privileged
, Docker enables access to all devices on
the host, and reconfigures AppArmor or SELinux to allow the container
nearly all the same access to the host as processes running outside
containers on the host. Use this flag with caution.
For more information about the --privileged
flag, see the
docker run
reference.
If you want to limit access to a specific device or devices you can use
the --device
flag. It allows you to specify one or more devices that
will be accessible within the container.
$ docker run --device=/dev/snd:/dev/snd ...
By default, the container will be able to read
, write
, and mknod
these devices.
This can be overridden using a third :rwm
set of options to each --device
flag:
$ docker run --device=/dev/sda:/dev/xvdc --rm -it ubuntu fdisk /dev/xvdc
Command (m for help): q
$ docker run --device=/dev/sda:/dev/xvdc:r --rm -it ubuntu fdisk /dev/xvdc
You will not be able to write the partition table.
Command (m for help): q
$ docker run --device=/dev/sda:/dev/xvdc:w --rm -it ubuntu fdisk /dev/xvdc
crash....
$ docker run --device=/dev/sda:/dev/xvdc:m --rm -it ubuntu fdisk /dev/xvdc
fdisk: unable to open /dev/xvdc: Operation not permitted
In addition to --privileged
, the operator can have fine grain control over the
capabilities using --cap-add
and --cap-drop
. By default, Docker has a default
list of capabilities that are kept. The following table lists the Linux capability
options which are allowed by default and can be dropped.
| Capability Key | Capability Description |
|---|---|
| AUDIT_WRITE | Write records to kernel auditing log. |
| CHOWN | Make arbitrary changes to file UIDs and GIDs (see chown(2)). |
| DAC_OVERRIDE | Bypass file read, write, and execute permission checks. |
| FOWNER | Bypass permission checks on operations that normally require the file system UID of the process to match the UID of the file. |
| FSETID | Don't clear set-user-ID and set-group-ID permission bits when a file is modified. |
| KILL | Bypass permission checks for sending signals. |
| MKNOD | Create special files using mknod(2). |
| NET_BIND_SERVICE | Bind a socket to internet domain privileged ports (port numbers less than 1024). |
| NET_RAW | Use RAW and PACKET sockets. |
| SETFCAP | Set file capabilities. |
| SETGID | Make arbitrary manipulations of process GIDs and supplementary GID list. |
| SETPCAP | Modify process capabilities. |
| SETUID | Make arbitrary manipulations of process UIDs. |
| SYS_CHROOT | Use chroot(2), change root directory. |
The next table shows the capabilities which are not granted by default and may be added.
| Capability Key | Capability Description |
|---|---|
| AUDIT_CONTROL | Enable and disable kernel auditing; change auditing filter rules; retrieve auditing status and filtering rules. |
| AUDIT_READ | Allow reading the audit log via multicast netlink socket. |
| BLOCK_SUSPEND | Allow preventing system suspends. |
| BPF | Allow creating BPF maps, loading BPF Type Format (BTF) data, retrieve JITed code of BPF programs, and more. |
| CHECKPOINT_RESTORE | Allow checkpoint/restore related operations. Introduced in kernel 5.9. |
| DAC_READ_SEARCH | Bypass file read permission checks and directory read and execute permission checks. |
| IPC_LOCK | Lock memory (mlock(2), mlockall(2), mmap(2), shmctl(2)). |
| IPC_OWNER | Bypass permission checks for operations on System V IPC objects. |
| LEASE | Establish leases on arbitrary files (see fcntl(2)). |
| LINUX_IMMUTABLE | Set the FS_APPEND_FL and FS_IMMUTABLE_FL i-node flags. |
| MAC_ADMIN | Allow MAC configuration or state changes. Implemented for the Smack LSM. |
| MAC_OVERRIDE | Override Mandatory Access Control (MAC). Implemented for the Smack Linux Security Module (LSM). |
| NET_ADMIN | Perform various network-related operations. |
| NET_BROADCAST | Make socket broadcasts, and listen to multicasts. |
| PERFMON | Allow system performance and observability privileged operations using perf_events, i915_perf and other kernel subsystems |
| SYS_ADMIN | Perform a range of system administration operations. |
| SYS_BOOT | Use reboot(2) and kexec_load(2), reboot and load a new kernel for later execution. |
| SYS_MODULE | Load and unload kernel modules. |
| SYS_NICE | Raise process nice value (nice(2), setpriority(2)) and change the nice value for arbitrary processes. |
| SYS_PACCT | Use acct(2), switch process accounting on or off. |
| SYS_PTRACE | Trace arbitrary processes using ptrace(2). |
| SYS_RAWIO | Perform I/O port operations (iopl(2) and ioperm(2)). |
| SYS_RESOURCE | Override resource Limits. |
| SYS_TIME | Set system clock (settimeofday(2), stime(2), adjtimex(2)); set real-time (hardware) clock. |
| SYS_TTY_CONFIG | Use vhangup(2); employ various privileged ioctl(2) operations on virtual terminals. |
| SYSLOG | Perform privileged syslog(2) operations. |
| WAKE_ALARM | Trigger something that will wake up the system. |
Further reference information is available on the capabilities(7) - Linux man page, and in the Linux kernel source code.
Both flags support the value ALL
, so to allow a container to use all capabilities
except for MKNOD
:
$ docker run --cap-add=ALL --cap-drop=MKNOD ...
The --cap-add
and --cap-drop
flags accept capabilities to be specified with
a CAP_
prefix. The following examples are therefore equivalent:
$ docker run --cap-add=SYS_ADMIN ...
$ docker run --cap-add=CAP_SYS_ADMIN ...
For interacting with the network stack, instead of using --privileged
they
should use --cap-add=NET_ADMIN
to modify the network interfaces.
$ docker run -it --rm ubuntu:24.04 ip link add dummy0 type dummy
RTNETLINK answers: Operation not permitted
$ docker run -it --rm --cap-add=NET_ADMIN ubuntu:24.04 ip link add dummy0 type dummy
To mount a FUSE based filesystem, you need to combine both --cap-add
and
--device
:
$ docker run --rm -it --cap-add SYS_ADMIN sshfs sshfs sven@10.10.10.20:/home/sven /mnt
fuse: failed to open /dev/fuse: Operation not permitted
$ docker run --rm -it --device /dev/fuse sshfs sshfs sven@10.10.10.20:/home/sven /mnt
fusermount: mount failed: Operation not permitted
$ docker run --rm -it --cap-add SYS_ADMIN --device /dev/fuse sshfs
# sshfs sven@10.10.10.20:/home/sven /mnt
The authenticity of host '10.10.10.20 (10.10.10.20)' can't be established.
ECDSA key fingerprint is 25:34:85:75:25:b0:17:46:05:19:04:93:b5:dd:5f:c6.
Are you sure you want to continue connecting (yes/no)? yes
sven@10.10.10.20's password:
root@30aa0cfaf1b5:/# ls -la /mnt/src/docker
total 1516
drwxrwxr-x 1 1000 1000 4096 Dec 4 06:08 .
drwxrwxr-x 1 1000 1000 4096 Dec 4 11:46 ..
-rw-rw-r-- 1 1000 1000 16 Oct 8 00:09 .dockerignore
-rwxrwxr-x 1 1000 1000 464 Oct 8 00:09 .drone.yml
drwxrwxr-x 1 1000 1000 4096 Dec 4 06:11 .git
-rw-rw-r-- 1 1000 1000 461 Dec 4 06:08 .gitignore
....
The default seccomp profile will adjust to the selected capabilities, in order to allow use of facilities allowed by the capabilities, so you should not have to adjust this.
Overriding image defaults
When you build an image from a
Dockerfile,
or when committing it, you can set a number of default parameters that take
effect when the image starts up as a container. When you run an image, you can
override those defaults using flags for the docker run
command.
- Default entrypoint
- Default command and options
- Expose ports
- Environment variables
- Healthcheck
- User
- Working directory
Default command and options
The command syntax for docker run
supports optionally specifying commands and
arguments to the container's entrypoint, represented as [COMMAND]
and
[ARG...]
in the following synopsis example:
$ docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]
This command is optional because whoever created the IMAGE
may have already
provided a default COMMAND
, using the Dockerfile CMD
instruction. When you
run a container, you can override that CMD
instruction just by specifying a
new COMMAND
.
If the image also specifies an ENTRYPOINT
then the CMD
or COMMAND
get appended as arguments to the ENTRYPOINT
.
Default entrypoint
--entrypoint="""": Overwrite the default entrypoint set by the image
The entrypoint refers to the default executable that's invoked when you run a
container. A container's entrypoint is defined using the Dockerfile
ENTRYPOINT
instruction. It's similar to specifying a default command because
it specifies, but the difference is that you need to pass an explicit flag to
override the entrypoint, whereas you can override default commands with
positional arguments. The defines a container's default behavior, with the idea
that when you set an entrypoint you can run the container as if it were that
binary, complete with default options, and you can pass in more options as
commands. But there are cases where you may want to run something else inside
the container. This is when overriding the default entrypoint at runtime comes
in handy, using the --entrypoint
flag for the docker run
command.
The --entrypoint
flag expects a string value, representing the name or path
of the binary that you want to invoke when the container starts. The following
example shows you how to run a Bash shell in a container that has been set up
to automatically run some other binary (like /usr/bin/redis-server
):
$ docker run -it --entrypoint /bin/bash example/redis
The following examples show how to pass additional parameters to the custom entrypoint, using the positional command arguments:
$ docker run -it --entrypoint /bin/bash example/redis -c ls -l
$ docker run -it --entrypoint /usr/bin/redis-cli example/redis --help
You can reset a containers entrypoint by passing an empty string, for example:
$ docker run -it --entrypoint="""" mysql bash
Note
Passing
--entrypoint
clears out any default command set on the image. That is, anyCMD
instruction in the Dockerfile used to build it.
Exposed ports
By default, when you run a container, none of the container's ports are exposed to the host. This means you won't be able to access any ports that the container might be listening on. To make a container's ports accessible from the host, you need to publish the ports.
You can start the container with the -P
or -p
flags to expose its ports:
The
-P
(or--publish-all
) flag publishes all the exposed ports to the host. Docker binds each exposed port to a random port on the host.The
-P
flag only publishes port numbers that are explicitly flagged as exposed, either using the DockerfileEXPOSE
instruction or the--expose
flag for thedocker run
command.The
-p
(or--publish
) flag lets you explicitly map a single port or range of ports in the container to the host.
The port number inside the container (where the service listens) doesn't need
to match the port number published on the outside of the container (where
clients connect). For example, inside the container an HTTP service might be
listening on port 80. At runtime, the port might be bound to 42800 on the host.
To find the mapping between the host ports and the exposed ports, use the
docker port
command.
Environment variables
Docker automatically sets some environment variables when creating a Linux container. Docker doesn't set any environment variables when creating a Windows container.
The following environment variables are set for Linux containers:
| Variable | Value |
|---|---|
HOME | Set based on the value of USER |
HOSTNAME | The hostname associated with the container |
PATH | Includes popular directories, such as /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin |
TERM | xterm if the container is allocated a pseudo-TTY |
Additionally, you can set any environment variable in the container by using
one or more -e
flags. You can even override the variables mentioned above, or
variables defined using a Dockerfile ENV
instruction when building the image.
If the you name an environment variable without specifying a value, the current value of the named variable on the host is propagated into the container's environment:
$ export today=Wednesday
$ docker run -e ""deep=purple"" -e today --rm alpine env
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=d2219b854598
deep=purple
today=Wednesday
HOME=/root
PS C:\> docker run --rm -e ""foo=bar"" microsoft/nanoserver cmd /s /c set
ALLUSERSPROFILE=C:\ProgramData
APPDATA=C:\Users\ContainerAdministrator\AppData\Roaming
CommonProgramFiles=C:\Program Files\Common Files
CommonProgramFiles(x86)=C:\Program Files (x86)\Common Files
CommonProgramW6432=C:\Program Files\Common Files
COMPUTERNAME=C2FAEFCC8253
ComSpec=C:\Windows\system32\cmd.exe
foo=bar
LOCALAPPDATA=C:\Users\ContainerAdministrator\AppData\Local
NUMBER_OF_PROCESSORS=8
OS=Windows_NT
Path=C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Users\ContainerAdministrator\AppData\Local\Microsoft\WindowsApps
PATHEXT=.COM;.EXE;.BAT;.CMD
PROCESSOR_ARCHITECTURE=AMD64
PROCESSOR_IDENTIFIER=Intel64 Family 6 Model 62 Stepping 4, GenuineIntel
PROCESSOR_LEVEL=6
PROCESSOR_REVISION=3e04
ProgramData=C:\ProgramData
ProgramFiles=C:\Program Files
ProgramFiles(x86)=C:\Program Files (x86)
ProgramW6432=C:\Program Files
PROMPT=$P$G
PUBLIC=C:\Users\Public
SystemDrive=C:
SystemRoot=C:\Windows
TEMP=C:\Users\ContainerAdministrator\AppData\Local\Temp
TMP=C:\Users\ContainerAdministrator\AppData\Local\Temp
USERDOMAIN=User Manager
USERNAME=ContainerAdministrator
USERPROFILE=C:\Users\ContainerAdministrator
windir=C:\Windows
Healthchecks
The following flags for the docker run
command let you control the parameters
for container healthchecks:
| Option | Description |
|---|---|
--health-cmd | Command to run to check health |
--health-interval | Time between running the check |
--health-retries | Consecutive failures needed to report unhealthy |
--health-timeout | Maximum time to allow one check to run |
--health-start-period | Start period for the container to initialize before starting health-retries countdown |
--health-start-interval | Time between running the check during the start period |
--no-healthcheck | Disable any container-specified HEALTHCHECK |
Example:
$ docker run --name=test -d \
--health-cmd='stat /etc/passwd || exit 1' \
--health-interval=2s \
busybox sleep 1d
$ sleep 2; docker inspect --format='{{.State.Health.Status}}' test
healthy
$ docker exec test rm /etc/passwd
$ sleep 2; docker inspect --format='{{json .State.Health}}' test
{
""Status"": ""unhealthy"",
""FailingStreak"": 3,
""Log"": [
{
""Start"": ""2016-05-25T17:22:04.635478668Z"",
""End"": ""2016-05-25T17:22:04.7272552Z"",
""ExitCode"": 0,
""Output"": "" File: /etc/passwd\n Size: 334 \tBlocks: 8 IO Block: 4096 regular file\nDevice: 32h/50d\tInode: 12 Links: 1\nAccess: (0664/-rw-rw-r--) Uid: ( 0/ root) Gid: ( 0/ root)\nAccess: 2015-12-05 22:05:32.000000000\nModify: 2015...""
},
{
""Start"": ""2016-05-25T17:22:06.732900633Z"",
""End"": ""2016-05-25T17:22:06.822168935Z"",
""ExitCode"": 0,
""Output"": "" File: /etc/passwd\n Size: 334 \tBlocks: 8 IO Block: 4096 regular file\nDevice: 32h/50d\tInode: 12 Links: 1\nAccess: (0664/-rw-rw-r--) Uid: ( 0/ root) Gid: ( 0/ root)\nAccess: 2015-12-05 22:05:32.000000000\nModify: 2015...""
},
{
""Start"": ""2016-05-25T17:22:08.823956535Z"",
""End"": ""2016-05-25T17:22:08.897359124Z"",
""ExitCode"": 1,
""Output"": ""stat: can't stat '/etc/passwd': No such file or directory\n""
},
{
""Start"": ""2016-05-25T17:22:10.898802931Z"",
""End"": ""2016-05-25T17:22:10.969631866Z"",
""ExitCode"": 1,
""Output"": ""stat: can't stat '/etc/passwd': No such file or directory\n""
},
{
""Start"": ""2016-05-25T17:22:12.971033523Z"",
""End"": ""2016-05-25T17:22:13.082015516Z"",
""ExitCode"": 1,
""Output"": ""stat: can't stat '/etc/passwd': No such file or directory\n""
}
]
}
The health status is also displayed in the docker ps
output.
User
The default user within a container is root
(uid = 0). You can set a default
user to run the first process with the Dockerfile USER
instruction. When
starting a container, you can override the USER
instruction by passing the
-u
option.
-u="""", --user="""": Sets the username or UID used and optionally the groupname or GID for the specified command.
The followings examples are all valid:
--user=[ user | user:group | uid | uid:gid | user:gid | uid:group ]
Note
If you pass a numeric user ID, it must be in the range of 0-2147483647. If you pass a username, the user must exist in the container.
Working directory
The default working directory for running binaries within a container is the
root directory (/
). The default working directory of an image is set using
the Dockerfile WORKDIR
command. You can override the default working
directory for an image using the -w
(or --workdir
) flag for the docker run
command:
$ docker run --rm -w /my/workdir alpine pwd
/my/workdir
If the directory doesn't already exist in the container, it's created.",,,
1ed816c949feb764f03d266eafc968da631b0a6497c369a5bee9233f801323b6,"Publish in the Marketplace
Submit your extension to the Marketplace
Docker Desktop displays published extensions in the Extensions Marketplace on Docker Desktop and Docker Hub. The Extensions Marketplace is a space where developers can discover extensions to improve their developer experience and propose their own extension to be available for all Desktop users.
Whenever you are ready to publish your extension in the Marketplace, you can self-publish your extension
Note
As the Extension Marketplace continues to add new features for both Extension users and publishers, you are expected to maintain your extension over time to ensure it stays available in the Marketplace.
Important
The Docker manual review process for extensions is paused at the moment. Submit your extension through the automated submission process
Before you submit
Before you submit your extension, it must pass the validation checks.
It is highly recommended that your extension follows the guidelines outlined in this section before submitting your extension. If you request a review from the Docker Extensions team and have not followed the guidelines, the review process may take longer.
These guidelines don't replace Docker's terms of service or guarantee approval:
- Review the design guidelines
- Ensure the UI styling is in line with Docker Desktop guidelines
- Ensure your extensions support both light and dark mode
- Consider the needs of both new and existing users of your extension
- Test your extension with potential users
- Test your extension for crashes, bugs, and performance issues
- Test your extension on various platforms (Mac, Windows, Linux)
- Read the Terms of Service
Validation process
Submitted extensions go through an automated validation process. If all the validation checks pass successfully, the extension is published on the Marketplace and accessible to all users within a few hours. It is the fastest way to get developers the tools they need and to get feedback from them as you work to evolve/polish your extension.
Important
Docker Desktop caches the list of extensions available in the Marketplace for 12 hours. If you don't see your extension in the Marketplace, you can restart Docker Desktop to force the cache to refresh.",,,
05692381d68bd1b09858b9f377428c05135a330fe19144baff327920e6b712a2,"Networking overview
Container networking refers to the ability for containers to connect to and communicate with each other, or to non-Docker workloads.
Containers have networking enabled by default, and they can make outgoing
connections. A container has no information about what kind of network it's
attached to, or whether their peers are also Docker workloads or not. A
container only sees a network interface with an IP address, a gateway, a
routing table, DNS services, and other networking details. That is, unless the
container uses the none
network driver.
This page describes networking from the point of view of the container,
and the concepts around container networking.
This page doesn't describe OS-specific details about how Docker networks work.
For information about how Docker manipulates iptables
rules on Linux,
see
Packet filtering and firewalls.
User-defined networks
You can create custom, user-defined networks, and connect multiple containers to the same network. Once connected to a user-defined network, containers can communicate with each other using container IP addresses or container names.
The following example creates a network using the bridge
network driver and
running a container in the created network:
$ docker network create -d bridge my-net
$ docker run --network=my-net -itd --name=container3 busybox
Drivers
The following network drivers are available by default, and provide core networking functionality:
| Driver | Description |
|---|---|
bridge | The default network driver. |
host | Remove network isolation between the container and the Docker host. |
none | Completely isolate a container from the host and other containers. |
overlay | Overlay networks connect multiple Docker daemons together. |
ipvlan | IPvlan networks provide full control over both IPv4 and IPv6 addressing. |
macvlan | Assign a MAC address to a container. |
For more information about the different drivers, see Network drivers overview.
Connecting to multiple networks
A container can be connected to multiple networks.
For example, a frontend container may be connected to a bridge network
with external access, and a
--internal
network
to communicate with containers running backend services that do not need
external network access.
A container may also be connected to different types of network. For example,
an ipvlan
network to provide internet access, and a bridge
network for
access to local services.
When sending packets, if the destination is an address in a directly connected
network, packets are sent to that network. Otherwise, packets are sent to
a default gateway for routing to their destination. In the example above,
the ipvlan
network's gateway must be the default gateway.
The default gateway is selected by Docker, and may change whenever a
container's network connections change.
To make Docker choose a specific default gateway when creating the container
or connecting a new network, set a gateway priority. See option gw-priority
for the
docker run
and
docker network connect
commands.
The default gw-priority
is 0
and the gateway in the network with the
highest priority is the default gateway. So, when a network should always
be the default gateway, it is enough to set its gw-priority
to 1
.
$ docker run --network name=gwnet,gw-priority=1 --network anet1 --name myctr myimage
$ docker network connect anet2 myctr
Container networks
In addition to user-defined networks, you can attach a container to another
container's networking stack directly, using the --network container:<name|id>
flag format.
The following flags aren't supported for containers using the container:
networking mode:
--add-host
--hostname
--dns
--dns-search
--dns-option
--mac-address
--publish
--publish-all
--expose
The following example runs a Redis container, with Redis binding to
localhost
, then running the redis-cli
command and connecting to the Redis
server over the localhost
interface.
$ docker run -d --name redis example/redis --bind 127.0.0.1
$ docker run --rm -it --network container:redis example/redis-cli -h 127.0.0.1
Published ports
By default, when you create or run a container using docker create
or docker run
,
containers on bridge networks don't expose any ports to the outside world.
Use the --publish
or -p
flag to make a port available to services
outside the bridge network.
This creates a firewall rule in the host,
mapping a container port to a port on the Docker host to the outside world.
Here are some examples:
| Flag value | Description |
|---|---|
-p 8080:80 | Map port 8080 on the Docker host to TCP port 80 in the container. |
-p 192.168.1.100:8080:80 | Map port 8080 on the Docker host IP 192.168.1.100 to TCP port 80 in the container. |
-p 8080:80/udp | Map port 8080 on the Docker host to UDP port 80 in the container. |
-p 8080:80/tcp -p 8080:80/udp | Map TCP port 8080 on the Docker host to TCP port 80 in the container, and map UDP port 8080 on the Docker host to UDP port 80 in the container. |
Important
Publishing container ports is insecure by default. Meaning, when you publish a container's ports it becomes available not only to the Docker host, but to the outside world as well.
If you include the localhost IP address (
127.0.0.1
, or::1
) with the publish flag, only the Docker host and its containers can access the published container port.$ docker run -p 127.0.0.1:8080:80 -p '[::1]:8080:80' nginx
Warning
Hosts within the same L2 segment (for example, hosts connected to the same network switch) can reach ports published to localhost. For more information, see moby/moby#45610
If you want to make a container accessible to other containers, it isn't necessary to publish the container's ports. You can enable inter-container communication by connecting the containers to the same network, usually a bridge network.
Ports on the host's IPv6 addresses will map to the container's IPv4 address
if no host IP is given in a port mapping, the bridge network is IPv4-only,
and --userland-proxy=true
(default).
For more information about port mapping, including how to disable it and use direct routing to containers, see packet filtering and firewalls.
IP address and hostname
When creating a network, IPv4 address allocation is enabled by default, it
can be disabled using --ipv4=false
. IPv6 address allocation can be enabled
using --ipv6
.
$ docker network create --ipv6 --ipv4=false v6net
By default, the container gets an IP address for every Docker network it attaches to. A container receives an IP address out of the IP subnet of the network. The Docker daemon performs dynamic subnetting and IP address allocation for containers. Each network also has a default subnet mask and gateway.
You can connect a running container to multiple networks,
either by passing the --network
flag multiple times when creating the container,
or using the docker network connect
command for already running containers.
In both cases, you can use the --ip
or --ip6
flags to specify the container's IP address on that particular network.
In the same way, a container's hostname defaults to be the container's ID in Docker.
You can override the hostname using --hostname
.
When connecting to an existing network using docker network connect
,
you can use the --alias
flag to specify an additional network alias for the container on that network.
DNS services
Containers use the same DNS servers as the host by default, but you can
override this with --dns
.
By default, containers inherit the DNS settings as defined in the
/etc/resolv.conf
configuration file.
Containers that attach to the default bridge
network receive a copy of this file.
Containers that attach to a
custom network
use Docker's embedded DNS server.
The embedded DNS server forwards external DNS lookups to the DNS servers configured on the host.
You can configure DNS resolution on a per-container basis, using flags for the
docker run
or docker create
command used to start the container.
The following table describes the available docker run
flags related to DNS
configuration.
| Flag | Description |
|---|---|
--dns | The IP address of a DNS server. To specify multiple DNS servers, use multiple --dns flags. DNS requests will be forwarded from the container's network namespace so, for example, --dns=127.0.0.1 refers to the container's own loopback address. |
--dns-search | A DNS search domain to search non-fully qualified hostnames. To specify multiple DNS search prefixes, use multiple --dns-search flags. |
--dns-opt | A key-value pair representing a DNS option and its value. See your operating system's documentation for resolv.conf for valid options. |
--hostname | The hostname a container uses for itself. Defaults to the container's ID if not specified. |
Custom hosts
Your container will have lines in /etc/hosts
which define the hostname of the
container itself, as well as localhost
and a few other common things. Custom
hosts, defined in /etc/hosts
on the host machine, aren't inherited by
containers. To pass additional hosts into a container, refer to
add entries to
container hosts file in the
docker run
reference documentation.
Proxy server
If your container needs to use a proxy server, see Use a proxy server.",,,
fefa5351626a228e5eb6b9a659447833b7e83bcfad62d93b721f9207948bcc34,"SLSA definitions
BuildKit supports the creation of SLSA Provenance for builds that it runs.
The provenance format generated by BuildKit is defined by the SLSA Provenance format.
This page describes how BuildKit populate each field, and whether the field gets
included when you generate attestations mode=min
and mode=max
.
builder.id
Corresponds to
SLSA builder.id
.
Included with mode=min
and mode=max
.
The builder.id
field is set to the URL of the build, if available.
""builder"": {
""id"": ""https://github.com/docker/buildx/actions/runs/3709599520""
},
This value can be set using the builder-id
attestation parameter.
buildType
Corresponds to
SLSA buildType
.
Included with mode=min
and mode=max
.
The buildType
field is set to https://mobyproject.org/buildkit@v1
can be
used to determine the structure of the provenance content.
""buildType"": ""https://mobyproject.org/buildkit@v1"",
invocation.configSource
Corresponds to
SLSA invocation.configSource
.
Included with mode=min
and mode=max
.
Describes the config that initialized the build.
""invocation"": {
""configSource"": {
""uri"": ""https://github.com/moby/buildkit.git#refs/tags/v0.11.0"",
""digest"": {
""sha1"": ""4b220de5058abfd01ff619c9d2ff6b09a049bea0""
},
""entryPoint"": ""Dockerfile""
},
...
},
For builds initialized from a remote context, like a Git or HTTP URL, this
object defines the context URL and its immutable digest in the uri
and digest
fields.
For builds using a local frontend, such as a Dockerfile, the entryPoint
field defines the path
for the frontend file that initialized the build (filename
frontend option).
invocation.parameters
Corresponds to
SLSA invocation.parameters
.
Partially included with mode=min
.
Describes build inputs passed to the build.
""invocation"": {
""parameters"": {
""frontend"": ""gateway.v0"",
""args"": {
""build-arg:BUILDKIT_CONTEXT_KEEP_GIT_DIR"": ""1"",
""label:FOO"": ""bar"",
""source"": ""docker/dockerfile-upstream:master"",
""target"": ""release""
},
""secrets"": [
{
""id"": ""GIT_AUTH_HEADER"",
""optional"": true
},
...
],
""ssh"": [],
""locals"": []
},
...
},
The following fields are included with both mode=min
and mode=max
:
locals
lists any local sources used in the build, including the build context and frontend file.frontend
defines type of BuildKit frontend used for the build. Currently, this can bedockerfile.v0
orgateway.v0
.args
defines the build arguments passed to the BuildKit frontend.The keys inside the
args
object reflect the options as BuildKit receives them. For example,build-arg
andlabel
prefixes are used for build arguments and labels, andtarget
key defines the target stage that was built. Thesource
key defines the source image for the Gateway frontend, if used.
The following fields are only included with mode=max
:
secrets
defines secrets used during the build. Note that actual secret values are not included.ssh
defines the ssh forwards used during the build.
invocation.environment
Corresponds to
SLSA invocation.environment
.
Included with mode=min
and mode=max
.
""invocation"": {
""environment"": {
""platform"": ""linux/amd64""
},
...
},
The only value BuildKit currently sets is the platform
of the current build
machine. Note that this is not necessarily the platform of the build result that
can be determined from the in-toto
subject field.
materials
Corresponds to
SLSA materials
.
Included with mode=min
and mode=max
.
Defines all the external artifacts that were part of the build. The value depends on the type of artifact:
- The URL of Git repositories containing source code for the image
- HTTP URLs if you are building from a remote tarball, or that was included
using an
ADD
command in Dockerfile - Any Docker images used during the build
The URLs to the Docker images will be in Package URL format.
All the build materials will include the immutable checksum of the artifact. When building from a mutable tag, you can use the digest information to determine if the artifact has been updated compared to when the build ran.
""materials"": [
{
""uri"": ""pkg:docker/alpine@3.17?platform=linux%2Famd64"",
""digest"": {
""sha256"": ""8914eb54f968791faf6a8638949e480fef81e697984fba772b3976835194c6d4""
}
},
{
""uri"": ""https://github.com/moby/buildkit.git#refs/tags/v0.11.0"",
""digest"": {
""sha1"": ""4b220de5058abfd01ff619c9d2ff6b09a049bea0""
}
},
...
],
buildConfig
Corresponds to
SLSA buildConfig
.
Only included with mode=max
.
Defines the build steps performed during the build.
BuildKit internally uses LLB definition to execute the build steps. The LLB
definition of the build steps is defined in buildConfig.llbDefinition
field.
Each LLB step is the JSON definition of the
LLB ProtoBuf API.
The dependencies for a vertex in the LLB graph can be found in the inputs
field for every step.
""buildConfig"": {
""llbDefinition"": [
{
""id"": ""step0"",
""op"": {
""Op"": {
""exec"": {
""meta"": {
""args"": [
""/bin/sh"",
""-c"",
""go build .""
],
""env"": [
""PATH=/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"",
""GOPATH=/go"",
""GOFLAGS=-mod=vendor"",
],
""cwd"": ""/src"",
},
""mounts"": [...]
}
},
""platform"": {...},
},
""inputs"": [
""step8:0"",
""step2:0"",
]
},
...
]
},
metadata.buildInvocationId
Corresponds to
SLSA metadata.buildInvocationId
.
Included with mode=min
and mode=max
.
Unique identifier for the build invocation. When building a multi-platform image with a single build request, this value will be the shared by all the platform versions of the image.
""metadata"": {
""buildInvocationID"": ""rpv7a389uzil5lqmrgwhijwjz"",
...
},
metadata.buildStartedOn
Corresponds to
SLSA metadata.buildStartedOn
.
Included with mode=min
and mode=max
.
Timestamp when the build started.
""metadata"": {
""buildStartedOn"": ""2021-11-17T15:00:00Z"",
...
},
metadata.buildFinishedOn
Corresponds to
SLSA metadata.buildFinishedOn
.
Included with mode=min
and mode=max
.
Timestamp when the build finished.
""metadata"": {
""buildFinishedOn"": ""2021-11-17T15:01:00Z"",
...
},
metadata.completeness
Corresponds to
SLSA metadata.completeness
.
Included with mode=min
and mode=max
.
Defines if the provenance information is complete.
completeness.parameters
is true if all the build arguments are included in the
invocation.parameters
field. When building with min
mode, the build
arguments are not included in the provenance information and parameters are not
complete. Parameters are also not complete on direct LLB builds that did not use
a frontend.
completeness.environment
is always true for BuildKit builds.
completeness.materials
is true if materials
field includes all the
dependencies of the build. When building from un-tracked source in a local
directory, the materials are not complete, while when building from a remote Git
repository all materials can be tracked by BuildKit and completeness.materials
is true.
""metadata"": {
""completeness"": {
""parameters"": true,
""environment"": true,
""materials"": true
},
...
},
metadata.reproducible
Corresponds to
SLSA metadata.reproducible
.
Defines if the build result is supposed to be byte-by-byte reproducible. This
value can be set by the user with the reproducible=true
attestation parameter.
""metadata"": {
""reproducible"": false,
...
},
metadata.https://mobyproject.org/buildkit@v1#hermetic
Included with mode=min
and mode=max
.
This extension field is set to true if the build was hermetic and did not access
the network. In Dockerfiles, a build is hermetic if it does not use RUN
commands or disables network with --network=none
flag.
""metadata"": {
""https://mobyproject.org/buildkit@v1#hermetic"": true,
...
},
metadata.https://mobyproject.org/buildkit@v1#metadata
Partially included with mode=min
.
This extension field defines BuildKit-specific additional metadata that is not part of the SLSA provenance spec.
""metadata"": {
""https://mobyproject.org/buildkit@v1#metadata"": {
""source"": {...},
""layers"": {...},
""vcs"": {...},
},
...
},
source
Only included with mode=max
.
Defines a source mapping of LLB build steps, defined in the
buildConfig.llbDefinition
field, to their original source code (for example,
Dockerfile commands). The source.locations
field contains the ranges of all
the Dockerfile commands ran in an LLB step. source.infos
array contains the
source code itself. This mapping is present if the BuildKit frontend provided it
when creating the LLB definition.
layers
Only included with mode=max
.
Defines the layer mapping of LLB build step mounts defined in
buildConfig.llbDefinition
to the OCI descriptors of equivalent layers. This
mapping is present if the layer data was available, usually when attestation is
for an image or if the build step pulled in image data as part of the build.
vcs
Included with mode=min
and mode=max
.
Defines optional metadata for the version control system used for the build. If
a build uses a remote context from Git repository, BuildKit extracts the details
of the version control system automatically and displays it in the
invocation.configSource
field. But if the build uses a source from a local
directory, the VCS information is lost even if the directory contained a Git
repository. In this case, the build client can send additional vcs:source
and
vcs:revision
build options and BuildKit will add them to the provenance
attestations as extra metadata. Note that, contrary to the
invocation.configSource
field, BuildKit doesn't verify the vcs
values, and
as such they can't be trusted and should only be used as a metadata hint.
Output
To inspect the provenance that was generated and attached to a container image,
you can use the docker buildx imagetools
command to inspect the image in a
registry. Inspecting the attestation displays the format described in the
attestation storage specification.
For example, inspecting a simple Docker image based on alpine:latest
results
in a provenance attestation similar to the following, for a mode=min
build:
{
""_type"": ""https://in-toto.io/Statement/v0.1"",
""predicateType"": ""https://slsa.dev/provenance/v0.2"",
""subject"": [
{
""name"": ""pkg:docker/<registry>/<image>@<tag/digest>?platform=<platform>"",
""digest"": {
""sha256"": ""e8275b2b76280af67e26f068e5d585eb905f8dfd2f1918b3229db98133cb4862""
}
}
],
""predicate"": {
""builder"": {
""id"": """"
},
""buildType"": ""https://mobyproject.org/buildkit@v1"",
""materials"": [
{
""uri"": ""pkg:docker/alpine@latest?platform=linux%2Famd64"",
""digest"": {
""sha256"": ""8914eb54f968791faf6a8638949e480fef81e697984fba772b3976835194c6d4""
}
}
],
""invocation"": {
""configSource"": {
""entryPoint"": ""Dockerfile""
},
""parameters"": {
""frontend"": ""dockerfile.v0"",
""args"": {},
""locals"": [
{
""name"": ""context""
},
{
""name"": ""dockerfile""
}
]
},
""environment"": {
""platform"": ""linux/amd64""
}
},
""metadata"": {
""buildInvocationID"": ""yirbp1aosi1vqjmi3z6bc75nb"",
""buildStartedOn"": ""2022-12-08T11:48:59.466513707Z"",
""buildFinishedOn"": ""2022-12-08T11:49:01.256820297Z"",
""reproducible"": false,
""completeness"": {
""parameters"": true,
""environment"": true,
""materials"": false
},
""https://mobyproject.org/buildkit@v1#metadata"": {}
}
}
}
For a similar build, but with mode=max
:
{
""_type"": ""https://in-toto.io/Statement/v0.1"",
""predicateType"": ""https://slsa.dev/provenance/v0.2"",
""subject"": [
{
""name"": ""pkg:docker/<registry>/<image>@<tag/digest>?platform=<platform>"",
""digest"": {
""sha256"": ""e8275b2b76280af67e26f068e5d585eb905f8dfd2f1918b3229db98133cb4862""
}
}
],
""predicate"": {
""builder"": {
""id"": """"
},
""buildType"": ""https://mobyproject.org/buildkit@v1"",
""materials"": [
{
""uri"": ""pkg:docker/alpine@latest?platform=linux%2Famd64"",
""digest"": {
""sha256"": ""8914eb54f968791faf6a8638949e480fef81e697984fba772b3976835194c6d4""
}
}
],
""invocation"": {
""configSource"": {
""entryPoint"": ""Dockerfile""
},
""parameters"": {
""frontend"": ""dockerfile.v0"",
""args"": {},
""locals"": [
{
""name"": ""context""
},
{
""name"": ""dockerfile""
}
]
},
""environment"": {
""platform"": ""linux/amd64""
}
},
""buildConfig"": {
""llbDefinition"": [
{
""id"": ""step0"",
""op"": {
""Op"": {
""source"": {
""identifier"": ""docker-image://docker.io/library/alpine:latest@sha256:8914eb54f968791faf6a8638949e480fef81e697984fba772b3976835194c6d4""
}
},
""platform"": {
""Architecture"": ""amd64"",
""OS"": ""linux""
},
""constraints"": {}
}
},
{
""id"": ""step1"",
""op"": {
""Op"": null
},
""inputs"": [""step0:0""]
}
]
},
""metadata"": {
""buildInvocationID"": ""46ue2x93k3xj5l463dektwldw"",
""buildStartedOn"": ""2022-12-08T11:50:54.953375437Z"",
""buildFinishedOn"": ""2022-12-08T11:50:55.447841328Z"",
""reproducible"": false,
""completeness"": {
""parameters"": true,
""environment"": true,
""materials"": false
},
""https://mobyproject.org/buildkit@v1#metadata"": {
""source"": {
""locations"": {
""step0"": {
""locations"": [
{
""ranges"": [
{
""start"": {
""line"": 1
},
""end"": {
""line"": 1
}
}
]
}
]
}
},
""infos"": [
{
""filename"": ""Dockerfile"",
""data"": ""RlJPTSBhbHBpbmU6bGF0ZXN0Cg=="",
""llbDefinition"": [
{
""id"": ""step0"",
""op"": {
""Op"": {
""source"": {
""identifier"": ""local://dockerfile"",
""attrs"": {
""local.differ"": ""none"",
""local.followpaths"": ""[\""Dockerfile\"",\""Dockerfile.dockerignore\"",\""dockerfile\""]"",
""local.session"": ""q2jnwdkas0i0iu4knchd92jaz"",
""local.sharedkeyhint"": ""dockerfile""
}
}
},
""constraints"": {}
}
},
{
""id"": ""step1"",
""op"": {
""Op"": null
},
""inputs"": [""step0:0""]
}
]
}
]
},
""layers"": {
""step0:0"": [
[
{
""mediaType"": ""application/vnd.oci.image.layer.v1.tar+gzip"",
""digest"": ""sha256:c158987b05517b6f2c5913f3acef1f2182a32345a304fe357e3ace5fadcad715"",
""size"": 3370706
}
]
]
}
}
}
}
}",,,
dc1f942d7df02e2409a88183f0b4a8d1a5b5458d2de1bbeceda9b3642f555e5f,"Create an exception using the VEX
Vulnerability Exploitability eXchange (VEX) is a standard format for documenting vulnerabilities in the context of a software package or product. Docker Scout supports VEX documents to create exceptions for vulnerabilities in images.
Note
You can also create exceptions using the Docker Scout Dashboard or Docker Desktop. The GUI provides a user-friendly interface for creating exceptions, and it's easy to manage exceptions for multiple images. It also lets you create exceptions for multiple images, or your entire organization, all at once. For more information, see Create an exception using the GUI.
Prerequisites
To create exceptions using OpenVEX documents, you need:
- The latest version of Docker Desktop or the Docker Scout CLI plugin
- The
vexctl
command line tool. - The containerd image store must be enabled
- Write permissions to the registry repository where the image is stored
Introduction to VEX
The VEX standard is defined by a working group by the United States Cybersecurity and Infrastructure Security Agency (CISA). At the core of VEX are exploitability assessments. These assessments describe the status of a given CVE for a product. The possible vulnerability statuses in VEX are:
- Not affected: No remediation is required regarding this vulnerability.
- Affected: Actions are recommended to remediate or address this vulnerability.
- Fixed: These product versions contain a fix for the vulnerability.
- Under investigation: It is not yet known whether these product versions are affected by the vulnerability. An update will be provided in a later release.
There are multiple implementations and formats of VEX. Docker Scout supports the OpenVex implementation. Regardless of the specific implementation, the core idea is the same: to provide a framework for describing the impact of vulnerabilities. Key components of VEX regardless of implementation includes:
- VEX document
- A type of security advisory for storing VEX statements. The format of the document depends on the specific implementation.
- VEX statement
- Describes the status of a vulnerability in a product, whether it's exploitable, and whether there are ways to remediate the issue.
- Justification and impact
- Depending on the vulnerability status, statements include a justification or impact statement describing why a product is or isn't affected.
- Action statements
- Describe how to remediate or mitigate the vulnerability.
vexctl
example
The following example command creates a VEX document stating that:
- The software product described by this VEX document is the Docker image
example/app:v1
- The image contains the npm package
express@4.17.1
- The npm package is affected by a known vulnerability:
CVE-2022-24999
- The image is unaffected by the CVE, because the vulnerable code is never executed in containers that run this image
$ vexctl create \
--author=""author@example.com"" \
--product=""pkg:docker/example/app@v1"" \
--subcomponents=""pkg:npm/express@4.17.1"" \
--vuln=""CVE-2022-24999"" \
--status=""not_affected"" \
--justification=""vulnerable_code_not_in_execute_path"" \
--file=""CVE-2022-24999.vex.json""
Here's a description of the options in this example:
--author
- The email of the author of the VEX document.
--product
- Package URL (PURL) of the Docker image. A PURL is an identifier
for the image in a standardized format, defined in the PURL
specification.
Docker image PURL strings begin with a
pkg:docker
type prefix, followed by the image repository and version (the image tag or SHA256 digest). Unlike image tags, where the version is specified likeexample/app:v1
, in PURL the image repository and version are separated by an@
. --subcomponents
- PURL of the vulnerable package in the image. In this example, the
vulnerability exists in an npm package, so the
--subcomponents
PURL is the identifier for the npm package name and version (pkg:npm/express@4.17.1
).If the same vulnerability exists in multiple packages,
vexctl
lets you specify the--subcomponents
flag multiple times for a singlecreate
command.You can also omit
--subcomponents
, in which case the VEX statement applies to the entire image. --vuln
- ID of the CVE that the VEX statement addresses.
--status
- This is the status label of the vulnerability. This describes the
relationship between the software (
--product
) and the CVE (--vuln
). The possible values for the status label in OpenVEX are:not_affected
affected
fixed
under_investigation
In this example, the VEX statement asserts that the Docker image is
not_affected
by the vulnerability. Thenot_affected
status is the only status that results in CVE suppression, where the CVE is filtered out of the analysis results. The other statuses are useful for documentation purposes, but they do not work for creating exceptions. For more information about all the possible status labels, see Status Labels in the OpenVEX specification. --justification
- Justifies the
not_affected
status label, informing why the product is not affected by the vulnerability. In this case, the justification given isvulnerable_code_not_in_execute_path
, signalling that the vulnerability can't be executed as used by the product.In OpenVEX, status justifications can have one of the five possible values:
component_not_present
vulnerable_code_not_present
vulnerable_code_not_in_execute_path
vulnerable_code_cannot_be_controlled_by_adversary
inline_mitigations_already_exist
For more information about these values and their definitions, see Status Justifications in the OpenVEX specification.
--file
- Filename of the VEX document output
Example JSON document
Here's the OpenVEX JSON generated by this command:
{
""@context"": ""https://openvex.dev/ns/v0.2.0"",
""@id"": ""https://openvex.dev/docs/public/vex-749f79b50f5f2f0f07747c2de9f1239b37c2bda663579f87a35e5f0fdfc13de5"",
""author"": ""author@example.com"",
""timestamp"": ""2024-05-27T13:20:22.395824+02:00"",
""version"": 1,
""statements"": [
{
""vulnerability"": {
""name"": ""CVE-2022-24999""
},
""timestamp"": ""2024-05-27T13:20:22.395829+02:00"",
""products"": [
{
""@id"": ""pkg:docker/example/app@v1"",
""subcomponents"": [
{
""@id"": ""pkg:npm/express@4.17.1""
}
]
}
],
""status"": ""not_affected"",
""justification"": ""vulnerable_code_not_in_execute_path""
}
]
}
Understanding how VEX documents are supposed to be structured can be a bit of a mouthful. The OpenVEX specification describes the format and all the possible properties of documents and statements. For the full details, refer to the specification to learn more about the available fields and how to create a well-formed OpenVEX document.
To learn more about the available flags and syntax of the vexctl
CLI tool and
how to install it, refer to the
vexctl
GitHub repository.
Verifying VEX documents
To test whether the VEX documents you create are well-formed and produce the
expected results, use the docker scout cves
command with the --vex-location
flag to apply a VEX document to a local image analysis using the CLI.
The following command invokes a local image analysis that incorporates all VEX
documents in the specified location, using the --vex-location
flag. In this
example, the CLI is instructed to look for VEX documents in the current working
directory.
$ docker scout cves <IMAGE> --vex-location .
The output of the docker scout cves
command displays the results with any VEX
statements found in under the --vex-location
location factored into the
results. For example, CVEs assigned a status of not_affected
are filtered out
from the results. If the output doesn't seem to take the VEX statements into
account, that's an indication that the VEX documents might be invalid in some
way.
Things to look out for include:
- The PURL of a Docker image must begin with
pkg:docker/
followed by the image name. - In a Docker image PURL, the image name and version is separated by
@
. An image namedexample/myapp:1.0
has the following PURL:pkg:docker/example/myapp@1.0
. - Remember to specify an
author
(it's a mandatory field in OpenVEX) - The
OpenVEX specification describes how
and when to use
justification
,impact_statement
, and other fields in the VEX documents. Specifying these in an incorrect way results in an invalid document. Make sure your VEX documents comply with the OpenVEX specification.
Attach VEX documents to images
When you've created a VEX document, you can attach it to your image in the following ways:
- Attach the document as an attestation
- Embed the document in the image filesystem
You can't remove a VEX document from an image once it's been added. For documents attached as attestations, you can create a new VEX document and attach it to the image again. Doing so will overwrite the previous VEX document (but it won't remove the attestation). For images where the VEX document has been embedded in the image's filesystem, you need to rebuild the image to change the VEX document.
Attestation
To attach VEX documents as an attestation, you can use the docker scout attestation add
CLI command. Using attestations is the recommended option for
attaching exceptions to images when using VEX.
You can attach attestations to images that have already been pushed to a registry. You don't need to build or push the image again. Additionally, having the exceptions attached to the image as attestations means consumers can inspect the exceptions for an image, directly from the registry.
To attach an attestation to an image:
Build the image and push it to a registry.
$ docker build --provenance=true --sbom=true --tag <IMAGE> --push .
Attach the exception to the image as an attestation.
$ docker scout attestation add \ --file <cve-id>.vex.json \ --predicate-type https://openvex.dev/ns/v0.2.0 \ <IMAGE>
The options for this command are:
--file
: the location and filename of the VEX document--predicate-type
: the in-totopredicateType
for OpenVEX
Image filesystem
Embedding VEX documents directly on the image filesystem is a good option if
you know the exceptions ahead of time, before you build the image. And it's
relatively easy; just COPY
the VEX document to the image in your Dockerfile.
The downside with this approach is that you can't change or update the exception later. Image layers are immutable, so anything you put in the image's filesystem is there forever. Attaching the document as an attestation provides better flexibility.
Note
VEX documents embedded in the image filesystem are not considered for images that have attestations. If your image has any attestations, Docker Scout will only look for exceptions in the attestations, and not in the image filesystem.
If you want to use the VEX document embedded in the image filesystem, you must remove the attestation from the image. Note that provenance attestations may be added automatically for images. To ensure that no attestations are added to the image, you can explicitly disable both SBOM and provenance attestations using the
--provenance=false
and--sbom=false
flags when building the image.
To embed a VEX document on the image filesystem, COPY
the file into the image
as part of the image build. The following example shows how to copy all VEX
documents under .vex/
in the build context, to /var/lib/db
in the image.
# syntax=docker/dockerfile:1
FROM alpine
COPY .vex/* /var/lib/db/
The filename of the VEX document must match the *.vex.json
glob pattern.
It doesn't matter where on the image's filesystem you store the file.
Note that the copied files must be part of the filesystem of the final image, For multi-stage builds, the documents must persist in the final stage.",,,
2cd5f2aa2f9dc6098ba7400ae117b7a24fd79c9b30802c658a029a2a19cc7431,"Docker Hub pull usage and limits
Note
Starting April 1, 2025, all users with a Pro, Team, or Business subscription will have unlimited Docker Hub pulls with fair use. Unauthenticated users and users with a free Personal account have the following pull limits:
- Unauthenticated users: 10 pulls/hour
- Authenticated users with a free account: 100 pulls/hour
Unauthenticated and Docker Personal users are subject to hourly pull rate limits on Docker Hub. In contrast, Docker Pro, Team, and Business users benefit from unlimited pulls per hour.
The following pull usage and limits apply based on your subscription, subject to fair use:
| User type | Pull rate limit per hour |
|---|---|
| Business (authenticated) | Unlimited |
| Team (authenticated) | Unlimited |
| Pro (authenticated) | Unlimited |
| Personal (authenticated) | 100 |
| Unauthenticated Users | 10 per IPv4 address or IPv6 /64 subnet |
Pull definition
A pull is defined as the following:
- A Docker pull includes both a version check and any download that
occurs as a result of the pull. Depending on the client, a
docker pull
can verify the existence of an image or tag without downloading it by performing a version check. - Version checks do not count towards usage pricing.
- A pull for a normal image makes one pull for a single manifest.
- A pull for a multi-arch image will count as one pull for each different architecture.
Pull attribution
Pulls from authenticated users can be attributed to either a personal or an organization namespace.
Attribution is based on the following:
- Private pulls: Pulls for private repositories are attributed to the repository's namespace owner.
- Public pulls: When pulling images from a public repository, attribution is determined based on domain affiliation and organization membership.
- Verified domain ownership: When pulling an image from an account linked to a verified domain, the attribution is set to be the owner of that domain.
- Single organization membership:
- If the owner of the verified domain is a company and the user is part of only one organization within that company, the pull is attributed to that specific organization.
- If the user is part of only one organization, the pull is attributed to that specific organization.
- Multiple organization memberships: If the user is part of multiple organizations under the company, the pull is attributed to the user's personal namespace.
When pulling Docker Verified Publisher images, attribution towards rate limiting is not applied. For more details, see Docker Verified Publisher Program.
Authentication
To ensure correct attribution of your pulls, you must authenticate with Docker Hub. The following sections provide information on how to sign in to Docker Hub to authenticate your pulls.
Docker Desktop
If you are using Docker Desktop, you can sign in to Docker Hub from the Docker Desktop menu.
Select Sign in / Create Docker ID from the Docker Desktop menu and follow the on-screen instructions to complete the sign-in process.
Docker Engine
If you're using a standalone version of Docker Engine, run the docker login
command from a terminal to authenticate with Docker Hub. For information on how
to use the command, see
docker login.
Docker Swarm
If you're running Docker Swarm, you must use the --with-registry-auth
flag to
authenticate with Docker Hub. For more information, see
Create a
service. If you
are using a Docker Compose file to deploy an application stack, see
docker
stack deploy.
GitHub Actions
If you're using GitHub Actions to build and push Docker images to Docker Hub, see login action. If you are using another Action, you must add your username and access token in a similar way for authentication.
Kubernetes
If you're running Kubernetes, follow the instructions in Pull an Image from a Private Registry for information on authentication.
Third-party platforms
If you're using any third-party platforms, follow your provider’s instructions on using registry authentication.
Note
When pulling images via a third-party platform, the platform may use the same IPv4 address or IPv6 /64 subnet to pull images for multiple users. Even if you are authenticated, pulls attributed to a single IPv4 address or IPv6 /64 subnet may cause abuse rate limiting.
- Artifactory
- AWS CodeBuild
- AWS ECS/Fargate
- Azure Pipelines
- Chipper CI
- CircleCI
- Codefresh
- Drone.io
- GitLab
- LayerCI
- TeamCity
View monthly pulls and included usage
You can view your monthly pulls on the Usage page in Docker Hub.
On that page, you can also send a report to your email that contains a comma separated file with the following detailed information.
| CSV column | Definition | Usage guidance |
|---|---|---|
datehour | The date and hour (yyyy/mm/dd/hh ) of the pull that resulted in the data transfer. | This helps in identifying peak usage times and patterns. |
user_name | The Docker ID of the user that pulled the image | This lets organization owners track data consumption per user and manage resources effectively. |
repository | The name of the repository of the image that was pulled. | This lets you identify which repositories are most frequently accessed and consume most of the data transfer. |
access_token_name | Name of the access token that was used for authentication with Docker CLI. generated tokens are automatically generated by the Docker client when a user signs in. | Personal access tokens are usually used to authenticate automated tools (Docker Desktop, CI/CD tools, etc.). This is useful for identifying which automated system issued the pull. |
ips | The IP address that was used to pull the image. This field is aggregated, so more than one IP address may appear, representing all the IPs used to pull an image within the same date and hour. | This helps you understand the origin of the data transfer, which is useful for diagnosing and identifying patterns in automated or manual pulls. |
repository_privacy | The privacy state of the image repository that was pulled. This can either be public or private . | This distinguishes between public and private repositories to identify which data transfer threshold the pull impacts. |
tag | The tag for the image. The tag is only available if the pull included a tag. | This helps in identifying the image. Tags are often used to identify specific versions or variants of an image. |
digest | The unique image digest for the image. | This helps in identifying the image. |
version_checks | The number of version checks accumulated for the date and hour of each image repository. Depending on the client, a pull can do a version check to verify the existence of an image or tag without downloading it. | This helps identify the frequency of version checks, which you can use to analyze usage trends and potential unexpected behaviors. |
pulls | The number of pulls accumulated for the date and hour of each image repository. | This helps identify the frequency of repository pulls, which you can use to analyze usage trends and potential unexpected behaviors. |
View hourly pull rate and limit
The pull rate limit is calculated on a per hour basis. There is no pull rate limit for users or automated systems with a paid subscription. Unauthenticated and Docker Personal users using Docker Hub will experience rate limits on image pulls.
When you issue a pull and you are over the limit, Docker Hub returns a
429
response code with the following body when the manifest is requested:
You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limits
This error message appears in the Docker CLI or in the Docker Engine logs.
To view your current pull rate and limit:
Note
To check your limits, you need
curl
,grep
, andjq
installed.
Get a token.
To get a token anonymously, if you are pulling anonymously:
$ TOKEN=$(curl ""https://auth.docker.io/token?service=registry.docker.io&scope=repository:ratelimitpreview/test:pull"" | jq -r .token)
To get a token with a user account, if you are authenticated, insert your username and password in the following command:
$ TOKEN=$(curl --user 'username:password' ""https://auth.docker.io/token?service=registry.docker.io&scope=repository:ratelimitpreview/test:pull"" | jq -r .token)
Get the headers that contain your limits. These headers are returned on both GET and HEAD requests. Using GET emulates a real pull and counts towards the limit. Using HEAD won't.
$ curl --head -H ""Authorization: Bearer $TOKEN"" https://registry-1.docker.io/v2/ratelimitpreview/test/manifests/latest
Examine the headers. You should see the following headers.
ratelimit-limit: 100;w=3600 ratelimit-remaining: 20;w=3600 docker-ratelimit-source: 192.0.2.1
In the previous example, the pull limit is 100 pulls per 3600 seconds (1 hour), and there are 20 pulls remaining.
If you don't see any
ratelimit
header, it could be because the image or your IP is unlimited in partnership with a publisher, provider, or an open source organization. It could also mean that the user you are pulling as is part of a paid Docker plan. Pulling that image won't count toward pull rate limits if you don't see these headers.",,,
e61e3a2c878a99365fef9fc9fa6561fcd772ca99605c97eea3e5b7dd52ed90f2,"Explore the Containers view in Docker Desktop
The Containers view lists all your running containers and applications. You must have running or stopped containers and applications to see them listed.
Container actions
Use the Search field to search for any specific container.
From the Containers view you can perform the following actions:
- Pause/Resume
- Stop/Start/Restart
- View image packages and CVEs
- Delete
- Open the application in VS code
- Open the port exposed by the container in a browser
- Copy docker run. This lets you share container run details or modify certain parameters.
Resource usage
From the Containers view you can monitor your containers' CPU and memory usage over time. This can help you understand if something is wrong with your containers or if you need to allocate additional resources.
When you inspect a container, the Stats tab displays further information about a container's resource utilization. You can see how much CPU, memory, network and disk space your container is using over time.
Inspect a container
You can obtain detailed information about the container when you select it.
From here, you can use the quick action buttons to perform various actions such as pause, resume, start or stop, or explore the Logs, Inspect, Bind mounts, Exec, Files, and Stats tabs.
Logs
Select Logs to see logs from the container. You can also:
- Use
Cmd + f
/Ctrl + f
to open the search bar and find specific entries. Search matches are highlighted in yellow. - Press
Enter
orShift + Enter
to jump to the next or previous search match respectively. - Use the Copy icon in the top right-hand corner to copy all the logs to your clipboard.
- Automatically copy any logs content by highlighting a few lines or a section of the logs.
- Use the Clear terminal icon in the top right-hand corner to clear the logs terminal.
- Select and view external links that may be in your logs.
Inspect
Select Inspect to view low-level information about the container. It displays the local path, version number of the image, SHA-256, port mapping, and other details.
Integrated terminal
From the Exec tab, you can use the integrated terminal, on a running container, directly within Docker Desktop. You are able to quickly run commands within your container so you can understand its current state or debug when something goes wrong.
Using the integrated terminal is the same as running one of the following commands:
docker exec -it <container-id> /bin/sh
docker exec -it <container-id> cmd.exe
when accessing Windows containersdocker debug <container-id>
when using debug mode
The integrated terminal:
- Persists your session and Debug mode setting if you navigate to another part of the Docker Desktop Dashboard and then return.
- Supports copy, paste, search, and clearing your session.
- When not using debug mode, it automatically detects the default user for a
running container from the image's Dockerfile. If no user is specified, or
you're using debug mode, it defaults to
root
.
Open the integrated terminal
To open the integrated terminal, either:
- Hover over your running container and under the Actions column, select the Show container actions menu. From the drop-down menu, select Open in terminal.
- Or, select the container and then select the Exec tab.
To use your external terminal, navigate to the General tab in Settings and select the System default option under Choose your terminal.
Open the integrated terminal in debug mode
Debug mode requires a Pro, Team, or Business subscription. Debug mode has several advantages, such as:
- A customizable toolbox. The toolbox comes with many standard Linux tools
pre-installed, such as
vim
,nano
,htop
, andcurl
. For more details, see thedocker debug
CLI reference. - The ability to access containers that don't have a shell, for example, slim or distroless containers.
To open the integrated terminal in debug mode:
Sign in to Docker Desktop with an account that has a Pro, Team, or Business subscription.
After you're signed in, either:
- Hover over your running container and under the Actions column, select the Show container actions menu. From the drop-down menu, select Use Docker Debug.
- Or, select the container and then select the Debug tab. If the Debug tab isn't visible, select the Exec tab and then enable the Debug mode setting.
To use debug mode by default when accessing the integrated terminal, navigate to the General tab in Settings and select the Enable Docker Debug by default option.
Files
Select Files to explore the filesystem of running or stopped containers. You can also:
- See which files have been recently added, modified, or deleted
- Edit a file straight from the built-in editor
- Drag and drop files and folders between the host and the container
- Delete unnecessary files when you right-click on a file
- Download files and folders from the container straight to the host",,,
8131220fddeca1be409e606dd9581cbe5e913e6b5cb2b163fcaf47acd4cb77a0,"Organization settings
Table of contents
This section describes how to manage organization settings in the Docker Admin Console.
Configure general information
General organization information appears on your organization landing page in the Admin Console.
This information includes:
- Organization Name
- Company
- Location
- Website
- Gravatar email: To add an avatar to your Docker account, create a Gravatar account and create your avatar. Next, add your Gravatar email to your Docker account settings. It may take some time for your avatar to update in Docker.
To edit this information:
- Sign in to the Admin Console.
- Select your company on the Choose profile page.
- Under Organization settings, select General.
- Specify the organization information and select Save.
Next steps
In the Organization settings menu, you can also configure SSO and set up SCIM. If your organization isn't part of a company, from here you can also audit your domains or create a company.",,,
feaabb0ca110d838e6a595789f293b488663569083dc81f1fc0eee5efd7c8e5e,"Set up a dev environment
Important
Dev Environments is no longer under active development.
While the current functionality remains available, it may take us longer to respond to support requests.
Changes to Dev Environments with Docker Desktop 4.13
Docker has simplified how you configure your dev environment project. All you need to get started is a
compose-dev.yaml
file. If you have an existing project with a.docker/
folder this is automatically migrated the next time you launch.If you are using
.docker/docker-compose.yaml
, we move it to../compose-dev.yaml
. If you are using.docker/config.json
, we create a../compose-dev.yaml
file with a single service named ""app”. It is configured to use the image or Dockerfile referenced in the JSON as a starting point.
To set up a dev environment, there are additional configuration steps to tell Docker Desktop how to build, start, and use the right image for your services.
Dev Environments use a compose-dev.yaml
file located at the root of your project. This file allows you to define the image required for a dedicated service, the ports you'd like to expose, along with additional configuration options.
The following is an example compose-dev.yaml
file.
version: ""3.7""
services:
backend:
build:
context: backend
target: development
secrets:
- db-password
depends_on:
- db
db:
image: mariadb
restart: always
healthcheck:
test: [ ""CMD"", ""mysqladmin"", ""ping"", ""-h"", ""127.0.0.1"", ""--silent"" ]
interval: 3s
retries: 5
start_period: 30s
secrets:
- db-password
volumes:
- db-data:/var/lib/mysql
environment:
- MYSQL_DATABASE=example
- MYSQL_ROOT_PASSWORD_FILE=/run/secrets/db-password
expose:
- 3306
proxy:
build: proxy
ports:
- 8080:80
depends_on:
- backend
volumes:
db-data:
secrets:
db-password:
file: db/password.txt
In the yaml file, the build context backend
specifies that that the container should be built using the development
stage (target
attribute) of the Dockerfile located in the backend
directory (context
attribute)
The development
stage of the Dockerfile is defined as follows:
# syntax=docker/dockerfile:1
FROM golang:1.16-alpine AS build
WORKDIR /go/src/github.com/org/repo
COPY . .
RUN go build -o server .
FROM build AS development
RUN apk update \
&& apk add git
CMD [""go"", ""run"", ""main.go""]
FROM alpine:3.12
EXPOSE 8000
COPY --from=build /go/src/github.com/org/repo/server /server
CMD [""/server""]
The development
target uses a golang:1.16-alpine
image with all dependencies you need for development. You can start your project directly from VS Code and interact with the others applications or services such as the database or the frontend.
In the example, the Docker Compose files are the same. However, they could be different and the services defined in the main Compose file may use other targets to build or directly reference other images.
What's next?
Learn how to distribute your dev environment",,,
0584126546206e0945cd18c618622d3c333fa8b82d572b91dec061228a967957,"Dashboard
The Docker Scout Dashboard helps you share the analysis of images in an organization with your team. Developers can now see an overview of their security status across all their images from both Docker Hub and Artifactory, and get remediation advice at their fingertips. It helps team members in roles such as security, compliance, and operations to know what vulnerabilities and issues they need to focus on.
Overview
The Overview tab provides a summary for the repositories in the selected organization.
At the top of this page, you can select which Environment to view. By default, the most recently pushed images are shown. To learn more about environments, see Environment monitoring.
The Policy boxes show your current compliance rating for each policy, and a trend indication for the selected environment. The trend describes the policy delta for the most recent images compared to the previous version. For more information about policies, see Policy Evaluation.
The vulnerability chart shows the total number of vulnerabilities for images in the selected environment over time. You can configure the timescale for the chart using the drop-down menu.
Use the header menu at the top of the website to access the different main sections of the Docker Scout Dashboard:
- Policies: shows the policy compliance for the organization, see Policies
- Images: lists all Docker Scout-enabled repositories in the organization, see Images
- Base images: lists all base images used by repositories in an organization
- Packages: lists all packages across repositories in the organization
- Vulnerabilities: lists all CVEs in the organization's images, see Vulnerabilities
- Integrations: create and manage third-party integrations, see Integrations
- Settings: manage repository settings, see Settings
Policies
The Policies view shows a breakdown of policy compliance for all of the images in the selected organization and environment. You can use the Image drop-down menu to view a policy breakdown for a specific environment.
For more information about policies, see Policy Evaluation.
Images
The Images view shows all images in Scout-enabled repositories for the selected environment. You can filter the list by selecting a different environment, or by repository name using the text filter.
For each repository, the list displays the following details:
- The repository name (image reference without the tag or digest)
- The most recent tag of the image in the selected environment
- Operating systems and architectures for the most recent tag
- Vulnerabilities status for the most recent tag
- Policy status for the most recent tag
Selecting a repository link takes you to a list of all images in that repository that have been analyzed. From here you can view the full analysis results for a specific image, and compare tags to view the differences in packages and vulnerabilities
Selecting an image link takes you to a details view for the selected tag or digest. This view contains two tabs that detail the composition and policy compliance for the image:
Policy status shows the policy evaluation results for the selected image. Here you also have links for details about the policy violations.
For more information about policy, see Policy Evaluation.
Image layers shows a breakdown of the image analysis results. You can get a complete view of the vulnerabilities your image contains and understand how they got in.
Vulnerabilities
The Vulnerabilities view shows a list of all vulnerabilities for images in the organization. This list includes details about CVE such as the severity and Common Vulnerability Scoring System (CVSS) score, as well as whether there's a fix version available. The CVSS score displayed here is the highest score out of all available sources.
Selecting the links on this page opens the vulnerability details page, This page is a publicly visible page, and shows detailed information about a CVE. You can share the link to a particular CVE description with other people even if they're not a member of your Docker organization or signed in to Docker Scout.
If you are signed in, the My images tab on this page lists all of your images affected by the CVE.
Integrations
The Integrations page lets you create and manage your Docker Scout integrations, such as environment integrations and registry integrations. For more information on how to get started with integrations, see Integrating Docker Scout with other systems.
Settings
The settings menu in the Docker Scout Dashboard contains:
- Repository settings for enabling and disabling repositories
- Notifications for managing your notification preferences for Docker Scout.
Repository settings
When you enable Docker Scout for a repository, Docker Scout analyzes new tags automatically when you push to that repository. To enable repositories in Amazon ECR, Azure ACR, or other third-party registries, you first need to integrate them. See Container registry integrations
Notification settings
The Notification settings page is where you can change the preferences for receiving notifications from Docker Scout. Notification settings are personal, and changing notification settings only affects your personal account, not the entire organization.
The purpose of notifications in Docker Scout is to raise awareness about upstream changes that affect you. Docker Scout will notify you about when a new vulnerability is disclosed in a security advisory, and it affects one or more of your images. You will not receive notifications about changes to vulnerability exposure or policy compliance as a result of pushing a new image.
Note
Notifications are only triggered for the last pushed image tags for each repository. ""Last pushed"" refers to the image tag that was most recently pushed to the registry and analyzed by Docker Scout. If the last pushed image is not affected by a newly disclosed CVE, then no notification will be triggered.
The available notification settings are:
Repository scope
Here you can select whether you want to enable notifications for all repositories, or only for specific repositories. These settings apply to the currently selected organization, and can be changed for each organization you are a member of.
- All repositories: select this option to receive notifications for all repositories that you have access to.
- Specific repositories: select this option to receive notifications for specific repositories. You can then enter the names of repositories you want to receive notifications for.
Delivery preferences
These settings control how you receive notifications from Docker Scout. They apply to all organizations that you're a member of.
- Notification pop-ups: select this check-box to receive notification pop-up messages in the Docker Scout Dashboard.
- OS notifications: select this check-box to receive OS-level notifications from your browser if you have the Docker Scout Dashboard open in a browser tab.
To enable OS notifications, Docker Scout needs permissions to send notifications using the browser API.
From this page, you can also go to the settings for Team collaboration integrations, such as the Slack integration.
You can also configure your notification settings in Docker Desktop by going to Settings > Notifications.",,,
e987f72088ab641f2297f5446f11b314202ab19b604566c97204b76c9ed3e62c,"Advanced options for autobuild and autotest
Note
Automated builds require a Docker Pro, Team, or Business subscription.
The following options allow you to customize your automated build and automated test processes.
Environment variables for building and testing
Several utility environment variables are set by the build process, and are available during automated builds, automated tests, and while executing hooks.
Note
These environment variables are only available to the build and test processes and don't affect your service's run environment.
SOURCE_BRANCH
: the name of the branch or the tag that is currently being tested.SOURCE_COMMIT
: the SHA1 hash of the commit being tested.COMMIT_MSG
: the message from the commit being tested and built.DOCKER_REPO
: the name of the Docker repository being built.DOCKERFILE_PATH
: the dockerfile currently being built.DOCKER_TAG
: the Docker repository tag being built.IMAGE_NAME
: the name and tag of the Docker repository being built. (This variable is a combination ofDOCKER_REPO
:DOCKER_TAG
.)
If you are using these build environment variables in a
docker-compose.test.yml
file for automated testing, declare them in your sut
service's environment as shown below.
services:
sut:
build: .
command: run_tests.sh
environment:
- SOURCE_BRANCH
Override build, test or push commands
Docker Hub allows you to override and customize the build
, test
and push
commands during automated build and test processes using hooks. For example, you
might use a build hook to set build arguments used only during the build
process. You can also set up
custom build phase hooks
to perform actions in between these commands.
Important
Use these hooks with caution. The contents of these hook files replace the basic
docker
commands, so you must include a similar build, test or push command in the hook or your automated process does not complete.
To override these phases, create a folder called hooks
in your source code
repository at the same directory level as your Dockerfile. Create a file called
hooks/build
, hooks/test
, or hooks/push
and include commands that the
builder process can execute, such as docker
and bash
commands (prefixed
appropriately with #!/bin/bash
).
These hooks run on an instance of
Ubuntu,
which includes interpreters
such as Perl or Python, and utilities such as git
or curl
. Refer to the
Ubuntu documentation
for the full list of available interpreters and utilities.
Custom build phase hooks
You can run custom commands between phases of the build process by creating hooks. Hooks allow you to provide extra instructions to the autobuild and autotest processes.
Create a folder called hooks
in your source code repository at the same
directory level as your Dockerfile. Place files that define the hooks in that
folder. Hook files can include both docker
commands, and bash
commands as
long as they are prefixed appropriately with #!/bin/bash
. The builder executes
the commands in the files before and after each step.
The following hooks are available:
hooks/post_checkout
hooks/pre_build
hooks/post_build
hooks/pre_test
hooks/post_test
hooks/pre_push
(only used when executing a build rule or Automated build )hooks/post_push
(only used when executing a build rule or Automated build )
Build hook examples
Override the ""build"" phase to set variables
Docker Hub allows you to define build environment variables either in the hook files, or from the automated build interface, which you can then reference in hooks.
The following example defines a build hook that uses docker build
arguments to
set the variable CUSTOM
based on the value of variable defined using the
Docker Hub build settings. $DOCKERFILE_PATH
is a variable that you provide
with the name of the Dockerfile you want to build, and $IMAGE_NAME
is the name
of the image being built.
$ docker build --build-arg CUSTOM=$VAR -f $DOCKERFILE_PATH -t $IMAGE_NAME .
Important
A
hooks/build
file overrides the basicdocker build
command used by the builder, so you must include a similar build command in the hook or the automated build fails.
Refer to the docker build documentation to learn more about Docker build-time variables.
Push to multiple repositories
By default the build process pushes the image only to the repository where the
build settings are configured. If you need to push the same image to multiple
repositories, you can set up a post_push
hook to add additional tags and push
to more repositories.
$ docker tag $IMAGE_NAME $DOCKER_REPO:$SOURCE_COMMIT
$ docker push $DOCKER_REPO:$SOURCE_COMMIT
Source repository or branch clones
When Docker Hub pulls a branch from a source code repository, it performs a shallow clone, it clones only the tip of the specified branch. This has the advantage of minimizing the amount of data transfer necessary from the repository and speeding up the build because it pulls only the minimal code necessary.
As a result, if you need to perform a custom action that relies on a different
branch, such as a post_push
hook, you can't checkout that branch unless
you do one of the following:
You can get a shallow checkout of the target branch by doing the following:
$ git fetch origin branch:mytargetbranch --depth 1
You can also ""unshallow"" the clone, which fetches the whole Git history (and potentially takes a long time / moves a lot of data) by using the
--unshallow
flag on the fetch:$ git fetch --unshallow origin",,,
8a17d8db671fa4fe4f68dca54bff7aa94fdf412eb244544ea8175b31dbb4089c,"Docker Build
Docker Build is one of Docker Engine's most used features. Whenever you are creating an image you are using Docker Build. Build is a key part of your software development life cycle allowing you to package and bundle your code and ship it anywhere.
Docker Build is more than a command for building images, and it's not only about packaging your code. It's a whole ecosystem of tools and features that support not only common workflow tasks but also provides support for more complex and advanced scenarios.",,,
bf73f830bc0b7044f03bba35cecc8f8805150c119c8b1d77d6d40eb0281c7e2f,"Convert an account into an organization
You can convert an existing user account to an organization. This is useful if you need multiple users to access your account and the repositories that it’s connected to. Converting it to an organization gives you better control over permissions for these users through teams and roles.
When you convert a user account to an organization, the account is migrated to a Docker Team plan.
Important
Once you convert your account to an organization, you can’t revert it to a user account.
Prerequisites
Before you convert a user account to an organization, ensure that you meet the following requirements:
The user account that you want to convert must not be a member of a company or any teams or organizations. You must remove the account from all teams, organizations, or the company.
To do this:
- Navigate to Organizations and then select the organization(s) you need to leave.
- Find your username in the Members tab.
- Select the More options menu and then select Leave organization.
If the user account is the sole owner of any organization or company, assign another user the owner role and then remove yourself from the organization or company.
You must have a separate Docker ID ready to assign as the owner of the organization during conversion.
If you want to convert your user account into an organization account and you don't have any other user accounts, you need to create a new user account to assign it as the owner of the new organization. With the owner role assigned, this user account has full administrative access to configure and manage the organization. You can assign more users the owner role after the conversion.
Effects of converting an account into an organization
Consider the following effects of converting your account:
This process removes the email address for the account, and organization owners will receive notification emails instead. You'll be able to reuse the removed email address for another account after converting.
The current plan will cancel and your new subscription will start.
Repository namespaces and names won't change, but converting your account removes any repository collaborators. Once you convert the account, you'll need to add those users as team members.
Existing automated builds will appear as if they were set up by the first owner added to the organization. See Convert an account into an organization for steps on adding the first owner.
The user account that you add as the first owner will have full administrative access to configure and manage the organization.
Converting a user account to an organization will delete all of the user's personal access tokens. See Create an access token for steps on creating personal access tokens after converting the user account.
Convert an account into an organization
Ensure you have removed your user account from any company or teams or organizations. Also make sure that you have a new Docker ID before you convert an account. See the Prerequisites section for details.
Sign in to your Docker account.
In Docker Home, select your avatar in the top-right corner to open the drop-down.
Select Account settings.
Select Convert.
Review the warning displayed about converting a user account. This action cannot be undone and has considerable implications for your assets and the account.
Enter a Username of new owner to set an organization owner. This is the user account that will manage the organization, and the only way to access the organization settings after conversion. You cannot use the same Docker ID as the account you are trying to convert.
Select Confirm. The new owner receives a notification email. Use that owner account to sign in and manage the new organization.",,,
4fbc1a542f37435398a22beed46b4ec99516a1242abd3267a03fc79c3fce28ff,"Why use Compose?
Key benefits of Docker Compose
Using Docker Compose offers several benefits that streamline the development, deployment, and management of containerized applications:
Simplified control: Docker Compose allows you to define and manage multi-container applications in a single YAML file. This simplifies the complex task of orchestrating and coordinating various services, making it easier to manage and replicate your application environment.
Efficient collaboration: Docker Compose configuration files are easy to share, facilitating collaboration among developers, operations teams, and other stakeholders. This collaborative approach leads to smoother workflows, faster issue resolution, and increased overall efficiency.
Rapid application development: Compose caches the configuration used to create a container. When you restart a service that has not changed, Compose re-uses the existing containers. Re-using containers means that you can make changes to your environment very quickly.
Portability across environments: Compose supports variables in the Compose file. You can use these variables to customize your composition for different environments, or different users.
Extensive community and support: Docker Compose benefits from a vibrant and active community, which means abundant resources, tutorials, and support. This community-driven ecosystem contributes to the continuous improvement of Docker Compose and helps users troubleshoot issues effectively.
Common use cases of Docker Compose
Compose can be used in many different ways. Some common use cases are outlined below.
Development environments
When you're developing software, the ability to run an application in an isolated environment and interact with it is crucial. The Compose command line tool can be used to create the environment and interact with it.
The
Compose file provides a way to document and configure
all of the application's service dependencies (databases, queues, caches,
web service APIs, etc). Using the Compose command line tool you can create
and start one or more containers for each dependency with a single command
(docker compose up
).
Together, these features provide a convenient way for you to get started on a project. Compose can reduce a multi-page ""developer getting started guide"" to a single machine-readable Compose file and a few commands.
Automated testing environments
An important part of any Continuous Deployment or Continuous Integration process is the automated test suite. Automated end-to-end testing requires an environment in which to run tests. Compose provides a convenient way to create and destroy isolated testing environments for your test suite. By defining the full environment in a Compose file, you can create and destroy these environments in just a few commands:
$ docker compose up -d
$ ./run_tests
$ docker compose down
Single host deployments
Compose has traditionally been focused on development and testing workflows, but with each release we're making progress on more production-oriented features.
For details on using production-oriented features, see Compose in production.",,,
637bc7ae16c7268ddf0914279e7173927de16e953cc863a884ba9355fcc25695,"Manage an account
You can centrally manage the settings for your Docker account using Docker Home. Here you can also take administrative actions for your account and manage your account security.
Tip
If your account is associated with an organization that enforces single sign-on (SSO), you may not have permissions to update your account settings. You must contact your administrator to update your settings.
Update general settings
- Sign in to your Docker account.
- In Docker Home, select your avatar in the top-right corner to open the drop-down.
- Select Account settings.
From the Account Center page, you can take any of the following actions.
Update account information
To update your account information, select the arrow icon. You can edit the following settings here:
- Full name
- Company
- Location
- Website
- Gravatar email: To add an avatar to your Docker account, create a Gravatar account and create your avatar. Next, add your Gravatar email to your Docker account settings. It may take some time for your avatar to update in Docker.
This information is visible on your account profile in Docker Hub.
Make your changes here, then select Save to save your settings.
Update email address
To update your email address, select Email.
- Enter your new email address.
- Enter your password to confirm the change.
- Select Send verification email to send a verification email to your new email address.
Once you verify your email address, your account information will update.
Change your password
You can change your password by initiating a password reset via email.
To change your password, select Password and then Reset password.
Follow the instructions in the password reset email.
Manage security settings
To update your two-factor authentication (2FA) settings, select 2FA. For information on two-factor authentication (2FA) for your account, see Enable two-factor authentication to get started.
To manage personal access tokens, select Personal access tokens. For information on personal access tokens, see Create and manage access tokens.
Account management
To convert your account into an organization, select Convert. For more information on converting your account, see Convert an account into an organization.
To deactivate your account, select Deactivate. For information on deactivating your account, see Deactivating a user account.",,,
a4ab9fdd39ef157f3760d707fc6c0c23159b7a7c51eb9a3d317a5977f8cab37e,"Docker object labels
Labels are a mechanism for applying metadata to Docker objects, including:
- Images
- Containers
- Local daemons
- Volumes
- Networks
- Swarm nodes
- Swarm services
You can use labels to organize your images, record licensing information, annotate relationships between containers, volumes, and networks, or in any way that makes sense for your business or application.
Label keys and values
A label is a key-value pair, stored as a string. You can specify multiple labels for an object, but each key must be unique within an object. If the same key is given multiple values, the most-recently-written value overwrites all previous values.
Key format recommendations
A label key is the left-hand side of the key-value pair. Keys are alphanumeric
strings which may contain periods (.
), underscores (_
), slashes (/
), and hyphens (-
). Most Docker users use
images created by other organizations, and the following guidelines help to
prevent inadvertent duplication of labels across objects, especially if you plan
to use labels as a mechanism for automation.
Authors of third-party tools should prefix each label key with the reverse DNS notation of a domain they own, such as
com.example.some-label
.Don't use a domain in your label key without the domain owner's permission.
The
com.docker.*
,io.docker.*
, andorg.dockerproject.*
namespaces are reserved by Docker for internal use.Label keys should begin and end with a lower-case letter and should only contain lower-case alphanumeric characters, the period character (
.
), and the hyphen character (-
). Consecutive periods or hyphens aren't allowed.The period character (
.
) separates namespace ""fields"". Label keys without namespaces are reserved for CLI use, allowing users of the CLI to interactively label Docker objects using shorter typing-friendly strings.
These guidelines aren't currently enforced and additional guidelines may apply to specific use cases.
Value guidelines
Label values can contain any data type that can be represented as a string,
including (but not limited to) JSON, XML, CSV, or YAML. The only requirement is
that the value be serialized to a string first, using a mechanism specific to
the type of structure. For instance, to serialize JSON into a string, you might
use the JSON.stringify()
JavaScript method.
Since Docker doesn't deserialize the value, you can't treat a JSON or XML document as a nested structure when querying or filtering by label value unless you build this functionality into third-party tooling.
Manage labels on objects
Each type of object with support for labels has mechanisms for adding and managing them and using them as they relate to that type of object. These links provide a good place to start learning about how you can use labels in your Docker deployments.
Labels on images, containers, local daemons, volumes, and networks are static for the lifetime of the object. To change these labels you must recreate the object. Labels on Swarm nodes and services can be updated dynamically.
Images and containers
Local Docker daemons
Volumes
Networks
Swarm nodes
Swarm services",,,
deb51d451ed35adf10804fac6fa68b47828da2a5e7623cbafca5a48a8abe0948,"Collect Docker metrics with Prometheus
Prometheus is an open-source systems monitoring and alerting toolkit. You can configure Docker as a Prometheus target.
Warning
The available metrics and the names of those metrics are in active development and may change at any time.
Currently, you can only monitor Docker itself. You can't currently monitor your application using the Docker target.
Example
The following example shows you how to configure your Docker daemon, set up Prometheus to run as a container on your local machine, and monitor your Docker instance using Prometheus.
Configure the daemon
To configure the Docker daemon as a Prometheus target, you need to specify the
metrics-address
in the daemon.json
configuration file. This daemon expects
the file to be located at one of the following locations by default. If the
file doesn't exist, create it.
- Linux:
/etc/docker/daemon.json
- Windows Server:
C:\ProgramData\docker\config\daemon.json
- Docker Desktop: Open the Docker Desktop settings and select Docker Engine to edit the file.
Add the following configuration:
{
""metrics-addr"": ""127.0.0.1:9323""
}
Save the file, or in the case of Docker Desktop for Mac or Docker Desktop for Windows, save the configuration. Restart Docker.
Docker now exposes Prometheus-compatible metrics on port 9323 via the loopback
interface. You can configure it to use the wildcard address 0.0.0.0
instead,
but this will expose the Prometheus port to the wider network. Consider your
threat model carefully when deciding which option best suits your environment.
Create a Prometheus configuration
Copy the following configuration file and save it to a location of your choice,
for example /tmp/prometheus.yml
. This is a stock Prometheus configuration file,
except for the addition of the Docker job definition at the bottom of the file.
# my global config
global:
scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
# scrape_timeout is set to the global default (10s).
# Attach these labels to any time series or alerts when communicating with
# external systems (federation, remote storage, Alertmanager).
external_labels:
monitor: ""codelab-monitor""
# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
rule_files:
# - ""first.rules""
# - ""second.rules""
# A scrape configuration containing exactly one endpoint to scrape:
# Here it's Prometheus itself.
scrape_configs:
# The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
- job_name: prometheus
# metrics_path defaults to '/metrics'
# scheme defaults to 'http'.
static_configs:
- targets: [""localhost:9090""]
- job_name: docker
# metrics_path defaults to '/metrics'
# scheme defaults to 'http'.
static_configs:
- targets: [""host.docker.internal:9323""]
Run Prometheus in a container
Next, start a Prometheus container using this configuration.
$ docker run --name my-prometheus \
--mount type=bind,source=/tmp/prometheus.yml,destination=/etc/prometheus/prometheus.yml \
-p 9090:9090 \
--add-host host.docker.internal=host-gateway \
prom/prometheus
If you're using Docker Desktop, the --add-host
flag is optional. This flag
makes sure that the host's internal IP gets exposed to the Prometheus
container. Docker Desktop does this by default. The host IP is exposed as the
host.docker.internal
hostname. This matches the configuration defined in
prometheus.yml
in the previous step.
Open the Prometheus Dashboard
Verify that the Docker target is listed at http://localhost:9090/targets/
.
Note
You can't access the endpoint URLs on this page directly if you use Docker Desktop.
Use Prometheus
Create a graph. Select the Graphs link in the Prometheus UI. Choose a metric
from the combo box to the right of the Execute button, and click
Execute. The screenshot below shows the graph for
engine_daemon_network_actions_seconds_count
.
The graph shows a pretty idle Docker instance, unless you're already running active workloads on your system.
To make the graph more interesting, run a container that uses some network actions by starting downloading some packages using a package manager:
$ docker run --rm alpine apk add git make musl-dev go
Wait a few seconds (the default scrape interval is 15 seconds) and reload your graph. You should see an uptick in the graph, showing the increased network traffic caused by the container you just ran.
Next steps
The example provided here shows how to run Prometheus as a container on your
local system. In practice, you'll probably be running Prometheus on another
system or as a cloud service somewhere. You can set up the Docker daemon as a
Prometheus target in such contexts too. Configure the metrics-addr
of the
daemon and add the address of the daemon as a scrape endpoint in your
Prometheus configuration.
- job_name: docker
static_configs:
- targets: [""docker.daemon.example:<PORT>""]
For more information about Prometheus, refer to the Prometheus documentation",,,
34f3d0833abd00adf07abbb1951cfbcc19b62f0edb6a5458b36b15464c5b2a4f,"Docker Hub usage and limits
Note
Starting April 1, 2025, all users with a Pro, Team, or Business subscription will have unlimited Docker Hub pulls with fair use. Unauthenticated users and users with a free Personal account have the following pull limits:
- Unauthenticated users: 10 pulls/hour
- Authenticated users with a free account: 100 pulls/hour
The following table provides an overview of the included usage and limits for each user type, subject to fair use:
| User type | Pull rate limit per hour | Number of public repositories | Number of private repositories |
|---|---|---|---|
| Business (authenticated) | Unlimited | Unlimited | Unlimited |
| Team (authenticated) | Unlimited | Unlimited | Unlimited |
| Pro (authenticated) | Unlimited | Unlimited | Unlimited |
| Personal (authenticated) | 100 | Unlimited | Up to 1 |
| Unauthenticated users | 10 per IPv4 address or IPv6 /64 subnet | Not applicable | Not applicable |
For more details, see Pull usage and limits.
Fair use
When utilizing the Docker Platform, users should be aware that excessive data transfer, pull rates, or data storage can lead to throttling, or additional charges. To ensure fair resource usage and maintain service quality, we reserve the right to impose restrictions or apply additional charges to accounts exhibiting excessive data and storage consumption.
Abuse rate limit
Docker Hub has an abuse rate limit to protect the application and infrastructure. This limit applies to all requests to Hub properties including web pages, APIs, and image pulls. The limit is applied per IPv4 address or per IPv6 /64 subnet, and while the limit changes over time depending on load and other factors, it's in the order of thousands of requests per minute. The abuse limit applies to all users equally regardless of account level.
You can differentiate between the pull rate limit and abuse rate limit by
looking at the error code. The abuse limit returns a simple 429 Too Many Requests
response. The pull limit returns a longer error message that includes
a link to documentation.",,,
61c1bbf76186df38e64f5b6df7fcbfa1e432ca785a95f07e282aacb52470e618,"Custom kernels on WSL
Docker Desktop depends on several kernel features built into the default WSL 2 Linux kernel distributed by Microsoft. Consequently, using a custom kernel with Docker Desktop on WSL 2 is not officially supported and may cause issues with Docker Desktop startup or operation.
However, in some cases it may be necessary to run custom kernels; Docker Desktop does not block their use, and some users have reported success using them.
If you choose to use a custom kernel, it is recommended you start from the kernel tree distributed by Microsoft from their official repository and then add the features you need on top of that.
It's also recommended that you:
- Use the same kernel version as the one distributed by the latest WSL2
release. You can find the version by running
wsl.exe --system uname -r
in a terminal. - Start from the default kernel configuration as provided by Microsoft from their repository and add the features you need on top of that.
- Make sure that your kernel build environment includes
pahole
and its version is properly reflected in the corresponding kernel config (CONFIG_PAHOLE_VERSION
).",,,
e6ac2948cd952051bf4f0baf0d0878a724e6b485df57e4e2172edbc2f72a6998,"What is Docker?
Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker's methodologies for shipping, testing, and deploying code, you can significantly reduce the delay between writing code and running it in production.
The Docker platform
Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security lets you run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you don't need to rely on what's installed on the host. You can share containers while you work, and be sure that everyone you share with gets the same container that works in the same way.
Docker provides tooling and a platform to manage the lifecycle of your containers:
- Develop your application and its supporting components using containers.
- The container becomes the unit for distributing and testing your application.
- When you're ready, deploy your application into your production environment, as a container or an orchestrated service. This works the same whether your production environment is a local data center, a cloud provider, or a hybrid of the two.
What can I use Docker for?
Fast, consistent delivery of your applications
Docker streamlines the development lifecycle by allowing developers to work in standardized environments using local containers which provide your applications and services. Containers are great for continuous integration and continuous delivery (CI/CD) workflows.
Consider the following example scenario:
- Your developers write code locally and share their work with their colleagues using Docker containers.
- They use Docker to push their applications into a test environment and run automated and manual tests.
- When developers find bugs, they can fix them in the development environment and redeploy them to the test environment for testing and validation.
- When testing is complete, getting the fix to the customer is as simple as pushing the updated image to the production environment.
Responsive deployment and scaling
Docker's container-based platform allows for highly portable workloads. Docker containers can run on a developer's local laptop, on physical or virtual machines in a data center, on cloud providers, or in a mixture of environments.
Docker's portability and lightweight nature also make it easy to dynamically manage workloads, scaling up or tearing down applications and services as business needs dictate, in near real time.
Running more workloads on the same hardware
Docker is lightweight and fast. It provides a viable, cost-effective alternative to hypervisor-based virtual machines, so you can use more of your server capacity to achieve your business goals. Docker is perfect for high density environments and for small and medium deployments where you need to do more with fewer resources.
Docker architecture
Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. Another Docker client is Docker Compose, that lets you work with applications consisting of a set of containers.
The Docker daemon
The Docker daemon (dockerd
) listens for Docker API requests and manages Docker
objects such as images, containers, networks, and volumes. A daemon can also
communicate with other daemons to manage Docker services.
The Docker client
The Docker client (docker
) is the primary way that many Docker users interact
with Docker. When you use commands such as docker run
, the client sends these
commands to dockerd
, which carries them out. The docker
command uses the
Docker API. The Docker client can communicate with more than one daemon.
Docker Desktop
Docker Desktop is an easy-to-install application for your Mac, Windows or Linux environment that enables you to build and share containerized applications and microservices. Docker Desktop includes the Docker daemon (dockerd
), the Docker client (docker
), Docker Compose, Docker Content Trust, Kubernetes, and Credential Helper. For more information, see
Docker Desktop.
Docker registries
A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker looks for images on Docker Hub by default. You can even run your own private registry.
When you use the docker pull
or docker run
commands, Docker pulls the required images from your configured registry. When you use the docker push
command, Docker pushes
your image to your configured registry.
Docker objects
When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects. This section is a brief overview of some of those objects.
Images
An image is a read-only template with instructions for creating a Docker
container. Often, an image is based on another image, with some additional
customization. For example, you may build an image which is based on the ubuntu
image, but installs the Apache web server and your application, as well as the
configuration details needed to make your application run.
You might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.
Containers
A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state.
By default, a container is relatively well isolated from other containers and its host machine. You can control how isolated a container's network, storage, or other underlying subsystems are from other containers or from the host machine.
A container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that aren't stored in persistent storage disappear.
Example docker run
command
The following command runs an ubuntu
container, attaches interactively to your
local command-line session, and runs /bin/bash
.
$ docker run -i -t ubuntu /bin/bash
When you run this command, the following happens (assuming you are using the default registry configuration):
If you don't have the
ubuntu
image locally, Docker pulls it from your configured registry, as though you had rundocker pull ubuntu
manually.Docker creates a new container, as though you had run a
docker container create
command manually.Docker allocates a read-write filesystem to the container, as its final layer. This allows a running container to create or modify files and directories in its local filesystem.
Docker creates a network interface to connect the container to the default network, since you didn't specify any networking options. This includes assigning an IP address to the container. By default, containers can connect to external networks using the host machine's network connection.
Docker starts the container and executes
/bin/bash
. Because the container is running interactively and attached to your terminal (due to the-i
and-t
flags), you can provide input using your keyboard while Docker logs the output to your terminal.When you run
exit
to terminate the/bin/bash
command, the container stops but isn't removed. You can start it again or remove it.
The underlying technology
Docker is written in the
Go programming language and takes
advantage of several features of the Linux kernel to deliver its functionality.
Docker uses a technology called namespaces
to provide the isolated workspace
called the container. When you run a container, Docker creates a set of
namespaces for that container.
These namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is limited to that namespace.",,,
9b0090c178cb58273af2f9a762f68b31e392a6941630482f7463e252cb403e1d,"Docker Scout image analysis
When you activate image analysis for a repository, Docker Scout automatically analyzes new images that you push to that repository.
Image analysis extracts the Software Bill of Material (SBOM) and other image metadata,and evaluates it against vulnerability data from security advisories.
If you run image analysis as a one-off task using the CLI or Docker Desktop, Docker Scout won't store any data about your image. If you enable Docker Scout for your container image repositories however, Docker Scout saves a metadata snapshot of your images after the analysis. As new vulnerability data becomes available, Docker Scout recalibrates the analysis using the metadata snapshot, which means your security status for images is updated in real-time. This dynamic evaluation means there's no need to re-analyze images when new CVE information is disclosed.
Docker Scout image analysis is available by default for Docker Hub repositories. You can also integrate third-party registries and other services. To learn more, see Integrating Docker Scout with other systems.
Activate Docker Scout on a repository
Docker Personal comes with 1 Scout-enabled repository. You can upgrade your Docker subscription if you need additional repositories. See Subscriptions and features to learn how many Scout-enabled repositories come with each subscription tier.
Before you can activate image analysis on a repository in a third-party registry, the registry must be integrated with Docker Scout for your Docker organization. Docker Hub is integrated by default. For more information, see See Container registry integrations
Note
You must have the Editor or Owner role in the Docker organization to activate image analysis on a repository.
To activate image analysis:
- Go to Repository settings in the Docker Scout Dashboard.
- Select the repositories that you want to enable.
- Select Enable image analysis.
If your repositories already contain images, Docker Scout pulls and analyzes the latest images automatically.
Analyze registry images
To trigger image analysis for an image in a registry, push the image to a registry that's integrated with Docker Scout, to a repository where image analysis is activated.
Note
Image analysis on the Docker Scout platform has a maximum image file size limit of 10 GB, unless the image has an SBOM attestation. See Maximum image size.
Sign in with your Docker ID, either using the
docker login
command or the Sign in button in Docker Desktop.Build and push the image that you want to analyze.
$ docker build --push --tag <org>/<image:tag> --provenance=true --sbom=true .
Building with the
--provenance=true
and--sbom=true
flags attaches build attestations to the image. Docker Scout uses attestations to provide more fine-grained analysis results.Note
The default
docker
driver only supports build attestations if you use the containerd image store.Go to the Images page in the Docker Scout Dashboard.
The image appears in the list shortly after you push it to the registry. It may take a few minutes for the analysis results to appear.
Analyze images locally
You can analyze local images with Docker Scout using Docker Desktop or the
docker scout
commands for the Docker CLI.
Docker Desktop
Note
Docker Desktop background indexing supports images up to 10 GB in size. See Maximum image size.
To analyze an image locally using the Docker Desktop GUI:
Pull or build the image that you want to analyze.
Go to the Images view in the Docker Dashboard.
Select one of your local images in the list.
This opens the Image details view, showing a breakdown of packages and vulnerabilities found by the Docker Scout analysis for the image you selected.
CLI
The docker scout
CLI commands provide a command line interface for using Docker
Scout from your terminal.
docker scout quickview
: summary of the specified image, see Quickviewdocker scout cves
: local analysis of the specified image, see CVEsdocker scout compare
: analyzes and compares two images
By default, the results are printed to standard output. You can also export results to a file in a structured format, such as Static Analysis Results Interchange Format (SARIF).
Quickview
The docker scout quickview
command provides an overview of the
vulnerabilities found in a given image and its base image.
$ docker scout quickview traefik:latest
✓ SBOM of image already cached, 311 packages indexed
Your image traefik:latest │ 0C 2H 8M 1L
Base image alpine:3 │ 0C 0H 0M 0L
If your the base image is out of date, the quickview
command also shows how
updating your base image would change the vulnerability exposure of your image.
$ docker scout quickview postgres:13.1
✓ Pulled
✓ Image stored for indexing
✓ Indexed 187 packages
Your image postgres:13.1 │ 17C 32H 35M 33L
Base image debian:buster-slim │ 9C 14H 9M 23L
Refreshed base image debian:buster-slim │ 0C 1H 6M 29L
│ -9 -13 -3 +6
Updated base image debian:stable-slim │ 0C 0H 0M 17L
│ -9 -14 -9 -6
CVEs
The docker scout cves
command gives you a complete view of all the
vulnerabilities in the image. This command supports several flags that lets you
specify more precisely which vulnerabilities you're interested in, for example,
by severity or package type:
$ docker scout cves --format only-packages --only-vuln-packages \
--only-severity critical postgres:13.1
✓ SBOM of image already cached, 187 packages indexed
✗ Detected 10 vulnerable packages with a total of 17 vulnerabilities
Name Version Type Vulnerabilities
───────────────────────────────────────────────────────────────────────────
dpkg 1.19.7 deb 1C 0H 0M 0L
glibc 2.28-10 deb 4C 0H 0M 0L
gnutls28 3.6.7-4+deb10u6 deb 2C 0H 0M 0L
libbsd 0.9.1-2 deb 1C 0H 0M 0L
libksba 1.3.5-2 deb 2C 0H 0M 0L
libtasn1-6 4.13-3 deb 1C 0H 0M 0L
lz4 1.8.3-1 deb 1C 0H 0M 0L
openldap 2.4.47+dfsg-3+deb10u5 deb 1C 0H 0M 0L
openssl 1.1.1d-0+deb10u4 deb 3C 0H 0M 0L
zlib 1:1.2.11.dfsg-1 deb 1C 0H 0M 0L
For more information about these commands and how to use them, refer to the CLI reference documentation:
Vulnerability severity assessment
Docker Scout assigns a severity rating to vulnerabilities based on vulnerability data from advisory sources. Advisories are ranked and prioritized depending on the type of package that's affected by a vulnerability. For example, if a vulnerability affects an OS package, the severity level assigned by the distribution maintainer is prioritized.
If the preferred advisory source has assigned a severity rating to a CVE, but
not a CVSS score, Docker Scout falls back to displaying a CVSS score from
another source. The severity rating from the preferred advisory and the CVSS
score from the fallback advisory are displayed together. This means a
vulnerability can have a severity rating of LOW
with a CVSS score of 9.8, if
the preferred advisory assigns a LOW
rating but no CVSS score, and a fallback
advisory assigns a CVSS score of 9.8.
Vulnerabilities that haven't been assigned a CVSS score in any source are categorized as Unspecified (U).
Docker Scout doesn't implement a proprietary vulnerability metrics system. All metrics are inherited from security advisories that Docker Scout integrates with. Advisories may use different thresholds for classifying vulnerabilities, but most of them adhere to the CVSS v3.0 specification, which maps CVSS scores to severity ratings according to the following table:
| CVSS score | Severity rating |
|---|---|
| 0.1 – 3.9 | Low (L) |
| 4.0 – 6.9 | Medium (M) |
| 7.0 – 8.9 | High (H) |
| 9.0 – 10.0 | Critical (C) |
For more information, see Vulnerability Metrics (NIST).
Note that, given the advisory prioritization and fallback mechanism described earlier, severity ratings displayed in Docker Scout may deviate from this rating system.
Maximum image size
Image analysis on the Docker Scout platform, and analysis triggered by background indexing in Docker Desktop, has an image file size limit of 10 GB (uncompressed). To analyze images larger than that, you can either:
- Attach SBOM attestations at build-time
- Use the CLI to analyze the image locally
Images analyzed locally with the CLI and images with SBOM attestations have no maximum file size.",,,
fe9b30fa9d8a323416acc94083870c714c491563eac30ed0e0f0012c0532b2f0,"GPU support in Docker Desktop
Table of contents
Note
Currently GPU support in Docker Desktop is only available on Windows with the WSL2 backend.
Using NVIDIA GPUs with WSL2
Docker Desktop for Windows supports WSL 2 GPU Paravirtualization (GPU-PV) on NVIDIA GPUs. To enable WSL 2 GPU Paravirtualization, you need:
- A machine with an NVIDIA GPU
- Up to date Windows 10 or Windows 11 installation
- Up to date drivers from NVIDIA supporting WSL 2 GPU Paravirtualization
- The latest version of the WSL 2 Linux kernel. Use
wsl --update
on the command line - To make sure the WSL 2 backend is turned on in Docker Desktop
To validate that everything works as expected, execute a docker run
command with the --gpus=all
flag. For example, the following will run a short benchmark on your GPU:
$ docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark
The output will be similar to:
Run ""nbody -benchmark [-numbodies=<numBodies>]"" to measure performance.
-fullscreen (run n-body simulation in fullscreen mode)
-fp64 (use double precision floating point values for simulation)
-hostmem (stores simulation data in host memory)
-benchmark (run benchmark to measure performance)
-numbodies=<N> (number of bodies (>= 1) to run in simulation)
-device=<d> (where d=0,1,2.... for the CUDA device to use)
-numdevices=<i> (where i=(number of CUDA devices > 0) to use for simulation)
-compare (compares simulation results running once on the default GPU and once on the CPU)
-cpu (run n-body simulation on the CPU)
-tipsy=<file.bin> (load a tipsy model file for simulation)
> NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
> Windowed mode
> Simulation data stored in video memory
> Single precision floating point simulation
> 1 Devices used for simulation
MapSMtoCores for SM 7.5 is undefined. Default to use 64 Cores/SM
GPU Device 0: ""GeForce RTX 2060 with Max-Q Design"" with compute capability 7.5
> Compute 7.5 CUDA device: [GeForce RTX 2060 with Max-Q Design]
30720 bodies, total time for 10 iterations: 69.280 ms
= 136.219 billion interactions per second
= 2724.379 single-precision GFLOP/s at 20 flops per interaction
Or if you wanted to try something more useful you could use the official Ollama image to run the Llama2 large language model.
$ docker run --gpus=all -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
$ docker exec -it ollama ollama run llama2",,,
e2693aff8d425bc1afb9647b6653ecbde056d70ba3c8ee019ed014c0381da651,"Enhanced Container Isolation FAQs
Do I need to change the way I use Docker when ECI is switched on?
No, you can continue to use Docker as usual. ECI works under the covers by creating a more secure container.
Do all container workloads work well with ECI?
The great majority of container workloads run fine with ECI enabled, but a few do not (yet). For the few workloads that don't yet work with Enhanced Container Isolation, Docker is continuing to improve the feature to reduce this to a minimum.
Can I run privileged containers with ECI?
Yes, you can use the --privileged
flag in containers but unlike privileged
containers without ECI, the container can only use it's elevated privileges to
access resources assigned to the container. It can't access global kernel
resources in the Docker Desktop Linux VM. This lets you run privileged
containers securely (including Docker-in-Docker). For more information, see
Key features and benefits.
Will all privileged container workloads run with ECI?
No. Privileged container workloads that want to access global kernel resources inside the Docker Desktop Linux VM won't work. For example, you can't use a privileged container to load a kernel module.
Why not just restrict usage of the --privileged
flag?
Privileged containers are typically used to run advanced workloads in containers, for example Docker-in-Docker or Kubernetes-in-Docker, to perform kernel operations such as loading modules, or to access hardware devices.
ECI allows the running of advanced workloads, but denies the ability to perform kernel operations or access hardware devices.
Does ECI restrict bind mounts inside the container?
Yes, it restricts bind mounts of directories located in the Docker Desktop Linux VM into the container.
It doesn't restrict bind mounts of your host machine files into the container, as configured via Docker Desktop's Settings > Resources > File Sharing.
Can I mount the host's Docker Socket into a container when ECI is enabled?
By default, ECI blocks bind-mounting the host's Docker socket into containers, for security reasons. However, there are legitimate use cases for this, such as when using Testcontainers for local testing.
To enable such use cases, it's possible to configure ECI to allow Docker socket mounts into containers, but only for your chosen (i.e,. trusted) container images, and even restrict what commands the container can send to the Docker Engine via the socket. See ECI Docker socket mount permissions.
Does ECI protect all containers launched with Docker Desktop?
Not yet. It protects all containers launched by users via docker create
and
docker run
.
For containers implicitly created by docker build
as well as Docker
Desktop's integrated Kubernetes, protection varies depending on the Docker
Desktop version (see the following two FAQs).
ECI does not yet protect Docker Desktop Extension containers and Dev Environments containers.
Does ECI protect containers implicitly used by docker build
?
Prior to Docker Desktop 4.19, ECI did not protect containers used implicitly
by docker build
during the build process.
Since Docker Desktop 4.19, ECI protects containers used by docker build
when using the
Docker container build driver.
In addition, since Docker Desktop 4.30, ECI also protects containers used by
docker build
when using the default ""docker"" build driver, on all
platforms supported by Docker Desktop except Windows with WSL 2.
Does ECI protect Kubernetes in Docker Desktop?
Prior to Docker Desktop 4.38, ECI did not protect the Kubernetes cluster integrated in Docker Desktop.
Since Docker Desktop 4.38, ECI protects the integrated Kubernetes cluster when using the new kind provisioner (see Deploy On Kubernetes). In this case, each node in the multi-node Kubernetes cluster is actually an ECI protected container. With ECI disabled, each node in the Kubernetes cluster is a less-secure fully privileged container.
ECI does not protect the integrated Kubernetes cluster when using the older Kubeadm single-node cluster provisioner.
Does ECI protect containers launched prior to enabling ECI?
No. Containers created prior to switching on ECI are not protected. Therefore, it is recommended you remove all containers prior to switching on ECI.
Does ECI affect the performance of containers?
ECI has little impact on the performance of
containers. The exception is for containers that perform lots of mount
and
umount
system calls, as these are trapped and vetted by the Sysbox container
runtime to ensure they are not being used to breach the container's filesystem.
With ECI, can the user still override the --runtime
flag from the CLI ?
No. With ECI enabled, Sysbox is set as the default (and only) runtime for
containers deployed by Docker Desktop users. If a user attempts to override the
runtime (e.g., docker run --runtime=runc
), this request is ignored and the
container is created through the Sysbox runtime.
The reason runc
is disallowed is it lets users run as ""true
root"" on the Docker Desktop Linux VM, thereby providing them with implicit
control of the VM and the ability to modify the administrative configurations
for Docker Desktop.
How is ECI different from Docker Engine's userns-remap mode?
See How does it work.
How is ECI different from Rootless Docker?
See How does it work",,,
50f9cb8899402deedcdf7bc7b697944101e7795951f377063056440088829ce2,"Create and manage access tokens
You can create a personal access token (PAT) to use as an alternative to your password for Docker CLI authentication.
Compared to passwords, PATs provide the following advantages:
- You can investigate when the PAT was last used and then disable or delete it if you find any suspicious activity.
- When using an access token, you can't perform any administrative activity on the account, including changing the password. It protects your account if your computer is compromised.
- Access tokens are valuable for building integrations, as you can issue multiple tokens, one for each integration, and revoke them at any time.
Create an access token
Important
Treat access tokens like your password and keep them secret. Store your tokens securely in a credential manager for example.
Use the Docker Admin Console to create an access token.
Sign in to your Docker account.
Select your avatar in the top-right corner and from the drop-down menu select Account settings.
Select Personal access tokens.
Select Generate new token.
Add a description for your token. Use something that indicates the use case or purpose of the token.
Select the expiration date for the token.
Set the access permissions. The access permissions are scopes that set restrictions in your repositories. For example, for Read & Write permissions, an automation pipeline can build an image and then push it to a repository. However, it can't delete the repository.
Select Generate and then copy the token that appears on the screen and save it. You won't be able to retrieve the token once you close this prompt.
Use an access token
You can use an access token in place of your password when you sign in using Docker CLI.
Sign in from your Docker CLI client with the following command, replacing YOUR_USERNAME
with your Docker ID:
$ docker login --username <YOUR_USERNAME>
When prompted for a password, enter your personal access token instead of a password.
Note
If you have two-factor authentication (2FA) enabled, you must use a personal access token when logging in from the Docker CLI. 2FA is an optional, but more secure method of authentication.
Fair use
When utilizing PATs, users should be aware that excessive creation of PATs could lead to throttling, or additional charges. To ensure fair resource usage and maintain service quality, Docker reserves the right to impose restrictions or apply additional charges to accounts exhibiting excessive use of PATs.
Modify existing tokens
Note
You can't edit the expiration date on an existing token. You must create a new PAT if you need to set a new expiration date.
You can rename, activate, deactivate, or delete a token as needed. You can manage your tokens in your account settings.
Sign in to your Docker account.
Select your avatar in the top-right corner and from the drop-down menu select Account settings.
Select Personal access tokens.
This page shows an overview of all your tokens, and lists if the token was generated manually or if it was auto-generated. You can also view the scope of the tokens, which tokens are activate and inactive, when they were created, when they were last used, and their expiration date.
Select the actions menu on the far right of a token row, then select Deactivate or Activate, Edit, or Delete to modify the token.
After editing the token, select Save token.
Auto-generated tokens
When you sign in to your Docker account with Docker Desktop, Docker Desktop generates an authentication token on your behalf. When you interact with Docker Hub using the Docker CLI, the CLI uses this token for authentication. The token scope has Read, Write, and Delete access. If your Docker Desktop session expires, the token is automatically removed locally.
You can have up to 5 auto-generated tokens associated with your account. These are deleted and created automatically based on usage and creation dates. You can also delete your auto-generated tokens as needed. For more information, see Modify existing tokens.",,,
3494cc7fa69ad4b5d5e9367624468ac1c13137c000a3cbd6e51b10c936d9c0ba,"Docker Engine version 28 release notes
Table of contents
This page describes the latest changes, additions, known issues, and fixes for Docker Engine version 28.
For more information about:
- Deprecated and removed features, see Deprecated Engine Features.
- Changes to the Engine API, see Engine API version history.
28.0.1
2025-02-26For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Networking
- Remove dependency on kernel modules
ip_set
,ip_set_hash_net
andnetfilter_xt_set
.- The dependency was introduced in release 28.0.0 but proved too disruptive. The iptables rules using these modules have been replaced. moby/moby#49530
- Allow daemon startup on a host with IPv6 disabled without requiring
--ip6tables=false
. moby/moby#49525 - Fix a bug that was causing containers with
--restart=always
and a published port already in use to restart in a tight loop. moby/moby#49507 - Fix an issue with Swarm ingress, caused by incorrect ordering of iptables rules. moby/moby#49538
- Fix creation of a swarm-scoped network from a
--config-only
network. moby/moby#49521 - Fix
docker network inspect
reporting an IPv6 gateway with CIDR suffix for a newly created network with no specific IPAM config, until a daemon restart. moby/moby#49520 - Improve the error reported when kernel modules
ip_set
,ip_set_hash_net
andnetilter_xt_set
are not available. moby/moby#49524 - Move most of Docker's iptables rules out of the filter-FORWARD chain, so that other applications are free to append rules that must follow Docker's rules. moby/moby#49518
- Update
--help
output and man page lo state which options only apply to the default bridge network. moby/moby#49522
Bug fixes and enhancements
- Fix
docker context create
always returning an error when using the""skip-tls-verify""
option. docker/cli#5850 - Fix shell completion suggesting IDs instead of names for services and nodes. docker/cli#5848
- Fix unintentionally printing exit status to standard error output when
docker exec/run
returns a non-zero status. docker/cli#5854 - Fix regression
protocol ""tcp"" is not supported by the RootlessKit port driver ""slirp4netns""
. moby/moby#49514 - containerd image store: Fix
docker inspect
not being able to show multi-platform images with missing layers for all platforms. moby/moby#49533 - containerd image store: Fix
docker images --tree
reporting wrong content size. moby/moby#49535 - Fix compilation on i386 moby/moby#49526
Packaging updates
- Update
github.com/go-jose/go-jose/v4
to v4.0.5 to address GHSA-c6gw-w398-hv78 / CVE-2025-27144. docker/cli#5867 - Update Buildx to v0.21.1. docker/docker-ce-packaging#1167
- Update Compose to v2.33.1. docker/docker-ce-packaging#1168
API
- containerd image store: Fix
GET /images/json?manifests=1
not fillingManifests
for index-only images moby/moby#49533 - containerd image store: Fix
GET /images/json and /images/<name>/json
Size.Content
field including the size of content that's not available locally moby/moby#49535
28.0.0
2025-02-19For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 28.0.0 milestone
- moby/moby, 28.0.0 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
New
- Add ability to mount an image inside a container via
--mount type=image
. moby/moby#48798- You can also specify
--mount type=image,image-subpath=[subpath],...
option to mount a specific path from the image. docker/cli#5755
- You can also specify
docker images --tree
now shows metadata badges docker/cli#5744docker load
,docker save
, anddocker history
now support a--platform
flag allowing you to choose a specific platform for single-platform operations on multi-platform images. docker/cli#5331- Add
OOMScoreAdj
todocker service create
anddocker stack
. docker/cli#5145 docker buildx prune
now supportsreserved-space
,max-used-space
,min-free-space
andkeep-bytes
filters. moby/moby#48720- Windows: Add support for running containerd as a child process of the daemon, instead of using a system-installed containerd. moby/moby#47955
Networking
- The
docker-proxy
binary has been updated, older versions will not work with the updateddockerd
. moby/moby#48132- Close a window in which the userland proxy (
docker-proxy
) could accept TCP connections, that would then fail afteriptables
NAT rules were set up. - The executable
rootlesskit-docker-proxy
is no longer used, it has been removed from the build and distribution.
- Close a window in which the userland proxy (
- DNS nameservers read from the host's
/etc/resolv.conf
are now always accessed from the host's network namespace. moby/moby#48290- When the host's
/etc/resolv.conf
contains no nameservers and there are no--dns
overrides, Google's DNS servers are no longer used, apart from by the default bridge network and in build containers.
- When the host's
- Container interfaces in bridge and macvlan networks now use randomly generated MAC addresses.
moby/moby#48808
- Gratuitous ARP / Neighbour Advertisement messages will be sent when the interfaces are started so that, when IP addresses are reused, they're associated with the newly generated MAC address.
- IPv6 addresses in the default bridge network are now IPAM-assigned, rather than being derived from the MAC address.
- The deprecated OCI
prestart
hook is now only used by build containers. For other containers, network interfaces are added to the network namespace after task creation is complete, before the container task is started. moby/moby#47406 - Add a new
gw-priority
option todocker run
,docker container create
, anddocker network connect
. This option will be used by the Engine to determine which network provides the default gateway for a container. Ondocker run
, this option is only available through the extended--network
syntax. docker/cli#5664 - Add a new netlabel
com.docker.network.endpoint.ifname
to customize the interface name used when connecting a container to a network. It's supported by all built-in network drivers on Linux. moby/moby#49155- When a container is created with multiple networks specified, there's no guarantee on the order networks will be connected to the container. So, if a custom interface name uses the same prefix as the auto-generated names, for example
eth
, the container might fail to start. - The recommended practice is to use a different prefix, for example
en0
, or a numerical suffix high enough to never collide, for exampleeth100
. - This label can be specified on
docker network connect
via the--driver-opt
flag, for exampledocker network connect --driver-opt=com.docker.network.endpoint.ifname=foobar …
. - Or via the long-form
--network
flag ondocker run
, for exampledocker run --network=name=bridge,driver-opt=com.docker.network.endpoint.ifname=foobar …
- When a container is created with multiple networks specified, there's no guarantee on the order networks will be connected to the container. So, if a custom interface name uses the same prefix as the auto-generated names, for example
- If a custom network driver reports capability
GwAllocChecker
then, before a network is created, it will get aGwAllocCheckerRequest
with the network's options. The custom driver may then reply that no gateway IP address should be allocated. moby/moby#49372
Port publishing in bridge networks
dockerd
now requiresipset
support in the Linux kernel. moby/moby#48596- The
iptables
andip6tables
rules used to implement port publishing and network isolation have been extensively modified. This enables some of the following functional changes, and is a first step in refactoring to enable nativenftables
support in a future release. moby/moby#48815 - If it becomes necessary to downgrade to an earlier version of the daemon, some manual cleanup of the new rules will be necessary. The simplest and surest approach is to reboot the host, or use
iptables -F
andip6tables -F
to flush all existingiptables
rules from thefilter
table before starting the older version of the daemon. When that is not possible, run the following commands as root:iptables -D FORWARD -m set --match-set docker-ext-bridges-v4 dst -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT; ip6tables -D FORWARD -m set --match-set docker-ext-bridges-v6 dst -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
iptables -D FORWARD -m set --match-set docker-ext-bridges-v4 dst -j DOCKER; ip6tables -D FORWARD -m set --match-set docker-ext-bridges-v6 dst -j DOCKER
- If you were previously running with the iptables filter-FORWARD policy set to
ACCEPT
and need to restore access to unpublished ports, also delete per-bridge-network rules from theDOCKER
chains. For example,iptables -D DOCKER ! -i docker0 -o docker0 -j DROP
.
- The
- Fix a security issue that was allowing remote hosts to connect directly to a container on its published ports. moby/moby#49325
- Fix a security issue that was allowing neighbor hosts to connect to ports mapped on a loopback address. moby/moby#49325
- Fix an issue that prevented port publishing to link-local addresses. moby/moby#48570
- UDP ports published by a container are now reliably accessible by containers on other networks, via the host's public IP address. moby/moby#48571
- Docker will now only set the
ip6tables
policy for theFORWARD
chain in thefilter
table toDROP
if it enables IP forwarding on the host itself (sysctlsnet.ipv6.conf.all.forwarding
andnet.ipv6.conf.default.forwarding
). This is now aligned with existing IPv4 behaviour. moby/moby#48594- If IPv6 forwarding is enabled on your host, but you were depending on Docker to set the ip6tables filter-FORWARD policy to
DROP
, you may need to update your host's configuration to make sure it is secure.
- If IPv6 forwarding is enabled on your host, but you were depending on Docker to set the ip6tables filter-FORWARD policy to
- Direct routed access to container ports that are not exposed using
p
/-publish
is now blocked in theDOCKER
iptables chain. moby/moby#48724- If the default iptables filter-FORWARD policy was previously left at
ACCEPT
on your host, and direct routed access to a container's unpublished ports from a remote host is still required, options are:- Publish the ports you need.
- Use the new
gateway_mode_ipv[46]=nat-unprotected
, described below.
- Container ports published to host addresses will continue to be accessible via those host addresses, using NAT or the userland proxy.
- Unpublished container ports continue to be directly accessible from the Docker host via the container's IP address.
- If the default iptables filter-FORWARD policy was previously left at
- Networks created with
gateway_mode_ipv[46]=routed
are now accessible from other bridge networks running on the same Docker host, as well as from outside the host. moby/moby#48596 - Bridge driver options
com.docker.network.bridge.gateway_mode_ipv4
andcom.docker.network.bridge.gateway_mode_ipv6
now accept modenat-unprotected
. moby/moby#48597nat-unprotected
is similar to the defaultnat
mode, but no per port/protocol rules are set up. This means any port on a container can be accessed by direct-routing from a remote host.
- Bridge driver options
com.docker.network.bridge.gateway_mode_ipv4
andcom.docker.network.bridge.gateway_mode_ipv6
now accept modeisolated
, when the network is alsointernal
. moby/moby#49262- An address is normally assigned to the bridge device in an
internal
network. So, processes on the Docker host can access the network, and containers in the network can access host services listening on that bridge address (including services listening on ""any"" host address,0.0.0.0
or::
). - An
internal
bridge network created with gateway modeisolated
does not have an address on the Docker host.
- An address is normally assigned to the bridge device in an
- When a port mapping includes a host IP address or port number that cannot be used because NAT from the host is disabled using
--gateway_mode_ipv[46]
, container creation will no longer fail. The unused fields may be needed if the gateway endpoint changes when networks are connected or disconnected. A message about the unused fields will be logged. moby/moby#48575 - Do not create iptables nat-POSTROUTING masquerade rules for a container's own published ports, when the userland proxy is enabled. moby/moby#48854
IPv6
- Add
docker network create
option--ipv4
. To disable IPv4 address assignment for a network, usedocker network create --ipv4=false [...]
. docker/cli#5599 - Daemon option
--ipv6
(""ipv6"": true
indaemon.json
) can now be used withoutfixed-cidr-v6
. moby/moby#48319 - IPAM now handles subnets bigger than ""/64"". moby/moby#49223
- Duplicate address detection (DAD) is now disabled for addresses assigned to the bridges belonging to bridge networks. moby/moby#48609
- Modifications to
host-gateway
, for compatibility with IPv6-only networks. moby/moby#48807- When special value
host-gateway
is used in an--add-host
option in place of an address, it's replaced by an address on the Docker host to make it possible to refer to the host by name. The address used belongs to the default bridge (normallydocker0
). Until now it's always been an IPv4 address, because all containers on bridge networks had IPv4 addresses. - Now, if IPv6 is enabled on the default bridge network,
/etc/hosts
entries will be created for IPv4 and IPv6 addresses. So, a container that's only connected to IPv6-only networks can access the host by name. - The
--host-gateway-ip
option overrides the address used to replacehost-gateway
. Two of these options are now allowed on the command line, for one IPv4 gateway and one IPv6. - In the
daemon.json
file, to provide two addresses, use""host-gateway-ips""
. For example,""host-gateway-ips"": [""192.0.2.1"", ""2001:db8::1111""]
.
- When special value
Bug fixes and enhancements
- Add IPv6 loopback address as an insecure registry by default. moby/moby#48540
- Add support for Cobra-generated completion scripts for
dockerd
. moby/moby#49339 - Fix DNS queries failing when containers are launched via
systemd
auto-start on boot moby/moby#48812 - Fix Docker Swarm mode ignoring
volume.subpath
docker/cli#5833 - Fix
docker export
continuing the export after the operation is canceled. moby/moby#49265 - Fix
docker export
not releasing the container's writable layer after a failure. moby/moby#48517 - Fix
docker images --tree
unnecessary truncating long image names when multiple names are available docker/cli#5757 - Fix a bug where a container with a name matching another container's ID is not restored on daemon startup. moby/moby#48669
- Fix an issue preventing some IPv6 addresses shown by
docker ps
to be properly bracketed docker/cli#5468 - Fix bug preventing image pulls from being cancelled during
docker run
. docker/cli#5645 - Fix error-handling when running the daemon as a Windows service to prevent unclean exits. moby/moby#48518
- Fix issue causing output of
docker run
to be inconsistent when using--attach stdout
or--attach stderr
versusstdin
.docker run --attach stdin
now exits if the container exits. docker/cli#5662 - Fix rootless Docker setup with
subid
backed by NSS modules. moby/moby#49036 - Generated completion scripts from the CLI now show descriptions next to each command/flag suggestion. docker/cli#5756
- IPv6 addresses shown by
docker ps
in port bindings are now bracketed docker/cli#5363 - Implement the ports validation method for Compose docker/cli#5524
- Improve error-output for invalid flags on the command line. docker/cli#5233
- Improve errors when failing to start a container using anther container's network namespace. moby/moby#49367
- Improve handling of invalid API errors that could result in an empty error message being shown. moby/moby#49373
- Improve output and consistency for unknown (sub)commands and invalid arguments docker/cli#5234
- Improve validation of
exec-opts
in daemon configuration. moby/moby#48979 - Update the handling of the
--gpus=0
flag to be consistent with the NVIDIA Container Runtime. moby/moby#48482 client.ContainerCreate
now normalizesCapAdd
andCapDrop
fields inHostConfig
to their canonical form. moby/moby#48551docker image save
now produces stable timestamps. moby/moby#48611docker inspect
now lets you inspect Swarm configs docker/cli#5573- containerd image store: Add support for
Extracting
layer status indocker pull
. moby/moby#49064 - containerd image store: Fix
commit
,import
, andbuild
not preserving a replaced image as a dangling image. moby/moby#48316 - containerd image store: Make
docker load --platform
return an error when the requested platform isn't loaded. moby/moby#48718 - Fix validation of
--link
option. docker/cli#5739 - Add validation of network-diagnostic-port daemon configuration option. moby/moby#49305
- Unless explicitly configured, an IP address is no longer reserved for a gateway in cases where it is not required. Namely, “internal” bridge networks with option
com.docker.network.bridge.inhibit_ipv4
,ipvlan
ormacvlan
networks with no parent interface, and L3 IPvlan modes. moby/moby#49261 - If a custom network driver reports capability
GwAllocChecker
then, before a network is created, it will get aGwAllocCheckerRequest
with the network's options. The custom driver may then reply that no gateway IP address should be allocated. moby/moby#49372 - Fixed an issue that meant a container could not be attached to an L3 IPvlan at the same time as other network types. moby/moby#49130
- Remove the correct
/etc/hosts
entries when disconnecting a container from a network. moby/moby#48857 - Fix duplicate network disconnect events. moby/moby#48800
- Resolve issues related to changing
fixed-cidr
fordocker0
, and inferring configuration from a user-managed default bridge (--bridge
). moby/moby#48319 - Remove feature flag
windows-dns-proxy
, introduced in release 26.1.0 to control forwarding to external DNS resolvers from Windows containers, to makenslookup
work. It was enabled by default in release 27.0.0. moby/moby#48738 - Remove an
iptables
mangle rule for checksumming SCTP. The rule can be re-enabled by settingDOCKER_IPTABLES_SCTP_CHECKSUM=1
in the daemon's environment. This override will be removed in a future release. moby/moby#48149 - Faster connection to bridge networks, in most cases. moby/moby#49302
Packaging updates
- Update Go runtime to 1.23.6. docker/cli#5795, moby/moby#49393, docker/docker-ce-packaging#1161
- Update
runc
to v1.2.5 (static binaries only). moby/moby#49464 - Update containerd to v1.7.25. moby/moby#49252
- Update BuildKit to v0.20.0. moby/moby#49495
- Update Buildx to v0.21.0. docker/docker-ce-packaging#1166
- Update Compose to v2.32.4. docker/docker-ce-packaging#1143
- The canonical source for the
dockerd(8)
man page has been moved back to themoby/moby
repository itself. moby/moby#48298
Go SDK
- Improve validation of empty object IDs. The client now returns an ""Invalid Parameter"" error when trying to use an empty ID or name. This changes the error returned by some ""Inspect"" functions from a ""Not found"" error to an ""Invalid Parameter"". moby/moby#49381
Client.ImageBuild()
now omits default values from the API request's query string. moby/moby#48651api/types/container
: MergeStats
andStatsResponse
moby/moby#49287client.WithVersion
: Strip v-prefix when setting API version moby/moby#49352client
: AddWithTraceOptions
allowing to specify custom OTe1 trace options. moby/moby#49415client
: AddHijackDialer
interface. moby/moby#49388client
: AddSwarmManagementAPIClient
interface to describe all API client methods related to Swarm-specific objects. moby/moby#49388client
: AddWithTraceOptions
allowing to specify custom OTel trace options. moby/moby#49415client
:ImageHistory
,ImageLoad
andImageSave
now use variadic functional options moby/moby#49466pkg/containerfs
: Move to internal moby/moby#48097pkg/reexec
: Can now be used on platforms other than Linux, Windows, macOS and FreeBSD moby/moby#49118api/types/container
: introduceCommitResponse
type. This is currently an alias forIDResponse
, but may become a distinct type in a future release. moby/moby#49444api/types/container
: introduceExecCreateResponse
type. This is currently an alias forIDResponse
, but may become a distinct type in a future release. moby/moby#49444
API
- Update API version to v1.48 moby/moby#48476
GET /images/{name}/json
response now returns theManifests
field containing information about the sub-manifests contained in the image index. This includes things like platform-specific manifests and build attestations. moby/moby#48264POST /containers/create
now supportsMount
of typeimage
for mounting an image inside a container. moby/moby#48798GET /images/{name}/history
now supports aplatform
parameter (JSON encoded OCI Platform type) that lets you specify a platform to show the history of. moby/moby#48295POST /images/{name}/load
andGET /images/{name}/get
now supports aplatform
parameter (JSON encoded OCI Platform type) that lets you specify a platform to load/save. Not passing this parameter results in loading/saving the full multi-platform image. moby/moby#48295- Improve errors for invalid width/height on container resize and exec resize moby/moby#48679
- The
POST /containers/create
endpoint now includes a warning in the response when setting the container-wideVolumeDriver
option in combination with volumes defined throughMounts
because theVolumeDriver
option has no effect on those volumes. This warning was previously generated by the CLI. moby/moby#48789 - containerd image store:
GET /images/json
andGET /images/{name}/json
responses now includesDescriptor
field, which contains an OCI descriptor of the image target. The new field is only populated if the daemon provides a multi-platform image store. moby/moby#48894 - containerd image store:
GET /containers/{name}/json
now returns anImageManifestDescriptor
field containing the OCI descriptor of the platform-specific image manifest of the image that was used to create the container. moby/moby#48855 - Add debug endpoints (
GET /debug/vars
,GET /debug/pprof/
,GET /debug/pprof/cmdline
,GET /debug/pprof/profile
,GET /debug/pprof/symbol
,GET /debug/pprof/trace
,GET /debug/pprof/{name}
) are now also accessible through the versioned-API paths (/v<API-version>/<endpoint>
). moby/moby#49051 - Fix API returning a
500
status code instead of400
for validation errors. moby/moby#49217 - Fix status codes for archive endpoints
HEAD /containers/{name:.*}/archive
,GET /containers/{name:.*}/archive
,PUT /containers/{name:.*}/archive
returning a500
status instead of a400
status. moby/moby#49219 POST /containers/create
now accepts awritable-cgroups=true
option inHostConfig.SecurityOpt
to mount the container's cgroups writable. This provides a more granular approach thanHostConfig.Privileged
. moby/moby#48828POST /build/prune
renameskeep-bytes
toreserved-space
and now supports additional prune parametersmax-used-space
andmin-free-space
. moby/moby#48720POST /networks/create
now has anEnableIPv4
field. Setting it tofalse
disables IPv4 IPAM for the network. moby/moby#48271GET /networks/{id}
now returns anEnableIPv4
field showing whether the network has IPv4 IPAM enabled. moby/moby#48271- User-defined bridge networks require either IPv4 or IPv6 address assignment to be enabled. IPv4 cannot be disabled for the default bridge network (
docker0
). moby/moby#48323 macvlan
andipvlan
networks can be created with address assignment disabled for IPv4, IPv6, or both address families. moby/moby#48299- IPv4 cannot be disabled for Windows or Swarm networks. moby/moby#48278
- Add a way to specify which network should provide the default gateway for a container.
moby/moby#48936
POST /networks/{id}/connect
andPOST /containers/create
now accept aGwPriority
field inEndpointsConfig
. This value is used to determine which network endpoint provides the default gateway for the container. The endpoint with the highest priority is selected. If multiple endpoints have the same priority, endpoints are sorted lexicographically by their network name, and the one that sorts first is picked. moby/moby#48746GET /containers/json
now returns aGwPriority
field inNetworkSettings
for each network endpoint. TheGwPriority
field is used by the CLI’s newgw-priority
option fordocker run
anddocker network connect
. moby/moby#48746
- Settings for
eth0
in--sysctl
options are no longer automatically migrated to the network endpoint. moby/moby#48746- For example, in the Docker CLI,
docker run --network mynet --sysctl net.ipv4.conf.eth0.log_martians=1 ...
is rejected. Instead, you must usedocker run --network name=mynet,driver-opt=com.docker.network.endpoint.sysctls=net.ipv4.conf.IFNAME.log_martians=1 ...
- For example, in the Docker CLI,
GET /containers/json
now returns anImageManifestDescriptor
field matching the same field in/containers/{name}/json
. This field is only populated if the daemon provides a multi-platform image store. moby/moby#49407
Removed
- The Fluent logger option
fluentd-async-connect
has been deprecated in v20.10 and is now removed. moby/moby#46114 - The
--time
option ondocker stop
anddocker restart
is deprecated and renamed to--timeout
. docker/cli#5485 - Go-SDK:
pkg/ioutils
: RemoveNewReaderErrWrapper
as it was never used. moby/moby#49258 - Go-SDK:
pkg/ioutils
: Remove deprecatedBytesPipe
,NewBytesPipe
,ErrClosed
,WriteCounter
,NewWriteCounter
,NewReaderErrWrapper
,NopFlusher
. moby/moby#49245 - Go-SDK:
pkg/ioutils
: Remove deprecatedNopWriter
andNopWriteCloser
. moby/moby#49256 - Go-SDK:
pkg/sysinfo
: Remove deprecated NumCPU. moby/moby#49242 - Go-SDK: Remove
pkg/broadcaster
, as it was only used internally moby/moby#49172 - Go-SDK: Remove deprecated
cli.Errors
type docker/cli#5549 - Remove
pkg/ioutils.ReadCloserWrapper
, as it was only used in tests. moby/moby#49237 - Remove deprecated
api-cors-header
config parameter and thedockerd
--api-cors-header
option moby/moby#48209 - Remove deprecated
APIEndpoint.Version
field,APIVersion
type, andAPIVersion1
andAPIVersion2
consts. moby/moby#49004 - Remove deprecated
api-cors-header
config parameter and the Docker daemon's--api-cors-header
option. docker/cli#5437 - Remove deprecated
pkg/directory
package moby/moby#48779 - Remove deprecated
pkg/dmsg.Dmesg()
moby/moby#48109 - Remove deprecated image/spec package, which was moved to a separate module (
github.com/moby/docker-image-spec
) moby/moby#48460 - Remove migration code and errors for the deprecated
logentries
logging driver. moby/moby#48891 - Remove support for deprecated external graph-driver plugins. moby/moby#48072
api/types
: Remove deprecatedcontainer.ContainerNode
andContainerJSONBase.Node
field. moby/moby#48107api/types
: Remove deprecated aliases:ImagesPruneReport
,VolumesPruneReport
,NetworkCreateRequest
,NetworkCreate
,NetworkListOptions
,NetworkCreateResponse
,NetworkInspectOptions
,NetworkConnect
,NetworkDisconnect
,EndpointResource
,NetworkResource
,NetworksPruneReport
,ExecConfig
,ExecStartCheck
,ContainerExecInspect
,ContainersPruneReport
,ContainerPathStat
,CopyToContainerOptions
,ContainerStats
,ImageSearchOptions
,ImageImportSource
,ImageLoadResponse
,ContainerNode
. moby/moby#48107libnetwork/iptables
: Remove deprecatedIPV
,Iptables
,IP6Tables
andPassthrough()
. moby/moby#49121pkg/archive
: Remove deprecatedCanonicalTarNameForPath
,NewTempArchive
,TempArchive
moby/moby#48708pkg/fileutils
: Remove deprecatedGetTotalUsedFds
moby/moby#49210pkg/ioutils
: RemoveOnEOFReader
, which was only used internally moby/moby#49170pkg/longpath
: Remove deprecatedPrefix
constant. moby/moby#48779pkg/stringid
: Remove deprecatedIsShortID
andValidateID
functions moby/moby#48705runconfig/opts
: Remove deprecatedConvertKVStringsToMap
moby/moby#48102runconfig
: Remove deprecatedContainerConfigWrapper
,SetDefaultNetModeIfBlank
,DefaultDaemonNetworkMode
,IsPreDefinedNetwork
moby/moby#48102container
: Remove deprecatedErrNameReserved
,ErrNameNotReserved
. moby/moby#48728- Remove
Daemon.ContainerInspectCurrent()
method and changeDaemon.ContainerInspect()
signature to accept abackend.ContainerInspectOptions
struct moby/moby#48672 - Remove deprecated
Daemon.Exists()
andDaemon.IsPaused()
methods. moby/moby#48723
Deprecations
- API: The
BridgeNfIptables
andBridgeNfIp6tables
fields in theGET /info
response are now always befalse
and will be omitted in API v1.49. The netfilter module is now loaded on-demand, and no longer during daemon startup, making these fields obsolete. moby/moby#49114 - API: The
error
andprogress
fields in streaming responses for endpoints that return a JSON progress response, such asPOST /images/create
,POST /images/{name}/push
, andPOST /build
are deprecated. moby/moby#49447- Users should use the information in the
errorDetail
andprogressDetail
fields instead. - These fields were marked deprecated in API v1.4 (docker v0.6.0) and API v1.8 (docker v0.7.1) respectively, but still returned.
- These fields will be left empty or will be omitted in a future API version.
- Users should use the information in the
- Deprecate
Daemon.Register()
. This function is unused and will be removed in the next release. moby/moby#48702 - Deprecate
client.ImageInspectWithRaw
function in favor of the newclient.ImageInspect
. moby/moby#48264 - Deprecate
daemon/config.Config.ValidatePlatformConfig()
. This method was used as helper forconfig.Validate
, which should be used instead. moby/moby#48985 - Deprecate
pkg/reexec
. This package is deprecated and moved to a separate module. Usegithub.com/moby/sys/reexec
instead. moby/moby#49129 - Deprecate configuration for pushing non-distributable artifacts docker/cli#5724
- Deprecate the
--allow-nondistributable-artifacts
daemon flag and correspondingallow-nondistributable-artifacts
field indaemon.json
. Setting either option will no longer take an effect, but a deprecation warning log is added. moby/moby#49065 - Deprecate the
RegistryConfig.AllowNondistributableArtifactsCIDRs
andRegistryConfig.AllowNondistributableArtifactsHostnames
fields in theGET /info
API response. For API version v1.48 and older, the fields are still included in the response, but alwaysnull
. In API version v1.49 and later, the field will be omitted entirely. moby/moby#49065 - Go-SDK: Deprecate
registry.ServiceOptions.AllowNondistributableArtifacts
field. moby/moby#49065 - Go-SDK: The
BridgeNfIptables
,BridgeNfIp6tables
fields inapi/types/system.Info
andBridgeNFCallIPTablesDisabled
,BridgeNFCallIP6TablesDisabled
fields inpkg/sysinfo.SysInfo
are deprecated and will be removed in the next release. moby/moby#49114 - Go-SDK:
client
: DeprecateCommonAPIClient
interface in favor of theAPIClient
interface. TheCommonAPIClient
will be changed to an alias forAPIClient
in the next release, and removed in the release after. moby/moby#49388 - Go-SDK:
client
: DeprecateErrorConnectionFailed
helper. This function was only used internally, and will be removed in the next release. moby/moby#49389 - Go-SDK:
pkg/ioutils
: DeprecateNewAtomicFileWriter
,AtomicWriteFile
,AtomicWriteSet
,NewAtomicWriteSet
in favor ofpkg/atomicwriter
equivalents. moby/moby#49171 - Go-SDK:
pkg/sysinfo
: DeprecateNumCPU
. This utility has the same behavior asruntime.NumCPU
. moby/moby#49241 - Go-SDK:
pkg/system
: DeprecateMkdirAll
. This function provided custom handling for Windows GUID volume paths. Handling for such paths is now supported by Go standard library in go1.22 and newer, and this function is now an alias foros.MkdirAll
, which should be used instead. This alias will be removed in the next release. moby/moby#49162 - Go-SDK: Deprecate
pkg/parsers.ParseKeyValueOpt
. moby/moby#49177 - Go-SDK: Deprecate
pkg/parsers.ParseUintListMaximum
,pkg/parsers.ParseUintList
. These utilities were only used internally and will be removed in the next release. moby/moby#49222 - Go-SDK: Deprecate
api/type.IDResponse
in favor ofcontainer.CommitResponse
andcontainer.ExecCreateResponse
, which are currently an alias, but may become distinct types in a future release. This type will be removed in the next release. moby/moby#49446 - Go-SDK: Deprecate
api/types/container.ContainerUpdateOKBody
in favor ofUpdateResponse
. This type will be removed in the next release. moby/moby#49442 - Go-SDK: Deprecate
api/types/container.ContainerTopOKBody
in favor ofTopResponse
. This type will be removed in the next release. moby/moby#49442 - Go-SDK:
pkg/jsonmessage
: Fix deprecation ofProgressMessage
,ErrorMessage
, which were deprecated in Docker v0.6.0 and v0.7.1 respectively. moby/moby#49447 - Move
GraphDriverData
fromapi/types
toapi/types/storage
. The old type is deprecated and will be removed in the next release. moby/moby#48108 - Move
RequestPrivilegeFunc
fromapi/types
toapi/types/registry
. The old type is deprecated and will be removed in the next release. moby/moby#48119 - Move from
api/types
toapi/types/container
-NetworkSettings
,NetworkSettingsBase
,DefaultNetworkSettings
,SummaryNetworkSettings
,Health
,HealthcheckResult
,NoHealthcheck
,Starting
,Healthy
, andUnhealthy
constants,MountPoint
,Port
,ContainerState
,Container
,ContainerJSONBase
,ContainerJSON
,ContainerNode
. The old types are deprecated and will be removed in the next release. moby/moby#48108 - Move from
api/types
toapi/types/image
-ImageInspect
,RootFS
. The old types are deprecated and will be removed in the next release. moby/moby#48108 ContainerdCommit.Expected
,RuncCommit.Expected
, andInitCommit.Expected
fields in theGET /info
endpoint are deprecated and will be omitted in API v1.49. moby/moby#48478api/types/registry
: DeprecateServiceConfig.AllowNondistributableArtifactsCIDRs
andServiceConfig.AllowNondistributableArtifactsHostnames
fields. These fields will be removed in the next release. moby/moby#49065api/types/system/Commit.Expected
field is deprecated and should no longer be used. moby/moby#48478daemon/graphdriver
: DeprecateGetDriver()
moby/moby#48079libnetwork/iptables
: DeprecatePassthrough
. This function was only used internally, and will be removed in the next release. moby/moby#49115pkg/directory.Size()
function is deprecated, and will be removed in the next release. moby/moby#48057registry
: DeprecateAPIEndpoint.TrimHostName
; hostname is now trimmed unconditionally for remote names. This field will be removed in the next release. moby/moby#49005allow-nondistributable-artifacts
field indaemon.json
. Setting either option will no longer take effect, but a deprecation warning log is added to raise awareness about the deprecation. This warning is planned to become an error in the next release. moby/moby#49065",,,
6d660f316a4f68e1ff8663f1e6fc6670db3378937e6353efe355466f6911ff4b,"Amazon S3 cache
The s3
cache storage uploads your resulting build cache to
Amazon S3 file storage service
or other S3-compatible services, such as
MinIO.
This cache storage backend is not supported with the default docker
driver.
To use this feature, create a new builder using a different driver. See
Build drivers for more information.
Synopsis
$ docker buildx build --push -t <user>/<image> \
--cache-to type=s3,region=<region>,bucket=<bucket>,name=<cache-image>[,parameters...] \
--cache-from type=s3,region=<region>,bucket=<bucket>,name=<cache-image> .
The following table describes the available CSV parameters that you can pass to
--cache-to
and --cache-from
.
| Name | Option | Type | Default | Description |
|---|---|---|---|---|
region | cache-to ,cache-from | String | Required. Geographic location. | |
bucket | cache-to ,cache-from | String | Required. Name of the S3 bucket. | |
name | cache-to ,cache-from | String | Name of the cache image. | |
endpoint_url | cache-to ,cache-from | String | Endpoint of the S3 bucket. | |
blobs_prefix | cache-to ,cache-from | String | Prefix to prepend to blob filenames. | |
upload_parallelism | cache-to | Integer | 4 | Number of parallel layer uploads. |
touch_refresh | cache-to | Time | 24h | Interval for updating the timestamp of unchanged cache layers. |
manifests_prefix | cache-to ,cache-from | String | Prefix to prepend on manifest filenames. | |
use_path_style | cache-to ,cache-from | Boolean | false | When true , uses bucket in the URL instead of hostname. |
access_key_id | cache-to ,cache-from | String | See authentication. | |
secret_access_key | cache-to ,cache-from | String | See authentication. | |
session_token | cache-to ,cache-from | String | See authentication. | |
mode | cache-to | min ,max | min | Cache layers to export, see cache mode. |
ignore-error | cache-to | Boolean | false | Ignore errors caused by failed cache exports. |
Authentication
Buildx can reuse existing AWS credentials, configured either using a
credentials file or environment variables, for pushing and pulling cache to S3.
Alternatively, you can use the access_key_id
, secret_access_key
, and
session_token
attributes to specify credentials directly on the CLI.
Refer to AWS Go SDK, Specifying Credentials for details about authentication using environment variables and credentials file.
Further reading
For an introduction to caching see Docker build cache.
For more information on the s3
cache backend, see the
BuildKit README.",,,
6370cc0c5ff74ea5bc3ed7bf56bcdae4ce32cb5c06191f05f5e445c0204ac6c9,"Integrate Docker Scout with GitLab CI/CD
The following examples runs in GitLab CI in a repository containing a Docker image's definition and contents. Triggered by a commit, the pipeline builds the image. If the commit was to the default branch, it uses Docker Scout to get a CVE report. If the commit was to a different branch, it uses Docker Scout to compare the new version to the current published version.
Steps
First, set up the rest of the workflow. There's a lot that's not specific to Docker Scout but needed to create the images to compare.
Add the following to a .gitlab-ci.yml
file at the root of your repository.
docker-build:
image: docker:latest
stage: build
services:
- docker:dind
before_script:
- docker login -u ""$CI_REGISTRY_USER"" -p ""$CI_REGISTRY_PASSWORD"" $CI_REGISTRY
# Install curl and the Docker Scout CLI
- |
apk add --update curl
curl -sSfL https://raw.githubusercontent.com/docker/scout-cli/main/install.sh | sh -s --
apk del curl
rm -rf /var/cache/apk/*
# Login to Docker Hub required for Docker Scout CLI
- echo ""$DOCKER_HUB_PAT"" | docker login -u ""$DOCKER_HUB_USER"" --password-stdin
This sets up the workflow to build Docker images with Docker-in-Docker mode, running Docker inside a container.
It then downloads curl
and the Docker Scout CLI plugin, logs into the Docker
registry using environment variables defined in your repository's settings.
Add the following to the YAML file:
script:
- |
if [[ ""$CI_COMMIT_BRANCH"" == ""$CI_DEFAULT_BRANCH"" ]]; then
tag=""""
echo ""Running on default branch '$CI_DEFAULT_BRANCH': tag = 'latest'""
else
tag="":$CI_COMMIT_REF_SLUG""
echo ""Running on branch '$CI_COMMIT_BRANCH': tag = $tag""
fi
- docker build --pull -t ""$CI_REGISTRY_IMAGE${tag}"" .
- |
if [[ ""$CI_COMMIT_BRANCH"" == ""$CI_DEFAULT_BRANCH"" ]]; then
# Get a CVE report for the built image and fail the pipeline when critical or high CVEs are detected
docker scout cves ""$CI_REGISTRY_IMAGE${tag}"" --exit-code --only-severity critical,high
else
# Compare image from branch with latest image from the default branch and fail if new critical or high CVEs are detected
docker scout compare ""$CI_REGISTRY_IMAGE${tag}"" --to ""$CI_REGISTRY_IMAGE:latest"" --exit-code --only-severity critical,high --ignore-unchanged
fi
- docker push ""$CI_REGISTRY_IMAGE${tag}""
This creates the flow mentioned previously. If the commit was to the default branch, Docker Scout generates a CVE report. If the commit was to a different branch, Docker Scout compares the new version to the current published version. It only shows critical or high-severity vulnerabilities and ignores vulnerabilities that haven't changed since the last analysis.
Add the following to the YAML file:
rules:
- if: $CI_COMMIT_BRANCH
exists:
- Dockerfile
These final lines ensure that the pipeline only runs if the commit contains a Dockerfile and if the commit was to the CI branch.
Video walkthrough
The following is a video walkthrough of the process of setting up the workflow with GitLab.",,,
32d0e1aae4bb0f18b53a0799679dadbb0898c3f9b4c46d96003cfffcbc4322b3,"IPvlan network driver
The IPvlan driver gives users total control over both IPv4 and IPv6 addressing. The VLAN driver builds on top of that in giving operators complete control of layer 2 VLAN tagging and even IPvlan L3 routing for users interested in underlay network integration. For overlay deployments that abstract away physical constraints see the multi-host overlay driver.
IPvlan is a new twist on the tried and true network virtualization technique. The Linux implementations are extremely lightweight because rather than using the traditional Linux bridge for isolation, they are associated to a Linux Ethernet interface or sub-interface to enforce separation between networks and connectivity to the physical network.
IPvlan offers a number of unique features and plenty of room for further innovations with the various modes. Two high level advantages of these approaches are, the positive performance implications of bypassing the Linux bridge and the simplicity of having fewer moving parts. Removing the bridge that traditionally resides in between the Docker host NIC and container interface leaves a simple setup consisting of container interfaces, attached directly to the Docker host interface. This result is easy to access for external facing services as there is no need for port mappings in these scenarios.
Options
The following table describes the driver-specific options that you can pass to
--opt
when creating a network using the ipvlan
driver.
| Option | Default | Description |
|---|---|---|
ipvlan_mode | l2 | Sets the IPvlan operating mode. Can be one of: l2 , l3 , l3s |
ipvlan_flag | bridge | Sets the IPvlan mode flag. Can be one of: bridge , private , vepa |
parent | Specifies the parent interface to use. |
Examples
Prerequisites
- The examples on this page are all single host.
- All examples can be performed on a single host running Docker. Any
example using a sub-interface like
eth0.10
can be replaced witheth0
or any other valid parent interface on the Docker host. Sub-interfaces with a.
are created on the fly.-o parent
interfaces can also be left out of thedocker network create
all together and the driver will create adummy
interface that will enable local host connectivity to perform the examples. - Kernel requirements:
- IPvlan Linux kernel v4.2+ (support for earlier kernels exists but is buggy). To check your current kernel version, use
uname -r
- IPvlan Linux kernel v4.2+ (support for earlier kernels exists but is buggy). To check your current kernel version, use
IPvlan L2 mode example usage
An example of the IPvlan L2
mode topology is shown in the following image.
The driver is specified with -d driver_name
option. In this case -d ipvlan
.
The parent interface in the next example -o parent=eth0
is configured as follows:
$ ip addr show eth0
3: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
inet 192.168.1.250/24 brd 192.168.1.255 scope global eth0
Use the network from the host's interface as the --subnet
in the
docker network create
. The container will be attached to the same network as
the host interface as set via the -o parent=
option.
Create the IPvlan network and run a container attaching to it:
# IPvlan (-o ipvlan_mode= Defaults to L2 mode if not specified)
$ docker network create -d ipvlan \
--subnet=192.168.1.0/24 \
--gateway=192.168.1.1 \
-o ipvlan_mode=l2 \
-o parent=eth0 db_net
# Start a container on the db_net network
$ docker run --net=db_net -it --rm alpine /bin/sh
# NOTE: the containers can NOT ping the underlying host interfaces as
# they are intentionally filtered by Linux for additional isolation.
The default mode for IPvlan is l2
. If -o ipvlan_mode=
is left unspecified,
the default mode will be used. Similarly, if the --gateway
is left empty, the
first usable address on the network will be set as the gateway. For example, if
the subnet provided in the network create is --subnet=192.168.1.0/24
then the
gateway the container receives is 192.168.1.1
.
To help understand how this mode interacts with other hosts, the following figure shows the same layer 2 segment between two Docker hosts that applies to and IPvlan L2 mode.
The following will create the exact same network as the network db_net
created
earlier, with the driver defaults for --gateway=192.168.1.1
and -o ipvlan_mode=l2
.
# IPvlan (-o ipvlan_mode= Defaults to L2 mode if not specified)
$ docker network create -d ipvlan \
--subnet=192.168.1.0/24 \
-o parent=eth0 db_net_ipv
# Start a container with an explicit name in daemon mode
$ docker run --net=db_net_ipv --name=ipv1 -itd alpine /bin/sh
# Start a second container and ping using the container name
# to see the docker included name resolution functionality
$ docker run --net=db_net_ipv --name=ipv2 -it --rm alpine /bin/sh
$ ping -c 4 ipv1
# NOTE: the containers can NOT ping the underlying host interfaces as
# they are intentionally filtered by Linux for additional isolation.
The drivers also support the --internal
flag that will completely isolate
containers on a network from any communications external to that network. Since
network isolation is tightly coupled to the network's parent interface the result
of leaving the -o parent=
option off of a docker network create
is the exact
same as the --internal
option. If the parent interface is not specified or the
--internal
flag is used, a netlink type dummy
parent interface is created
for the user and used as the parent interface effectively isolating the network
completely.
The following two docker network create
examples result in identical networks
that you can attach container to:
# Empty '-o parent=' creates an isolated network
$ docker network create -d ipvlan \
--subnet=192.168.10.0/24 isolated1
# Explicit '--internal' flag is the same:
$ docker network create -d ipvlan \
--subnet=192.168.11.0/24 --internal isolated2
# Even the '--subnet=' can be left empty and the default
# IPAM subnet of 172.18.0.0/16 will be assigned
$ docker network create -d ipvlan isolated3
$ docker run --net=isolated1 --name=cid1 -it --rm alpine /bin/sh
$ docker run --net=isolated2 --name=cid2 -it --rm alpine /bin/sh
$ docker run --net=isolated3 --name=cid3 -it --rm alpine /bin/sh
# To attach to any use `docker exec` and start a shell
$ docker exec -it cid1 /bin/sh
$ docker exec -it cid2 /bin/sh
$ docker exec -it cid3 /bin/sh
IPvlan 802.1Q trunk L2 mode example usage
Architecturally, IPvlan L2 mode trunking is the same as Macvlan with regard to gateways and L2 path isolation. There are nuances that can be advantageous for CAM table pressure in ToR switches, one MAC per port and MAC exhaustion on a host's parent NIC to name a few. The 802.1Q trunk scenario looks the same. Both modes adhere to tagging standards and have seamless integration with the physical network for underlay integration and hardware vendor plugin integrations.
Hosts on the same VLAN are typically on the same subnet and almost always are grouped together based on their security policy. In most scenarios, a multi-tier application is tiered into different subnets because the security profile of each process requires some form of isolation. For example, hosting your credit card processing on the same virtual network as the frontend webserver would be a regulatory compliance issue, along with circumventing the long standing best practice of layered defense in depth architectures. VLANs or the equivocal VNI (Virtual Network Identifier) when using the Overlay driver, are the first step in isolating tenant traffic.
The Linux sub-interface tagged with a VLAN can either already exist or will be
created when you call a docker network create
. docker network rm
will delete
the sub-interface. Parent interfaces such as eth0
are not deleted, only
sub-interfaces with a netlink parent index > 0.
For the driver to add/delete the VLAN sub-interfaces the format needs to be
interface_name.vlan_tag
. Other sub-interface naming can be used as the
specified parent, but the link will not be deleted automatically when
docker network rm
is invoked.
The option to use either existing parent VLAN sub-interfaces or let Docker manage
them enables the user to either completely manage the Linux interfaces and
networking or let Docker create and delete the VLAN parent sub-interfaces
(netlink ip link
) with no effort from the user.
For example: use eth0.10
to denote a sub-interface of eth0
tagged with the
VLAN id of 10
. The equivalent ip link
command would be
ip link add link eth0 name eth0.10 type vlan id 10
.
The example creates the VLAN tagged networks and then starts two containers to test connectivity between containers. Different VLANs cannot ping one another without a router routing between the two networks. The default namespace is not reachable per IPvlan design in order to isolate container namespaces from the underlying host.
VLAN ID 20
In the first network tagged and isolated by the Docker host, eth0.20
is the
parent interface tagged with VLAN id 20
specified with -o parent=eth0.20
.
Other naming formats can be used, but the links need to be added and deleted
manually using ip link
or Linux configuration files. As long as the -o parent
exists, anything can be used if it is compliant with Linux netlink.
# now add networks and hosts as you would normally by attaching to the master (sub)interface that is tagged
$ docker network create -d ipvlan \
--subnet=192.168.20.0/24 \
--gateway=192.168.20.1 \
-o parent=eth0.20 ipvlan20
# in two separate terminals, start a Docker container and the containers can now ping one another.
$ docker run --net=ipvlan20 -it --name ivlan_test1 --rm alpine /bin/sh
$ docker run --net=ipvlan20 -it --name ivlan_test2 --rm alpine /bin/sh
VLAN ID 30
In the second network, tagged and isolated by the Docker host, eth0.30
is the
parent interface tagged with VLAN id 30
specified with -o parent=eth0.30
. The
ipvlan_mode=
defaults to l2 mode ipvlan_mode=l2
. It can also be explicitly
set with the same result as shown in the next example.
# now add networks and hosts as you would normally by attaching to the master (sub)interface that is tagged.
$ docker network create -d ipvlan \
--subnet=192.168.30.0/24 \
--gateway=192.168.30.1 \
-o parent=eth0.30 \
-o ipvlan_mode=l2 ipvlan30
# in two separate terminals, start a Docker container and the containers can now ping one another.
$ docker run --net=ipvlan30 -it --name ivlan_test3 --rm alpine /bin/sh
$ docker run --net=ipvlan30 -it --name ivlan_test4 --rm alpine /bin/sh
The gateway is set inside of the container as the default gateway. That gateway would typically be an external router on the network.
$$ ip route
default via 192.168.30.1 dev eth0
192.168.30.0/24 dev eth0 src 192.168.30.2
Example: Multi-Subnet IPvlan L2 Mode starting two containers on the same subnet
and pinging one another. In order for the 192.168.114.0/24
to reach
192.168.116.0/24
it requires an external router in L2 mode. L3 mode can route
between subnets that share a common -o parent=
.
Secondary addresses on network routers are common as an address space becomes exhausted to add another secondary to an L3 VLAN interface or commonly referred to as a ""switched virtual interface"" (SVI).
$ docker network create -d ipvlan \
--subnet=192.168.114.0/24 --subnet=192.168.116.0/24 \
--gateway=192.168.114.254 --gateway=192.168.116.254 \
-o parent=eth0.114 \
-o ipvlan_mode=l2 ipvlan114
$ docker run --net=ipvlan114 --ip=192.168.114.10 -it --rm alpine /bin/sh
$ docker run --net=ipvlan114 --ip=192.168.114.11 -it --rm alpine /bin/sh
A key takeaway is, operators have the ability to map their physical network into
their virtual network for integrating containers into their environment with no
operational overhauls required. NetOps drops an 802.1Q trunk into the
Docker host. That virtual link would be the -o parent=
passed in the network
creation. For untagged (non-VLAN) links, it is as simple as -o parent=eth0
or
for 802.1Q trunks with VLAN IDs each network gets mapped to the corresponding
VLAN/Subnet from the network.
An example being, NetOps provides VLAN ID and the associated subnets for VLANs
being passed on the Ethernet link to the Docker host server. Those values are
plugged into the docker network create
commands when provisioning the
Docker networks. These are persistent configurations that are applied every time
the Docker engine starts which alleviates having to manage often complex
configuration files. The network interfaces can also be managed manually by
being pre-created and Docker networking will never modify them, and use them
as parent interfaces. Example mappings from NetOps to Docker network commands
are as follows:
- VLAN: 10, Subnet: 172.16.80.0/24, Gateway: 172.16.80.1
--subnet=172.16.80.0/24 --gateway=172.16.80.1 -o parent=eth0.10
- VLAN: 20, IP subnet: 172.16.50.0/22, Gateway: 172.16.50.1
--subnet=172.16.50.0/22 --gateway=172.16.50.1 -o parent=eth0.20
- VLAN: 30, Subnet: 10.1.100.0/16, Gateway: 10.1.100.1
--subnet=10.1.100.0/16 --gateway=10.1.100.1 -o parent=eth0.30
IPvlan L3 mode example
IPvlan will require routes to be distributed to each endpoint. The driver only builds the IPvlan L3 mode port and attaches the container to the interface. Route distribution throughout a cluster is beyond the initial implementation of this single host scoped driver. In L3 mode, the Docker host is very similar to a router starting new networks in the container. They are on networks that the upstream network will not know about without route distribution. For those curious how IPvlan L3 will fit into container networking, see the following examples.
IPvlan L3 mode drops all broadcast and multicast traffic. This reason alone makes IPvlan L3 mode a prime candidate for those looking for massive scale and predictable network integrations. It is predictable and in turn will lead to greater uptimes because there is no bridging involved. Bridging loops have been responsible for high profile outages that can be hard to pinpoint depending on the size of the failure domain. This is due to the cascading nature of BPDUs (Bridge Port Data Units) that are flooded throughout a broadcast domain (VLAN) to find and block topology loops. Eliminating bridging domains, or at the least, keeping them isolated to a pair of ToRs (top of rack switches) will reduce hard to troubleshoot bridging instabilities. IPvlan L2 modes is well suited for isolated VLANs only trunked into a pair of ToRs that can provide a loop-free non-blocking fabric. The next step further is to route at the edge via IPvlan L3 mode that reduces a failure domain to a local host only.
- L3 mode needs to be on a separate subnet as the default namespace since it requires a netlink route in the default namespace pointing to the IPvlan parent interface.
- The parent interface used in this example is
eth0
and it is on the subnet192.168.1.0/24
. Notice thedocker network
is not on the same subnet aseth0
. - Unlike IPvlan l2 modes, different subnets/networks can ping one another as
long as they share the same parent interface
-o parent=
.
$$ ip a show eth0
3: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
link/ether 00:50:56:39:45:2e brd ff:ff:ff:ff:ff:ff
inet 192.168.1.250/24 brd 192.168.1.255 scope global eth0
- A traditional gateway doesn't mean much to an L3 mode IPvlan interface since
there is no broadcast traffic allowed. Because of that, the container default
gateway points to the containers
eth0
device. See below for CLI output ofip route
orip -6 route
from inside an L3 container for details.
The mode -o ipvlan_mode=l3
must be explicitly specified since the default
IPvlan mode is l2
.
The following example does not specify a parent interface. The network drivers will create a dummy type link for the user rather than rejecting the network creation and isolating containers from only communicating with one another.
# Create the IPvlan L3 network
$ docker network create -d ipvlan \
--subnet=192.168.214.0/24 \
--subnet=10.1.214.0/24 \
-o ipvlan_mode=l3 ipnet210
# Test 192.168.214.0/24 connectivity
$ docker run --net=ipnet210 --ip=192.168.214.10 -itd alpine /bin/sh
$ docker run --net=ipnet210 --ip=10.1.214.10 -itd alpine /bin/sh
# Test L3 connectivity from 10.1.214.0/24 to 192.168.214.0/24
$ docker run --net=ipnet210 --ip=192.168.214.9 -it --rm alpine ping -c 2 10.1.214.10
# Test L3 connectivity from 192.168.214.0/24 to 10.1.214.0/24
$ docker run --net=ipnet210 --ip=10.1.214.9 -it --rm alpine ping -c 2 192.168.214.10
Note
Notice that there is no
--gateway=
option in the network create. The field is ignored if one is specifiedl3
mode. Take a look at the container routing table from inside of the container:# Inside an L3 mode container $$ ip route default dev eth0 192.168.214.0/24 dev eth0 src 192.168.214.10
In order to ping the containers from a remote Docker host or the container be able to ping a remote host, the remote host or the physical network in between need to have a route pointing to the host IP address of the container's Docker host eth interface.
Dual stack IPv4 IPv6 IPvlan L2 mode
Not only does Libnetwork give you complete control over IPv4 addressing, but it also gives you total control over IPv6 addressing as well as feature parity between the two address families.
The next example will start with IPv6 only. Start two containers on the same VLAN
139
and ping one another. Since the IPv4 subnet is not specified, the default IPAM will provision a default IPv4 subnet. That subnet is isolated unless the upstream network is explicitly routing it on VLAN139
.
# Create a v6 network
$ docker network create -d ipvlan \
--ipv6 --subnet=2001:db8:abc2::/64 --gateway=2001:db8:abc2::22 \
-o parent=eth0.139 v6ipvlan139
# Start a container on the network
$ docker run --net=v6ipvlan139 -it --rm alpine /bin/sh
View the container eth0 interface and v6 routing table:
# Inside the IPv6 container
$$ ip a show eth0
75: eth0@if55: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default
link/ether 00:50:56:2b:29:40 brd ff:ff:ff:ff:ff:ff
inet 172.18.0.2/16 scope global eth0
valid_lft forever preferred_lft forever
inet6 2001:db8:abc4::250:56ff:fe2b:2940/64 scope link
valid_lft forever preferred_lft forever
inet6 2001:db8:abc2::1/64 scope link nodad
valid_lft forever preferred_lft forever
$$ ip -6 route
2001:db8:abc4::/64 dev eth0 proto kernel metric 256
2001:db8:abc2::/64 dev eth0 proto kernel metric 256
default via 2001:db8:abc2::22 dev eth0 metric 1024
Start a second container and ping the first container's v6 address.
# Test L2 connectivity over IPv6
$ docker run --net=v6ipvlan139 -it --rm alpine /bin/sh
# Inside the second IPv6 container
$$ ip a show eth0
75: eth0@if55: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default
link/ether 00:50:56:2b:29:40 brd ff:ff:ff:ff:ff:ff
inet 172.18.0.3/16 scope global eth0
valid_lft forever preferred_lft forever
inet6 2001:db8:abc4::250:56ff:fe2b:2940/64 scope link tentative dadfailed
valid_lft forever preferred_lft forever
inet6 2001:db8:abc2::2/64 scope link nodad
valid_lft forever preferred_lft forever
$$ ping6 2001:db8:abc2::1
PING 2001:db8:abc2::1 (2001:db8:abc2::1): 56 data bytes
64 bytes from 2001:db8:abc2::1%eth0: icmp_seq=0 ttl=64 time=0.044 ms
64 bytes from 2001:db8:abc2::1%eth0: icmp_seq=1 ttl=64 time=0.058 ms
2 packets transmitted, 2 packets received, 0% packet loss
round-trip min/avg/max/stddev = 0.044/0.051/0.058/0.000 ms
The next example with setup a dual stack IPv4/IPv6 network with an example
VLAN ID of 140
.
Next create a network with two IPv4 subnets and one IPv6 subnets, all of which have explicit gateways:
$ docker network create -d ipvlan \
--subnet=192.168.140.0/24 --subnet=192.168.142.0/24 \
--gateway=192.168.140.1 --gateway=192.168.142.1 \
--subnet=2001:db8:abc9::/64 --gateway=2001:db8:abc9::22 \
-o parent=eth0.140 \
-o ipvlan_mode=l2 ipvlan140
Start a container and view eth0 and both v4 & v6 routing tables:
$ docker run --net=ipvlan140 --ip6=2001:db8:abc2::51 -it --rm alpine /bin/sh
$ ip a show eth0
78: eth0@if77: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default
link/ether 00:50:56:2b:29:40 brd ff:ff:ff:ff:ff:ff
inet 192.168.140.2/24 scope global eth0
valid_lft forever preferred_lft forever
inet6 2001:db8:abc4::250:56ff:fe2b:2940/64 scope link
valid_lft forever preferred_lft forever
inet6 2001:db8:abc9::1/64 scope link nodad
valid_lft forever preferred_lft forever
$$ ip route
default via 192.168.140.1 dev eth0
192.168.140.0/24 dev eth0 proto kernel scope link src 192.168.140.2
$$ ip -6 route
2001:db8:abc4::/64 dev eth0 proto kernel metric 256
2001:db8:abc9::/64 dev eth0 proto kernel metric 256
default via 2001:db8:abc9::22 dev eth0 metric 1024
Start a second container with a specific --ip4
address and ping the first host
using IPv4 packets:
$ docker run --net=ipvlan140 --ip=192.168.140.10 -it --rm alpine /bin/sh
Note
Different subnets on the same parent interface in IPvlan
L2
mode cannot ping one another. That requires a router to proxy-arp the requests with a secondary subnet. However, IPvlanL3
will route the unicast traffic between disparate subnets as long as they share the same-o parent
parent link.
Dual stack IPv4 IPv6 IPvlan L3 mode
Example: IPvlan L3 Mode Dual Stack IPv4/IPv6, Multi-Subnet w/ 802.1Q VLAN Tag:118
As in all of the examples, a tagged VLAN interface does not have to be used. The
sub-interfaces can be swapped with eth0
, eth1
, bond0
or any other valid
interface on the host other then the lo
loopback.
The primary difference you will see is that L3 mode does not create a default
route with a next-hop but rather sets a default route pointing to dev eth
only
since ARP/Broadcasts/Multicast are all filtered by Linux as per the design. Since
the parent interface is essentially acting as a router, the parent interface IP
and subnet needs to be different from the container networks. That is the opposite
of bridge and L2 modes, which need to be on the same subnet (broadcast domain)
in order to forward broadcast and multicast packets.
# Create an IPv6+IPv4 Dual Stack IPvlan L3 network
# Gateways for both v4 and v6 are set to a dev e.g. 'default dev eth0'
$ docker network create -d ipvlan \
--subnet=192.168.110.0/24 \
--subnet=192.168.112.0/24 \
--subnet=2001:db8:abc6::/64 \
-o parent=eth0 \
-o ipvlan_mode=l3 ipnet110
# Start a few of containers on the network (ipnet110)
# in separate terminals and check connectivity
$ docker run --net=ipnet110 -it --rm alpine /bin/sh
# Start a second container specifying the v6 address
$ docker run --net=ipnet110 --ip6=2001:db8:abc6::10 -it --rm alpine /bin/sh
# Start a third specifying the IPv4 address
$ docker run --net=ipnet110 --ip=192.168.112.30 -it --rm alpine /bin/sh
# Start a 4th specifying both the IPv4 and IPv6 addresses
$ docker run --net=ipnet110 --ip6=2001:db8:abc6::50 --ip=192.168.112.50 -it --rm alpine /bin/sh
Interface and routing table outputs are as follows:
$$ ip a show eth0
63: eth0@if59: <BROADCAST,MULTICAST,NOARP,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN group default
link/ether 00:50:56:2b:29:40 brd ff:ff:ff:ff:ff:ff
inet 192.168.112.2/24 scope global eth0
valid_lft forever preferred_lft forever
inet6 2001:db8:abc4::250:56ff:fe2b:2940/64 scope link
valid_lft forever preferred_lft forever
inet6 2001:db8:abc6::10/64 scope link nodad
valid_lft forever preferred_lft forever
# Note the default route is the eth device because ARPs are filtered.
$$ ip route
default dev eth0 scope link
192.168.112.0/24 dev eth0 proto kernel scope link src 192.168.112.2
$$ ip -6 route
2001:db8:abc4::/64 dev eth0 proto kernel metric 256
2001:db8:abc6::/64 dev eth0 proto kernel metric 256
default dev eth0 metric 1024
Note
There may be a bug when specifying
--ip6=
addresses when you delete a container with a specified v6 address and then start a new container with the same v6 address it throws the following like the address isn't properly being released to the v6 pool. It will fail to unmount the container and be left dead.
docker: Error response from daemon: Address already in use.
Manually create 802.1Q links
VLAN ID 40
If a user does not want the driver to create the VLAN sub-interface, it needs to
exist before running docker network create
. If you have sub-interface
naming that is not interface.vlan_id
it is honored in the -o parent=
option
again as long as the interface exists and is up.
Links, when manually created, can be named anything as long as they exist when
the network is created. Manually created links do not get deleted regardless of
the name when the network is deleted with docker network rm
.
# create a new sub-interface tied to dot1q vlan 40
$ ip link add link eth0 name eth0.40 type vlan id 40
# enable the new sub-interface
$ ip link set eth0.40 up
# now add networks and hosts as you would normally by attaching to the master (sub)interface that is tagged
$ docker network create -d ipvlan \
--subnet=192.168.40.0/24 \
--gateway=192.168.40.1 \
-o parent=eth0.40 ipvlan40
# in two separate terminals, start a Docker container and the containers can now ping one another.
$ docker run --net=ipvlan40 -it --name ivlan_test5 --rm alpine /bin/sh
$ docker run --net=ipvlan40 -it --name ivlan_test6 --rm alpine /bin/sh
Example: VLAN sub-interface manually created with any name:
# create a new sub interface tied to dot1q vlan 40
$ ip link add link eth0 name foo type vlan id 40
# enable the new sub-interface
$ ip link set foo up
# now add networks and hosts as you would normally by attaching to the master (sub)interface that is tagged
$ docker network create -d ipvlan \
--subnet=192.168.40.0/24 --gateway=192.168.40.1 \
-o parent=foo ipvlan40
# in two separate terminals, start a Docker container and the containers can now ping one another.
$ docker run --net=ipvlan40 -it --name ivlan_test5 --rm alpine /bin/sh
$ docker run --net=ipvlan40 -it --name ivlan_test6 --rm alpine /bin/sh
Manually created links can be cleaned up with:
$ ip link del foo
As with all of the Libnetwork drivers, they can be mixed and matched, even as far as running 3rd party ecosystem drivers in parallel for maximum flexibility to the Docker user.",,,
af06123e45bf07a202a84acd58c5392a0242bdc56a9614c1c37e131be3627f55,"Change your subscription
Important
Starting July 1, 2024, Docker will begin collecting sales tax on subscription fees in compliance with state regulations for customers in the United States. For our global customers subject to VAT, the implementation will start rolling out on July 1, 2024. Note that while the roll out begins on this date, VAT charges may not apply to all applicable subscriptions immediately.
To ensure that tax assessments are correct, make sure that your billing information and VAT/Tax ID, if applicable, are updated. If you're exempt from sales tax, see Register a tax certificate.
The following sections describe how to change plans when you have a Docker subscription plan or legacy Docker subscription plan.
Note
Legacy Docker plans apply to Docker subscribers who last purchased or renewed their subscription before December 10, 2024. These subscribers will keep their current plan and pricing until their next renewal date that falls on or after December 10, 2024. To see purchase or renewal history, view your billing history. For more details about legacy subscriptions, see Announcing Upgraded Docker Plans.
Upgrade your subscription
When you upgrade a Docker plan, you immediately have access to all the features and entitlements available in your Docker subscription plan. For detailed information on features available in each subscription, see Docker Pricing.
To upgrade your Docker subscription:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- Optional. If you're upgrading from a free Personal plan to a Team plan and want to keep your username, convert your user account into an organization.
- Select the account you want to upgrade in the drop-down at the top-left of the page.
- Select Upgrade.
- Follow the on-screen instructions to complete your upgrade.
You can upgrade a legacy Docker Core, Docker Build Cloud, or Docker Scout subscription plan to a Docker subscription plan that includes access to all tools.
Contact Docker sales to upgrade your legacy Docker plan.
Downgrade your subscription
You can downgrade your Docker subscription at anytime before the renewal date. The unused portion of the subscription isn't refundable or creditable.
When you downgrade your subscription, access to paid features is available until the next billing cycle. The downgrade takes effect on the next billing cycle.
Important
If you downgrade your personal account from a Pro subscription to a Personal subscription, note that Personal subscriptions don't include collaborators for private repositories. Only one private repository is included with a Personal subscription. When you downgrade, all collaborators will be removed and additional private repositories are locked. Before you downgrade, consider the following:
- Team size: You may need to reduce the number of team members and convert any private repositories to public repositories or delete them. For information on features available in each tier, see Docker Pricing.
- SSO and SCIM: If you want to downgrade a Docker Business subscription and your organization uses single sign-on (SSO) for user authentication, you need to remove your SSO connection and verified domains before downgrading. After removing the SSO connection, any organization members that were auto-provisioned (for example, with SCIM) need to set up a password to sign in without SSO. To do this, users can reset their password at sign in.
If you have a sales-assisted Docker Business subscription, contact your account manager to downgrade your subscription.
To downgrade your Docker subscription:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- Select the account you want to downgrade in the drop-down at the top-left of the page.
- Select the action icon and then Cancel subscription.
- Review the cancellation warnings, then select Confirm cancellation.
- Optional. Fill out the feedback survey, or select Skip.
If you have a sales-assisted Docker Business subscription, contact your account manager to downgrade your subscription.
Downgrade Legacy Docker plan
To downgrade your legacy Docker subscription:
- Sign in to Docker Hub Billing.
- Select the account you want to downgrade in the drop-down at the top-left of the page.
- Select the link to Manage this account on Docker Hub.
- In the plan section, select Change plan.
- Follow the on-screen instructions to complete your downgrade.
Downgrade Docker Build Cloud subscription
To downgrade your Docker Build Cloud subscription:
- Sign in to Docker Home and open Docker Build Cloud.
- Select Account settings, then Downgrade.
- To confirm your downgrade, type DOWNGRADE in the text field and select Yes, continue.
- The account settings page will update with a notification bar notifying you of your downgrade date (start of next billing cycle).
Pause a subscription
You can't pause or delay a subscription. If a subscription invoice hasn't been paid on the due date, there's a 15 day grace period, including the due date.",,,
d85259735a310898ed6a4fd726c3d3b086788c938233ee26eacd02f219b7c0e5,"containerd image store with Docker Engine
containerd, the industry-standard container runtime, uses snapshotters instead
of the classic storage drivers for storing image and container data.
While the overlay2
driver still remains the default driver for Docker Engine,
you can opt in to using containerd snapshotters as an experimental feature.
To learn more about the containerd image store and its benefits, refer to containerd image store on Docker Desktop.
Enable containerd image store on Docker Engine
Switching to containerd snapshotters causes you to temporarily lose images and containers created using the classic storage drivers. Those resources still exist on your filesystem, and you can retrieve them by turning off the containerd snapshotters feature.
The following steps explain how to enable the containerd snapshotters feature.
Add the following configuration to your
/etc/docker/daemon.json
configuration file:{ ""features"": { ""containerd-snapshotter"": true } }
Save the file.
Restart the daemon for the changes to take effect.
$ sudo systemctl restart docker
After restarting the daemon, running docker info
shows that you're using
containerd snapshotter storage drivers.
$ docker info -f '{{ .DriverStatus }}'
[[driver-type io.containerd.snapshotter.v1]]
Docker Engine uses the overlayfs
containerd snapshotter by default.",,,
4ffb44368cc33d409f58d555561d7d1fa49ccf57e18aa7d98799f847342bd85b,"Use a proxy server with the Docker CLI
This page describes how to configure the Docker CLI to use proxies via environment variables in containers.
This page doesn't describe how to configure proxies for Docker Desktop. For instructions, see configuring Docker Desktop to use HTTP/HTTPS proxies.
If you're running Docker Engine without Docker Desktop, refer to
Configure the Docker daemon to use a proxy
to learn how to configure a proxy server for the Docker daemon (dockerd
) itself.
If your container needs to use an HTTP, HTTPS, or FTP proxy server, you can configure it in different ways:
Note
Unfortunately, there's no standard that defines how web clients should handle proxy environment variables, or the format for defining them.
If you're interested in the history of these variables, check out this blog post on the subject, by the GitLab team: We need to talk: Can we standardize NO_PROXY?.
Configure the Docker client
You can add proxy configurations for the Docker client using a JSON
configuration file, located in ~/.docker/config.json
.
Builds and containers use the configuration specified in this file.
{
""proxies"": {
""default"": {
""httpProxy"": ""http://proxy.example.com:3128"",
""httpsProxy"": ""https://proxy.example.com:3129"",
""noProxy"": ""*.test.example.com,.example.org,127.0.0.0/8""
}
}
}
Warning
Proxy settings may contain sensitive information. For example, some proxy servers require authentication information to be included in their URL, or their address may expose IP-addresses or hostnames of your company's environment.
Environment variables are stored as plain text in the container's configuration, and as such can be inspected through the remote API or committed to an image when using
docker commit
.
The configuration becomes active after saving the file, you don't need to restart Docker. However, the configuration only applies to new containers and builds, and doesn't affect existing containers.
The following table describes the available configuration parameters.
| Property | Description |
|---|---|
httpProxy | Sets the HTTP_PROXY and http_proxy environment variables and build arguments. |
httpsProxy | Sets the HTTPS_PROXY and https_proxy environment variables and build arguments. |
ftpProxy | Sets the FTP_PROXY and ftp_proxy environment variables and build arguments. |
noProxy | Sets the NO_PROXY and no_proxy environment variables and build arguments. |
allProxy | Sets the ALL_PROXY and all_proxy environment variables and build arguments. |
These settings are used to configure proxy environment variables for containers only, and not used as proxy settings for the Docker CLI or the Docker Engine itself. Refer to the environment variables and configure the Docker daemon to use a proxy server sections for configuring proxy settings for the CLI and daemon.
Run containers with a proxy configuration
When you start a container, its proxy-related environment variables are set
to reflect your proxy configuration in ~/.docker/config.json
.
For example, assuming a proxy configuration like the example shown in the earlier section, environment variables for containers that you run are set as follows:
$ docker run --rm alpine sh -c 'env | grep -i _PROXY'
https_proxy=http://proxy.example.com:3129
HTTPS_PROXY=http://proxy.example.com:3129
http_proxy=http://proxy.example.com:3128
HTTP_PROXY=http://proxy.example.com:3128
no_proxy=*.test.example.com,.example.org,127.0.0.0/8
NO_PROXY=*.test.example.com,.example.org,127.0.0.0/8
Build with a proxy configuration
When you invoke a build, proxy-related build arguments are pre-populated automatically, based on the proxy settings in your Docker client configuration file.
Assuming a proxy configuration like the example shown in the earlier section, environment are set as follows during builds:
$ docker build \
--no-cache \
--progress=plain \
- <<EOF
FROM alpine
RUN env | grep -i _PROXY
EOF
#5 [2/2] RUN env | grep -i _PROXY
#5 0.100 HTTPS_PROXY=https://proxy.example.com:3129
#5 0.100 no_proxy=*.test.example.com,.example.org,127.0.0.0/8
#5 0.100 NO_PROXY=*.test.example.com,.example.org,127.0.0.0/8
#5 0.100 https_proxy=https://proxy.example.com:3129
#5 0.100 http_proxy=http://proxy.example.com:3128
#5 0.100 HTTP_PROXY=http://proxy.example.com:3128
#5 DONE 0.1s
Configure proxy settings per daemon
The default
key under proxies
in ~/.docker/config.json
configures the proxy
settings for all daemons that the client connects to.
To configure the proxies for individual daemons,
use the address of the daemon instead of the default
key.
The following example configures both a default proxy config,
and a no-proxy override for the Docker daemon on address
tcp://docker-daemon1.example.com
:
{
""proxies"": {
""default"": {
""httpProxy"": ""http://proxy.example.com:3128"",
""httpsProxy"": ""https://proxy.example.com:3129"",
""noProxy"": ""*.test.example.com,.example.org,127.0.0.0/8""
},
""tcp://docker-daemon1.example.com"": {
""noProxy"": ""*.internal.example.net""
}
}
}
Set proxy using the CLI
Instead of
configuring the Docker client,
you can specify proxy configurations on the command-line when you invoke the
docker build
and docker run
commands.
Proxy configuration on the command-line uses the --build-arg
flag for builds,
and the --env
flag for when you want to run containers with a proxy.
$ docker build --build-arg HTTP_PROXY=""http://proxy.example.com:3128"" .
$ docker run --env HTTP_PROXY=""http://proxy.example.com:3128"" redis
For a list of all the proxy-related build arguments that you can use with the
docker build
command, see
Predefined ARGs.
These proxy values are only available in the build container.
They're not included in the build output.
Proxy as environment variable for builds
Don't use the ENV
Dockerfile instruction to specify proxy settings for builds.
Use build arguments instead.
Using environment variables for proxies embeds the configuration into the image. If the proxy is an internal proxy, it might not be accessible for containers created from that image.
Embedding proxy settings in images also poses a security risk, as the values may include sensitive information.",,,
47f27328558fe6ff2040fa5318863435eb76d68f477316101c065a590b463cf8,"Multi-stage builds
Multi-stage builds are useful to anyone who has struggled to optimize Dockerfiles while keeping them easy to read and maintain.
Use multi-stage builds
With multi-stage builds, you use multiple FROM
statements in your Dockerfile.
Each FROM
instruction can use a different base, and each of them begins a new
stage of the build. You can selectively copy artifacts from one stage to
another, leaving behind everything you don't want in the final image.
The following Dockerfile has two separate stages: one for building a binary, and another where the binary gets copied from the first stage into the next stage.
# syntax=docker/dockerfile:1
FROM golang:1.23
WORKDIR /src
COPY <<EOF ./main.go
package main
import ""fmt""
func main() {
fmt.Println(""hello, world"")
}
EOF
RUN go build -o /bin/hello ./main.go
FROM scratch
COPY --from=0 /bin/hello /bin/hello
CMD [""/bin/hello""]
You only need the single Dockerfile. No need for a separate build script. Just
run docker build
.
$ docker build -t hello .
The end result is a tiny production image with nothing but the binary inside. None of the build tools required to build the application are included in the resulting image.
How does it work? The second FROM
instruction starts a new build stage with
the scratch
image as its base. The COPY --from=0
line copies just the
built artifact from the previous stage into this new stage. The Go SDK and any
intermediate artifacts are left behind, and not saved in the final image.
Name your build stages
By default, the stages aren't named, and you refer to them by their integer
number, starting with 0 for the first FROM
instruction. However, you can
name your stages, by adding an AS <NAME>
to the FROM
instruction. This
example improves the previous one by naming the stages and using the name in
the COPY
instruction. This means that even if the instructions in your
Dockerfile are re-ordered later, the COPY
doesn't break.
# syntax=docker/dockerfile:1
FROM golang:1.23 AS build
WORKDIR /src
COPY <<EOF /src/main.go
package main
import ""fmt""
func main() {
fmt.Println(""hello, world"")
}
EOF
RUN go build -o /bin/hello ./main.go
FROM scratch
COPY --from=build /bin/hello /bin/hello
CMD [""/bin/hello""]
Stop at a specific build stage
When you build your image, you don't necessarily need to build the entire
Dockerfile including every stage. You can specify a target build stage. The
following command assumes you are using the previous Dockerfile
but stops at
the stage named build
:
$ docker build --target build -t hello .
A few scenarios where this might be useful are:
- Debugging a specific build stage
- Using a
debug
stage with all debugging symbols or tools enabled, and a leanproduction
stage - Using a
testing
stage in which your app gets populated with test data, but building for production using a different stage which uses real data
Use an external image as a stage
When using multi-stage builds, you aren't limited to copying from stages you
created earlier in your Dockerfile. You can use the COPY --from
instruction to
copy from a separate image, either using the local image name, a tag available
locally or on a Docker registry, or a tag ID. The Docker client pulls the image
if necessary and copies the artifact from there. The syntax is:
COPY --from=nginx:latest /etc/nginx/nginx.conf /nginx.conf
Use a previous stage as a new stage
You can pick up where a previous stage left off by referring to it when using
the FROM
directive. For example:
# syntax=docker/dockerfile:1
FROM alpine:latest AS builder
RUN apk --no-cache add build-base
FROM builder AS build1
COPY source1.cpp source.cpp
RUN g++ -o /binary source.cpp
FROM builder AS build2
COPY source2.cpp source.cpp
RUN g++ -o /binary source.cpp
Differences between legacy builder and BuildKit
The legacy Docker Engine builder processes all stages of a Dockerfile leading
up to the selected --target
. It will build a stage even if the selected
target doesn't depend on that stage.
BuildKit only builds the stages that the target stage depends on.
For example, given the following Dockerfile:
# syntax=docker/dockerfile:1
FROM ubuntu AS base
RUN echo ""base""
FROM base AS stage1
RUN echo ""stage1""
FROM base AS stage2
RUN echo ""stage2""
With
BuildKit enabled, building the
stage2
target in this Dockerfile means only base
and stage2
are processed.
There is no dependency on stage1
, so it's skipped.
$ DOCKER_BUILDKIT=1 docker build --no-cache -f Dockerfile --target stage2 .
[+] Building 0.4s (7/7) FINISHED
=> [internal] load build definition from Dockerfile 0.0s
=> => transferring dockerfile: 36B 0.0s
=> [internal] load .dockerignore 0.0s
=> => transferring context: 2B 0.0s
=> [internal] load metadata for docker.io/library/ubuntu:latest 0.0s
=> CACHED [base 1/2] FROM docker.io/library/ubuntu 0.0s
=> [base 2/2] RUN echo ""base"" 0.1s
=> [stage2 1/1] RUN echo ""stage2"" 0.2s
=> exporting to image 0.0s
=> => exporting layers 0.0s
=> => writing image sha256:f55003b607cef37614f607f0728e6fd4d113a4bf7ef12210da338c716f2cfd15 0.0s
On the other hand, building the same target without BuildKit results in all stages being processed:
$ DOCKER_BUILDKIT=0 docker build --no-cache -f Dockerfile --target stage2 .
Sending build context to Docker daemon 219.1kB
Step 1/6 : FROM ubuntu AS base
---> a7870fd478f4
Step 2/6 : RUN echo ""base""
---> Running in e850d0e42eca
base
Removing intermediate container e850d0e42eca
---> d9f69f23cac8
Step 3/6 : FROM base AS stage1
---> d9f69f23cac8
Step 4/6 : RUN echo ""stage1""
---> Running in 758ba6c1a9a3
stage1
Removing intermediate container 758ba6c1a9a3
---> 396baa55b8c3
Step 5/6 : FROM base AS stage2
---> d9f69f23cac8
Step 6/6 : RUN echo ""stage2""
---> Running in bbc025b93175
stage2
Removing intermediate container bbc025b93175
---> 09fc3770a9c4
Successfully built 09fc3770a9c4
The legacy builder processes stage1
, even if stage2
doesn't depend on it.",,,
33d32bca98548551214f5f534f87e2f85a5c06c01a372eef8df3faabf5f16a88,"Deprecated and retired Docker products and features
This document provides an overview of Docker features, products, and open-source projects that have been deprecated, retired, or transitioned.
Note
This page does not cover deprecated and removed Docker Engine features. For a detailed list of deprecated Docker Engine features, refer to the Docker Engine Deprecated Features documentation.
Products and features
Support for these deprecated or retired features is no longer provided by Docker, Inc. The projects that have been transitioned to third parties continue to receive updates from their new maintainers.
Docker Machine
Docker Machine was a tool for provisioning and managing Docker hosts across various platforms, including virtual machines and cloud providers. It is no longer maintained, and users are encouraged to use Docker Desktop or Docker Engine directly on supported platforms. Machine's approach to creating and configuring hosts has been superseded by more modern workflows that integrate more closely with Docker Desktop.
Docker Toolbox
Docker Toolbox was used on older systems where Docker Desktop could not run. It bundled Docker Machine, Docker Engine, and Docker Compose into a single installer. Toolbox is no longer maintained and is effectively replaced by Docker Desktop on current systems. References to Docker Toolbox occasionally appear in older documentation or community tutorials, but it is not recommended for new installations.
Docker Cloud integrations
Docker previously offered integrations for Amazon's Elastic Container Service (ECS) and Azure Container Instances (ACI) to streamline container workflows. These integrations have been deprecated, and users should now rely on native cloud tools or third-party solutions to manage their workloads. The move toward platform-specific or universal orchestration tools reduced the need for specialized Docker Cloud integrations.
You can still view the relevant documentation for these integrations in the Compose CLI repository.
Docker Enterprise Edition
Docker Enterprise Edition (EE) was Docker's commercial platform for deploying and managing large-scale container environments. It was acquired by Mirantis in 2019, and users looking for enterprise-level functionality can now explore Mirantis Kubernetes Engine or other products offered by Mirantis. Much of the technology and features found in Docker EE have been absorbed into the Mirantis product line.
Note
For information about enterprise-level features offered by Docker today, see the Docker Business subscription.
Docker Data Center and Docker Trusted Registry
Docker Data Center (DDC) was an umbrella term that encompassed Docker Universal Control Plane (UCP) and Docker Trusted Registry (DTR). These components provided a full-stack solution for managing containers, security, and registry services in enterprise environments. They are now under the Mirantis portfolio following the Docker Enterprise acquisition. Users still encountering references to DDC, UCP, or DTR should refer to Mirantis's documentation for guidance on modern equivalents.
Dev Environments
Dev Environments was a feature introduced in Docker Desktop that allowed developers to spin up development environments quickly. This feature is no longer under active development. Similar workflows can be achieved through Docker Compose or by creating custom configurations tailored to specific project requirements.
Open source projects
Several open-source projects originally maintained by Docker have been archived, discontinued, or transitioned to other maintainers or organizations.
Registry (now CNCF Distribution)
The Docker Registry served as the open-source implementation of a container image registry. It was donated to the Cloud Native Computing Foundation (CNCF) in 2019 and is maintained under the name ""Distribution."" It remains a cornerstone for managing and distributing container images.
Docker Compose v1 (replaced by Compose v2)
Docker Compose v1 (docker-compose
), a Python-based tool for defining
multi-container applications, has been superseded by Compose v2 (docker compose
), which is written in Go and integrates with the Docker CLI. Compose
v1 is no longer maintained, and users should migrate to Compose v2.
InfraKit
InfraKit was an open-source toolkit designed to manage declarative infrastructure and automate container deployments. It has been archived, and users are encouraged to explore tools such as Terraform for infrastructure provisioning and orchestration.
Docker Notary (now CNCF Notary)
Docker Notary was a system for signing and verifying the authenticity of container content. It was donated to the CNCF in 2017 and continues to be developed as ""Notary."" Users seeking secure content verification should consult the CNCF Notary project.
SwarmKit
SwarmKit powers Docker Swarm mode by providing orchestration for container deployments. While Swarm mode remains functional, development has slowed in favor of Kubernetes-based solutions. Individuals evaluating container orchestration options should investigate whether SwarmKit meets modern workload requirements.",,,
5d538e83e002e246a8acdd3d0bef0759da29afedc82478b47ebd5326501f3063,"Docker Official Images
Docker, Inc. sponsors a dedicated team that's responsible for reviewing and publishing all content in Docker Official Images. This team works in collaboration with upstream software maintainers, security experts, and the broader Docker community.
While it's preferable to have upstream software authors maintaining their Docker Official Images, this isn't a strict requirement. Creating and maintaining images for Docker Official Images is a collaborative process. It takes place openly on GitHub where participation is encouraged. Anyone can provide feedback, contribute code, suggest process changes, or even propose a new Official Image.
Creating a Docker Official Image
From a high level, an Official Image starts out as a proposal in the form of a set of GitHub pull requests. The following GitHub repositories detail the proposal requirements:
The Docker Official Images team, with help from community contributors, formally review each proposal and provide feedback to the author. This initial review process can be lengthy, often requiring a bit of back-and-forth before the proposal is accepted.
There are subjective considerations during the review process. These subjective concerns boil down to the basic question: ""is this image generally useful?"" For example, the Python Docker Official Image is ""generally useful"" to the larger Python developer community, whereas an obscure text adventure game written in Python last week is not.
Once a new proposal is accepted, the author is responsible for keeping their images and documentation up-to-date and responding to user feedback. Docker is responsible for building and publishing the images on Docker Hub. Updates to Docker Official Images follow the same pull request process as for new images, although the review process for updates is more streamlined. The Docker Official Images team ultimately acts as a gatekeeper for all changes, which helps ensures consistency, quality, and security.
Submitting feedback for Docker Official Images
All Docker Official Images contain a User Feedback section in their documentation which covers the details for that specific repository. In most cases, the GitHub repository which contains the Dockerfiles for an Official Image also has an active issue tracker.
General feedback and support questions about Docker Official Images
should be directed to the #general
channel in the
Docker Community Slack.
If you're a maintainer or contributor to Docker Official Images and you're
looking for help or advice, use the #docker-library
channel on
Libera.Chat IRC.",,,
9bd4ae03b17528d8f3a9c890749e58e9527695f8b71a06e9e687ab48c8d08451,"Linux post-installation steps for Docker Engine
These optional post-installation procedures describe how to configure your Linux host machine to work better with Docker.
Manage Docker as a non-root user
The Docker daemon binds to a Unix socket, not a TCP port. By default it's the
root
user that owns the Unix socket, and other users can only access it using
sudo
. The Docker daemon always runs as the root
user.
If you don't want to preface the docker
command with sudo
, create a Unix
group called docker
and add users to it. When the Docker daemon starts, it
creates a Unix socket accessible by members of the docker
group. On some Linux
distributions, the system automatically creates this group when installing
Docker Engine using a package manager. In that case, there is no need for you to
manually create the group.
Warning
The
docker
group grants root-level privileges to the user. For details on how this impacts security in your system, see Docker Daemon Attack Surface.
Note
To run Docker without root privileges, see Run the Docker daemon as a non-root user (Rootless mode).
To create the docker
group and add your user:
Create the
docker
group.$ sudo groupadd docker
Add your user to the
docker
group.$ sudo usermod -aG docker $USER
Log out and log back in so that your group membership is re-evaluated.
If you're running Linux in a virtual machine, it may be necessary to restart the virtual machine for changes to take effect.
You can also run the following command to activate the changes to groups:
$ newgrp docker
Verify that you can run
docker
commands withoutsudo
.$ docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a message and exits.
If you initially ran Docker CLI commands using
sudo
before adding your user to thedocker
group, you may see the following error:WARNING: Error loading config file: /home/user/.docker/config.json - stat /home/user/.docker/config.json: permission denied
This error indicates that the permission settings for the
~/.docker/
directory are incorrect, due to having used thesudo
command earlier.To fix this problem, either remove the
~/.docker/
directory (it's recreated automatically, but any custom settings are lost), or change its ownership and permissions using the following commands:$ sudo chown ""$USER"":""$USER"" /home/""$USER""/.docker -R $ sudo chmod g+rwx ""$HOME/.docker"" -R
Configure Docker to start on boot with systemd
Many modern Linux distributions use systemd to manage which services start when the system boots. On Debian and Ubuntu, the Docker service starts on boot by default. To automatically start Docker and containerd on boot for other Linux distributions using systemd, run the following commands:
$ sudo systemctl enable docker.service
$ sudo systemctl enable containerd.service
To stop this behavior, use disable
instead.
$ sudo systemctl disable docker.service
$ sudo systemctl disable containerd.service
You can use systemd unit files to configure the Docker service on startup, for example to add an HTTP proxy, set a different directory or partition for the Docker runtime files, or other customizations. For an example, see Configure the daemon to use a proxy.
Configure default logging driver
Docker provides
logging drivers for
collecting and viewing log data from all containers running on a host. The
default logging driver, json-file
, writes log data to JSON-formatted files on
the host filesystem. Over time, these log files expand in size, leading to
potential exhaustion of disk resources.
To avoid issues with overusing disk for log data, consider one of the following options:
- Configure the
json-file
logging driver to turn on log rotation. - Use an alternative logging driver such as the ""local"" logging driver that performs log rotation by default.
- Use a logging driver that sends logs to a remote logging aggregator.
Next steps
- Take a look at the Docker workshop to learn how to build an image and run it as a containerized application.",,,
0ae93bf5e3c16d0caac8fa3042da42d96283115ac24bdcd8d26c4f6c7777142c,"Lock your swarm to protect its encryption key
The Raft logs used by swarm managers are encrypted on disk by default. This at-rest encryption protects your service's configuration and data from attackers who gain access to the encrypted Raft logs. One of the reasons this feature was introduced was in support of the Docker secrets feature.
When Docker restarts, both the TLS key used to encrypt communication among swarm nodes and the key used to encrypt and decrypt Raft logs on disk are loaded into each manager node's memory. Docker has the ability to protect the mutual TLS encryption key and the key used to encrypt and decrypt Raft logs at rest, by allowing you to take ownership of these keys and to require manual unlocking of your managers. This feature is called autolock.
When Docker restarts, you must unlock the swarm first, using a key encryption key generated by Docker when the swarm was locked. You can rotate this key encryption key at any time.
Note
You don't need to unlock the swarm when a new node joins the swarm, because the key is propagated to it over mutual TLS.
Initialize a swarm with autolocking enabled
When you initialize a new swarm, you can use the --autolock
flag to
enable autolocking of swarm manager nodes when Docker restarts.
$ docker swarm init --autolock
Swarm initialized: current node (k1q27tfyx9rncpixhk69sa61v) is now a manager.
To add a worker to this swarm, run the following command:
docker swarm join \
--token SWMTKN-1-0j52ln6hxjpxk2wgk917abcnxywj3xed0y8vi1e5m9t3uttrtu-7bnxvvlz2mrcpfonjuztmtts9 \
172.31.46.109:2377
To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
To unlock a swarm manager after it restarts, run the `docker swarm unlock`
command and provide the following key:
SWMKEY-1-WuYH/IX284+lRcXuoVf38viIDK3HJEKY13MIHX+tTt8
Store the key in a safe place, such as in a password manager.
When Docker restarts, you need to unlock the swarm. A locked swarm causes an error like the following when you try to start or restart a service:
$ sudo service docker restart
$ docker service ls
Error response from daemon: Swarm is encrypted and needs to be unlocked before it can be used. Use ""docker swarm unlock"" to unlock it.
Enable or disable autolock on an existing swarm
To enable autolock on an existing swarm, set the autolock
flag to true
.
$ docker swarm update --autolock=true
Swarm updated.
To unlock a swarm manager after it restarts, run the `docker swarm unlock`
command and provide the following key:
SWMKEY-1-+MrE8NgAyKj5r3NcR4FiQMdgu+7W72urH0EZeSmP/0Y
Please remember to store this key in a password manager, since without it you
will not be able to restart the manager.
To disable autolock, set --autolock
to false
. The mutual TLS key and the
encryption key used to read and write Raft logs are stored unencrypted on
disk. There is a trade-off between the risk of storing the encryption key
unencrypted at rest and the convenience of restarting a swarm without
needing to unlock each manager.
$ docker swarm update --autolock=false
Keep the unlock key around for a short time after disabling autolocking, in case a manager goes down while it is still configured to lock using the old key.
Unlock a swarm
To unlock a locked swarm, use docker swarm unlock
.
$ docker swarm unlock
Please enter unlock key:
Enter the encryption key that was generated and shown in the command output when you locked the swarm or rotated the key, and the swarm unlocks.
View the current unlock key for a running swarm
Consider a situation where your swarm is running as expected, then a manager node becomes unavailable. You troubleshoot the problem and bring the physical node back online, but you need to unlock the manager by providing the unlock key to read the encrypted credentials and Raft logs.
If the key has not been rotated since the node left the swarm, and you have a
quorum of functional manager nodes in the swarm, you can view the current unlock
key using docker swarm unlock-key
without any arguments.
$ docker swarm unlock-key
To unlock a swarm manager after it restarts, run the `docker swarm unlock`
command and provide the following key:
SWMKEY-1-8jDgbUNlJtUe5P/lcr9IXGVxqZpZUXPzd+qzcGp4ZYA
Please remember to store this key in a password manager, since without it you
will not be able to restart the manager.
If the key was rotated after the swarm node became unavailable and you do not have a record of the previous key, you may need to force the manager to leave the swarm and join it back to the swarm as a new manager.
Rotate the unlock key
You should rotate the locked swarm's unlock key on a regular schedule.
$ docker swarm unlock-key --rotate
Successfully rotated manager unlock key.
To unlock a swarm manager after it restarts, run the `docker swarm unlock`
command and provide the following key:
SWMKEY-1-8jDgbUNlJtUe5P/lcr9IXGVxqZpZUXPzd+qzcGp4ZYA
Please remember to store this key in a password manager, since without it you
will not be able to restart the manager.
Warning
When you rotate the unlock key, keep a record of the old key around for a few minutes, so that if a manager goes down before it gets the new key, it may still be unlocked with the old one.",,,
ca0a48e072e8566daa4ebfa4bf6d678f27787704336fdfa22b83195b6397c3f6,"Exporters overview
Exporters save your build results to a specified output type. You specify the
exporter to use with the
--output
CLI option.
Buildx supports the following exporters:
image
: exports the build result to a container image.registry
: exports the build result into a container image, and pushes it to the specified registry.local
: exports the build root filesystem into a local directory.tar
: packs the build root filesystem into a local tarball.oci
: exports the build result to the local filesystem in the OCI image layout format.docker
: exports the build result to the local filesystem in the Docker Image Specification v1.2.0 format.cacheonly
: doesn't export a build output, but runs the build and creates a cache.
Using exporters
To specify an exporter, use the following command syntax:
$ docker buildx build --tag <registry>/<image> \
--output type=<TYPE> .
Most common use cases don't require that you specify which exporter to use
explicitly. You only need to specify the exporter if you intend to customize
the output, or if you want to save it to disk. The --load
and --push
options allow Buildx to infer the exporter settings to use.
For example, if you use the --push
option in combination with --tag
, Buildx
automatically uses the image
exporter, and configures the exporter to push the
results to the specified registry.
To get the full flexibility out of the various exporters BuildKit has to offer,
you use the --output
flag that lets you configure exporter options.
Use cases
Each exporter type is designed for different use cases. The following sections describe some common scenarios, and how you can use exporters to generate the output that you need.
Load to image store
Buildx is often used to build container images that can be loaded to an image
store. That's where the docker
exporter comes in. The following example shows
how to build an image using the docker
exporter, and have that image loaded to
the local image store, using the --output
option:
$ docker buildx build \
--output type=docker,name=<registry>/<image> .
Buildx CLI will automatically use the docker
exporter and load it to the image
store if you supply the --tag
and --load
options:
$ docker buildx build --tag <registry>/<image> --load .
Building images using the docker
driver are automatically loaded to the local
image store.
Images loaded to the image store are available to docker run
immediately
after the build finishes, and you'll see them in the list of images when you run
the docker images
command.
Push to registry
To push a built image to a container registry, you can use the registry
or
image
exporters.
When you pass the --push
option to the Buildx CLI, you instruct BuildKit to
push the built image to the specified registry:
$ docker buildx build --tag <registry>/<image> --push .
Under the hood, this uses the image
exporter, and sets the push
parameter.
It's the same as using the following long-form command using the --output
option:
$ docker buildx build \
--output type=image,name=<registry>/<image>,push=true .
You can also use the registry
exporter, which does the same thing:
$ docker buildx build \
--output type=registry,name=<registry>/<image> .
Export image layout to file
You can use either the oci
or docker
exporters to save the build results to
image layout on your local filesystem. Both of these exporters generate a tar
archive file containing the corresponding image layout. The dest
parameter
defines the target output path for the tarball.
$ docker buildx build --output type=oci,dest=./image.tar .
[+] Building 0.8s (7/7) FINISHED
...
=> exporting to oci image format 0.0s
=> exporting layers 0.0s
=> exporting manifest sha256:c1ef01a0a0ef94a7064d5cbce408075730410060e253ff8525d1e5f7e27bc900 0.0s
=> exporting config sha256:eadab326c1866dd247efb52cb715ba742bd0f05b6a205439f107cf91b3abc853 0.0s
=> sending tarball 0.0s
$ mkdir -p out && tar -C out -xf ./image.tar
$ tree out
out
├── blobs
│ └── sha256
│ ├── 9b18e9b68314027565b90ff6189d65942c0f7986da80df008b8431276885218e
│ ├── c78795f3c329dbbbfb14d0d32288dea25c3cd12f31bd0213be694332a70c7f13
│ ├── d1cf38078fa218d15715e2afcf71588ee482352d697532cf316626164699a0e2
│ ├── e84fa1df52d2abdfac52165755d5d1c7621d74eda8e12881f6b0d38a36e01775
│ └── fe9e23793a27fe30374308988283d40047628c73f91f577432a0d05ab0160de7
├── index.json
├── manifest.json
└── oci-layout
Export filesystem
If you don't want to build an image from your build results, but instead export
the filesystem that was built, you can use the local
and tar
exporters.
The local
exporter unpacks the filesystem into a directory structure in the
specified location. The tar
exporter creates a tarball archive file.
$ docker buildx build --output type=local,dest=<path/to/output> .
The local
exporter is useful in
multi-stage builds
since it allows you to export only a minimal number of build artifacts, such as
self-contained binaries.
Cache-only export
The cacheonly
exporter can be used if you just want to run a build, without
exporting any output. This can be useful if, for example, you want to run a test
build. Or, if you want to run the build first, and create exports using
subsequent commands. The cacheonly
exporter creates a build cache, so any
successive builds are instant.
$ docker buildx build --output type=cacheonly
If you don't specify an exporter, and you don't provide short-hand options like
--load
that automatically selects the appropriate exporter, Buildx defaults to
using the cacheonly
exporter. Except if you build using the docker
driver,
in which case you use the docker
exporter.
Buildx logs a warning message when using cacheonly
as a default:
$ docker buildx build .
WARNING: No output specified with docker-container driver.
Build result will only remain in the build cache.
To push result image into registry use --push or
to load image into docker use --load
Multiple exporters
You can use multiple exporters for any given build by specifying the --output
flag multiple times. This requires both Buildx and BuildKit version 0.13.0
or later.
The following example runs a single build, using three different exporters:
- The
registry
exporter to push the image to a registry - The
local
exporter to extract the build results to the local filesystem - The
--load
flag (a shorthand for theimage
exporter) to load the results to the local image store.
$ docker buildx build \
--output type=registry,tag=<registry>/<image> \
--output type=local,dest=<path/to/output> \
--load .
Configuration options
This section describes some configuration options available for exporters.
The options described here are common for at least two or more exporter types. Additionally, the different exporters types support specific parameters as well. See the detailed page about each exporter for more information about which configuration parameters apply.
The common parameters described here are:
Compression
When you export a compressed output, you can configure the exact compression algorithm and level to use. While the default values provide a good out-of-the-box experience, you may wish to tweak the parameters to optimize for storage vs compute costs. Changing the compression parameters can reduce storage space required, and improve image download times, but will increase build times.
To select the compression algorithm, you can use the compression
option. For
example, to build an image
with compression=zstd
:
$ docker buildx build \
--output type=image,name=<registry>/<image>,push=true,compression=zstd .
Use the compression-level=<value>
option alongside the compression
parameter
to choose a compression level for the algorithms which support it:
- 0-9 for
gzip
andestargz
- 0-22 for
zstd
As a general rule, the higher the number, the smaller the resulting file will be, and the longer the compression will take to run.
Use the force-compression=true
option to force re-compressing layers imported
from a previous image, if the requested compression algorithm is different from
the previous compression algorithm.
Note
The
gzip
andestargz
compression methods use thecompress/gzip
package, whilezstd
uses thegithub.com/klauspost/compress/zstd
package.
OCI media types
The image
, registry
, oci
and docker
exporters create container images.
These exporters support both Docker media types (default) and OCI media types
To export images with OCI media types set, use the oci-mediatypes
property.
$ docker buildx build \
--output type=image,name=<registry>/<image>,push=true,oci-mediatypes=true .
What's next
Read about each of the exporters to learn about how they work and how to use them:",,,
7378a7d9a1676c471ff96bc76e8ec120d9fcb546fbc8dab791338c49337243dc,"Start the daemon
This page shows how to start the daemon, either manually or using OS utilities.
Start the daemon using operating system utilities
On a typical installation the Docker daemon is started by a system utility, not manually by a user. This makes it easier to automatically start Docker when the machine reboots.
The command to start Docker depends on your operating system. Check the correct page under Install Docker.
Start with systemd
On some operating systems, like Ubuntu and Debian, the Docker daemon service starts automatically. Use the following command to start it manually:
$ sudo systemctl start docker
If you want Docker to start at boot, see Configure Docker to start on boot.
Start the daemon manually
If you don't want to use a system utility to manage the Docker daemon, or just
want to test things out, you can manually run it using the dockerd
command.
You may need to use sudo
, depending on your operating system configuration.
When you start Docker this way, it runs in the foreground and sends its logs directly to your terminal.
$ dockerd
INFO[0000] +job init_networkdriver()
INFO[0000] +job serveapi(unix:///var/run/docker.sock)
INFO[0000] Listening for HTTP on unix (/var/run/docker.sock)
To stop Docker when you have started it manually, issue a Ctrl+C
in your
terminal.",,,
4ce448e203b4ed22700bceca923f07039a365432de736c9019cac200f03c9faf,"Networking using the host network
This series of tutorials deals with networking standalone containers which bind directly to the Docker host's network, with no network isolation. For other networking topics, see the overview.
Goal
The goal of this tutorial is to start a nginx
container which binds directly
to port 80 on the Docker host. From a networking point of view, this is the
same level of isolation as if the nginx
process were running directly on the
Docker host and not in a container. However, in all other ways, such as storage,
process namespace, and user namespace, the nginx
process is isolated from the
host.
Prerequisites
This procedure requires port 80 to be available on the Docker host. To make Nginx listen on a different port, see the documentation for the
nginx
imageThe
host
networking driver only works on Linux hosts, and as an opt-in feature in Docker Desktop version 4.34 and later. To enable this feature in Docker Desktop, navigate to the Resources tab in Settings, and then under Network select Enable host networking.
Procedure
Create and start the container as a detached process. The
--rm
option means to remove the container once it exits/stops. The-d
flag means to start the container detached (in the background).$ docker run --rm -d --network host --name my_nginx nginx
Access Nginx by browsing to http://localhost:80/.
Examine your network stack using the following commands:
Examine all network interfaces and verify that a new one was not created.
$ ip addr show
Verify which process is bound to port 80, using the
netstat
command. You need to usesudo
because the process is owned by the Docker daemon user and you otherwise won't be able to see its name or PID.$ sudo netstat -tulpn | grep :80
Stop the container. It will be removed automatically as it was started using the
--rm
option.docker container stop my_nginx",,,
14899127d4b18e7b28802a75ebb00855529887fadf8ea0c31ac876c74c34d65d,"Build dependent images
To reduce push/pull time and image weight, a common practice for Compose applications is to have services share base layers as much as possible. You will typically select the same operating system base image for all services. But you also can get one step further sharing image layers when your images share the same system packages. The challenge to address is then to avoid repeating the exact same Dockerfile instruction in all services.
For illustration, this page assumes you want all your services to be built with an alpine
base
image and install system package openssl
.
Multi-stage Dockerfile
The recommended approach is to group the shared declaration in a single Dockerfile, and use multi-stage features so that service images are built from this shared declaration.
Dockerfile:
FROM alpine as base
RUN /bin/sh -c apk add --update --no-cache openssl
FROM base as service_a
# build service a
...
FROM base as service_b
# build service b
...
Compose file:
services:
a:
build:
target: service_a
b:
build:
target: service_b
Use another service's image as the base image
A popular pattern is to reuse a service image as a base image in another service. As Compose does not parse the Dockerfile, it can't automatically detect this dependency between services to correctly order the build execution.
a.Dockerfile:
FROM alpine
RUN /bin/sh -c apk add --update --no-cache openssl
b.Dockerfile:
FROM service_a
# build service b
Compose file:
services:
a:
image: service_a
build:
dockerfile: a.Dockerfile
b:
image: service_b
build:
dockerfile: b.Dockerfile
Legacy Docker Compose v1 used to build images sequentially, which made this pattern usable out of the box. Compose v2 uses BuildKit to optimise builds and build images in parallel and requires an explicit declaration.
The recommended approach is to declare the dependent base image as an additional build context:
Compose file:
services:
a:
image: service_a
build:
dockerfile: a.Dockerfile
b:
image: service_b
build:
context: b/
dockerfile: b.Dockerfile
additional_contexts:
# `FROM service_a` will be resolved as a dependency on service a which has to be built first
service_a: ""service:a""
Build with Bake
Using Bake let you pass the complete build definition for all services and to orchestrate build execution in the most efficient way.
To enable this feature, run Compose with the COMPOSE_BAKE=true
variable set in your environment.
$ COMPOSE_BAKE=true docker compose build
[+] Building 0.0s (0/1)
=> [internal] load local bake definitions 0.0s
...
[+] Building 2/2 manifest list sha256:4bd2e88a262a02ddef525c381a5bdb08c83 0.0s
✔ service_b Built 0.7s
✔ service_a Built
Bake can also be selected as the default builder by editing your $HOME/.docker/config.json
config file:
{
...
""plugins"": {
""compose"": {
""build"": ""bake""
}
}
...
}",,,
b2cb5990c9fbc541c1ba083ad4810b6a53aef2c1b57186115531b2d4ca328fea,"What is Settings Management?
Settings Management helps you control key Docker Desktop settings, like proxies and network configurations, on your developers' machines within your organization.
For an extra layer of security, you can also use Settings Management to enable and lock in Enhanced Container Isolation, which prevents containers from modifying any Settings Management configurations.
Who is it for?
- For organizations that want to configure Docker Desktop to be within their organization's centralized control.
- For organizations that want to create a standardized Docker Desktop environment at scale.
- For Docker Business customers who want to confidently manage their use of Docker Desktop within tightly regulated environments.
How does it work?
You can configure several Docker Desktop settings using either:
- An
admin-settings.json
file. This file is located on the Docker Desktop host and can only be accessed by developers with root or administrator privileges. - Creating a settings policy in the Docker Admin Console
Settings that are defined by an administrator override any previous values set by developers and ensure that these cannot be modified.
What features can I configure with Settings Management?
Using the admin-settings.json
file, you can:
- Turn on and lock in Enhanced Container Isolation
- Configure HTTP proxies
- Configure network settings
- Configure Kubernetes settings
- Enforce the use of WSL 2 based engine or Hyper-V
- Enforce the use of Rosetta for x86_64/amd64 emulation on Apple Silicon
- Configure Docker Engine
- Turn off Docker Desktop's ability to checks for updates
- Turn off Docker Extensions
- Turn off Docker Scout SBOM indexing
- Turn off beta and experimental features
- Turn off Docker AI ( Ask Gordon)
- Turn off Docker Desktop's onboarding survey
- Control whether developers can use the Docker terminal
- Control the file sharing implementation for your developers on macOS
- Specify which paths your developers can add file shares to
- Configure Air-gapped containers
For more details on the syntax and options, see Configure Settings Management.
How do I set up and enforce Settings Management?
You first need to enforce sign-in to ensure that all Docker Desktop developers authenticate with your organization. Since the Settings Management feature requires a Docker Business subscription, enforced sign-in guarantees that only authenticated users have access and that the feature consistently takes effect across all users, even though it may still work without enforced sign-in.
Next, you must either:
- Manually
create and configure the
admin-settings.json
file, or use the--admin-settings
installer flag on macOS or Windows to automatically create theadmin-settings.json
and save it in the correct location. - Fill out the Settings policy creation form in the Docker Admin Console.
Once this is done, Docker Desktop developers receive the changed settings when they either:
- Quit, re-launch, and sign in to Docker Desktop
- Launch and sign in to Docker Desktop for the first time
To avoid disrupting your developers' workflows, Docker doesn't automatically require that developers re-launch and re-authenticate once a change has been made.
What do developers see when the settings are enforced?
Enforced settings appear grayed out in Docker Desktop. They can't be edited via the Docker Desktop Dashboard, CLI, or settings-store.json
(or settings.json
for Docker Desktop 4.34 and earlier).
In addition, if Enhanced Container Isolation is enforced, developers can't use privileged containers or similar techniques to modify enforced settings within the Docker Desktop Linux VM. For example, they can't reconfigure proxy and networking, or Docker Engine.",,,
6c04808c26ce18814e2e931fd29901875791893acd095b020c6c873fb1855641,"Configure Docker Scout with environment variables
Table of contents
The following environment variables are available to configure the Docker Scout
CLI commands, and the corresponding docker/scout-cli
container image:
| Name | Format | Description |
|---|---|---|
| DOCKER_SCOUT_CACHE_FORMAT | String | Format of the local image cache; can be oci or tar (default: oci ) |
| DOCKER_SCOUT_CACHE_DIR | String | Directory where the local SBOM cache is stored (default: $HOME/.docker/scout ) |
| DOCKER_SCOUT_NO_CACHE | Boolean | When set to true , disables the use of local SBOM cache |
| DOCKER_SCOUT_OFFLINE | Boolean | Use offline mode when indexing SBOM |
| DOCKER_SCOUT_REGISTRY_TOKEN | String | Token for authenticating to a registry when pulling images |
| DOCKER_SCOUT_REGISTRY_USER | String | Username for authenticating to a registry when pulling images |
| DOCKER_SCOUT_REGISTRY_PASSWORD | String | Password or personal access token for authenticating to a registry when pulling images |
| DOCKER_SCOUT_HUB_USER | String | Docker Hub username for authenticating to the Docker Scout backend |
| DOCKER_SCOUT_HUB_PASSWORD | String | Docker Hub password or personal access token for authenticating to the Docker Scout backend |
| DOCKER_SCOUT_NEW_VERSION_WARN | Boolean | Warn about new versions of the Docker Scout CLI |
| DOCKER_SCOUT_EXPERIMENTAL_WARN | Boolean | Warn about experimental features |
| DOCKER_SCOUT_EXPERIMENTAL_POLICY_OUTPUT | Boolean | Disable experimental output for policy evaluation |
Offline mode
Under normal operation, Docker Scout cross-references external systems, such as npm, NuGet, or proxy.golang.org, to retrieve additional information about packages found in your image.
When DOCKER_SCOUT_OFFLINE
is set to true
, Docker Scout image analysis runs
in offline mode. Offline mode means Docker Scout doesn't make outbound requests
to external systems.
To use offline mode:
$ export DOCKER_SCOUT_OFFLINE=true",,,
7163aac4cfea40189b0cc905dbee46d68a1d2f84bf70edabe229f8b6a9e77972,"Include
With include
, you can incorporate a separate compose.yaml
file directly in your current compose.yaml
file. This makes it easy to modularize complex applications into sub-Compose files, which in turn enables application configurations to be made simpler and more explicit.
The
include
top-level element helps to reflect the engineering team responsible for the code directly in the config file's organization. It also solves the relative path problem that
extends
and
merge present.
Each path listed in the include
section loads as an individual Compose application model, with its own project directory, in order to resolve relative paths.
Once the included Compose application loads, all resources are copied into the current Compose application model.
Note
include
applies recursively so an included Compose file which declares its owninclude
section, results in those other files being included as well.
Example
include:
- my-compose-include.yaml #with serviceB declared
services:
serviceA:
build: .
depends_on:
- serviceB #use serviceB directly as if it was declared in this Compose file
my-compose-include.yaml
manages serviceB
which details some replicas, web UI to inspect data, isolated networks, volumes for data persistence, etc. The application relying on serviceB
doesn’t need to know about the infrastructure details, and consumes the Compose file as a building block it can rely on.
This means the team managing serviceB
can refactor its own database component to introduce additional services without impacting any dependent teams. It also means that the dependent teams don't need to include additional flags on each Compose command they run.
Include and overrides
Compose reports an error if any resource from include
conflicts with resources from the included Compose file. This rule prevents
unexpected conflicts with resources defined by the included compose file author. However, there may be some circumstances where you might want to tweak the
included model. This can be achieved by adding an override file to the include directive:
include:
- path :
- third-party/compose.yaml
- override.yaml # local override for third-party model
The main limitation with this approach is that you need to maintain a dedicated override file per include. For complex projects with multiple includes this would result into many Compose files.
The other option is to use a compose.override.yaml
file. While conflicts will be rejected from the file using include
when same
resource is declared, a global Compose override file can override the resulting merged model, as demonstrated in following example:
Main compose.yaml
file:
include:
- team-1/compose.yaml # declare service-1
- team-2/compose.yaml # declare service-2
Override compose.override.yaml
file:
services:
service-1:
# override included service-1 to enable debugger port
ports:
- 2345:2345
service-2:
# override included service-2 to use local data folder containing test data
volumes:
- ./data:/data
Combined together, this allows you to benefit from third-party reusable components, and adjust the Compose model for your needs.",,,
abadc49616c826c780fbf88256166455650267d6ca1fa29503027c49321f5c04,"Amazon CloudWatch Logs logging driver
The awslogs
logging driver sends container logs to
Amazon CloudWatch Logs.
Log entries can be retrieved through the
AWS Management
Console or the
AWS SDKs
and Command Line Tools.
Usage
To use the awslogs
driver as the default logging driver, set the log-driver
and log-opt
keys to appropriate values in the daemon.json
file, which is
located in /etc/docker/
on Linux hosts or
C:\ProgramData\docker\config\daemon.json
on Windows Server. For more about
configuring Docker using daemon.json
, see
daemon.json.
The following example sets the log driver to awslogs
and sets the
awslogs-region
option.
{
""log-driver"": ""awslogs"",
""log-opts"": {
""awslogs-region"": ""us-east-1""
}
}
Restart Docker for the changes to take effect.
You can set the logging driver for a specific container by using the
--log-driver
option to docker run
:
$ docker run --log-driver=awslogs ...
If you are using Docker Compose, set awslogs
using the following declaration example:
myservice:
logging:
driver: awslogs
options:
awslogs-region: us-east-1
Amazon CloudWatch Logs options
You can add logging options to the daemon.json
to set Docker-wide defaults,
or use the --log-opt NAME=VALUE
flag to specify Amazon CloudWatch Logs
logging driver options when starting a container.
awslogs-region
The awslogs
logging driver sends your Docker logs to a specific region. Use
the awslogs-region
log option or the AWS_REGION
environment variable to set
the region. By default, if your Docker daemon is running on an EC2 instance
and no region is set, the driver uses the instance's region.
$ docker run --log-driver=awslogs --log-opt awslogs-region=us-east-1 ...
awslogs-endpoint
By default, Docker uses either the awslogs-region
log option or the
detected region to construct the remote CloudWatch Logs API endpoint.
Use the awslogs-endpoint
log option to override the default endpoint
with the provided endpoint.
Note
The
awslogs-region
log option or detected region controls the region used for signing. You may experience signature errors if the endpoint you've specified withawslogs-endpoint
uses a different region.
awslogs-group
You must specify a
log group
for the awslogs
logging driver. You can specify the log group with the
awslogs-group
log option:
$ docker run --log-driver=awslogs --log-opt awslogs-region=us-east-1 --log-opt awslogs-group=myLogGroup ...
awslogs-stream
To configure which
log stream
should be used, you can specify the awslogs-stream
log option. If not
specified, the container ID is used as the log stream.
Note
Log streams within a given log group should only be used by one container at a time. Using the same log stream for multiple containers concurrently can cause reduced logging performance.
awslogs-create-group
Log driver returns an error by default if the log group doesn't exist. However, you can set the
awslogs-create-group
to true
to automatically create the log group as needed.
The awslogs-create-group
option defaults to false
.
$ docker run \
--log-driver=awslogs \
--log-opt awslogs-region=us-east-1 \
--log-opt awslogs-group=myLogGroup \
--log-opt awslogs-create-group=true \
...
Note
Your AWS IAM policy must include the
logs:CreateLogGroup
permission before you attempt to useawslogs-create-group
.
awslogs-create-stream
By default, the log driver creates the AWS CloudWatch Logs stream used for container log persistence.
Set awslogs-create-stream
to false
to disable log stream creation. When disabled, the Docker daemon assumes
the log stream already exists. A use case where this is beneficial is when log stream creation is handled by
another process avoiding redundant AWS CloudWatch Logs API calls.
If awslogs-create-stream
is set to false
and the log stream does not exist, log persistence to CloudWatch
fails during container runtime, resulting in Failed to put log events
error messages in daemon logs.
$ docker run \
--log-driver=awslogs \
--log-opt awslogs-region=us-east-1 \
--log-opt awslogs-group=myLogGroup \
--log-opt awslogs-stream=myLogStream \
--log-opt awslogs-create-stream=false \
...
awslogs-datetime-format
The awslogs-datetime-format
option defines a multi-line start pattern in
Python
strftime
format. A log message consists of a line that
matches the pattern and any following lines that don't match the pattern. Thus
the matched line is the delimiter between log messages.
One example of a use case for using this format is for parsing output such as a stack dump, which might otherwise be logged in multiple entries. The correct pattern allows it to be captured in a single entry.
This option always takes precedence if both awslogs-datetime-format
and
awslogs-multiline-pattern
are configured.
Note
Multi-line logging performs regular expression parsing and matching of all log messages, which may have a negative impact on logging performance.
Consider the following log stream, where new log messages start with a timestamp:
[May 01, 2017 19:00:01] A message was logged
[May 01, 2017 19:00:04] Another multi-line message was logged
Some random message
with some random words
[May 01, 2017 19:01:32] Another message was logged
The format can be expressed as a strftime
expression of
[%b %d, %Y %H:%M:%S]
, and the awslogs-datetime-format
value can be set to
that expression:
$ docker run \
--log-driver=awslogs \
--log-opt awslogs-region=us-east-1 \
--log-opt awslogs-group=myLogGroup \
--log-opt awslogs-datetime-format='\[%b %d, %Y %H:%M:%S\]' \
...
This parses the logs into the following CloudWatch log events:
# First event
[May 01, 2017 19:00:01] A message was logged
# Second event
[May 01, 2017 19:00:04] Another multi-line message was logged
Some random message
with some random words
# Third event
[May 01, 2017 19:01:32] Another message was logged
The following strftime
codes are supported:
| Code | Meaning | Example |
|---|---|---|
%a | Weekday abbreviated name. | Mon |
%A | Weekday full name. | Monday |
%w | Weekday as a decimal number where 0 is Sunday and 6 is Saturday. | 0 |
%d | Day of the month as a zero-padded decimal number. | 08 |
%b | Month abbreviated name. | Feb |
%B | Month full name. | February |
%m | Month as a zero-padded decimal number. | 02 |
%Y | Year with century as a decimal number. | 2008 |
%y | Year without century as a zero-padded decimal number. | 08 |
%H | Hour (24-hour clock) as a zero-padded decimal number. | 19 |
%I | Hour (12-hour clock) as a zero-padded decimal number. | 07 |
%p | AM or PM. | AM |
%M | Minute as a zero-padded decimal number. | 57 |
%S | Second as a zero-padded decimal number. | 04 |
%L | Milliseconds as a zero-padded decimal number. | .123 |
%f | Microseconds as a zero-padded decimal number. | 000345 |
%z | UTC offset in the form +HHMM or -HHMM. | +1300 |
%Z | Time zone name. | PST |
%j | Day of the year as a zero-padded decimal number. | 363 |
awslogs-multiline-pattern
The awslogs-multiline-pattern
option defines a multi-line start pattern using a
regular expression. A log message consists of a line that matches the pattern
and any following lines that don't match the pattern. Thus the matched line is
the delimiter between log messages.
This option is ignored if awslogs-datetime-format
is also configured.
Note
Multi-line logging performs regular expression parsing and matching of all log messages. This may have a negative impact on logging performance.
Consider the following log stream, where each log message should start with the
pattern INFO
:
INFO A message was logged
INFO Another multi-line message was logged
Some random message
INFO Another message was logged
You can use the regular expression of ^INFO
:
$ docker run \
--log-driver=awslogs \
--log-opt awslogs-region=us-east-1 \
--log-opt awslogs-group=myLogGroup \
--log-opt awslogs-multiline-pattern='^INFO' \
...
This parses the logs into the following CloudWatch log events:
# First event
INFO A message was logged
# Second event
INFO Another multi-line message was logged
Some random message
# Third event
INFO Another message was logged
tag
Specify tag
as an alternative to the awslogs-stream
option. tag
interprets
Go template markup, such as {{.ID}}
, {{.FullID}}
or {{.Name}}
docker.{{.ID}}
. See
the
tag option documentation for details on supported template
substitutions.
When both awslogs-stream
and tag
are specified, the value supplied for
awslogs-stream
overrides the template specified with tag
.
If not specified, the container ID is used as the log stream.
Note
The CloudWatch log API doesn't support
:
in the log name. This can cause some issues when using the{{ .ImageName }}
as a tag, since a Docker image has a format ofIMAGE:TAG
, such asalpine:latest
. Template markup can be used to get the proper format. To get the image name and the first 12 characters of the container ID, you can use:--log-opt tag='{{ with split .ImageName "":"" }}{{join . ""_""}}{{end}}-{{.ID}}'
the output is something like:
alpine_latest-bf0072049c76
awslogs-force-flush-interval-seconds
The awslogs
driver periodically flushes logs to CloudWatch.
The awslogs-force-flush-interval-seconds
option changes log flush interval seconds.
Default is 5 seconds.
awslogs-max-buffered-events
The awslogs
driver buffers logs.
The awslogs-max-buffered-events
option changes log buffer size.
Default is 4K.
Credentials
You must provide AWS credentials to the Docker daemon to use the awslogs
logging driver. You can provide these credentials with the AWS_ACCESS_KEY_ID
,
AWS_SECRET_ACCESS_KEY
, and AWS_SESSION_TOKEN
environment variables, the
default AWS shared credentials file (~/.aws/credentials
of the root user), or
if you are running the Docker daemon on an Amazon EC2 instance, the Amazon EC2
instance profile.
Credentials must have a policy applied that allows the logs:CreateLogStream
and logs:PutLogEvents
actions, as shown in the following example.
{
""Version"": ""2012-10-17"",
""Statement"": [
{
""Action"": [""logs:CreateLogStream"", ""logs:PutLogEvents""],
""Effect"": ""Allow"",
""Resource"": ""*""
}
]
}",,,
1f1fc2e7eb16d16066cab315f01677717734bf02a0c91081e20bf317178d5563,"Manage organization members
Learn how to manage members for your organization in Docker Hub and the Docker Admin Console.
Invite members
Owners can invite new members to an organization via Docker ID, email address, or with a CSV file containing email addresses. If an invitee does not have a Docker account, they must create an account and verify their email address before they can accept an invitation to join the organization. When inviting members, their pending invitation occupies a seat.
Invite members via Docker ID or email address
Use the following steps to invite members to your organization via Docker ID or email address. To invite a large amount of members to your organization via CSV file, see the next section.
Sign in to Docker Hub.
Select Organizations, your organization, and then Members.
Select Invite members.
Select Emails or usernames.
Follow the on-screen instructions to invite members. Invite a maximum of 1000 members and separate multiple entries by comma, semicolon, or space.
Note
When you invite members, you assign them a role. See Roles and permissions for details about the access permissions for each role.
Pending invitations appear in the table. The invitees receive an email with a link to Docker Hub where they can accept or decline the invitation.
Invite members via CSV file
To invite multiple members to an organization via a CSV file containing email addresses:
Sign in to Docker Hub.
Select Organizations, your organization, and then Members.
Select Invite members.
Select CSV upload.
Select Download the template CSV file to optionally download an example CSV file. The following is an example of the contents of a valid CSV file.
email docker.user-0@example.com docker.user-1@example.com
CSV file requirements:
- The file must contain a header row with at least one heading named
email
. Additional columns are allowed and are ignored in the import. - The file must contain a maximum of 1000 email addresses (rows). To invite more than 1000 users, create multiple CSV files and perform all steps in this task for each file.
- The file must contain a header row with at least one heading named
Create a new CSV file or export a CSV file from another application.
- To export a CSV file from another application, see the application’s documentation.
- To create a new CSV file, open a new file in a text editor, type
email
on the first line, type the user email addresses one per line on the following lines, and then save the file with a .csv extension.
Select Browse files and then select your CSV file, or drag and drop the CSV file into the Select a CSV file to upload box. You can only select one CSV file at a time.
Note
If the amount of email addresses in your CSV file exceeds the number of available seats in your organization, you cannot continue to invite members. To invite members, you can purchase more seats, or remove some email addresses from the CSV file and re-select the new file. To purchase more seats, see Add seats to your subscription or Contact sales.
After the CSV file has been uploaded, select Review.
Valid email addresses and any email addresses that have issues appear. Email addresses may have the following issues:
- Invalid email: The email address is not a valid address. The email address will be ignored if you send invites. You can correct the email address in the CSV file and re-import the file.
- Already invited: The user has already been sent an invite email and another invite email will not be sent.
- Member: The user is already a member of your organization and an invite email will not be sent.
- Duplicate: The CSV file has multiple occurrences of the same email address. The user will be sent only one invite email.
Follow the on-screen instructions to invite members.
Note
When you invite members, you assign them a role. See Roles and permissions for details about the access permissions for each role.
Pending invitations appear in the table. The invitees receive an email with a link to Docker Hub where they can accept or decline the invitation.
Owners can invite new members to an organization via Docker ID, email address, or with a CSV file containing email addresses. If an invitee does not have a Docker account, they must create an account and verify their email address before they can accept an invitation to join the organization. When inviting members, their pending invitation occupies a seat.
Invite members via Docker ID or email address
Use the following steps to invite members to your organization via Docker ID or email address. To invite a large amount of members to your organization via CSV file, see the next section.
Sign in to the Admin Console.
Select your organization from the Choose profile page, and then select Members.
Select Invite.
Select Emails or usernames.
Follow the on-screen instructions to invite members. Invite a maximum of 1000 members and separate multiple entries by comma, semicolon, or space.
Note
When you invite members, you assign them a role. See Roles and permissions for details about the access permissions for each role.
Pending invitations appear in the table. The invitees receive an email with a link to Docker Hub where they can accept or decline the invitation.
Invite members via CSV file
To invite multiple members to an organization via a CSV file containing email addresses:
Sign in to the Admin Console.
Select your organization from the Choose profile page, and then select Members.
Select Invite.
Select CSV upload.
Select Download the template CSV file to optionally download an example CSV file. The following is an example of the contents of a valid CSV file.
email docker.user-0@example.com docker.user-1@example.com
CSV file requirements:
- The file must contain a header row with at least one heading named
email
. Additional columns are allowed and are ignored in the import. - The file must contain a maximum of 1000 email addresses (rows). To invite more than 1000 users, create multiple CSV files and perform all steps in this task for each file.
- The file must contain a header row with at least one heading named
Create a new CSV file or export a CSV file from another application.
- To export a CSV file from another application, see the application’s documentation.
- To create a new CSV file, open a new file in a text editor, type
email
on the first line, type the user email addresses one per line on the following lines, and then save the file with a .csv extension.
Select Browse files and then select your CSV file, or drag and drop the CSV file into the Select a CSV file to upload box. You can only select one CSV file at a time.
Note
If the amount of email addresses in your CSV file exceeds the number of available seats in your organization, you cannot continue to invite members. To invite members, you can purchase more seats, or remove some email addresses from the CSV file and re-select the new file. To purchase more seats, see Add seats to your subscription or Contact sales.
After the CSV file has been uploaded, select Review.
Valid email addresses and any email addresses that have issues appear. Email addresses may have the following issues:
- Invalid email: The email address is not a valid address. The email address will be ignored if you send invites. You can correct the email address in the CSV file and re-import the file.
- Already invited: The user has already been sent an invite email and another invite email will not be sent.
- Member: The user is already a member of your organization and an invite email will not be sent.
- Duplicate: The CSV file has multiple occurrences of the same email address. The user will be sent only one invite email.
Follow the on-screen instructions to invite members.
Note
When you invite members, you assign them a role. See Roles and permissions for details about the access permissions for each role.
Pending invitations appear in the table. The invitees receive an email with a link to Docker Hub where they can accept or decline the invitation.
Accept invitation
When an invitation is to a user's email address, they receive a link to Docker Hub where they can accept or decline the invitation. To accept an invitation:
Navigate to your email inbox and open the Docker email with an invitation to join the Docker organization.
To open the link to Docker Hub, select the click here link.
Warning
Invitation email links expire after 14 days. If your email link has expired, you can sign in to Docker Hub with the email address the link was sent to and accept the invitation from the Notifications panel.
The Docker create an account page will open. If you already have an account, select Already have an account? Sign in. If you do not have an account yet, create an account using the same email address you received the invitation through.
Optional. If you do not have an account and created one, you must navigate back to your email inbox and verify your email address using the Docker verification email.
Once you are signed in to Docker Hub, select Organizations from the top-level navigation menu.
The organizations page will display your invitation. Select Accept.
After accepting an invitation, you are now a member of the organization.
Manage invitations
After inviting members, you can resend or remove invitations as needed.
Resend an invitation
To resend an invitation from Docker Hub:
- Sign in to Docker Hub.
- Select Organizations, your organization, and then Members.
- In the table, locate the invitee, select the Actions icon, and then select Resend invitation.
- Select Invite to confirm.
To resend an invitation from the Admin Console:
- In the Admin Console, select your organization.
- Select Members.
- Select the action menu next to the invitee and select Resend invitation.
- Select Invite to confirm.
Remove an invitation
To remove a member's invitation from Docker Hub:
- Sign in to Docker Hub.
- Select Organizations, your organization, and then Members.
- In the table, select the Action icon, and then select Remove member or Remove invitee.
- Follow the on-screen instructions to remove the member or invitee.
To remove an invitation from the Admin Console:
- In the Admin Console, select your organization.
- Select Members.
- Select the action menu next to the invitee and select Remove invitee.
- Select Remove to confirm.
Manage members on a team
Use Docker Hub or the Admin Console to add or remove team members. Organization owners can add a member to one or more teams within an organization.
Add a member to a team
To add a member to a team with Docker Hub:
Sign in to Docker Hub.
Select Organizations, your organization, and then Members.
Select the Action icon, and then select Add to team.
Note
You can also navigate to Organizations > Your Organization > Teams > Your Team Name and select Add Member. Select a member from the drop-down list to add them to the team or search by Docker ID or email.
Select the team and then select Add.
Note
An invitee must first accept the invitation to join the organization before being added to the team.
To add a member to a team with the Admin Console:
In the Admin Console, select your organization.
Select the team name.
Select Add member. You can add the member by searching for their email address or username.
Note
An invitee must first accept the invitation to join the organization before being added to the team.
Remove a member from a team
Note
If your organization uses single sign-on (SSO) with SCIM enabled, you should remove members from your identity provider (IdP). This will automatically remove members from Docker. If SCIM is disabled, you must manually manage members in Docker.
Organization owners can remove a member from a team in Docker Hub or Admin Console. Removing the member from the team will revoke their access to the permitted resources.
To remove a member from a specific team with Docker Hub:
- Sign in to Docker Hub.
- Select Organizations, your organization, Teams, and then the team.
- Select the X next to the user’s name to remove them from the team.
- When prompted, select Remove to confirm.
To remove a member from a specific team with the Admin Console:
- In the Admin Console, select your organization.
- Select the team name.
- Select the X next to the user's name to remove them from the team.
- When prompted, select Remove to confirm.
Update a member role
Organization owners can manage roles within an organization. If an organization is part of a company, the company owner can also manage that organization's roles. If you have SSO enabled, you can use SCIM for role mapping.
Note
If you're the only owner of an organization, you need to assign a new owner before you can edit your role.
To update a member role:
- Sign in to Docker Hub.
- Select Organizations, your organization, and then Members.
- Find the username of the member whose role you want to edit. In the table, select the Actions icon.
- Select Edit role.
- Select their organization, select the role you want to assign, and then select Save.
Export members CSV file
Owners can export a CSV file containing all members. The CSV file for a company contains the following fields:
- Name: The user's name
- Username: The user's Docker ID
- Email: The user's email address
- Member of Organizations: All organizations the user is a member of within a company
- Invited to Organizations: All organizations the user is an invitee of within a company
- Account Created: The time and date when the user account was created
To export a CSV file of your members:
- Sign in to Docker Hub.
- Select Organizations, your organization, and then Members.
- Select the Action icon and then select Export users as CSV.
To export a CSV file of your members:
- In the Admin Console, select your organization.
- Select Members.
- Select the download icon to export a CSV file of all members.",,,
e4458551f430d29a6dd8e21a61a54cb1b653d521b453cfcfdf2234081c3555c5,"Inspect a service on the swarm
When you have deployed a service to your swarm, you can use the Docker CLI to see details about the service running in the swarm.
If you haven't already, open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named
manager1
.Run
docker service inspect --pretty <SERVICE-ID>
to display the details about a service in an easily readable format.To see the details on the
helloworld
service:[manager1]$ docker service inspect --pretty helloworld ID: 9uk4639qpg7npwf3fn2aasksr Name: helloworld Service Mode: REPLICATED Replicas: 1 Placement: UpdateConfig: Parallelism: 1 ContainerSpec: Image: alpine Args: ping docker.com Resources: Endpoint Mode: vip
Tip
To return the service details in json format, run the same command without the
--pretty
flag.[manager1]$ docker service inspect helloworld [ { ""ID"": ""9uk4639qpg7npwf3fn2aasksr"", ""Version"": { ""Index"": 418 }, ""CreatedAt"": ""2016-06-16T21:57:11.622222327Z"", ""UpdatedAt"": ""2016-06-16T21:57:11.622222327Z"", ""Spec"": { ""Name"": ""helloworld"", ""TaskTemplate"": { ""ContainerSpec"": { ""Image"": ""alpine"", ""Args"": [ ""ping"", ""docker.com"" ] }, ""Resources"": { ""Limits"": {}, ""Reservations"": {} }, ""RestartPolicy"": { ""Condition"": ""any"", ""MaxAttempts"": 0 }, ""Placement"": {} }, ""Mode"": { ""Replicated"": { ""Replicas"": 1 } }, ""UpdateConfig"": { ""Parallelism"": 1 }, ""EndpointSpec"": { ""Mode"": ""vip"" } }, ""Endpoint"": { ""Spec"": {} } } ]
Run
docker service ps <SERVICE-ID>
to see which nodes are running the service:[manager1]$ docker service ps helloworld NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS helloworld.1.8p1vev3fq5zm0mi8g0as41w35 alpine worker2 Running Running 3 minutes
In this case, the one instance of the
helloworld
service is running on theworker2
node. You may see the service running on your manager node. By default, manager nodes in a swarm can execute tasks just like worker nodes.Swarm also shows you the
DESIRED STATE
andCURRENT STATE
of the service task so you can see if tasks are running according to the service definition.Run
docker ps
on the node where the task is running to see details about the container for the task.Tip
If
helloworld
is running on a node other than your manager node, you must ssh to that node.[worker2]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e609dde94e47 alpine:latest ""ping docker.com"" 3 minutes ago Up 3 minutes helloworld.1.8p1vev3fq5zm0mi8g0as41w35
Next steps
Next, you'll change the scale for the service running in the swarm.",,,
b91165f7e448a2d7671c889b4bac899482f3d24942468cbe09d5d4e315c0f65f,"Runtime metrics
Docker stats
You can use the docker stats
command to live stream a container's
runtime metrics. The command supports CPU, memory usage, memory limit,
and network IO metrics.
The following is a sample output from the docker stats
command
$ docker stats redis1 redis2
CONTAINER CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O
redis1 0.07% 796 KB / 64 MB 1.21% 788 B / 648 B 3.568 MB / 512 KB
redis2 0.07% 2.746 MB / 64 MB 4.29% 1.266 KB / 648 B 12.4 MB / 0 B
The
docker stats
reference
page has more details about the docker stats
command.
Control groups
Linux Containers rely on control groups which not only track groups of processes, but also expose metrics about CPU, memory, and block I/O usage. You can access those metrics and obtain network usage metrics as well. This is relevant for ""pure"" LXC containers, as well as for Docker containers.
Control groups are exposed through a pseudo-filesystem. In modern distributions, you
should find this filesystem under /sys/fs/cgroup
. Under that directory, you
see multiple sub-directories, called devices
, freezer
, blkio
, and so on.
Each sub-directory actually corresponds to a different cgroup hierarchy.
On older systems, the control groups might be mounted on /cgroup
, without
distinct hierarchies. In that case, instead of seeing the sub-directories,
you see a bunch of files in that directory, and possibly some directories
corresponding to existing containers.
To figure out where your control groups are mounted, you can run:
$ grep cgroup /proc/mounts
Enumerate cgroups
The file layout of cgroups is significantly different between v1 and v2.
If /sys/fs/cgroup/cgroup.controllers
is present on your system, you are using v2,
otherwise you are using v1.
Refer to the subsection that corresponds to your cgroup version.
cgroup v2 is used by default on the following distributions:
- Fedora (since 31)
- Debian GNU/Linux (since 11)
- Ubuntu (since 21.10)
cgroup v1
You can look into /proc/cgroups
to see the different control group subsystems
known to the system, the hierarchy they belong to, and how many groups they contain.
You can also look at /proc/<pid>/cgroup
to see which control groups a process
belongs to. The control group is shown as a path relative to the root of
the hierarchy mountpoint. /
means the process hasn't been assigned to a
group, while /lxc/pumpkin
indicates that the process is a member of a
container named pumpkin
.
cgroup v2
On cgroup v2 hosts, the content of /proc/cgroups
isn't meaningful.
See /sys/fs/cgroup/cgroup.controllers
to the available controllers.
Changing cgroup version
Changing cgroup version requires rebooting the entire system.
On systemd-based systems, cgroup v2 can be enabled by adding systemd.unified_cgroup_hierarchy=1
to the kernel command line.
To revert the cgroup version to v1, you need to set systemd.unified_cgroup_hierarchy=0
instead.
If grubby
command is available on your system (e.g. on Fedora), the command line can be modified as follows:
$ sudo grubby --update-kernel=ALL --args=""systemd.unified_cgroup_hierarchy=1""
If grubby
command isn't available, edit the GRUB_CMDLINE_LINUX
line in /etc/default/grub
and run sudo update-grub
.
Running Docker on cgroup v2
Docker supports cgroup v2 since Docker 20.10. Running Docker on cgroup v2 also requires the following conditions to be satisfied:
- containerd: v1.4 or later
- runc: v1.0.0-rc91 or later
- Kernel: v4.15 or later (v5.2 or later is recommended)
Note that the cgroup v2 mode behaves slightly different from the cgroup v1 mode:
- The default cgroup driver (
dockerd --exec-opt native.cgroupdriver
) issystemd
on v2,cgroupfs
on v1. - The default cgroup namespace mode (
docker run --cgroupns
) isprivate
on v2,host
on v1. - The
docker run
flags--oom-kill-disable
and--kernel-memory
are discarded on v2.
Find the cgroup for a given container
For each container, one cgroup is created in each hierarchy. On
older systems with older versions of the LXC userland tools, the name of
the cgroup is the name of the container. With more recent versions
of the LXC tools, the cgroup is lxc/<container_name>.
For Docker containers using cgroups, the cgroup name is the full
ID or long ID of the container. If a container shows up as ae836c95b4c3
in docker ps
, its long ID might be something like
ae836c95b4c3c9e9179e0e91015512da89fdec91612f63cebae57df9a5444c79
. You can
look it up with docker inspect
or docker ps --no-trunc
.
Putting everything together to look at the memory metrics for a Docker container, take a look at the following paths:
/sys/fs/cgroup/memory/docker/<longid>/
on cgroup v1,cgroupfs
driver/sys/fs/cgroup/memory/system.slice/docker-<longid>.scope/
on cgroup v1,systemd
driver/sys/fs/cgroup/docker/<longid>/
on cgroup v2,cgroupfs
driver/sys/fs/cgroup/system.slice/docker-<longid>.scope/
on cgroup v2,systemd
driver
Metrics from cgroups: memory, CPU, block I/O
Note
This section isn't yet updated for cgroup v2. For further information about cgroup v2, refer to the kernel documentation.
For each subsystem (memory, CPU, and block I/O), one or more pseudo-files exist and contain statistics.
Memory metrics: memory.stat
Memory metrics are found in the memory
cgroup. The memory
control group adds a little overhead, because it does very fine-grained
accounting of the memory usage on your host. Therefore, many distributions
chose to not enable it by default. Generally, to enable it, all you have
to do is to add some kernel command-line parameters:
cgroup_enable=memory swapaccount=1
.
The metrics are in the pseudo-file memory.stat
.
Here is what it looks like:
cache 11492564992
rss 1930993664
mapped_file 306728960
pgpgin 406632648
pgpgout 403355412
swap 0
pgfault 728281223
pgmajfault 1724
inactive_anon 46608384
active_anon 1884520448
inactive_file 7003344896
active_file 4489052160
unevictable 32768
hierarchical_memory_limit 9223372036854775807
hierarchical_memsw_limit 9223372036854775807
total_cache 11492564992
total_rss 1930993664
total_mapped_file 306728960
total_pgpgin 406632648
total_pgpgout 403355412
total_swap 0
total_pgfault 728281223
total_pgmajfault 1724
total_inactive_anon 46608384
total_active_anon 1884520448
total_inactive_file 7003344896
total_active_file 4489052160
total_unevictable 32768
The first half (without the total_
prefix) contains statistics relevant
to the processes within the cgroup, excluding sub-cgroups. The second half
(with the total_
prefix) includes sub-cgroups as well.
Some metrics are ""gauges"", or values that can increase or decrease. For instance,
swap
is the amount of swap space used by the members of the cgroup.
Some others are ""counters"", or values that can only go up, because
they represent occurrences of a specific event. For instance, pgfault
indicates the number of page faults since the creation of the cgroup.
cache
- The amount of memory used by the processes of this control group that can be
associated precisely with a block on a block device. When you read from and
write to files on disk, this amount increases. This is the case if you use
""conventional"" I/O (
open
,read
,write
syscalls) as well as mapped files (withmmap
). It also accounts for the memory used bytmpfs
mounts, though the reasons are unclear. rss
- The amount of memory that doesn't correspond to anything on disk: stacks, heaps, and anonymous memory maps.
mapped_file
- Indicates the amount of memory mapped by the processes in the control group. It doesn't give you information about how much memory is used; it rather tells you how it's used.
pgfault
,pgmajfault
- Indicate the number of times that a process of the cgroup triggered a ""page
fault"" and a ""major fault"", respectively. A page fault happens when a process
accesses a part of its virtual memory space which is nonexistent or protected.
The former can happen if the process is buggy and tries to access an invalid
address (it is sent a
SIGSEGV
signal, typically killing it with the famousSegmentation fault
message). The latter can happen when the process reads from a memory zone which has been swapped out, or which corresponds to a mapped file: in that case, the kernel loads the page from disk, and let the CPU complete the memory access. It can also happen when the process writes to a copy-on-write memory zone: likewise, the kernel preempts the process, duplicate the memory page, and resume the write operation on the process's own copy of the page. ""Major"" faults happen when the kernel actually needs to read the data from disk. When it just duplicates an existing page, or allocate an empty page, it's a regular (or ""minor"") fault. swap
- The amount of swap currently used by the processes in this cgroup.
active_anon
,inactive_anon
- The amount of anonymous memory that has been identified has respectively
active and inactive by the kernel. ""Anonymous"" memory is the memory that is
not linked to disk pages. In other words, that's the equivalent of the rss
counter described above. In fact, the very definition of the rss counter is
active_anon
+inactive_anon
-tmpfs
(where tmpfs is the amount of memory used up bytmpfs
filesystems mounted by this control group). Now, what's the difference between ""active"" and ""inactive""? Pages are initially ""active""; and at regular intervals, the kernel sweeps over the memory, and tags some pages as ""inactive"". Whenever they're accessed again, they're immediately re-tagged ""active"". When the kernel is almost out of memory, and time comes to swap out to disk, the kernel swaps ""inactive"" pages. active_file
,inactive_file
- Cache memory, with active and inactive similar to the anon memory
above. The exact formula is
cache
=active_file
+inactive_file
+tmpfs
. The exact rules used by the kernel to move memory pages between active and inactive sets are different from the ones used for anonymous memory, but the general principle is the same. When the kernel needs to reclaim memory, it's cheaper to reclaim a clean (=non modified) page from this pool, since it can be reclaimed immediately (while anonymous pages and dirty/modified pages need to be written to disk first). unevictable
- The amount of memory that cannot be reclaimed; generally, it accounts for
memory that has been ""locked"" with
mlock
. It's often used by crypto frameworks to make sure that secret keys and other sensitive material never gets swapped out to disk. memory_limit
,memsw_limit
- These aren't really metrics, but a reminder of the limits applied to this cgroup. The first one indicates the maximum amount of physical memory that can be used by the processes of this control group; the second one indicates the maximum amount of RAM+swap.
Accounting for memory in the page cache is very complex. If two processes in different control groups both read the same file (ultimately relying on the same blocks on disk), the corresponding memory charge is split between the control groups. It's nice, but it also means that when a cgroup is terminated, it could increase the memory usage of another cgroup, because they're not splitting the cost anymore for those memory pages.
CPU metrics: cpuacct.stat
Now that we've covered memory metrics, everything else is
simple in comparison. CPU metrics are in the
cpuacct
controller.
For each container, a pseudo-file cpuacct.stat
contains the CPU usage
accumulated by the processes of the container, broken down into user
and
system
time. The distinction is:
user
time is the amount of time a process has direct control of the CPU, executing process code.system
time is the time the kernel is executing system calls on behalf of the process.
Those times are expressed in ticks of 1/100th of a second, also called ""user
jiffies"". There are USER_HZ
""jiffies"" per second, and on x86 systems,
USER_HZ
is 100. Historically, this mapped exactly to the number of scheduler
""ticks"" per second, but higher frequency scheduling and
tickless kernels have made the number of
ticks irrelevant.
Block I/O metrics
Block I/O is accounted in the blkio
controller.
Different metrics are scattered across different files. While you can
find in-depth details in the
blkio-controller
file in the kernel documentation, here is a short list of the most
relevant ones:
blkio.sectors
- Contains the number of 512-bytes sectors read and written by the processes member of the cgroup, device by device. Reads and writes are merged in a single counter.
blkio.io_service_bytes
- Indicates the number of bytes read and written by the cgroup. It has 4 counters per device, because for each device, it differentiates between synchronous vs. asynchronous I/O, and reads vs. writes.
blkio.io_serviced
- The number of I/O operations performed, regardless of their size. It also has 4 counters per device.
blkio.io_queued
- Indicates the number of I/O operations currently queued for this cgroup. In other words, if the cgroup isn't doing any I/O, this is zero. The opposite is not true. In other words, if there is no I/O queued, it doesn't mean that the cgroup is idle (I/O-wise). It could be doing purely synchronous reads on an otherwise quiescent device, which can therefore handle them immediately, without queuing. Also, while it's helpful to figure out which cgroup is putting stress on the I/O subsystem, keep in mind that it's a relative quantity. Even if a process group doesn't perform more I/O, its queue size can increase just because the device load increases because of other devices.
Network metrics
Network metrics aren't exposed directly by control groups. There is a
good explanation for that: network interfaces exist within the context
of network namespaces. The kernel could probably accumulate metrics
about packets and bytes sent and received by a group of processes, but
those metrics wouldn't be very useful. You want per-interface metrics
(because traffic happening on the local lo
interface doesn't really count). But since processes in a single cgroup
can belong to multiple network namespaces, those metrics would be harder
to interpret: multiple network namespaces means multiple lo
interfaces, potentially multiple eth0
interfaces, etc.; so this is why there is no easy way to gather network
metrics with control groups.
Instead you can gather network metrics from other sources.
iptables
iptables (or rather, the netfilter framework for which iptables is just an interface) can do some serious accounting.
For instance, you can setup a rule to account for the outbound HTTP traffic on a web server:
$ iptables -I OUTPUT -p tcp --sport 80
There is no -j
or -g
flag,
so the rule just counts matched packets and goes to the following
rule.
Later, you can check the values of the counters, with:
$ iptables -nxvL OUTPUT
Technically, -n
isn't required, but it
prevents iptables from doing DNS reverse lookups, which are probably
useless in this scenario.
Counters include packets and bytes. If you want to setup metrics for
container traffic like this, you could execute a for
loop to add two iptables
rules per
container IP address (one in each direction), in the FORWARD
chain. This only meters traffic going through the NAT
layer; you also need to add traffic going through the userland
proxy.
Then, you need to check those counters on a regular basis. If you
happen to use collectd
, there is a
nice plugin
to automate iptables counters collection.
Interface-level counters
Since each container has a virtual Ethernet interface, you might want to check
directly the TX and RX counters of this interface. Each container is associated
to a virtual Ethernet interface in your host, with a name like vethKk8Zqi
.
Figuring out which interface corresponds to which container is, unfortunately,
difficult.
But for now, the best way is to check the metrics from within the containers. To accomplish this, you can run an executable from the host environment within the network namespace of a container using ip-netns magic.
The ip-netns exec
command allows you to execute any
program (present in the host system) within any network namespace
visible to the current process. This means that your host can
enter the network namespace of your containers, but your containers
can't access the host or other peer containers.
Containers can interact with their sub-containers, though.
The exact format of the command is:
$ ip netns exec <nsname> <command...>
For example:
$ ip netns exec mycontainer netstat -i
ip netns
finds the mycontainer
container by
using namespaces pseudo-files. Each process belongs to one network
namespace, one PID namespace, one mnt
namespace,
etc., and those namespaces are materialized under
/proc/<pid>/ns/
. For example, the network
namespace of PID 42 is materialized by the pseudo-file
/proc/42/ns/net
.
When you run ip netns exec mycontainer ...
, it
expects /var/run/netns/mycontainer
to be one of
those pseudo-files. (Symlinks are accepted.)
In other words, to execute a command within the network namespace of a container, we need to:
- Find out the PID of any process within the container that we want to investigate;
- Create a symlink from
/var/run/netns/<somename>
to/proc/<thepid>/ns/net
- Execute
ip netns exec <somename> ....
Review
Enumerate Cgroups for how to find
the cgroup of an in-container process whose network usage you want to measure.
From there, you can examine the pseudo-file named
tasks
, which contains all the PIDs in the
cgroup (and thus, in the container). Pick any one of the PIDs.
Putting everything together, if the ""short ID"" of a container is held in
the environment variable $CID
, then you can do this:
$ TASKS=/sys/fs/cgroup/devices/docker/$CID*/tasks
$ PID=$(head -n 1 $TASKS)
$ mkdir -p /var/run/netns
$ ln -sf /proc/$PID/ns/net /var/run/netns/$CID
$ ip netns exec $CID netstat -i
Tips for high-performance metric collection
Running a new process each time you want to update metrics is (relatively) expensive. If you want to collect metrics at high resolutions, and/or over a large number of containers (think 1000 containers on a single host), you don't want to fork a new process each time.
Here is how to collect metrics from a single process. You need to
write your metric collector in C (or any language that lets you do
low-level system calls). You need to use a special system call,
setns()
, which lets the current process enter any
arbitrary namespace. It requires, however, an open file descriptor to
the namespace pseudo-file (remember: that's the pseudo-file in
/proc/<pid>/ns/net
).
However, there is a catch: you must not keep this file descriptor open. If you do, when the last process of the control group exits, the namespace isn't destroyed, and its network resources (like the virtual interface of the container) stays around forever (or until you close that file descriptor).
The right approach would be to keep track of the first PID of each container, and re-open the namespace pseudo-file each time.
Collect metrics when a container exits
Sometimes, you don't care about real time metric collection, but when a container exits, you want to know how much CPU, memory, etc. it has used.
Docker makes this difficult because it relies on lxc-start
, which carefully
cleans up after itself. It is usually easier to collect metrics at regular
intervals, and this is the way the collectd
LXC plugin works.
But, if you'd still like to gather the stats when a container stops, here is how:
For each container, start a collection process, and move it to the control groups that you want to monitor by writing its PID to the tasks file of the cgroup. The collection process should periodically re-read the tasks file to check if it's the last process of the control group. (If you also want to collect network statistics as explained in the previous section, you should also move the process to the appropriate network namespace.)
When the container exits, lxc-start
attempts to
delete the control groups. It fails, since the control group is
still in use; but that's fine. Your process should now detect that it is
the only one remaining in the group. Now is the right time to collect
all the metrics you need!
Finally, your process should move itself back to the root control group,
and remove the container control group. To remove a control group, just
rmdir
its directory. It's counter-intuitive to
rmdir
a directory as it still contains files; but
remember that this is a pseudo-filesystem, so usual rules don't apply.
After the cleanup is done, the collection process can exit safely.",,,
239a57c0ef6c8228ed74d179c6214621408bc571612528b80e26868e922994f5,"Daemon proxy configuration
If your organization uses a proxy server to connect to the internet, you may need to configure the Docker daemon to use the proxy server. The daemon uses a proxy server to access images stored on Docker Hub and other registries, and to reach other nodes in a Docker swarm.
This page describes how to configure a proxy for the Docker daemon. For instructions on configuring proxy settings for the Docker CLI, see Configure Docker CLI to use a proxy server.
Important
Proxy configurations specified in the
daemon.json
are ignored by Docker Desktop. If you use Docker Desktop, you can configure proxies using the Docker Desktop settings.
There are two ways you can configure these settings:
- Configuring the daemon through a configuration file or CLI flags
- Setting environment variables on the system
Configuring the daemon directly takes precedence over environment variables.
Daemon configuration
You may configure proxy behavior for the daemon in the daemon.json
file,
or using CLI flags for the --http-proxy
or --https-proxy
flags for the
dockerd
command. Configuration using daemon.json
is recommended.
{
""proxies"": {
""http-proxy"": ""http://proxy.example.com:3128"",
""https-proxy"": ""https://proxy.example.com:3129"",
""no-proxy"": ""*.test.example.com,.example.org,127.0.0.0/8""
}
}
After changing the configuration file, restart the daemon for the proxy configuration to take effect:
$ sudo systemctl restart docker
Environment variables
The Docker daemon checks the following environment variables in its start-up environment to configure HTTP or HTTPS proxy behavior:
HTTP_PROXY
http_proxy
HTTPS_PROXY
https_proxy
NO_PROXY
no_proxy
systemd unit file
If you're running the Docker daemon as a systemd service, you can create a
systemd drop-in file that sets the variables for the docker
service.
Note for rootless mode
The location of systemd configuration files are different when running Docker in rootless mode. When running in rootless mode, Docker is started as a user-mode systemd service, and uses files stored in each users' home directory in
~/.config/systemd/<user>/docker.service.d/
. In addition,systemctl
must be executed withoutsudo
and with the--user
flag. Select the ""Rootless mode"" tab if you are running Docker in rootless mode.
Create a systemd drop-in directory for the
docker
service:$ sudo mkdir -p /etc/systemd/system/docker.service.d
Create a file named
/etc/systemd/system/docker.service.d/http-proxy.conf
that adds theHTTP_PROXY
environment variable:[Service] Environment=""HTTP_PROXY=http://proxy.example.com:3128""
If you are behind an HTTPS proxy server, set the
HTTPS_PROXY
environment variable:[Service] Environment=""HTTPS_PROXY=https://proxy.example.com:3129""
Multiple environment variables can be set; to set both a non-HTTPS and a HTTPs proxy;
[Service] Environment=""HTTP_PROXY=http://proxy.example.com:3128"" Environment=""HTTPS_PROXY=https://proxy.example.com:3129""
Note
Special characters in the proxy value, such as
#?!()[]{}
, must be double escaped using%%
. For example:[Service] Environment=""HTTP_PROXY=http://domain%%5Cuser:complex%%23pass@proxy.example.com:3128/""
If you have internal Docker registries that you need to contact without proxying, you can specify them via the
NO_PROXY
environment variable.The
NO_PROXY
variable specifies a string that contains comma-separated values for hosts that should be excluded from proxying. These are the options you can specify to exclude hosts:- IP address prefix (
1.2.3.4
) - Domain name, or a special DNS label (
*
) - A domain name matches that name and all subdomains. A domain name with a
leading ""."" matches subdomains only. For example, given the domains
foo.example.com
andexample.com
:example.com
matchesexample.com
andfoo.example.com
, and.example.com
matches onlyfoo.example.com
- A single asterisk (
*
) indicates that no proxying should be done - Literal port numbers are accepted by IP address prefixes (
1.2.3.4:80
) and domain names (foo.example.com:80
)
Example:
[Service] Environment=""HTTP_PROXY=http://proxy.example.com:3128"" Environment=""HTTPS_PROXY=https://proxy.example.com:3129"" Environment=""NO_PROXY=localhost,127.0.0.1,docker-registry.example.com,.corp""
- IP address prefix (
Flush changes and restart Docker
$ sudo systemctl daemon-reload $ sudo systemctl restart docker
Verify that the configuration has been loaded and matches the changes you made, for example:
$ sudo systemctl show --property=Environment docker Environment=HTTP_PROXY=http://proxy.example.com:3128 HTTPS_PROXY=https://proxy.example.com:3129 NO_PROXY=localhost,127.0.0.1,docker-registry.example.com,.corp
Create a systemd drop-in directory for the
docker
service:$ mkdir -p ~/.config/systemd/user/docker.service.d
Create a file named
~/.config/systemd/user/docker.service.d/http-proxy.conf
that adds theHTTP_PROXY
environment variable:[Service] Environment=""HTTP_PROXY=http://proxy.example.com:3128""
If you are behind an HTTPS proxy server, set the
HTTPS_PROXY
environment variable:[Service] Environment=""HTTPS_PROXY=https://proxy.example.com:3129""
Multiple environment variables can be set; to set both a non-HTTPS and a HTTPs proxy;
[Service] Environment=""HTTP_PROXY=http://proxy.example.com:3128"" Environment=""HTTPS_PROXY=https://proxy.example.com:3129""
Note
Special characters in the proxy value, such as
#?!()[]{}
, must be double escaped using%%
. For example:[Service] Environment=""HTTP_PROXY=http://domain%%5Cuser:complex%%23pass@proxy.example.com:3128/""
If you have internal Docker registries that you need to contact without proxying, you can specify them via the
NO_PROXY
environment variable.The
NO_PROXY
variable specifies a string that contains comma-separated values for hosts that should be excluded from proxying. These are the options you can specify to exclude hosts:- IP address prefix (
1.2.3.4
) - Domain name, or a special DNS label (
*
) - A domain name matches that name and all subdomains. A domain name with a
leading ""."" matches subdomains only. For example, given the domains
foo.example.com
andexample.com
:example.com
matchesexample.com
andfoo.example.com
, and.example.com
matches onlyfoo.example.com
- A single asterisk (
*
) indicates that no proxying should be done - Literal port numbers are accepted by IP address prefixes (
1.2.3.4:80
) and domain names (foo.example.com:80
)
Example:
[Service] Environment=""HTTP_PROXY=http://proxy.example.com:3128"" Environment=""HTTPS_PROXY=https://proxy.example.com:3129"" Environment=""NO_PROXY=localhost,127.0.0.1,docker-registry.example.com,.corp""
- IP address prefix (
Flush changes and restart Docker
$ systemctl --user daemon-reload $ systemctl --user restart docker
Verify that the configuration has been loaded and matches the changes you made, for example:
$ systemctl --user show --property=Environment docker Environment=HTTP_PROXY=http://proxy.example.com:3128 HTTPS_PROXY=https://proxy.example.com:3129 NO_PROXY=localhost,127.0.0.1,docker-registry.example.com,.corp",,,
8a3ce4c64603d88a29d18bf059e4888b35c522aa394cf921a80ae70cc435cb31,"Remote driver
The Buildx remote driver allows for more complex custom build workloads, allowing you to connect to externally managed BuildKit instances. This is useful for scenarios that require manual management of the BuildKit daemon, or where a BuildKit daemon is exposed from another source.
Synopsis
$ docker buildx create \
--name remote \
--driver remote \
tcp://localhost:1234
The following table describes the available driver-specific options that you can
pass to --driver-opt
:
| Parameter | Type | Default | Description |
|---|---|---|---|
key | String | Sets the TLS client key. | |
cert | String | Absolute path to the TLS client certificate to present to buildkitd . | |
cacert | String | Absolute path to the TLS certificate authority used for validation. | |
servername | String | Endpoint hostname. | TLS server name used in requests. |
default-load | Boolean | false | Automatically load images to the Docker Engine image store. |
Example: Remote BuildKit over Unix sockets
This guide shows you how to create a setup with a BuildKit daemon listening on a Unix socket, and have Buildx connect through it.
Ensure that BuildKit is installed.
For example, you can launch an instance of buildkitd with:
$ sudo ./buildkitd --group $(id -gn) --addr unix://$HOME/buildkitd.sock
Alternatively, see here for running buildkitd in rootless mode or here for examples of running it as a systemd service.
Check that you have a Unix socket that you can connect to.
$ ls -lh /home/user/buildkitd.sock srw-rw---- 1 root user 0 May 5 11:04 /home/user/buildkitd.sock
Connect Buildx to it using the remote driver:
$ docker buildx create \ --name remote-unix \ --driver remote \ unix://$HOME/buildkitd.sock
List available builders with
docker buildx ls
. You should then seeremote-unix
among them:$ docker buildx ls NAME/NODE DRIVER/ENDPOINT STATUS PLATFORMS remote-unix remote remote-unix0 unix:///home/.../buildkitd.sock running linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386 default * docker default default running linux/amd64, linux/386
You can switch to this new builder as the default using
docker buildx use remote-unix
, or specify it per build using --builder
:
$ docker buildx build --builder=remote-unix -t test --load .
Remember that you need to use the --load
flag if you want to load the build
result into the Docker daemon.
Example: Remote BuildKit in Docker container
This guide will show you how to create setup similar to the docker-container
driver, by manually booting a BuildKit Docker container and connecting to it
using the Buildx remote driver. This procedure will manually create a container
and access it via it's exposed port. (You'd probably be better of just using the
docker-container
driver that connects to BuildKit through the Docker daemon,
but this is for illustration purposes.)
Generate certificates for BuildKit.
You can use this bake definition as a starting point:
SAN=""localhost 127.0.0.1"" docker buildx bake ""https://github.com/moby/buildkit.git#master:examples/create-certs""
Note that while it's possible to expose BuildKit over TCP without using TLS, it's not recommended. Doing so allows arbitrary access to BuildKit without credentials.
With certificates generated in
.certs/
, startup the container:$ docker run -d --rm \ --name=remote-buildkitd \ --privileged \ -p 1234:1234 \ -v $PWD/.certs:/etc/buildkit/certs \ moby/buildkit:latest \ --addr tcp://0.0.0.0:1234 \ --tlscacert /etc/buildkit/certs/daemon/ca.pem \ --tlscert /etc/buildkit/certs/daemon/cert.pem \ --tlskey /etc/buildkit/certs/daemon/key.pem
This command starts a BuildKit container and exposes the daemon's port 1234 to localhost.
Connect to this running container using Buildx:
$ docker buildx create \ --name remote-container \ --driver remote \ --driver-opt cacert=${PWD}/.certs/client/ca.pem,cert=${PWD}/.certs/client/cert.pem,key=${PWD}/.certs/client/key.pem,servername=<TLS_SERVER_NAME> \ tcp://localhost:1234
Alternatively, use the
docker-container://
URL scheme to connect to the BuildKit container without specifying a port:$ docker buildx create \ --name remote-container \ --driver remote \ docker-container://remote-container
Example: Remote BuildKit in Kubernetes
This guide will show you how to create a setup similar to the kubernetes
driver by manually creating a BuildKit Deployment
. While the kubernetes
driver will do this under-the-hood, it might sometimes be desirable to scale
BuildKit manually. Additionally, when executing builds from inside Kubernetes
pods, the Buildx builder will need to be recreated from within each pod or
copied between them.
Create a Kubernetes deployment of
buildkitd
, as per the instructions here.Following the guide, create certificates for the BuildKit daemon and client using create-certs.sh, and create a deployment of BuildKit pods with a service that connects to them.
Assuming that the service is called
buildkitd
, create a remote builder in Buildx, ensuring that the listed certificate files are present:$ docker buildx create \ --name remote-kubernetes \ --driver remote \ --driver-opt cacert=${PWD}/.certs/client/ca.pem,cert=${PWD}/.certs/client/cert.pem,key=${PWD}/.certs/client/key.pem \ tcp://buildkitd.default.svc:1234
Note that this only works internally, within the cluster, since the BuildKit
setup guide only creates a ClusterIP
service. To access a builder remotely,
you can set up and use an ingress, which is outside the scope of this guide.
Debug a remote builder in Kubernetes
If you're having trouble accessing a remote builder deployed in Kubernetes, you
can use the kube-pod://
URL scheme to connect directly to a BuildKit pod
through the Kubernetes API. Note that this method only connects to a single pod
in the deployment.
$ kubectl get pods --selector=app=buildkitd -o json | jq -r '.items[].metadata.name'
buildkitd-XXXXXXXXXX-xxxxx
$ docker buildx create \
--name remote-container \
--driver remote \
kube-pod://buildkitd-XXXXXXXXXX-xxxxx
Alternatively, use the port forwarding mechanism of kubectl
:
$ kubectl port-forward svc/buildkitd 1234:1234
Then you can point the remote driver at tcp://localhost:1234
.",,,
4b30275a53b52a943094c5b77cd31c86d1fd2854ee8616def418e437e3f3436e,"Building with Docker Build Cloud
To build using Docker Build Cloud, invoke a build command and specify the name of the
builder using the --builder
flag.
$ docker buildx build --builder cloud-<ORG>-<BUILDER_NAME> --tag <IMAGE> .
Use by default
If you want to use Docker Build Cloud without having to specify the --builder
flag
each time, you can set it as the default builder.
Run the following command:
$ docker buildx use cloud-<ORG>-<BUILDER_NAME> --global
Open the Docker Desktop settings and navigate to the Builders tab.
Find the cloud builder under Available builders.
Open the drop-down menu and select Use.
Changing your default builder with docker buildx use
only changes the default
builder for the docker buildx build
command. The docker build
command still
uses the default
builder, unless you specify the --builder
flag explicitly.
If you use build scripts, such as make
, we recommend that you update your
build commands from docker build
to docker buildx build
, to avoid any
confusion with regards to builder selection. Alternatively, you can run docker buildx install
to make the default docker build
command behave like docker buildx build
, without discrepancies.
Use with Docker Compose
To build with Docker Build Cloud using docker compose build
, first set the
cloud builder as your selected builder, then run your build.
Note
Make sure you're using a supported version of Docker Compose, see Prerequisites.
$ docker buildx use cloud-<ORG>-<BUILDER_NAME>
$ docker compose build
In addition to docker buildx use
, you can also use the docker compose build --builder
flag or the
BUILDX_BUILDER
environment
variable to select the cloud builder.
Loading build results
Building with --tag
loads the build result to the local image store
automatically when the build finishes. To build without a tag and load the
result, you must pass the --load
flag.
Loading the build result for multi-platform images is not supported. Use the
docker buildx build --push
flag when building multi-platform images to push
the output to a registry.
$ docker buildx build --builder cloud-<ORG>-<BUILDER_NAME> \
--platform linux/amd64,linux/arm64 \
--tag <IMAGE> \
--push .
If you want to build with a tag, but you don't want to load the results to your local image store, you can export the build results to the build cache only:
$ docker buildx build --builder cloud-<ORG>-<BUILDER_NAME> \
--platform linux/amd64,linux/arm64 \
--tag <IMAGE> \
--output type=cacheonly .
Multi-platform builds
To run multi-platform builds, you must specify all of the platforms that you
want to build for using the --platform
flag.
$ docker buildx build --builder cloud-<ORG>-<BUILDER_NAME> \
--platform linux/amd64,linux/arm64 \
--tag <IMAGE> \
--push .
If you don't specify the platform, the cloud builder automatically builds for the architecture matching your local environment.
To learn more about building for multiple platforms, refer to Multi-platform builds.
Cloud builds in Docker Desktop
The Docker Desktop Builds view works with Docker Build Cloud out of the box. This view can show information about not only your own builds, but also builds initiated by your team members using the same builder.
Teams using a shared builder get access to information such as:
- Ongoing and completed builds
- Build configuration, statistics, dependencies, and results
- Build source (Dockerfile)
- Build logs and errors
This lets you and your team work collaboratively on troubleshooting and improving build speeds, without having to send build logs and benchmarks back and forth between each other.
Use secrets with Docker Build Cloud
To use build secrets with Docker Build Cloud,
such as authentication credentials or tokens,
use the --secret
and --ssh
CLI flags for the docker buildx
command.
The traffic is encrypted and secrets are never stored in the build cache.
Warning
If you're misusing build arguments to pass credentials, authentication tokens, or other secrets, you should refactor your build to pass the secrets using secret mounts instead. Build arguments are stored in the cache and their values are exposed through attestations. Secret mounts don't leak outside of the build and are never included in attestations.
For more information, refer to:
Managing build cache
You don't need to manage Docker Build Cloud cache manually. The system manages it for you through garbage collection.
Old cache is automatically removed if you hit your storage limit.
You can check your current cache state using the
docker buildx du
command.
To clear the builder's cache manually,
use the
docker buildx prune
command.
This works like pruning the cache for any other builder.
Warning
Pruning a cloud builder's cache also removes the cache for other team members using the same builder.
Unset Docker Build Cloud as the default builder
If you've set a cloud builder as the default builder
and want to revert to the default docker
builder,
run the following command:
$ docker context use default
This doesn't remove the builder from your system. It only changes the builder that's automatically selected to run your builds.
Registries on internal networks
It is possible to use Docker Build Cloud with a private registry or registry mirror on an internal network.",,,
c721b64c8876b4e1587b6264c70856c638ed6d05ab1f96948e656aa41098de19,"Ways to enforce sign-in for Docker Desktop
This page outlines the different methods for enforcing sign-in for Docker Desktop.
Registry key method (Windows only)
Note
The registry key method is available with Docker Desktop version 4.32 and later.
To enforce sign-in for Docker Desktop on Windows, you can configure a registry key that specifies your organization's allowed users. The following steps guide you through creating and deploying the registry key to enforce this policy:
Create the registry key. Your new key should look like the following:
$ HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Docker\Docker Desktop
Create a multi-string value
allowedOrgs
.Important
As of Docker Desktop version 4.36 and later, you can add more than one organization. With Docker Desktop version 4.35 and earlier, if you add more than one organization sign-in enforcement silently fails.
Use your organization's name, all lowercase as string data. If you're adding more than one organization, make sure there is an empty space between each organization name.
Restart Docker Desktop.
When Docker Desktop restarts, verify that the Sign in required! prompt appears.
In some cases, a system reboot may be necessary for enforcement to take effect.
Note
If a registry key and a
registry.json
file both exist, the registry key takes precedence.
Example deployment via Group Policy
The following example outlines how to deploy a registry key to enforce sign-in on Docker Desktop using Group Policy. There are multiple ways to deploy this configuration depending on your organization's infrastructure, security policies, and management tools.
- Create the registry script. Write a script to create the
HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Docker\Docker Desktop
key, add theallowedOrgs
multi-string, and then set the value to your organization’s name. - Within Group Policy, create or edit a Group Policy Objective (GPO) that applies to the machines or users you want to target.
- Within the GPO, navigate to Computer Configuration and select Preferences.
- Select Windows Settings then Registry.
- To add the registry item, right-click on the Registry node, select New, and then Registry Item.
- Configure the new registry item to match the registry script you created, specifying the action as Update. Make sure you input the correct path, value name (
allowedOrgs
), and value data (your organization names). - Link the GPO to an Organizational Unit (OU) that contains the machines you want to apply this setting to.
- Test the GPO on a small set of machines first to ensure it behaves as expected. You can use the
gpupdate /force
command on a test machine to manually refresh its group policy settings and check the registry to confirm the changes. - Once verified, you can proceed with broader deployment. Monitor the deployment to ensure the settings are applied correctly across the organization's computers.
Configuration profiles method (Mac only)
Configuration profiles are a feature of macOS that let you distribute configuration information to the Macs you manage. It is the safest method to enforce sign-in on macOS because the installed configuration profiles are protected by Apples' System Integrity Protection (SIP) and therefore can't be tampered with by the users.
Save the following XML file with the extension
.mobileconfig
, for exampledocker.mobileconfig
:<?xml version=""1.0"" encoding=""UTF-8""?> <!DOCTYPE plist PUBLIC ""-//Apple//DTD PLIST 1.0//EN"" ""http://www.apple.com/DTDs/PropertyList-1.0.dtd""> <plist version=""1.0""> <dict> <key>PayloadContent</key> <array> <dict> <key>PayloadType</key> <string>com.docker.config</string> <key>PayloadVersion</key> <integer>1</integer> <key>PayloadIdentifier</key> <string>com.docker.config</string> <key>PayloadUUID</key> <string>eed295b0-a650-40b0-9dda-90efb12be3c7</string> <key>PayloadDisplayName</key> <string>Docker Desktop Configuration</string> <key>PayloadDescription</key> <string>Configuration profile to manage Docker Desktop settings.</string> <key>PayloadOrganization</key> <string>Your Company Name</string> <key>allowedOrgs</key> <string>first_org;second_org</string> </dict> </array> <key>PayloadType</key> <string>Configuration</string> <key>PayloadVersion</key> <integer>1</integer> <key>PayloadIdentifier</key> <string>com.yourcompany.docker.config</string> <key>PayloadUUID</key> <string>0deedb64-7dc9-46e5-b6bf-69d64a9561ce</string> <key>PayloadDisplayName</key> <string>Docker Desktop Config Profile</string> <key>PayloadDescription</key> <string>Config profile to enforce Docker Desktop settings for allowed organizations.</string> <key>PayloadOrganization</key> <string>Your Company Name</string> </dict> </plist>
Change the placeholders
com.yourcompany.docker.config
andYour Company Name
to the name of your company.Add your organization name. The names of the allowed organizations are stored in the
allowedOrgs
property. It can contain either the name of a single organization or a list of organization names, separated by a semicolon:<key>allowedOrgs</key> <string>first_org;second_org</string>
Use a MDM solution to distribute your modified
.mobileconfig
file to your macOS clients.
plist method (Mac only)
Note
The
plist
method is available with Docker Desktop version 4.32 and later.
To enforce sign-in for Docker Desktop on macOS, you can use a plist
file that defines the required settings. The following steps guide you through the process of creating and deploying the necessary plist
file to enforce this policy:
Create the file
/Library/Application Support/com.docker.docker/desktop.plist
.Open
desktop.plist
in a text editor and add the following content, wheremyorg
is replaced with your organization’s name all lowercase:<?xml version=""1.0"" encoding=""UTF-8""?> <!DOCTYPE plist PUBLIC ""-//Apple//DTD PLIST 1.0//EN"" ""http://www.apple.com/DTDs/PropertyList-1.0.dtd""> <plist version=""1.0""> <dict> <key>allowedOrgs</key> <array> <string>myorg1</string> <string>myorg2</string> </array> </dict> </plist>
Important
As of Docker Desktop version 4.36 and later, you can add more than one organization. With Docker Desktop version 4.35 and earlier, sign-in enforcement silently fails if you add more than one organization.
Modify the file permissions to ensure the file cannot be edited by any non-administrator users.
Restart Docker Desktop.
When Docker Desktop restarts, verify that the Sign in required! prompt appears.
Note
If a
plist
andregistry.json
file both exist, theplist
file takes precedence.
Example deployment
The following example outlines how to create and distribute the plist
file to enforce sign-in on Docker Desktop. There are multiple ways to deploy this configuration depending on your organization's infrastructure, security policies, and management tools.
- Follow the steps previously outlined to create the
desktop.plist
file. - Use an MDM tool like Jamf or Fleet to distribute the
desktop.plist
file to/Library/Application Support/com.docker.docker/
on target macOS devices. - Through the MDM tool, set the file permissions to permit editing by administrators only.
- Create a Bash script that can check for the existence of the
.plist
file in the correct directory, create or modify it as needed, and set the appropriate permissions. Include commands in your script to:- Navigate to the
/Library/Application Support/com.docker.docker/
directory or create it if it doesn't exist. - Use the
defaults
command to write the required keys and values to thedesktop.plist
file. For example:$ defaults write /Library/Application\ Support/com.docker.docker/desktop.plist allowedOrgs -string ""myorg""
- Change permissions of the
plist
file to restrict editing, usingchmod
and possiblychown
to set the owner to root or another administrator account, ensuring it can't be easily modified by unauthorized users.
- Navigate to the
- Before deploying the script across the organization, test it on a local macOS machine to ensure it behaves as expected. Pay attention to directory paths, permissions, and the successful application of
plist
settings. - Ensure that you have the capability to execute scripts remotely on macOS devices. This might involve setting up SSH access or using a remote support tool that supports macOS.
- Use a method of remote script execution that fits your organization's infrastructure. Options include:
- SSH: If SSH is enabled on the target machines, you can use it to execute the script remotely. This method requires knowledge of the device's IP address and appropriate credentials.
- Remote support tool: For organizations using a remote support tool, you can add the script to a task and execute it across all selected machines.
- Ensure the script is running as expected on all targeted devices. You may have to check log files or implement logging within the script itself to report its success or failure.
registry.json method (All)
The following instructions explain how to create and deploy a registry.json
file to a single device. There are many ways to deploy the registry.json
file. You can follow the example deployments outlined in the .plist
file section. The method you choose is dependent on your organization's infrastructure, security policies, and the administrative rights of the end-users.
Option 1: Create a registry.json file to enforce sign-in
Ensure the user is a member of your organization in Docker. For more details, see Manage members.
Create the
registry.json
file.Based on the user's operating system, create a file named
registry.json
at the following location and make sure the file can't be edited by the user.Platform Location Windows /ProgramData/DockerDesktop/registry.json
Mac /Library/Application Support/com.docker.docker/registry.json
Linux /usr/share/docker-desktop/registry/registry.json
Specify your organization in the
registry.json
file.Open the
registry.json
file in a text editor and add the following contents, wheremyorg
is replaced with your organization’s name. The file contents are case-sensitive and you must use lowercase letters for your organization's name.{ ""allowedOrgs"": [""myorg1"", ""myorg2""] }
Important
As of Docker Desktop version 4.36 and later, you can add more than one organization. With Docker Desktop version 4.35 and earlier, if you add more than one organization sign-in enforcement silently fails.
Verify that sign-in is enforced.
To activate the
registry.json
file, restart Docker Desktop on the user’s machine. When Docker Desktop starts, verify that the Sign in required! prompt appears.In some cases, a system reboot may be necessary for the enforcement to take effect.
Tip
If your users have issues starting Docker Desktop after you enforce sign-in, they may need to update to the latest version.
Option 2: Create a registry.json file when installing Docker Desktop
To create a registry.json
file when installing Docker Desktop, use the following instructions based on your user's operating system.
To automatically create a registry.json
file when installing Docker Desktop,
download Docker Desktop Installer.exe
and run one of the following commands
from the directory containing Docker Desktop Installer.exe
. Replace myorg
with your organization's name. You must use lowercase letters for your
organization's name.
If you're using PowerShell:
PS> Start-Process '.\Docker Desktop Installer.exe' -Wait 'install --allowed-org=myorg'
If you're using the Windows Command Prompt:
C:\Users\Admin> ""Docker Desktop Installer.exe"" install --allowed-org=myorg
Important
As of Docker Desktop version 4.36 and later, you can add more than one organization to a single
registry.json
file. With Docker Desktop version 4.35 and earlier, if you add more than one organization sign-in enforcement silently fails.
To automatically create a registry.json
file when installing Docker Desktop,
download Docker.dmg
and run the following commands in a terminal from the
directory containing Docker.dmg
. Replace myorg
with your organization's name. You must use lowercase letters for your organization's name.
$ sudo hdiutil attach Docker.dmg
$ sudo /Volumes/Docker/Docker.app/Contents/MacOS/install --allowed-org=myorg
$ sudo hdiutil detach /Volumes/Docker
Option 3: Create a registry.json file using the command line
To create a registry.json
using the command line, use the following instructions based on your user's operating system.
To use the CLI to create a registry.json
file, run the following PowerShell
command as an administrator and replace myorg
with your organization's name. The file
contents are case-sensitive and you must use lowercase letters for your
organization's name.
PS> Set-Content /ProgramData/DockerDesktop/registry.json '{""allowedOrgs"":[""myorg""]}'
This creates the registry.json
file at
C:\ProgramData\DockerDesktop\registry.json
and includes the organization
information the user belongs to. Make sure that the user can't edit this file, but only the administrator can:
PS C:\ProgramData\DockerDesktop> Get-Acl .\registry.json
Directory: C:\ProgramData\DockerDesktop
Path Owner Access
---- ----- ------
registry.json BUILTIN\Administrators NT AUTHORITY\SYSTEM Allow FullControl...
Important
As of Docker Desktop version 4.36 and later, you can add more than one organization to a single
registry.json
file. With Docker Desktop version 4.35 and earlier, if you add more than one organization sign-in enforcement silently fails.
To use the CLI to create a registry.json
file, run the following commands in a
terminal and replace myorg
with your organization's name. The file contents
are case-sensitive and you must use lowercase letters for your organization's
name.
$ sudo mkdir -p ""/Library/Application Support/com.docker.docker""
$ echo '{""allowedOrgs"":[""myorg""]}' | sudo tee ""/Library/Application Support/com.docker.docker/registry.json""
This creates (or updates, if the file already exists) the registry.json
file
at /Library/Application Support/com.docker.docker/registry.json
and includes
the organization information the user belongs to. Make sure that the file has the
expected content, and that the user can't edit this file, but only the administrator can.
Verify that the content of the file contains the correct information:
$ sudo cat ""/Library/Application Support/com.docker.docker/registry.json""
{""allowedOrgs"":[""myorg""]}
Verify that the file has the expected permissions (-rw-r--r--
) and ownership
(root
and admin
):
$ sudo ls -l ""/Library/Application Support/com.docker.docker/registry.json""
-rw-r--r-- 1 root admin 26 Jul 27 22:01 /Library/Application Support/com.docker.docker/registry.json
Important
As of Docker Desktop version 4.36 and later, you can add more than one organization to a single
registry.json
file. With Docker Desktop version 4.35 and earlier, if you add more than one organization sign-in enforcement silently fails.
To use the CLI to create a registry.json
file, run the following commands in a
terminal and replace myorg
with your organization's name. The file contents
are case-sensitive and you must use lowercase letters for your organization's
name.
$ sudo mkdir -p /usr/share/docker-desktop/registry
$ echo '{""allowedOrgs"":[""myorg""]}' | sudo tee /usr/share/docker-desktop/registry/registry.json
This creates (or updates, if the file already exists) the registry.json
file
at /usr/share/docker-desktop/registry/registry.json
and includes the
organization information to which the user belongs. Make sure the file has the
expected content and that the user can't edit this file, only the root can.
Verify that the content of the file contains the correct information:
$ sudo cat /usr/share/docker-desktop/registry/registry.json
{""allowedOrgs"":[""myorg""]}
Verify that the file has the expected permissions (-rw-r--r--
) and ownership
(root
):
$ sudo ls -l /usr/share/docker-desktop/registry/registry.json
-rw-r--r-- 1 root root 26 Jul 27 22:01 /usr/share/docker-desktop/registry/registry.json
Important
As of Docker Desktop version 4.36 and later, you can add more than one organization to a single
registry.json
file. With Docker Desktop version 4.35 and earlier, if you add more than one organization sign-in enforcement silently fails.",,,
8c144677b37cdde7d0a02f0fa7a1ec945ce525b1d749535e8648df5dfaa5a1b1,"Manage company users
You can manage users at the company-level in the Docker Admin Console.
Owners can invite new members to an organization via Docker ID, email address, or with a CSV file containing email addresses. If an invitee does not have a Docker account, they must create an account and verify their email address before they can accept an invitation to join the organization. When inviting members, their pending invitation occupies a seat.
Invite members via Docker ID or email address
Use the following steps to invite members to your organization via Docker ID or email address. To invite a large amount of members to your organization via CSV file, see the next section.
Sign in to the Admin Console.
Select your organization from the Choose profile page, and then select Members.
Select Invite.
Select Emails or usernames.
Follow the on-screen instructions to invite members. Invite a maximum of 1000 members and separate multiple entries by comma, semicolon, or space.
Note
When you invite members, you assign them a role. See Roles and permissions for details about the access permissions for each role.
Pending invitations appear in the table. The invitees receive an email with a link to Docker Hub where they can accept or decline the invitation.
Invite members via CSV file
To invite multiple members to an organization via a CSV file containing email addresses:
Sign in to the Admin Console.
Select your organization from the Choose profile page, and then select Members.
Select Invite.
Select CSV upload.
Select Download the template CSV file to optionally download an example CSV file. The following is an example of the contents of a valid CSV file.
email docker.user-0@example.com docker.user-1@example.com
CSV file requirements:
- The file must contain a header row with at least one heading named
email
. Additional columns are allowed and are ignored in the import. - The file must contain a maximum of 1000 email addresses (rows). To invite more than 1000 users, create multiple CSV files and perform all steps in this task for each file.
- The file must contain a header row with at least one heading named
Create a new CSV file or export a CSV file from another application.
- To export a CSV file from another application, see the application’s documentation.
- To create a new CSV file, open a new file in a text editor, type
email
on the first line, type the user email addresses one per line on the following lines, and then save the file with a .csv extension.
Select Browse files and then select your CSV file, or drag and drop the CSV file into the Select a CSV file to upload box. You can only select one CSV file at a time.
Note
If the amount of email addresses in your CSV file exceeds the number of available seats in your organization, you cannot continue to invite members. To invite members, you can purchase more seats, or remove some email addresses from the CSV file and re-select the new file. To purchase more seats, see Add seats to your subscription or Contact sales.
After the CSV file has been uploaded, select Review.
Valid email addresses and any email addresses that have issues appear. Email addresses may have the following issues:
- Invalid email: The email address is not a valid address. The email address will be ignored if you send invites. You can correct the email address in the CSV file and re-import the file.
- Already invited: The user has already been sent an invite email and another invite email will not be sent.
- Member: The user is already a member of your organization and an invite email will not be sent.
- Duplicate: The CSV file has multiple occurrences of the same email address. The user will be sent only one invite email.
Follow the on-screen instructions to invite members.
Note
When you invite members, you assign them a role. See Roles and permissions for details about the access permissions for each role.
Pending invitations appear in the table. The invitees receive an email with a link to Docker Hub where they can accept or decline the invitation.
Manage members on a team
Use Docker Hub to add a member to a team or remove a member from a team. For more details, see Manage members in Docker Hub.",,,
42e32416b6fb6bd41666bd85c69d01eb3923568341c01f199736349fb76f2726,"Validating build configuration with GitHub Actions
Table of contents
Build checks let you validate your docker build
configuration without actually running the build.
Run checks with docker/build-push-action
To run build checks in a GitHub Actions workflow with the build-push-action
,
set the call
input parameter to check
. With this set, the workflow fails if
any check warnings are detected for your build's configuration.
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ secrets.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Validate build configuration
uses: docker/build-push-action@v6
with:
call: check
- name: Build and push
uses: docker/build-push-action@v6
with:
push: true
tags: user/app:latest
Run checks with docker/bake-action
If you're using Bake and docker/bake-action
to run your builds, you don't
need to specify any special inputs in your GitHub Actions workflow
configuration. Instead, define a Bake target that calls the check
method,
and invoke that target in your CI.
target ""build"" {
dockerfile = ""Dockerfile""
args = {
FOO = ""bar""
}
}
target ""validate-build"" {
inherits = [""build""]
call = ""check""
}
name: ci
on:
push:
env:
IMAGE_NAME: user/app
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Validate build configuration
uses: docker/bake-action@v6
with:
targets: validate-build
- name: Build
uses: docker/bake-action@v6
with:
targets: build
push: true",,,
5c7b3c76f6b107f249a64cecaf9354e05a9f735f6617f9f7a4171e2a5c05274c,"Device Mapper storage driver (deprecated)
Deprecated
The Device Mapper driver has been deprecated, and is removed in Docker Engine v25.0. If you are using Device Mapper, you must migrate to a supported storage driver before upgrading to Docker Engine v25.0. Read the Docker storage drivers page for supported storage drivers.
Device Mapper is a kernel-based framework that underpins many advanced
volume management technologies on Linux. Docker's devicemapper
storage driver
leverages the thin provisioning and snapshotting capabilities of this framework
for image and container management. This article refers to the Device Mapper
storage driver as devicemapper
, and the kernel framework as Device Mapper.
For the systems where it is supported, devicemapper
support is included in
the Linux kernel. However, specific configuration is required to use it with
Docker.
The devicemapper
driver uses block devices dedicated to Docker and operates at
the block level, rather than the file level. These devices can be extended by
adding physical storage to your Docker host, and they perform better than using
a filesystem at the operating system (OS) level.
Prerequisites
devicemapper
is supported on Docker Engine - Community running on CentOS, Fedora, SLES 15, Ubuntu, Debian, or RHEL.devicemapper
requires thelvm2
anddevice-mapper-persistent-data
packages to be installed.- Changing the storage driver makes any containers you have already
created inaccessible on the local system. Use
docker save
to save containers, and push existing images to Docker Hub or a private repository, so you do not need to recreate them later.
Configure Docker with the devicemapper
storage driver
Before following these procedures, you must first meet all the prerequisites.
Configure loop-lvm
mode for testing
This configuration is only appropriate for testing. The loop-lvm
mode makes
use of a 'loopback' mechanism that allows files on the local disk to be
read from and written to as if they were an actual physical disk or block
device.
However, the addition of the loopback mechanism, and interaction with the OS
filesystem layer, means that IO operations can be slow and resource-intensive.
Use of loopback devices can also introduce race conditions.
However, setting up loop-lvm
mode can help identify basic issues (such as
missing user space packages, kernel drivers, etc.) ahead of attempting the more
complex set up required to enable direct-lvm
mode. loop-lvm
mode should
therefore only be used to perform rudimentary testing prior to configuring
direct-lvm
.
For production systems, see Configure direct-lvm mode for production.
Stop Docker.
$ sudo systemctl stop docker
Edit
/etc/docker/daemon.json
. If it does not yet exist, create it. Assuming that the file was empty, add the following contents.{ ""storage-driver"": ""devicemapper"" }
See all storage options for each storage driver in the daemon reference documentation
Docker does not start if the
daemon.json
file contains badly-formed JSON.Start Docker.
$ sudo systemctl start docker
Verify that the daemon is using the
devicemapper
storage driver. Use thedocker info
command and look forStorage Driver
.$ docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 17.03.1-ce Storage Driver: devicemapper Pool Name: docker-202:1-8413957-pool Pool Blocksize: 65.54 kB Base Device Size: 10.74 GB Backing Filesystem: xfs Data file: /dev/loop0 Metadata file: /dev/loop1 Data Space Used: 11.8 MB Data Space Total: 107.4 GB Data Space Available: 7.44 GB Metadata Space Used: 581.6 KB Metadata Space Total: 2.147 GB Metadata Space Available: 2.147 GB Thin Pool Minimum Free Space: 10.74 GB Udev Sync Supported: true Deferred Removal Enabled: false Deferred Deletion Enabled: false Deferred Deleted Device Count: 0 Data loop file: /var/lib/docker/devicemapper/data Metadata loop file: /var/lib/docker/devicemapper/metadata Library Version: 1.02.135-RHEL7 (2016-11-16) <...>
This host is running in loop-lvm
mode, which is not supported on
production systems. This is indicated by the fact that the Data loop file
and a Metadata loop file
are on files under
/var/lib/docker/devicemapper
. These are loopback-mounted
sparse files. For production systems, see
Configure direct-lvm mode for production.
Configure direct-lvm mode for production
Production hosts using the devicemapper
storage driver must use direct-lvm
mode. This mode uses block devices to create the thin pool. This is faster than
using loopback devices, uses system resources more efficiently, and block
devices can grow as needed. However, more setup is required than in loop-lvm
mode.
After you have satisfied the
prerequisites, follow the steps
below to configure Docker to use the devicemapper
storage driver in
direct-lvm
mode.
Warning
Changing the storage driver makes any containers you have already created inaccessible on the local system. Use
docker save
to save containers, and push existing images to Docker Hub or a private repository, so you do not need to recreate them later.
Allow Docker to configure direct-lvm mode
Docker can manage the block device for you, simplifying configuration of direct-lvm
mode. This is appropriate for fresh Docker setups only. You can only use a
single block device. If you need to use multiple block devices,
configure direct-lvm mode manually instead.
The following new configuration options are available:
| Option | Description | Required? | Default | Example |
|---|---|---|---|---|
dm.directlvm_device | The path to the block device to configure for direct-lvm . | Yes | dm.directlvm_device=""/dev/xvdf"" | |
dm.thinp_percent | The percentage of space to use for storage from the passed in block device. | No | 95 | dm.thinp_percent=95 |
dm.thinp_metapercent | The percentage of space to use for metadata storage from the passed-in block device. | No | 1 | dm.thinp_metapercent=1 |
dm.thinp_autoextend_threshold | The threshold for when lvm should automatically extend the thin pool as a percentage of the total storage space. | No | 80 | dm.thinp_autoextend_threshold=80 |
dm.thinp_autoextend_percent | The percentage to increase the thin pool by when an autoextend is triggered. | No | 20 | dm.thinp_autoextend_percent=20 |
dm.directlvm_device_force | Whether to format the block device even if a filesystem already exists on it. If set to false and a filesystem is present, an error is logged and the filesystem is left intact. | No | false | dm.directlvm_device_force=true |
Edit the daemon.json
file and set the appropriate options, then restart Docker
for the changes to take effect. The following daemon.json
configuration sets all of the
options in the table above.
{
""storage-driver"": ""devicemapper"",
""storage-opts"": [
""dm.directlvm_device=/dev/xdf"",
""dm.thinp_percent=95"",
""dm.thinp_metapercent=1"",
""dm.thinp_autoextend_threshold=80"",
""dm.thinp_autoextend_percent=20"",
""dm.directlvm_device_force=false""
]
}
See all storage options for each storage driver in the daemon reference documentation
Restart Docker for the changes to take effect. Docker invokes the commands to configure the block device for you.
Warning
Changing these values after Docker has prepared the block device for you is not supported and causes an error.
You still need to perform periodic maintenance tasks.
Configure direct-lvm mode manually
The procedure below creates a logical volume configured as a thin pool to
use as backing for the storage pool. It assumes that you have a spare block
device at /dev/xvdf
with enough free space to complete the task. The device
identifier and volume sizes may be different in your environment and you
should substitute your own values throughout the procedure. The procedure also
assumes that the Docker daemon is in the stopped
state.
Identify the block device you want to use. The device is located under
/dev/
(such as/dev/xvdf
) and needs enough free space to store the images and container layers for the workloads that host runs. A solid state drive is ideal.Stop Docker.
$ sudo systemctl stop docker
Install the following packages:
RHEL / CentOS:
device-mapper-persistent-data
,lvm2
, and all dependenciesUbuntu / Debian / SLES 15:
thin-provisioning-tools
,lvm2
, and all dependencies
Create a physical volume on your block device from step 1, using the
pvcreate
command. Substitute your device name for/dev/xvdf
.Warning
The next few steps are destructive, so be sure that you have specified the correct device.
$ sudo pvcreate /dev/xvdf Physical volume ""/dev/xvdf"" successfully created.
Create a
docker
volume group on the same device, using thevgcreate
command.$ sudo vgcreate docker /dev/xvdf Volume group ""docker"" successfully created
Create two logical volumes named
thinpool
andthinpoolmeta
using thelvcreate
command. The last parameter specifies the amount of free space to allow for automatic expanding of the data or metadata if space runs low, as a temporary stop-gap. These are the recommended values.$ sudo lvcreate --wipesignatures y -n thinpool docker -l 95%VG Logical volume ""thinpool"" created. $ sudo lvcreate --wipesignatures y -n thinpoolmeta docker -l 1%VG Logical volume ""thinpoolmeta"" created.
Convert the volumes to a thin pool and a storage location for metadata for the thin pool, using the
lvconvert
command.$ sudo lvconvert -y \ --zero n \ -c 512K \ --thinpool docker/thinpool \ --poolmetadata docker/thinpoolmeta WARNING: Converting logical volume docker/thinpool and docker/thinpoolmeta to thin pool's data and metadata volumes with metadata wiping. THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.) Converted docker/thinpool to thin pool.
Configure autoextension of thin pools via an
lvm
profile.$ sudo vi /etc/lvm/profile/docker-thinpool.profile
Specify
thin_pool_autoextend_threshold
andthin_pool_autoextend_percent
values.thin_pool_autoextend_threshold
is the percentage of space used beforelvm
attempts to autoextend the available space (100 = disabled, not recommended).thin_pool_autoextend_percent
is the amount of space to add to the device when automatically extending (0 = disabled).The example below adds 20% more capacity when the disk usage reaches 80%.
activation { thin_pool_autoextend_threshold=80 thin_pool_autoextend_percent=20 }
Save the file.
Apply the LVM profile, using the
lvchange
command.$ sudo lvchange --metadataprofile docker-thinpool docker/thinpool Logical volume docker/thinpool changed.
Ensure monitoring of the logical volume is enabled.
$ sudo lvs -o+seg_monitor LV VG Attr LSize Pool Origin Data% Meta% Move Log Cpy%Sync Convert Monitor thinpool docker twi-a-t--- 95.00g 0.00 0.01 not monitored
If the output in the
Monitor
column reports, as above, that the volume isnot monitored
, then monitoring needs to be explicitly enabled. Without this step, automatic extension of the logical volume will not occur, regardless of any settings in the applied profile.$ sudo lvchange --monitor y docker/thinpool
Double check that monitoring is now enabled by running the
sudo lvs -o+seg_monitor
command a second time. TheMonitor
column should now report the logical volume is beingmonitored
.If you have ever run Docker on this host before, or if
/var/lib/docker/
exists, move it out of the way so that Docker can use the new LVM pool to store the contents of image and containers.$ sudo su - # mkdir /var/lib/docker.bk # mv /var/lib/docker/* /var/lib/docker.bk # exit
If any of the following steps fail and you need to restore, you can remove
/var/lib/docker
and replace it with/var/lib/docker.bk
.Edit
/etc/docker/daemon.json
and configure the options needed for thedevicemapper
storage driver. If the file was previously empty, it should now contain the following contents:{ ""storage-driver"": ""devicemapper"", ""storage-opts"": [ ""dm.thinpooldev=/dev/mapper/docker-thinpool"", ""dm.use_deferred_removal=true"", ""dm.use_deferred_deletion=true"" ] }
Start Docker.
systemd:
$ sudo systemctl start docker
service:
$ sudo service docker start
Verify that Docker is using the new configuration using
docker info
.$ docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 17.03.1-ce Storage Driver: devicemapper Pool Name: docker-thinpool Pool Blocksize: 524.3 kB Base Device Size: 10.74 GB Backing Filesystem: xfs Data file: Metadata file: Data Space Used: 19.92 MB Data Space Total: 102 GB Data Space Available: 102 GB Metadata Space Used: 147.5 kB Metadata Space Total: 1.07 GB Metadata Space Available: 1.069 GB Thin Pool Minimum Free Space: 10.2 GB Udev Sync Supported: true Deferred Removal Enabled: true Deferred Deletion Enabled: true Deferred Deleted Device Count: 0 Library Version: 1.02.135-RHEL7 (2016-11-16) <...>
If Docker is configured correctly, the
Data file
andMetadata file
is blank, and the pool name isdocker-thinpool
.After you have verified that the configuration is correct, you can remove the
/var/lib/docker.bk
directory which contains the previous configuration.$ sudo rm -rf /var/lib/docker.bk
Manage devicemapper
Monitor the thin pool
Do not rely on LVM auto-extension alone. The volume group
automatically extends, but the volume can still fill up. You can monitor
free space on the volume using lvs
or lvs -a
. Consider using a monitoring
tool at the OS level, such as Nagios.
To view the LVM logs, you can use journalctl
:
$ sudo journalctl -fu dm-event.service
If you run into repeated problems with thin pool, you can set the storage option
dm.min_free_space
to a value (representing a percentage) in
/etc/docker/daemon.json
. For instance, setting it to 10
ensures
that operations fail with a warning when the free space is at or near 10%.
See the
storage driver options in the Engine daemon reference.
Increase capacity on a running device
You can increase the capacity of the pool on a running thin-pool device. This is useful if the data's logical volume is full and the volume group is at full capacity. The specific procedure depends on whether you are using a loop-lvm thin pool or a direct-lvm thin pool.
Resize a loop-lvm thin pool
The easiest way to resize a loop-lvm
thin pool is to
use the device_tool utility,
but you can
use operating system utilities
instead.
Use the device_tool utility
A community-contributed script called device_tool.go
is available in the
moby/moby
Github repository. You can use this tool to resize a loop-lvm
thin pool,
avoiding the long process above. This tool is not guaranteed to work, but you
should only be using loop-lvm
on non-production systems.
If you do not want to use device_tool
, you can
resize the thin pool manually instead.
To use the tool, clone the Github repository, change to the
contrib/docker-device-tool
, and follow the instructions in theREADME.md
to compile the tool.Use the tool. The following example resizes the thin pool to 200GB.
$ ./device_tool resize 200GB
Use operating system utilities
If you do not want to
use the device-tool utility,
you can resize a loop-lvm
thin pool manually using the following procedure.
In loop-lvm
mode, a loopback device is used to store the data, and another
to store the metadata. loop-lvm
mode is only supported for testing, because
it has significant performance and stability drawbacks.
If you are using loop-lvm
mode, the output of docker info
shows file
paths for Data loop file
and Metadata loop file
:
$ docker info |grep 'loop file'
Data loop file: /var/lib/docker/devicemapper/data
Metadata loop file: /var/lib/docker/devicemapper/metadata
Follow these steps to increase the size of the thin pool. In this example, the thin pool is 100 GB, and is increased to 200 GB.
List the sizes of the devices.
$ sudo ls -lh /var/lib/docker/devicemapper/ total 1175492 -rw------- 1 root root 100G Mar 30 05:22 data -rw------- 1 root root 2.0G Mar 31 11:17 metadata
Increase the size of the
data
file to 200 G using thetruncate
command, which is used to increase or decrease the size of a file. Note that decreasing the size is a destructive operation.$ sudo truncate -s 200G /var/lib/docker/devicemapper/data
Verify the file size changed.
$ sudo ls -lh /var/lib/docker/devicemapper/ total 1.2G -rw------- 1 root root 200G Apr 14 08:47 data -rw------- 1 root root 2.0G Apr 19 13:27 metadata
The loopback file has changed on disk but not in memory. List the size of the loopback device in memory, in GB. Reload it, then list the size again. After the reload, the size is 200 GB.
$ echo $[ $(sudo blockdev --getsize64 /dev/loop0) / 1024 / 1024 / 1024 ] 100 $ sudo losetup -c /dev/loop0 $ echo $[ $(sudo blockdev --getsize64 /dev/loop0) / 1024 / 1024 / 1024 ] 200
Reload the devicemapper thin pool.
a. Get the pool name first. The pool name is the first field, delimited by
:
. This command extracts it.$ sudo dmsetup status | grep ' thin-pool ' | awk -F ': ' {'print $1'} docker-8:1-123141-pool
b. Dump the device mapper table for the thin pool.
$ sudo dmsetup table docker-8:1-123141-pool 0 209715200 thin-pool 7:1 7:0 128 32768 1 skip_block_zeroing
c. Calculate the total sectors of the thin pool using the second field of the output. The number is expressed in 512-k sectors. A 100G file has 209715200 512-k sectors. If you double this number to 200G, you get 419430400 512-k sectors.
d. Reload the thin pool with the new sector number, using the following three
dmsetup
commands.$ sudo dmsetup suspend docker-8:1-123141-pool $ sudo dmsetup reload docker-8:1-123141-pool --table '0 419430400 thin-pool 7:1 7:0 128 32768 1 skip_block_zeroing' $ sudo dmsetup resume docker-8:1-123141-pool
Resize a direct-lvm thin pool
To extend a direct-lvm
thin pool, you need to first attach a new block device
to the Docker host, and make note of the name assigned to it by the kernel. In
this example, the new block device is /dev/xvdg
.
Follow this procedure to extend a direct-lvm
thin pool, substituting your
block device and other parameters to suit your situation.
Gather information about your volume group.
Use the
pvdisplay
command to find the physical block devices currently in use by your thin pool, and the volume group's name.$ sudo pvdisplay |grep 'VG Name' PV Name /dev/xvdf VG Name docker
In the following steps, substitute your block device or volume group name as appropriate.
Extend the volume group, using the
vgextend
command with theVG Name
from the previous step, and the name of your new block device.$ sudo vgextend docker /dev/xvdg Physical volume ""/dev/xvdg"" successfully created. Volume group ""docker"" successfully extended
Extend the
docker/thinpool
logical volume. This command uses 100% of the volume right away, without auto-extend. To extend the metadata thinpool instead, usedocker/thinpool_tmeta
.$ sudo lvextend -l+100%FREE -n docker/thinpool Size of logical volume docker/thinpool_tdata changed from 95.00 GiB (24319 extents) to 198.00 GiB (50688 extents). Logical volume docker/thinpool_tdata successfully resized.
Verify the new thin pool size using the
Data Space Available
field in the output ofdocker info
. If you extended thedocker/thinpool_tmeta
logical volume instead, look forMetadata Space Available
.Storage Driver: devicemapper Pool Name: docker-thinpool Pool Blocksize: 524.3 kB Base Device Size: 10.74 GB Backing Filesystem: xfs Data file: Metadata file: Data Space Used: 212.3 MB Data Space Total: 212.6 GB Data Space Available: 212.4 GB Metadata Space Used: 286.7 kB Metadata Space Total: 1.07 GB Metadata Space Available: 1.069 GB <...>
Activate the devicemapper
after reboot
If you reboot the host and find that the docker
service failed to start,
look for the error, ""Non existing device"". You need to re-activate the
logical volumes with this command:
$ sudo lvchange -ay docker/thinpool
How the devicemapper
storage driver works
Warning
Do not directly manipulate any files or directories within
/var/lib/docker/
. These files and directories are managed by Docker.
Use the lsblk
command to see the devices and their pools, from the operating
system's point of view:
$ sudo lsblk
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda 202:0 0 8G 0 disk
└─xvda1 202:1 0 8G 0 part /
xvdf 202:80 0 100G 0 disk
├─docker-thinpool_tmeta 253:0 0 1020M 0 lvm
│ └─docker-thinpool 253:2 0 95G 0 lvm
└─docker-thinpool_tdata 253:1 0 95G 0 lvm
└─docker-thinpool 253:2 0 95G 0 lvm
Use the mount
command to see the mount-point Docker is using:
$ mount |grep devicemapper
/dev/xvda1 on /var/lib/docker/devicemapper type xfs (rw,relatime,seclabel,attr2,inode64,noquota)
When you use devicemapper
, Docker stores image and layer contents in the
thinpool, and exposes them to containers by mounting them under
subdirectories of /var/lib/docker/devicemapper/
.
Image and container layers on-disk
The /var/lib/docker/devicemapper/metadata/
directory contains metadata about
the Devicemapper configuration itself and about each image and container layer
that exist. The devicemapper
storage driver uses snapshots, and this metadata
include information about those snapshots. These files are in JSON format.
The /var/lib/docker/devicemapper/mnt/
directory contains a mount point for each image
and container layer that exists. Image layer mount points are empty, but a
container's mount point shows the container's filesystem as it appears from
within the container.
Image layering and sharing
The devicemapper
storage driver uses dedicated block devices rather than
formatted filesystems, and operates on files at the block level for maximum
performance during copy-on-write (CoW) operations.
Snapshots
Another feature of devicemapper
is its use of snapshots (also sometimes called
thin devices or virtual devices), which store the differences introduced in
each layer as very small, lightweight thin pools. Snapshots provide many
benefits:
Layers which are shared in common between containers are only stored on disk once, unless they are writable. For instance, if you have 10 different images which are all based on
alpine
, thealpine
image and all its parent images are only stored once each on disk.Snapshots are an implementation of a copy-on-write (CoW) strategy. This means that a given file or directory is only copied to the container's writable layer when it is modified or deleted by that container.
Because
devicemapper
operates at the block level, multiple blocks in a writable layer can be modified simultaneously.Snapshots can be backed up using standard OS-level backup utilities. Just make a copy of
/var/lib/docker/devicemapper/
.
Devicemapper workflow
When you start Docker with the devicemapper
storage driver, all objects
related to image and container layers are stored in
/var/lib/docker/devicemapper/
, which is backed by one or more block-level
devices, either loopback devices (testing only) or physical disks.
The base device is the lowest-level object. This is the thin pool itself. You can examine it using
docker info
. It contains a filesystem. This base device is the starting point for every image and container layer. The base device is a Device Mapper implementation detail, rather than a Docker layer.Metadata about the base device and each image or container layer is stored in
/var/lib/docker/devicemapper/metadata/
in JSON format. These layers are copy-on-write snapshots, which means that they are empty until they diverge from their parent layers.Each container's writable layer is mounted on a mountpoint in
/var/lib/docker/devicemapper/mnt/
. An empty directory exists for each read-only image layer and each stopped container.
Each image layer is a snapshot of the layer below it. The lowest layer of each
image is a snapshot of the base device that exists in the pool. When you run a
container, it is a snapshot of the image the container is based on. The following
example shows a Docker host with two running containers. The first is a ubuntu
container and the second is a busybox
container.
How container reads and writes work with devicemapper
Reading files
With devicemapper
, reads happen at the block level. The diagram below shows
the high level process for reading a single block (0x44f
) in an example
container.
An application makes a read request for block 0x44f
in the container. Because
the container is a thin snapshot of an image, it doesn't have the block, but it
has a pointer to the block on the nearest parent image where it does exist, and
it reads the block from there. The block now exists in the container's memory.
Writing files
Writing a new file: With the devicemapper
driver, writing new data to a
container is accomplished by an allocate-on-demand operation. Each block of
the new file is allocated in the container's writable layer and the block is
written there.
Updating an existing file: The relevant block of the file is read from the nearest layer where it exists. When the container writes the file, only the modified blocks are written to the container's writable layer.
Deleting a file or directory: When you delete a file or directory in a
container's writable layer, or when an image layer deletes a file that exists
in its parent layer, the devicemapper
storage driver intercepts further read
attempts on that file or directory and responds that the file or directory does
not exist.
Writing and then deleting a file: If a container writes to a file and later
deletes the file, all of those operations happen in the container's writable
layer. In that case, if you are using direct-lvm
, the blocks are freed. If you
use loop-lvm
, the blocks may not be freed. This is another reason not to use
loop-lvm
in production.
Device Mapper and Docker performance
allocate-on demand
performance impact:The
devicemapper
storage driver uses anallocate-on-demand
operation to allocate new blocks from the thin pool into a container's writable layer. Each block is 64KB, so this is the minimum amount of space that is used for a write.Copy-on-write performance impact: The first time a container modifies a specific block, that block is written to the container's writable layer. Because these writes happen at the level of the block rather than the file, performance impact is minimized. However, writing a large number of blocks can still negatively impact performance, and the
devicemapper
storage driver may actually perform worse than other storage drivers in this scenario. For write-heavy workloads, you should use data volumes, which bypass the storage driver completely.
Performance best practices
Keep these things in mind to maximize performance when using the devicemapper
storage driver.
Use
direct-lvm
: Theloop-lvm
mode is not performant and should never be used in production.Use fast storage: Solid-state drives (SSDs) provide faster reads and writes than spinning disks.
Memory usage: the
devicemapper
uses more memory than some other storage drivers. Each launched container loads one or more copies of its files into memory, depending on how many blocks of the same file are being modified at the same time. Due to the memory pressure, thedevicemapper
storage driver may not be the right choice for certain workloads in high-density use cases.Use volumes for write-heavy workloads: Volumes provide the best and most predictable performance for write-heavy workloads. This is because they bypass the storage driver and do not incur any of the potential overheads introduced by thin provisioning and copy-on-write. Volumes have other benefits, such as allowing you to share data among containers and persisting even when no running container is using them.
Note
When using
devicemapper
and thejson-file
log driver, the log files generated by a container are still stored in Docker's dataroot directory, by default/var/lib/docker
. If your containers generate lots of log messages, this may lead to increased disk usage or the inability to manage your system due to a full disk. You can configure a log driver to store your container logs externally.",,,
73c7a2ca81c50baf1e5efbf3af0cf65870373ca822b10fd0fbdc058465d2850f,"Docker Engine API
Docker provides an API for interacting with the Docker daemon (called the Docker Engine API), as well as SDKs for Go and Python. The SDKs allow you to efficiently build and scale Docker apps and solutions. If Go or Python don't work for you, you can use the Docker Engine API directly.
For information about Docker Engine SDKs, see Develop with Docker Engine SDKs.
The Docker Engine API is a RESTful API accessed by an HTTP client such as wget
or
curl
, or the HTTP library which is part of most modern programming languages.
View the API reference
You can view the reference for the latest version of the API or choose a specific version.
Versioned API and SDK
The version of the Docker Engine API you should use depends upon the version of your Docker daemon and Docker client.
A given version of the Docker Engine SDK supports a specific version of the Docker Engine API, as well as all earlier versions. If breaking changes occur, they are documented prominently.
Note
The Docker daemon and client don't necessarily need to be the same version at all times. However, keep the following in mind.
If the daemon is newer than the client, the client doesn't know about new features or deprecated API endpoints in the daemon.
If the client is newer than the daemon, the client can request API endpoints that the daemon doesn't know about.
A new version of the API is released when new features are added. The Docker API is backward-compatible, so you don't need to update code that uses the API unless you need to take advantage of new features.
To see the highest version of the API your Docker daemon and client support, use
docker version
:
$ docker version
Client: Docker Engine - Community
Version: 28.0.0
API version: 1.48
Go version: go1.23.6
Git commit: f9ced58
Built: Wed Feb 19 22:11:04 2025
OS/Arch: linux/amd64
Context: default
Server: Docker Engine - Community
Engine:
Version: 28.0.0
API version: 1.48 (minimum version 1.24)
Go version: go1.23.6
Git commit: af898ab
Built: Wed Feb 19 22:11:04 2025
OS/Arch: linux/amd64
...
You can specify the API version to use in any of the following ways:
When using the SDK, use the latest version. At a minimum, use the version that incorporates the API version with the features you need.
When using
curl
directly, specify the version as the first part of the URL. For instance, if the endpoint is/containers/
you can use/v1.48/containers/
.To force the Docker CLI or the Docker Engine SDKs to use an older version of the API than the version reported by
docker version
, set the environment variableDOCKER_API_VERSION
to the correct version. This works on Linux, Windows, or macOS clients.$ DOCKER_API_VERSION=1.47
While the environment variable is set, that version of the API is used, even if the Docker daemon supports a newer version. This environment variable disables API version negotiation, so you should only use it if you must use a specific version of the API, or for debugging purposes.
The Docker Go SDK allows you to enable API version negotiation, automatically selects an API version that's supported by both the client and the Docker Engine that's in use.
For the SDKs, you can also specify the API version programmatically as a parameter to the
client
object. See the Go constructor or the Python SDK documentation forclient
.
API version matrix
| Docker version | Maximum API version | Change log |
|---|---|---|
| 28.0 | 1.48 | changes |
| 27.5 | 1.47 | changes |
| 27.4 | 1.47 | changes |
| 27.3 | 1.47 | changes |
| 27.2 | 1.47 | changes |
| 27.1 | 1.46 | changes |
| 27.0 | 1.46 | changes |
| 26.1 | 1.45 | changes |
| 26.0 | 1.45 | changes |
| 25.0 | 1.44 | changes |
| 24.0 | 1.43 | changes |
| 23.0 | 1.42 | changes |
| 20.10 | 1.41 | changes |
| 19.03 | 1.40 | changes |
| 18.09 | 1.39 | changes |
| 18.06 | 1.38 | changes |
| 18.05 | 1.37 | changes |
| 18.04 | 1.37 | changes |
| 18.03 | 1.37 | changes |
| 18.02 | 1.36 | changes |
| 17.12 | 1.35 | changes |
| 17.11 | 1.34 | changes |
| 17.10 | 1.33 | changes |
| 17.09 | 1.32 | changes |
| 17.07 | 1.31 | changes |
| 17.06 | 1.30 | changes |
| 17.05 | 1.29 | changes |
| 17.04 | 1.28 | changes |
| 17.03.1 | 1.27 | changes |
| 17.03 | 1.26 | changes |
| 1.13.1 | 1.26 | changes |
| 1.13 | 1.25 | changes |
| 1.12 | 1.24 | changes |
Deprecated API versions
API versions before v1.24 are deprecated. You can find archived documentation for deprecated versions of the API in the code repository on GitHub:",,,
4fd5807d8bbea444d5680e6b487c2082ce68cd598688fcfea1406ef6d103b1de,"Configure policies
Some policy types are configurable. This means that you can create new, customized version of that policy type with your own configuration parameters. You can also disable a policy if you need to temporarily disregard it, or delete a policy altogether if it doesn't match your needs.
Note
Historic evaluation results for the default policy configuration are removed if you delete or customize a policy.
Add a policy
To add a new policy, select the policy type that you want to customize. All custom policies use a policy type as a base.
You can edit the display name and description of the new policy to help better communicate the compliant and non-compliant states of the policy. You can not change the name of the policy type, only its display names.
The available configuration parameters for a policy depends on the policy type that you're editing. For more information, refer to Policy types.
To add a policy:
Go to the Policies page in the Docker Scout Dashboard.
Select the Add policy button to open the policy configuration screen.
On the policy configuration screen, locate the policy type that you want to configure, and select Configure to open the policy configuration page.
- If the Configure button is grayed out, it means the current policy has no configurable parameters.
- If the button reads Integrate, it indicates that setup is required before the policy can be enabled. Selecting Integrate will direct you to the integration's setup guide.
Update the policy parameters.
Save the changes:
- Select Save policy to commit the changes and enable the policy for your current organization.
- Select Save and disable to save the policy configuration without enabling it.
Edit a policy
Editing a policy lets you to modify its configuration without creating a new one from scratch. This can be useful when policy parameters need adjustments due to evolving requirements or changes in your organization's compliance goals.
To edit a policy:
- Go to the Policies page in the Docker Scout Dashboard.
- Select the policy you want to edit.
- Select the Edit button.
- Update the policy parameters.
- Save the changes.
Disable a policy
When you disable a policy, evaluation results for that policy are hidden, and no longer appear in the Docker Scout Dashboard or in the CLI. Historic evaluation results aren't deleted if you disable a policy, so if you change your mind and re-enable a policy later, results from earlier evaluations will still be available.
To disable a policy:
- Go to the Policies page in the Docker Scout Dashboard.
- Select the policy you want to disable.
- Select the Disable button.
Delete a policy
When you delete a policy, evaluation results for that policy are deleted as well, and no longer appear in the Docker Scout Dashboard or in the CLI.
To delete a policy:
- Go to the Policies page in the Docker Scout Dashboard.
- Select the policy you want to delete.
- Select the Delete button.
Recover a deleted policy
If you've deleted a policy, you can recreate it by following the steps in Add a policy. On the policy configuration screen, select Configure on the deleted policy that you wish to recreate.",,,
3d8c4efea4f582e0e23d90caf306476104c345df50425b3e0dd0ffeb9cd5caca,"Test and debug
In order to improve the developer experience, Docker Desktop provides a set of tools to help you test and debug your extension.
Open Chrome DevTools
In order to open the Chrome DevTools for your extension when you select the Extensions tab, run:
$ docker extension dev debug <name-of-your-extensions>
Each subsequent click on the extension tab also opens Chrome DevTools. To stop this behaviour, run:
$ docker extension dev reset <name-of-your-extensions>
After an extension is deployed, it is also possible to open Chrome DevTools from the UI extension part using a variation of the
Konami Code. Select the Extensions tab, and then hit the key sequence up, up, down, down, left, right, left, right, p, d, t
.
Hot reloading whilst developing the UI
During UI development, it’s helpful to use hot reloading to test your changes without rebuilding your entire
extension. To do this, you can configure Docker Desktop to load your UI from a development server, such as the one
Vite starts when invoked with npm start
.
Assuming your app runs on the default port, start your UI app and then run:
$ cd ui
$ npm run dev
This starts a development server that listens on port 3000.
You can now tell Docker Desktop to use this as the frontend source. In another terminal run:
$ docker extension dev ui-source <name-of-your-extensions> http://localhost:3000
Close and reopen the Docker Desktop dashboard and go to your extension. All the changes to the frontend code are immediately visible.
Once finished, you can reset the extension configuration to the original settings. This will also reset opening Chrome DevTools if you used docker extension dev debug <name-of-your-extensions>
:
$ docker extension dev reset <name-of-your-extensions>
Show the extension containers
If your extension is composed of one or more services running as containers in the Docker Desktop VM, you can access them easily from the dashboard in Docker Desktop.
- In Docker Desktop, navigate to Settings.
- Under the Extensions tab, select the Show Docker Desktop Extensions system containers option. You can now view your extension containers and their logs.
Clean up
To remove the extension, run:
$ docker extension rm <name-of-your-extension>
What's next
- Build an advanced frontend extension.
- Learn more about extensions architecture.
- Explore our design principles.
- Take a look at our UI styling guidelines.
- Learn how to setup CI for your extension.",,,
e5543ada0ad3108760a2a655690e69e6985fbf31085e9e5eed2dcc729f33af0c,"Use IPv6 networking
IPv6 is only supported on Docker daemons running on Linux hosts.
Create an IPv6 network
Using
docker network create
:$ docker network create --ipv6 ip6net
Using
docker network create
, specifying an IPv6 subnet:$ docker network create --ipv6 --subnet 2001:db8::/64 ip6net
Using a Docker Compose file:
networks: ip6net: enable_ipv6: true ipam: config: - subnet: 2001:db8::/64
You can now run containers that attach to the ip6net
network.
$ docker run --rm --network ip6net -p 80:80 traefik/whoami
This publishes port 80 on both IPv6 and IPv4. You can verify the IPv6 connection by running curl, connecting to port 80 on the IPv6 loopback address:
$ curl http://[::1]:80
Hostname: ea1cfde18196
IP: 127.0.0.1
IP: ::1
IP: 172.17.0.2
IP: 2001:db8::2
IP: fe80::42:acff:fe11:2
RemoteAddr: [2001:db8::1]:37574
GET / HTTP/1.1
Host: [::1]
User-Agent: curl/8.1.2
Accept: */*
Use IPv6 for the default bridge network
The following steps show you how to use IPv6 on the default bridge network.
Edit the Docker daemon configuration file, located at
/etc/docker/daemon.json
. Configure the following parameters:{ ""ipv6"": true, ""fixed-cidr-v6"": ""2001:db8:1::/64"" }
ipv6
enables IPv6 networking on the default network.fixed-cidr-v6
assigns a subnet to the default bridge network, enabling dynamic IPv6 address allocation.ip6tables
enables additional IPv6 packet filter rules, providing network isolation and port mapping. It is enabled by-default, but can be disabled.
Save the configuration file.
Restart the Docker daemon for your changes to take effect.
$ sudo systemctl restart docker
You can now run containers on the default bridge network.
$ docker run --rm -p 80:80 traefik/whoami
This publishes port 80 on both IPv6 and IPv4. You can verify the IPv6 connection by making a request to port 80 on the IPv6 loopback address:
$ curl http://[::1]:80
Hostname: ea1cfde18196
IP: 127.0.0.1
IP: ::1
IP: 172.17.0.2
IP: 2001:db8:1::242:ac12:2
IP: fe80::42:acff:fe12:2
RemoteAddr: [2001:db8:1::1]:35558
GET / HTTP/1.1
Host: [::1]
User-Agent: curl/8.1.2
Accept: */*
Dynamic IPv6 subnet allocation
If you don't explicitly configure subnets for user-defined networks,
using docker network create --subnet=<your-subnet>
,
those networks use the default address pools of the daemon as a fallback.
This also applies to networks created from a Docker Compose file,
with enable_ipv6
set to true
.
If no IPv6 pools are included in Docker Engine's default-address-pools
,
and no --subnet
option is given,
Unique Local Addresses (ULAs)
will be used when IPv6 is enabled. These /64
subnets include a 40-bit
Global ID based on the Docker Engine's randomly generated ID, to give a
high probability of uniqueness.
To use different pools of IPv6 subnets for dynamic address allocation, you must manually configure address pools of the daemon to include:
- The default IPv4 address pools
- One or more IPv6 pools of your own
The default address pool configuration is:
{
""default-address-pools"": [
{ ""base"": ""172.17.0.0/16"", ""size"": 16 },
{ ""base"": ""172.18.0.0/16"", ""size"": 16 },
{ ""base"": ""172.19.0.0/16"", ""size"": 16 },
{ ""base"": ""172.20.0.0/14"", ""size"": 16 },
{ ""base"": ""172.24.0.0/14"", ""size"": 16 },
{ ""base"": ""172.28.0.0/14"", ""size"": 16 },
{ ""base"": ""192.168.0.0/16"", ""size"": 20 }
]
}
The following example shows a valid configuration with the default values and
an IPv6 pool. The IPv6 pool in the example provides up to 256 IPv6 subnets of
size /64
, from an IPv6 pool of prefix length /56
.
{
""default-address-pools"": [
{ ""base"": ""172.17.0.0/16"", ""size"": 16 },
{ ""base"": ""172.18.0.0/16"", ""size"": 16 },
{ ""base"": ""172.19.0.0/16"", ""size"": 16 },
{ ""base"": ""172.20.0.0/14"", ""size"": 16 },
{ ""base"": ""172.24.0.0/14"", ""size"": 16 },
{ ""base"": ""172.28.0.0/14"", ""size"": 16 },
{ ""base"": ""192.168.0.0/16"", ""size"": 20 },
{ ""base"": ""2001:db8::/56"", ""size"": 64 }
]
}
Note
The address
2001:db8::
in this example is reserved for use in documentation. Replace it with a valid IPv6 network.The default IPv4 pools are from the private address range, similar to the default IPv6 ULA networks.
Docker in Docker
On a host using xtables
(legacy iptables
) instead of nftables
, kernel
module ip6_tables
must be loaded before an IPv6 Docker network can be created,
It is normally loaded automatically when Docker starts.
However, if you running Docker in Docker that is not based on a recent
version of the
official docker
image, you
may need to run modprobe ip6_tables
on your host. Alternatively, use daemon
option --ip6tables=false
to disable ip6tables
for the containerized Docker
Engine.",,,
f9d50c566c2edd3b2a28ec818c4388eec697d799ac6ce348e050e0e2eb5f6c46,"Repositories
A Docker Hub repository is a collection of container images, enabling you to store, manage, and share Docker images publicly or privately. Each repository serves as a dedicated space where you can store images associated with a particular application, microservice, or project. Content in repositories is organized by tags, which represent different versions of the same application, allowing users to pull the right version when needed.
In this section, learn how to:
Create a repository.
Manage a repository, including how to manage:
Repository information: Add descriptions, overviews, and categories to help users understand the purpose and usage of your repository. Clear repository information aids discoverability and usability.
Access: Control who can access your repositories with flexible options. Make repositories public or private, add collaborators, and, for organizations, manage roles and teams to maintain security and control.
Images: Repositories support diverse content types, including OCI artifacts, and allow version control through tagging. Push new images and manage existing content across repositories for flexibility.
Image security insights: Utilize continuous Docker Scout analysis and static vulnerability scanning to detect, understand, and address security issues within container images.
Webhooks: Automate responses to repository events like image pushes or updates by setting up webhooks, which can trigger notifications or actions in external systems, streamlining workflows.
Automated builds: Integrate with GitHub or Bitbucket for automated builds. Every code change triggers an image rebuild, supporting continuous integration and delivery.
Trusted content: Contribute to Docker Official Images or manage repositories in the Verified Publisher and Sponsored Open Source programs, including tasks like setting logos, accessing analytics, and enabling vulnerability scanning.
Archive an outdated or unsupported repository.
Delete a repository.
Manage personal settings: For your account, you can set personal settings for repositories, including default repository privacy and autobuild notifications.",,,
c2a46b0707a99381db2ace21e8c23f6ef233875f8a0f1ac36204389ceba86b49,"Storage
By default all files created inside a container are stored on a writable container layer that sits on top of the read-only, immutable image layers.
Data written to the container layer doesn't persist when the container is destroyed. This means that it can be difficult to get the data out of the container if another process needs it.
The writable layer is unique per container. You can't easily extract the data from the writeable layer to the host, or to another container.
Storage mount options
Docker supports the following types of storage mounts for storing data outside of the writable layer of the container:
No matter which type of mount you choose to use, the data looks the same from within the container. It is exposed as either a directory or an individual file in the container's filesystem.
Volume mounts
Volumes are persistent storage mechanisms managed by the Docker daemon. They retain data even after the containers using them are removed. Volume data is stored on the filesystem on the host, but in order to interact with the data in the volume, you must mount the volume to a container. Directly accessing or interacting with the volume data is unsupported, undefined behavior, and may result in the volume or its data breaking in unexpected ways.
Volumes are ideal for performance-critical data processing and long-term storage needs. Since the storage location is managed on the daemon host, volumes provide the same raw file performance as accessing the host filesystem directly.
Bind mounts
Bind mounts create a direct link between a host system path and a container, allowing access to files or directories stored anywhere on the host. Since they aren't isolated by Docker, both non-Docker processes on the host and container processes can modify the mounted files simultaneously.
Use bind mounts when you need to be able to access files from both the container and the host.
tmpfs mounts
A tmpfs mount stores files directly in the host machine's memory, ensuring the data is not written to disk. This storage is ephemeral: the data is lost when the container is stopped or restarted, or when the host is rebooted. tmpfs mounts do not persist data either on the Docker host or within the container's filesystem.
These mounts are suitable for scenarios requiring temporary, in-memory storage, such as caching intermediate data, handling sensitive information like credentials, or reducing disk I/O. Use tmpfs mounts only when the data does not need to persist beyond the current container session.
Named pipes
Named pipes can be used for communication between the Docker host and a container. Common use case is to run a third-party tool inside of a container and connect to the Docker Engine API using a named pipe.
Next steps
- Learn more about volumes.
- Learn more about bind mounts.
- Learn more about tmpfs mounts.
- Learn more about storage drivers, which are not related to bind mounts or volumes, but allow you to store data in a container's writable layer.",,,
bd47ceab61abc878d88c265cd61e642158b0a18505dcd0a642511aeae017eb14,"Build release notes
This page contains information about the new features, improvements, and bug fixes in Docker Buildx.
0.21.0
2025-02-19The full release note for this release is available on GitHub.
New
- New command
buildx history trace
lets you inspect traces of a build in a Jaeger UI-based viewer and compare one trace with another. docker/buildx#2904
Enhancements
- The history inspection command
buildx history inspect
now supports custom formatting with--format
flag and JSON formatting for machine-readable output. docker/buildx#2964 - Support for CDI device entitlement in build and bake. docker/buildx#2994
- Supported CDI devices are now shown in the builder inspection. docker/buildx#2983
- When using
GitHub Cache backend
type=gha
, the URL for the Version 2 or API is now read from the environment and sent to BuildKit. Version 2 backend requires BuildKit v0.20.0 or later. docker/buildx#2983, docker/buildx#3001
Bug fixes
- Avoid unnecessary warnings and prompts when using
--progress=rawjson
. docker/buildx#2957 - Fix regression with debug shell sometimes not working correctly on
--on=error
. docker/buildx#2958 - Fix possible panic errors when using an unknown variable in the Bake definition. docker/buildx#2960
- Fix invalid duplicate output on JSON format formatting of
buildx ls
command. docker/buildx#2970 - Fix bake handling cache imports with CSV string containing multiple registry references. docker/buildx#2944
- Fix issue where error from pulling BuildKit image could be ignored. docker/buildx#2988
- Fix race on pausing progress on debug shell. docker/buildx#3003
0.20.1
2025-01-23The full release note for this release is available on GitHub.
Bug fixes
- Fix
bake --print
output after missing some attributes for attestations. docker/buildx#2937 - Fix allowing comma-separated image reference strings for cache import and export values. docker/buildx#2944
0.20.0
2025-01-20The full release note for this release is available on GitHub.
Note
This version of buildx enables filesystem entitlement checks for
buildx bake
command by default. If your Bake definition needs to read or write files outside your current working directory, you need to allow access to these paths with--allow fs=<path|*>
. On the terminal, you can also interactively approve these paths with the provided prompt. Optionally, you can disable these checks by settingBUILDX_BAKE_ENTITLEMENTS_FS=0
. This validation produced a warning in Buildx v0.19.0+, but starting from current release it produces an error. For more information, see the reference documentation.
New
- New
buildx history
command has been added that allows working with build records of completed and running builds. You can use these commands to list, inspect, remove your builds, replay the logs of already completed builds, and quickly open your builds in Docker Desktop Build UI for further debugging. This is an early version of this command and we expect to add more features in the future releases. #2891, #2925
Enhancements
- Bake: Definition now supports new object notation for the fields that previously required CSV strings as inputs (
attest
,output
,cache-from
,cache-to
,secret
,ssh
). docker/buildx#2758, docker/buildx#2848, docker/buildx#2871, docker/buildx#2814 - Bake: Filesystem entitlements now error by default. To disable this behavior, you can set
BUILDX_BAKE_ENTITLEMENTS_FS=0
. docker/buildx#2875 - Bake: Infer Git authentication token from remote files to build request. docker/buildx#2905
- Bake: Add support for
--list
flag to list targets and variables. docker/buildx#2900, docker/buildx#2907 - Bake: Update lookup order for default definition files to load the files with ""override"" suffix later. docker/buildx#2886
Bug fixes
- Bake: Fix entitlements check for default SSH socket. docker/buildx#2898
- Bake: Fix missing default target in group's default targets. docker/buildx#2863
- Bake: Fix named context from target platform matching. docker/buildx#2877
- Fix missing documentation for quiet progress mode. docker/buildx#2899
- Fix missing last progress from loading layers. docker/buildx#2876
- Validate BuildKit configuration before creating a builder. docker/buildx#2864
Packaging
- Compose compatibility has been updated to v2.4.7. docker/buildx#2893, docker/buildx#2857, docker/buildx#2829
0.19.1
2024-11-27The full release note for this release is available on GitHub.
Bug fixes
- Reverted the change in v0.19.0 that added new object notation for the fields that previously required CSV strings in Bake definition. This enhancement was reverted because of backwards incompatibility issues were discovered in some edge cases. This feature has now been postponed to the v0.20.0 release. docker/buildx#2824
0.19.0
2024-11-27The full release note for this release is available on GitHub.
New
Bake now requires you to allow filesystem entitlements when your build needs to read or write files outside of your current working directory. docker/buildx#2796, docker/buildx#2812.
To allow filesystem entitlements, use the
--allow fs.read=<path>
flag for thedocker buildx bake
command.This feature currently only reports a warning when using a local Bake definition, but will start to produce an error starting from the v0.20 release. To enable the error in the current release, you can set
BUILDX_BAKE_ENTITLEMENTS_FS=1
.
Enhancements
Bake definition now supports new object notation for the fields that previously required CSV strings as inputs. docker/buildx#2758
Note
This enhancement was reverted in v0.19.1 due to a bug.
Bake definition now allows defining validation conditions to variables. docker/buildx#2794
Metadata file values can now contain JSON array values. docker/buildx#2777
Improved error messages when using an incorrect format for labels. docker/buildx#2778
FreeBSD and OpenBSD artifacts are now included in the release. docker/buildx#2774, docker/buildx#2775, docker/buildx#2781
Bug fixes
- Fixed an issue with printing Bake definitions containing empty Compose networks. docker/buildx#2790.
Packaging
- Compose support has been updated to v2.4.4. docker/buildx#2806 docker/buildx#2780.
0.18.0
2024-10-31The full release note for this release is available on GitHub.
New
- The
docker buildx inspect
command now displays BuildKit daemon configuration options set with a TOML file. docker/buildx#2684 - The
docker buildx ls
command output is now more compact by default by compacting the platform list. A new--no-trunc
option can be used for the full list. docker/buildx#2138, docker/buildx#2717 - The
docker buildx prune
command now supports new--max-used-space
and--min-free-space
filters with BuildKit v0.17.0+ builders. docker/buildx#2766
Enhancements
- Allow capturing of CPU and memory profiles with
pprof
using theBUILDX_CPU_PROFILE
andBUILDX_MEM_PROFILE
environment variables. docker/buildx#2746 - Maximum Dockerfile size from standard input has increased. docker/buildx#2716, docker/buildx#2719
- Memory allocations have been reduced. docker/buildx#2724, docker/buildx#2713
- The
--list-targets
and--list-variables
flags fordocker buildx bake
no longer require initialization of the builder. docker/buildx#2763
Bug fixes
- Check warnings now print the full filepath to the offending Dockerfile, relative to the current working directory. docker/buildx#2672
- Fallback images for the
--check
and--call
options have been updated to correct references. docker/buildx#2705 - Fix issue with the build details link not showing in experimental mode. docker/buildx#2722
- Fix validation issue with invalid target linking for Bake. docker/buildx#2700
- Fix missing error message when running an invalid command. docker/buildx#2741
- Fix possible false warnings for local state in
--call
requests. docker/buildx#2754 - Fix potential issues with entitlements when using linked targets in Bake. docker/buildx#2701
- Fix possible permission issues when accessing local state after running Buildx with
sudo
. docker/buildx#2745
Packaging
- Compose compatibility has been updated to v2.4.1. docker/buildx#2760
0.17.1
2024-09-13The full release note for this release is available on GitHub.
Bug fixes
- Do not set
network.host
entitlement flag automatically on builder creation for thedocker-container
andkubernetes
drivers if the entitlement is set in the BuildKit configuration file. docker/buildx#2685 - Do not print the
network
field withdocker buildx bake --print
when empty. docker/buildx#2689 - Fix telemetry socket path under WSL2. docker/buildx#2698
0.17.0
2024-09-10The full release note for this release is available on GitHub.
New
- Add
basename
,dirname
andsanitize
functions to Bake. docker/buildx#2649 - Enable support for Bake entitlements to allow privileged operations during builds. docker/buildx#2666
Enhancements
- Introduce CLI metrics tracking for Bake commands. docker/buildx#2610
- Add
--debug
to all build commands. Previously, it was only available on the top-leveldocker
anddocker buildx
commands. docker/buildx#2660 - Allow builds from stdin for multi-node builders. docker/buildx#2656
- Improve
kubernetes
driver initialization. docker/buildx#2606 - Include target name in the error message when building multiple targets with Bake. docker/buildx#2651
- Optimize metrics handling to reduce performance overhead during progress tracking. docker/buildx#2641
- Display the number of warnings after completing a rule check. docker/buildx#2647
- Skip build ref and provenance metadata for frontend methods. docker/buildx#2650
- Add support for setting network mode in Bake files (HCL and JSON). docker/buildx#2671
- Support the
--metadata-file
flag when set along the--call
flag. docker/buildx#2640 - Use shared session for local contexts used by multiple Bake targets. docker/buildx#2615, docker/buildx#2607, docker/buildx#2663
Bug fixes
- Improve memory management to avoid unnecessary allocations. docker/buildx#2601
Packaging updates
- Compose support has been updated to v2.1.6. docker/buildx#2547
0.16.2
2024-07-25The full release note for this release is available on GitHub.
Bug fixes
- Fix possible ""bad file descriptor"" error when exporting local cache to NFS volume docker/buildx#2629
0.16.1
2024-07-18The full release note for this release is available on GitHub.
Bug fixes
- Fix possible panic due to data race in
buildx bake --print
command docker/buildx#2603 - Improve messaging about using
--debug
flag to inspect build warnings docker/buildx#2612
0.16.0
2024-07-11The full release note for this release is available on GitHub.
New
- Bake command now supports
--call
and--check
flags andcall
attribute in target definitions for selecting custom frontend methods. docker/buildx#2556, docker/buildx#2576 - Experimental
Bake now supports
--list-targets
and--list-variables
flags for inspecting the definition and possible configuration options for your project. docker/buildx#2556 - Bake definition variables and targets supports new
description
attribute for defining text-based description that can be inspected using e.g.--list-targets
and--list-variables
. docker/buildx#2556 - Bake now supports printing warnings for build check violations. docker/buildx#2501
Enhancements
- The build command now ensures that multi-node builds use the same build reference for each node. docker/buildx#2572
- Avoid duplicate requests and improve the performance of remote driver. docker/buildx#2501
- Build warnings can now be saved to the metadata file by setting the
BUILDX_METADATA_WARNINGS=1
environment variable. docker/buildx#2551, docker/buildx#2521, docker/buildx#2550 - Improve message of the
--check
flag when no warnings are detected. docker/buildx#2549
Bug fixes
- Fix support for multi-type annotations during build. docker/buildx#2522
- Fix a regression where possible inefficient transfer of files would occur when switching projects due to incremental transfer reuse. docker/buildx#2558
- Fix incorrect default load for chained Bake targets. docker/buildx#2583
- Fix incorrect
COMPOSE_PROJECT_NAME
handling in Bake. docker/buildx#2579 - Fix index annotations support for multi-node builds. docker/buildx#2546
- Fix capturing provenance metadata for builds from remote context. docker/buildx#2560
Packaging updates
- Compose support has been updated to v2.1.3. docker/buildx#2547
0.15.1
2024-06-18The full release note for this release is available on GitHub.
Bug fixes
- Fix missing build error and exit code for some validation requests with
--check
. docker/buildx#2518 - Update fallback image for
--check
to Dockerfile v1.8.1. docker/buildx#2538
0.15.0
2024-06-11The full release note for this release is available on GitHub.
New
New
--call
option allows setting evaluation method for a build, replacing the previous experimental--print
flag. docker/buildx#2498, docker/buildx#2487, docker/buildx#2513In addition to the default
build
method, the following methods are implemented by Dockerfile frontend:--call=check
: Run validation routines for your build configuration. For more information about build checks, see Build checks--call=outline
: Show configuration that would be used by current build, including all build arguments, secrets, SSH mounts, etc., that your build would use.--call=targets
: Show all available targets and their descriptions.
New
--prefer-index
flag has been added to thedocker buildx imagetools create
command to control the behavior of creating image out of one single-platform image manifest. docker/buildx#2482The
kubernetes
driver now supports atimeout
option for configuring deployment timeout. docker/buildx#2492New metrics definitions have been added for build warning types. docker/buildx#2482, docker/buildx#2507
The
buildx prune
andbuildx du
commands now support negative and prefix filters. docker/buildx#2473Building Compose files with Bake now supports passing SSH forwarding configuration. docker/buildx#2445
Fix issue with configuring the
kubernetes
driver with custom TLS certificates. docker/buildx#2454Fix concurrent kubeconfig access when loading nodes. docker/buildx#2497
Packaging updates
- Compose support has been updated to v2.1.2. docker/buildx#2502, docker/buildx#2425
0.14.0
2024-04-18The full release note for this release is available on GitHub.
Enhancements
- Add support for
--print=lint
(experimental). docker/buildx#2404, docker/buildx#2406 - Fix JSON formatting for custom implementations of print sub-requests in frontends. docker/buildx#2374
- Provenance records are now set when building with
--metadata-file
. docker/buildx#2280 - Add Git authentication support for remote definitions. docker/buildx#2363
- New
default-load
driver option for thedocker-container
,remote
, andkubernetes
drivers to load build results to the Docker Engine image store by default. docker/buildx#2259 - Add
requests.ephemeral-storage
,limits.ephemeral-storage
andschedulername
options to thekubernetes
driver. docker/buildx#2370, docker/buildx#2415 - Add
indexof
function fordocker-bake.hcl
files. docker/buildx#2384 - OpenTelemetry metrics for Buildx now measure durations of idle time, image exports, run operations, and image transfers for image source operations during build. docker/buildx#2316, docker/buildx#2317, docker/buildx#2323, docker/buildx#2271
- Build progress metrics to the OpenTelemetry endpoint associated with the
desktop-linux
context no longer requires Buildx in experimental mode (BUILDX_EXPERIMENTAL=1
). docker/buildx#2344
Bug fixes
- Fix
--load
and--push
incorrectly overriding outputs when used with multiple Bake file definitions. docker/buildx#2336 - Fix build from stdin with experimental mode enabled. docker/buildx#2394
- Fix an issue where delegated traces could be duplicated. docker/buildx#2362
Packaging updates
- Compose support has been updated to
v2.26.1
(via
compose-go
v2.0.2). docker/buildx#2391
0.13.1
2024-03-13The full release note for this release is available on GitHub.
Bug fixes
- Fix connecting to
docker-container://
andkube-pod://
style URLs with remote driver. docker/buildx#2327 - Fix handling of
--push
with Bake when a target has already defined a non-image output. docker/buildx#2330
0.13.0
2024-03-06The full release note for this release is available on GitHub.
New
- New
docker buildx dial-stdio
command for directly contacting BuildKit daemon of the configured builder instance. docker/buildx#2112 - Windows container builders can now be created using the
remote
driver and npipe connections. docker/buildx#2287 - Npipe URL scheme is now supported on Windows. docker/buildx#2250
- Experimental Buildx can now export OpenTelemetry metrics for build duration and transfer sizes. docker/buildx#2235, docker/buildx#2258 docker/buildx#2225 docker/buildx#2224 docker/buildx#2155
Enhancements
- Bake command now supports defining
shm-size
andulimit
values. docker/buildx#2279, docker/buildx#2242 - Better handling of connecting to unhealthy nodes with remote driver. docker/buildx#2130
- Builders using the
docker-container
andkubernetes
drivers now allownetwork.host
entitlement by default (allowing access to the container's network). docker/buildx#2266 - Builds can now use multiple outputs with a single command (requires BuildKit v0.13+). docker/buildx#2290, docker/buildx#2302
- Default Git repository path is now found via configured tracking branch. docker/buildx#2146
- Fix possible cache invalidation when using linked targets in Bake. docker/buildx#2265
- Fixes for Git repository path sanitization in WSL. docker/buildx#2167
- Multiple builders can now be removed with a single command. docker/buildx#2140
- New cancellation signal handling via Unix socket. docker/buildx#2184 docker/buildx#2289
- The Compose spec support has been updated to v2.0.0-rc.8. docker/buildx#2205
- The
--config
flag fordocker buildx create
was renamed to--buildkitd-config
. docker/buildx#2268 - The
--metadata-file
flag fordocker buildx build
can now also return build reference that can be used for further build debugging, for example, in Docker Desktop. docker/buildx#2263 - The
docker buildx bake
command now shares the same authentication provider for all targets for improved performance. docker/buildx#2147 - The
docker buildx imagetools inspect
command now shows DSSE-signed SBOM and Provenance attestations. docker/buildx#2194 - The
docker buildx ls
command now supports--format
options for controlling the output. docker/buildx#1787 - The
docker-container
driver now supports driver options for defining restart policy for BuildKit container. docker/buildx#1271 - VCS attributes exported from Buildx now include the local directory sub-paths if they're relative to the current Git repository. docker/buildx#2156
--add-host
flag now permits a=
separator for IPv6 addresses. docker/buildx#2121
Bug fixes
- Fix additional output when exporting progress with
--progress=rawjson
docker/buildx#2252 - Fix possible console warnings on Windows. docker/buildx#2238
- Fix possible inconsistent configuration merge order when using Bake with many configurations. docker/buildx#2237
- Fix possible panic in the
docker buildx imagetools create
command. docker/buildx#2230
0.12.1
2024-01-12The full release note for this release is available on GitHub.
Bug fixes and enhancements
- Fix incorrect validation of some
--driver-opt
values that could cause a panic and corrupt state to be stored. docker/buildx#2176
0.12.0
2023-11-16The full release note for this release is available on GitHub.
New
- New
--annotation
flag for thebuildx build
, and anannotations
key in the Bake file, that lets you add OCI Annotations to build results. #2020, #2098 - New experimental debugging features, including a new
debug
command and an interactive debugging console. This feature currently requires settingBUILDX_EXPERIMENTAL=1
. #2006, #1896, #1970, #1914, #2026, #2086
Bug fixes and enhancements
- The special
host-gateway
IP mapping can now be used with the--add-host
flag during build. #1894, #2083 - Bake now allows adding local source files when building from remote definition. #1838
- The status of uploading build results to Docker is now shown interactively on progress bar. #1994
- Error handling has been improved when bootstrapping multi-node build clusters. #1869
- The
buildx imagetools create
command now allows adding annotation when creating new images in the registry. #1965 - OpenTelemetry build trace delegation from buildx is now possible with Docker and Remote driver. #2034
- Bake command now shows all files where the build definition was loaded from on the progress bar. #2076
- Bake files now allow the same attributes to be defined in multiple definition files. #1062
- Using the Bake command with a remote definition now allows this definition to use local Dockerfiles. #2015
- Docker container driver now explicitly sets BuildKit config path to make sure configurations are loaded from same location for both mainline and rootless images. #2093
- Improve performance of detecting when BuildKit instance has completed booting. #1934
- Container driver now accepts many new driver options for defining the resource limits for BuildKit container. #2048
- Inspection commands formatting has been improved. #2068
- Error messages about driver capabilities have been improved. #1998
- Improve errors when invoking Bake command without targets. #2100
- Allow enabling debug logs with environment variables when running in standalone mode. #1821
- When using Docker driver the default image resolve mode has been updated to prefer local Docker images for backward compatibility. #1886
- Kubernetes driver now allows setting custom annotations and labels to the BuildKit deployments and pods. #1938
- Kubernetes driver now allows setting authentication token with endpoint configuration. #1891
- Fix possible issue with chained targets in Bake that could result in build failing or local source for a target uploaded multiple times. #2113
- Fix issue when accessing global target properties when using the matrix feature of the Bake command. #2106
- Fixes for formatting validation of certain build flags #2040
- Fixes to avoid locking certain commands unnecessarily while booting builder nodes. #2066
- Fix cases where multiple builds try to bootstrap the same builder instance in parallel. #2000
- Fix cases where errors on uploading build results to Docker could be dropped in some cases. #1927
- Fix detecting capabilities for missing attestation support based on build output. #1988
- Fix the build for loading in Bake remote definition to not show up in build history records. #1961, #1954
- Fix errors when building Compose files using the that define profiles with Bake. #1903
- Fix possible time correction errors on progress bar. #1968
- Fix passing custom cgroup parent to builds that used the new controller interface. #1913
Packaging
- Compose support has been updated to 1.20, enabling ""include"" functionality when using the Bake command. #1971, #2065, #2094
0.11.2
2023-07-18The full release note for this release is available on GitHub.
Bug fixes and enhancements
- Fix a regression that caused buildx to not read the
KUBECONFIG
path from the instance store. docker/buildx#1941 - Fix a regression with result handle builds showing up in the build history incorrectly. docker/buildx#1954
0.11.1
2023-07-05The full release note for this release is available on GitHub.
Bug fixes and enhancements
- Fix a regression for bake where services in profiles would not be loaded. docker/buildx#1903
- Fix a regression where
--cgroup-parent
option had no effect during build. docker/buildx#1913 - Fix a regression where valid docker contexts could fail buildx builder name validation. docker/buildx#1879
- Fix a possible panic when terminal is resized during the build. docker/buildx#1929
0.11.0
2023-06-13The full release note for this release is available on GitHub.
New
- Bake now supports
matrix builds.
The new matrix field on
target
lets you create multiple similar targets to remove duplication in bake files. docker/buildx#1690 - New experimental
--detach
flag for running builds in detached mode. docker/buildx#1296, docker/buildx#1620, docker/buildx#1614, docker/buildx#1737, docker/buildx#1755 - New experimental debug monitor mode that lets you start a debug session in your builds. docker/buildx#1626, docker/buildx#1640
- New
EXPERIMENTAL_BUILDKIT_SOURCE_POLICY
environment variable for applying a BuildKit source policy file. docker/buildx#1628
Bug fixes and enhancements
--load
now supports loading multi-platform images when the containerd image store is enabled. docker/buildx#1813- Build progress output now displays the name of the builder being used. docker/buildx#1177
- Bake now supports detecting
compose.{yml,yaml}
files. docker/buildx#1752 - Bake now supports new compose build keys
dockerfile_inline
andadditional_contexts
. docker/buildx#1784 - Bake now supports replace HCL function. docker/buildx#1720
- Bake now allows merging multiple similar attestation parameters into a single parameter to allow overriding with a single global value. docker/buildx#1699
- Initial support for shell completion. docker/buildx#1727
- BuildKit versions now correctly display in
buildx ls
andbuildx inspect
for builders using thedocker
driver. docker/buildx#1552 - Display additional builder node details in buildx inspect view. docker/buildx#1440, docker/buildx#1854
- Builders using the
remote
driver allow using TLS without proving its own key/cert (if BuildKit remote is configured to support it) docker/buildx#1693 - Builders using the
kubernetes
driver support a newserviceaccount
option, which sets theserviceAccountName
of the Kubernetes pod. docker/buildx#1597 - Builders using the
kubernetes
driver support theproxy-url
option in the kubeconfig file. docker/buildx#1780 - Builders using the
kubernetes
are now automatically assigned a node name if no name is explicitly provided. docker/buildx#1673 - Fix invalid path when writing certificates for
docker-container
driver on Windows. docker/buildx#1831 - Fix bake failure when remote bake file is accessed using SSH. docker/buildx#1711, docker/buildx#1734
- Fix bake failure when remote bake context is incorrectly resolved. docker/buildx#1783
- Fix path resolution of
BAKE_CMD_CONTEXT
andcwd://
paths in bake contexts. docker/buildx#1840 - Fix mixed OCI and Docker media types when creating images using
buildx imagetools create
. docker/buildx#1797 - Fix mismatched image id between
--iidfile
and-q
. docker/buildx#1844 - Fix AWS authentication when mixing static creds and IAM profiles. docker/buildx#1816
0.10.4
2023-03-06Note
Buildx v0.10 enables support for a minimal SLSA Provenance attestation, which requires support for OCI-compliant multi-platform images. This may introduce issues with registry and runtime support (e.g. Google Cloud Run and AWS Lambda). You can optionally disable the default provenance attestation functionality using
--provenance=false
.
Bug fixes and enhancements
- Add
BUILDX_NO_DEFAULT_ATTESTATIONS
as alternative to--provenance false
. docker/buildx#1645 - Disable dirty Git checkout detection by default for performance. Can be enabled with
BUILDX_GIT_CHECK_DIRTY
opt-in. docker/buildx#1650 - Strip credentials from VCS hint URL before sending to BuildKit. docker/buildx#1664
0.10.3
2023-02-16Note
Buildx v0.10 enables support for a minimal SLSA Provenance attestation, which requires support for OCI-compliant multi-platform images. This may introduce issues with registry and runtime support (e.g. Google Cloud Run and AWS Lambda). You can optionally disable the default provenance attestation functionality using
--provenance=false
.
Bug fixes and enhancements
- Fix reachable commit and warnings on collecting Git provenance info. docker/buildx#1592, docker/buildx#1634
- Fix a regression where docker context was not being validated. docker/buildx#1596
- Fix function resolution with JSON bake definition. docker/buildx#1605
- Fix case where original HCL bake diagnostic is discarded. docker/buildx#1607
- Fix labels not correctly set with bake and compose file. docker/buildx#1631
0.10.2
2023-01-30Note
Buildx v0.10 enables support for a minimal SLSA Provenance attestation, which requires support for OCI-compliant multi-platform images. This may introduce issues with registry and runtime support (e.g. Google Cloud Run and AWS Lambda). You can optionally disable the default provenance attestation functionality using
--provenance=false
.
Bug fixes and enhancements
- Fix preferred platforms order not taken into account in multi-node builds. docker/buildx#1561
- Fix possible panic on handling
SOURCE_DATE_EPOCH
environment variable. docker/buildx#1564 - Fix possible push error on multi-node manifest merge since BuildKit v0.11 on some registries. docker/buildx#1566
- Improve warnings on collecting Git provenance info. docker/buildx#1568
0.10.1
2023-01-27Note
Buildx v0.10 enables support for a minimal SLSA Provenance attestation, which requires support for OCI-compliant multi-platform images. This may introduce issues with registry and runtime support (e.g. Google Cloud Run and AWS Lambda). You can optionally disable the default provenance attestation functionality using
--provenance=false
.
Bug fixes and enhancements
- Fix sending the correct origin URL as
vsc:source
metadata. docker/buildx#1548 - Fix possible panic from data-race. docker/buildx#1504
- Fix regression with
rm --all-inactive
. docker/buildx#1547 - Improve attestation access in
imagetools inspect
by lazily loading data. docker/buildx#1546 - Correctly mark capabilities request as internal. docker/buildx#1538
- Detect invalid attestation configuration. docker/buildx#1545
- Update containerd patches to fix possible push regression affecting
imagetools
commands. docker/buildx#1559
0.10.0
2023-01-10Note
Buildx v0.10 enables support for a minimal SLSA Provenance attestation, which requires support for OCI-compliant multi-platform images. This may introduce issues with registry and runtime support (e.g. Google Cloud Run and AWS Lambda). You can optionally disable the default provenance attestation functionality using
--provenance=false
.
New
- The
buildx build
command supports new--attest
flag, along with shorthands--sbom
and--provenance
, for adding attestations for your current build. docker/buildx#1412 docker/buildx#1475--attest type=sbom
or--sbom=true
adds SBOM attestations.--attest type=provenance
or--provenance=true
adds SLSA provenance attestation.- When creating OCI images, a minimal provenance attestation is included with the image by default.
- When building with BuildKit that supports provenance attestations Buildx will
automatically share the version control information of your build context, so
it can be shown in provenance for later debugging. Previously this only
happened when building from a Git URL directly. To opt-out of this behavior
you can set
BUILDX_GIT_INFO=0
. Optionally you can also automatically define labels with VCS info by settingBUILDX_GIT_LABELS=1
. docker/buildx#1462, docker/buildx#1297, docker/buildx#1341, docker/buildx#1468, docker/buildx#1477 - Named contexts with
--build-context
now supportoci-layout://
protocol for initializing the context with a value of a local OCI layout directory. E.g.--build-context stagename=oci-layout://path/to/dir
. This feature requires BuildKit v0.11.0+ and Dockerfile 1.5.0+. docker/buildx#1456 - Bake now supports resource interpolation where you can reuse the values from other target definitions. docker/buildx#1434
- Buildx will now automatically forward
SOURCE_DATE_EPOCH
environment variable if it is defined in your environment. This feature is meant to be used with updated reproducible builds support in BuildKit v0.11.0+. docker/buildx#1482 - Buildx now remembers the last activity for a builder for better organization of builder instances. docker/buildx#1439
- Bake definition now supports null values for variables and labels for build arguments and labels to use the defaults set in the Dockerfile. docker/buildx#1449
- The
buildx imagetools inspect
command now supports showing SBOM and Provenance data. docker/buildx#1444, docker/buildx#1498 - Increase performance of
ls
command and inspect flows. docker/buildx#1430, docker/buildx#1454, docker/buildx#1455, docker/buildx#1345 - Adding extra hosts with
Docker driver now supports
Docker-specific
host-gateway
special value. docker/buildx#1446 - OCI exporter now supports
tar=false
option for exporting OCI format directly in a directory. docker/buildx#1420
Upgrades
- Updated the Compose Specification to 1.6.0. docker/buildx#1387
Bug fixes and enhancements
--invoke
can now load default launch environment from the image metadata. docker/buildx#1324- Fix container driver behavior in regards to UserNS. docker/buildx#1368
- Fix possible panic in Bake when using wrong variable value type. docker/buildx#1442
- Fix possible panic in
imagetools inspect
. docker/buildx#1441 docker/buildx#1406 - Fix sending empty
--add-host
value to BuildKit by default. docker/buildx#1457 - Fix handling progress prefixes with progress groups. docker/buildx#1305
- Fix recursively resolving groups in Bake. docker/buildx#1313
- Fix possible wrong indentation on multi-node builder manifests. docker/buildx#1396
- Fix possible panic from missing OpenTelemetry configuration. docker/buildx#1383
- Fix
--progress=tty
behavior when TTY is not available. docker/buildx#1371 - Fix connection error conditions in
prune
anddu
commands. docker/buildx#1307
0.9.1
2022-08-18Bug fixes and enhancements
- The
inspect
command now displays the BuildKit version in use. docker/buildx#1279 - Fixed a regression when building Compose files that contain services without a build block. docker/buildx#1277
For more details, see the complete release notes in the Buildx GitHub repository.
0.9.0
2022-08-17New
- Support for a new
remote
driver that you can use to connect to any already running BuildKit instance. docker/buildx#1078, docker/buildx#1093, docker/buildx#1094, docker/buildx#1103, docker/buildx#1134, docker/buildx#1204 - You can now load Dockerfile from standard input even when the build context is coming from external Git or HTTP URL. docker/buildx#994
- Build commands now support new the build context type
oci-layout://
for loading build context from local OCI layout directories. Note that this feature depends on an unreleased BuildKit feature and builder instance frommoby/buildkit:master
needs to be used until BuildKit v0.11 is released. docker/buildx#1173 - You can now use the new
--print
flag to run helper functions supported by the BuildKit frontend performing the build and print their results. You can use this feature in Dockerfile to show the build arguments and secrets that the current build supports with--print=outline
and list all available Dockerfile stages with--print=targets
. This feature is experimental for gathering early feedback and requires enablingBUILDX_EXPERIMENTAL=1
environment variable. We plan to update/extend this feature in the future without keeping backward compatibility. docker/buildx#1100, docker/buildx#1272 - You can now use the new
--invoke
flag to launch interactive containers from build results for an interactive debugging cycle. You can reload these containers with code changes or restore them to an initial state from the special monitor mode. This feature is experimental for gathering early feedback and requires enablingBUILDX_EXPERIMENTAL=1
environment variable. We plan to update/extend this feature in the future without enabling backward compatibility. docker/buildx#1168, docker/buildx#1257, docker/buildx#1259 - Buildx now understands environment variable
BUILDKIT_COLORS
andNO_COLOR
to customize/disable the colors of interactive build progressbar. docker/buildx#1230, docker/buildx#1226 buildx ls
command now shows the current BuildKit version of each builder instance. docker/buildx#998- The
bake
command now loads.env
file automatically when building Compose files for compatibility. docker/buildx#1261 - Bake now supports Compose files with
cache_to
definition. docker/buildx#1155 - Bake now supports new builtin function
timestamp()
to access current time. docker/buildx#1214 - Bake now supports Compose build secrets definition. docker/buildx#1069
- Additional build context configuration is now supported in Compose files via
x-bake
. docker/buildx#1256 - Inspecting builder now shows current driver options configuration. docker/buildx#1003, docker/buildx#1066
Updates
- Updated the Compose Specification to 1.4.0. docker/buildx#1246, docker/buildx#1251
Bug fixes and enhancements
- The
buildx ls
command output has been updated with better access to errors from different builders. docker/buildx#1109 - The
buildx create
command now performs additional validation of builder parameters to avoid creating a builder instance with invalid configuration. docker/buildx#1206 - The
buildx imagetools create
command can now create new multi-platform images even if the source subimages are located on different repositories or registries. docker/buildx#1137 - You can now set the default builder config that is used when creating
builder instances without passing custom
--config
value. docker/buildx#1111 - Docker driver can now detect if
dockerd
instance supports initially disabled Buildkit features like multi-platform images. docker/buildx#1260, docker/buildx#1262 - Compose files using targets with
.
in the name are now converted to use_
so the selector keys can still be used in such targets. docker/buildx#1011 - Included an additional validation for checking valid driver configurations. docker/buildx#1188, docker/buildx#1273
- The
remove
command now displays the removed builder and forbids removing context builders. docker/buildx#1128 - Enable Azure authentication when using Kubernetes driver. docker/buildx#974
- Add tolerations handling for kubernetes driver. docker/buildx#1045 docker/buildx#1053
- Replace deprecated seccomp annotations with
securityContext
in thekubernetes
driver. docker/buildx#1052 - Fix panic on handling manifests with nil platform. docker/buildx#1144
- Fix using duration filter with
prune
command. docker/buildx#1252 - Fix merging multiple JSON files on Bake definition. docker/buildx#1025
- Fix issues with implicit builder created from Docker context had invalid configuration or dropped connection. docker/buildx#1129
- Fix conditions for showing no-output warning when using named contexts. docker/buildx#968
- Fix duplicating builders when builder instance and docker context have the same name. docker/buildx#1131
- Fix printing unnecessary SSH warning logs. docker/buildx#1085
- Fix possible panic when using an empty variable block with Bake JSON definition. docker/buildx#1080
- Fix image tools commands not handling
--builder
flag correctly. docker/buildx#1067 - Fix using custom image together with rootless option. docker/buildx#1063
For more details, see the complete release notes in the Buildx GitHub repository.
0.8.2
2022-04-04Updates
- Update Compose spec used by
buildx bake
to v1.2.1 to fix parsing ports definition. docker/buildx#1033
Bug fixes and enhancements
- Fix possible crash on handling progress streams from BuildKit v0.10. docker/buildx#1042
- Fix parsing groups in
buildx bake
when already loaded by a parent group. docker/buildx#1021
For more details, see the complete release notes in the Buildx GitHub repository.
0.8.1
2022-03-21Bug fixes and enhancements
- Fix possible panic on handling build context scanning errors. docker/buildx#1005
- Allow
.
on Compose target names inbuildx bake
for backward compatibility. docker/buildx#1018
For more details, see the complete release notes in the Buildx GitHub repository.
0.8.0
2022-03-09New
- Build command now accepts
--build-context
flag to define additional named build contexts for your builds. docker/buildx#904 - Bake definitions now support defining dependencies between targets and using the result of one target in another build. docker/buildx#928, docker/buildx#965, docker/buildx#963, docker/buildx#962, docker/buildx#981
imagetools inspect
now accepts--format
flag allowing access to config and buildinfo for specific images. docker/buildx#854, docker/buildx#972- New flag
--no-cache-filter
allows configuring build, so it ignores cache only for specified Dockerfile stages. docker/buildx#860 - Builds can now show a summary of warnings sets by the building frontend. docker/buildx#892
- The new build argument
BUILDKIT_INLINE_BUILDINFO_ATTRS
allows opting-in to embed building attributes to resulting image. docker/buildx#908 - The new flag
--keep-buildkitd
allows keeping BuildKit daemon running when removing a builder
Bug fixes and enhancements
--metadata-file
output now supports embedded structure types. docker/buildx#946buildx rm
now accepts new flag--all-inactive
for removing all builders that are not currently running. docker/buildx#885- Proxy config is now read from Docker configuration file and sent with build requests for backward compatibility. docker/buildx#959
- Support host networking in Compose. docker/buildx#905, docker/buildx#880
- Bake files can now be read from stdin with
-f -
. docker/buildx#864 --iidfile
now always writes the image config digest independently of the driver being used (use--metadata-file
for digest). docker/buildx#980- Target names in Bake are now restricted to not use special characters. docker/buildx#929
- Image manifest digest can be read from metadata when pushed with
docker
driver. docker/buildx#989 - Fix environment file handling in Compose files. docker/buildx#905
- Show last access time in
du
command. docker/buildx#867 - Fix possible double output logs when multiple Bake targets run same build steps. docker/buildx#977
- Fix possible errors on multi-node builder building multiple targets with mixed platform. docker/buildx#985
- Fix some nested inheritance cases in Bake. docker/buildx#914
- Fix printing default group on Bake files. docker/buildx#884
- Fix
UsernsMode
when using rootless container. docker/buildx#887
For more details, see the complete release notes in the Buildx GitHub repository.
0.7.1
2021-08-25Fixes
- Fix issue with matching exclude rules in
.dockerignore
. docker/buildx#858 - Fix
bake --print
JSON output for current group. docker/buildx#857
For more details, see the complete release notes in the Buildx GitHub repository.
0.7.0
2021-11-10New features
- TLS certificates from BuildKit configuration are now transferred to build
container with
docker-container
andkubernetes
drivers. docker/buildx#787 - Builds support
--ulimit
flag for feature parity. docker/buildx#800 - Builds support
--shm-size
flag for feature parity. docker/buildx#790 - Builds support
--quiet
for feature parity. docker/buildx#740 - Builds support
--cgroup-parent
flag for feature parity. docker/buildx#814 - Bake supports builtin variable
BAKE_LOCAL_PLATFORM
. docker/buildx#748 - Bake supports
x-bake
extension field in Compose files. docker/buildx#721 kubernetes
driver now supports colon-separatedKUBECONFIG
. docker/buildx#761kubernetes
driver now supports setting Buildkit config file with--config
. docker/buildx#682kubernetes
driver now supports installing QEMU emulators with driver-opt. docker/buildx#682
Enhancements
- Allow using custom registry configuration for multi-node pushes from the client. docker/buildx#825
- Allow using custom registry configuration for
buildx imagetools
command. docker/buildx#825 - Allow booting builder after creating with
buildx create --bootstrap
. docker/buildx#692 - Allow
registry:insecure
output option for multi-node pushes. docker/buildx#825 - BuildKit config and TLS files are now kept in Buildx state directory and reused if BuildKit instance needs to be recreated. docker/buildx#824
- Ensure different projects use separate destination directories for incremental context transfer for better performance. docker/buildx#817
- Build containers are now placed on separate cgroup by default. docker/buildx#782
- Bake now prints the default group with
--print
. docker/buildx#720 docker
driver now dials build session over HTTP for better performance. docker/buildx#804
Fixes
- Fix using
--iidfile
together with a multi-node push. docker/buildx#826 - Using
--push
in Bake does not clear other image export options in the file. docker/buildx#773 - Fix Git URL detection for
buildx bake
whenhttps
protocol was used. docker/buildx#822 - Fix pushing image with multiple names on multi-node builds. docker/buildx#815
- Avoid showing
--builder
flags for commands that don't use it. docker/buildx#818 - Unsupported build flags now show a warning. docker/buildx#810
- Fix reporting error details in some OpenTelemetry traces. docker/buildx#812
For more details, see the complete release notes in the Buildx GitHub repository.
0.6.3
2021-08-30Fixes
- Fix BuildKit state volume location for Windows clients. docker/buildx#751
For more details, see the complete release notes in the Buildx GitHub repository.
0.6.2
2021-08-21For more details, see the complete release notes in the Buildx GitHub repository.
Fixes
- Fix connection error showing up in some SSH configurations. docker/buildx#741
0.6.1
2021-07-30Enhancements
- Set
ConfigFile
to parse compose files with Bake. docker/buildx#704
Fixes
- Duplicate progress env var. docker/buildx#693
- Should ignore nil client. docker/buildx#686
For more details, see the complete release notes in the Buildx GitHub repository.
0.6.0
2021-07-16New features
- Support for OpenTelemetry traces and forwarding Buildx client traces to BuildKit. docker/buildx#635
- Experimental GitHub Actions remote cache backend with
--cache-to type=gha
and--cache-from type=gha
. docker/buildx#535 - New
--metadata-file
flag has been added to build and Bake command that allows saving build result metadata in JSON format. docker/buildx#605 - This is the first release supporting Windows ARM64. docker/buildx#654
- This is the first release supporting Linux Risc-V. docker/buildx#652
- Bake now supports building from remote definition with local files or another remote source as context. docker/buildx#671
- Bake now allows variables to reference each other and using user functions in variables and vice-versa. docker/buildx#575, docker/buildx#539, docker/buildx#532
- Bake allows defining attributes in the global scope. docker/buildx#541
- Bake allows variables across multiple files. docker/buildx#538
- New quiet mode has been added to progress printer. docker/buildx#558
kubernetes
driver now supports defining resources/limits. docker/buildx#618- Buildx binaries can now be accessed through buildx-bin Docker image. docker/buildx#656
Enhancements
docker-container
driver now keeps BuildKit state in volume. Enabling updates with keeping state. docker/buildx#672- Compose parser is now based on new compose-go parser fixing support for some newer syntax. docker/buildx#669
- SSH socket is now automatically forwarded when building an ssh-based git URL. docker/buildx#581
- Bake HCL parser has been rewritten. docker/buildx#645
- Extend HCL support with more functions. docker/buildx#491 docker/buildx#503
- Allow secrets from environment variables. docker/buildx#488
- Builds with an unsupported multi-platform and load configuration now fail fast. docker/buildx#582
- Store Kubernetes config file to make buildx builder switchable. docker/buildx#497
- Kubernetes now lists all pods as nodes on inspection. docker/buildx#477
- Default Rootless image has been set to
moby/buildkit:buildx-stable-1-rootless
. docker/buildx#480
Fixes
imagetools create
command now correctly merges JSON descriptor with old one. docker/buildx#592- Fix building with
--network=none
not requiring extra security entitlements. docker/buildx#531
For more details, see the complete release notes in the Buildx GitHub repository.
0.5.1
2020-12-15Fixes
- Fix regression on setting
--platform
onbuildx create
outsidekubernetes
driver. docker/buildx#475
For more details, see the complete release notes in the Buildx GitHub repository.
0.5.0
2020-12-15New features
- The
docker
driver now supports the--push
flag. docker/buildx#442 - Bake supports inline Dockerfiles. docker/buildx#398
- Bake supports building from remote URLs and Git repositories. docker/buildx#398
BUILDX_CONFIG
env var allow users to have separate buildx state from Docker config. docker/buildx#385BUILDKIT_MULTI_PLATFORM
build arg allows to force building multi-platform return objects even if only one--platform
specified. docker/buildx#467
Enhancements
- Allow
--append
to be used withkubernetes
driver. docker/buildx#370 - Build errors show error location in source files and system stacktraces
with
--debug
. docker/buildx#389 - Bake formats HCL errors with source definition. docker/buildx#391
- Bake allows empty string values in arrays that will be discarded. docker/buildx#428
- You can now use the Kubernetes cluster config with the
kubernetes
driver. docker/buildx#368 docker/buildx#460 - Creates a temporary token for pulling images instead of sharing credentials when possible. docker/buildx#469
- Ensure credentials are passed when pulling BuildKit container image. docker/buildx#441 docker/buildx#433
- Disable user namespace remapping in
docker-container
driver. docker/buildx#462 - Allow
--builder
flag to switch to default instance. docker/buildx#425 - Avoid warn on empty
BUILDX_NO_DEFAULT_LOAD
config value. docker/buildx#390 - Replace error generated by
quiet
option by a warning. docker/buildx#403 - CI has been switched to GitHub Actions. docker/buildx#451, docker/buildx#463, docker/buildx#466, docker/buildx#468, docker/buildx#471
Fixes
- Handle lowercase Dockerfile name as a fallback for backward compatibility. docker/buildx#444
For more details, see the complete release notes in the Buildx GitHub repository.
0.4.2
2020-08-22New features
- Support
cacheonly
exporter. docker/buildx#337
Enhancements
- Update
go-cty
to pull in morestdlib
functions. docker/buildx#277 - Improve error checking on load. docker/buildx#281
Fixes
- Fix parsing json config with HCL. docker/buildx#280
- Ensure
--builder
is wired from root options. docker/buildx#321 - Remove warning for multi-platform iidfile. docker/buildx#351
For more details, see the complete release notes in the Buildx GitHub repository.
0.4.1
2020-05-01Fixes
- Fix regression on flag parsing. docker/buildx#268
- Fix using pull and no-cache keys in HCL targets. docker/buildx#268
For more details, see the complete release notes in the Buildx GitHub repository.
0.4.0
2020-04-30New features
- Add
kubernetes
driver. docker/buildx#167 - New global
--builder
flag to override builder instance for a single command. docker/buildx#246 - New
prune
anddu
commands for managing local builder cache. docker/buildx#249 - You can now set the new
pull
andno-cache
options for HCL targets. docker/buildx#165
Enhancements
- Upgrade Bake to HCL2 with support for variables and functions. docker/buildx#192
- Bake now supports
--load
and--push
. docker/buildx#164 - Bake now supports wildcard overrides for multiple targets. docker/buildx#164
- Container driver allows setting environment variables via
driver-opt
. docker/buildx#170
For more details, see the complete release notes in the Buildx GitHub repository.
0.3.1
2019-09-27Enhancements
- Handle copying unix sockets instead of erroring. docker/buildx#155 moby/buildkit#1144
Fixes
- Running Bake with multiple Compose files now merges targets correctly. docker/buildx#134
- Fix bug when building a Dockerfile from stdin (
build -f -
). docker/buildx#153
For more details, see the complete release notes in the Buildx GitHub repository.
0.3.0
2019-08-02New features
- Custom
buildkitd
daemon flags. docker/buildx#102 - Driver-specific options on
create
. docker/buildx#122
Enhancements
- Environment variables are used in Compose files. docker/buildx#117
- Bake now honors
--no-cache
and--pull
. docker/buildx#118 - Custom BuildKit config file. docker/buildx#121
- Entitlements support with
build --allow
. docker/buildx#104
Fixes
- Fix bug where
--build-arg foo
would not readfoo
from environment. docker/buildx#116
For more details, see the complete release notes in the Buildx GitHub repository.
0.2.2
2019-05-30Enhancements
- Change Compose file handling to require valid service specifications. docker/buildx#87
For more details, see the complete release notes in the Buildx GitHub repository.
0.2.1
2019-05-25New features
- Add
BUILDKIT_PROGRESS
env var. docker/buildx#69 - Add
local
platform. docker/buildx#70
Enhancements
- Keep arm variant if one is defined in the config. docker/buildx#68
- Make dockerfile relative to context. docker/buildx#83
Fixes
- Fix parsing target from compose files. docker/buildx#53
For more details, see the complete release notes in the Buildx GitHub repository.
0.2.0
2019-04-25New features
- First release
For more details, see the complete release notes in the Buildx GitHub repository.",,,
ab3ae7c980ce09c2186c7e3f509a352da88f0155a5798c32701c7ac5168c602d,"Share built image between jobs with GitHub Actions
As each job is isolated in its own runner, you can't use your built image between jobs, except if you're using self-hosted runners or Docker Build Cloud. However, you can pass data between jobs in a workflow using the actions/upload-artifact and actions/download-artifact actions:
name: ci
on:
push:
jobs:
build:
runs-on: ubuntu-latest
steps:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build and export
uses: docker/build-push-action@v6
with:
tags: myimage:latest
outputs: type=docker,dest=${{ runner.temp }}/myimage.tar
- name: Upload artifact
uses: actions/upload-artifact@v4
with:
name: myimage
path: ${{ runner.temp }}/myimage.tar
use:
runs-on: ubuntu-latest
needs: build
steps:
- name: Download artifact
uses: actions/download-artifact@v4
with:
name: myimage
path: ${{ runner.temp }}
- name: Load image
run: |
docker load --input ${{ runner.temp }}/myimage.tar
docker image ls -a",,,
b7050a7d9a3b6b6f32352ada3492d20ca5e8d905f01082abdab0b9883dfe0425,"Manage sensitive data with Docker secrets
About secrets
In terms of Docker Swarm services, a secret is a blob of data, such as a password, SSH private key, SSL certificate, or another piece of data that should not be transmitted over a network or stored unencrypted in a Dockerfile or in your application's source code. You can use Docker secrets to centrally manage this data and securely transmit it to only those containers that need access to it. Secrets are encrypted during transit and at rest in a Docker swarm. A given secret is only accessible to those services which have been granted explicit access to it, and only while those service tasks are running.
You can use secrets to manage any sensitive data which a container needs at runtime but you don't want to store in the image or in source control, such as:
- Usernames and passwords
- TLS certificates and keys
- SSH keys
- Other important data such as the name of a database or internal server
- Generic strings or binary content (up to 500 kb in size)
Note
Docker secrets are only available to swarm services, not to standalone containers. To use this feature, consider adapting your container to run as a service. Stateful containers can typically run with a scale of 1 without changing the container code.
Another use case for using secrets is to provide a layer of abstraction between the container and a set of credentials. Consider a scenario where you have separate development, test, and production environments for your application. Each of these environments can have different credentials, stored in the development, test, and production swarms with the same secret name. Your containers only need to know the name of the secret to function in all three environments.
You can also use secrets to manage non-sensitive data, such as configuration files. However, Docker supports the use of configs for storing non-sensitive data. Configs are mounted into the container's filesystem directly, without the use of a RAM disk.
Windows support
Docker includes support for secrets on Windows containers. Where there are differences in the implementations, they are called out in the examples below. Keep the following notable differences in mind:
Microsoft Windows has no built-in driver for managing RAM disks, so within running Windows containers, secrets are persisted in clear text to the container's root disk. However, the secrets are explicitly removed when a container stops. In addition, Windows does not support persisting a running container as an image using
docker commit
or similar commands.On Windows, we recommend enabling BitLocker on the volume containing the Docker root directory on the host machine to ensure that secrets for running containers are encrypted at rest.
Secret files with custom targets are not directly bind-mounted into Windows containers, since Windows does not support non-directory file bind-mounts. Instead, secrets for a container are all mounted in
C:\ProgramData\Docker\internal\secrets
(an implementation detail which should not be relied upon by applications) within the container. Symbolic links are used to point from there to the desired target of the secret within the container. The default target isC:\ProgramData\Docker\secrets
.When creating a service which uses Windows containers, the options to specify UID, GID, and mode are not supported for secrets. Secrets are currently only accessible by administrators and users with
system
access within the container.
How Docker manages secrets
When you add a secret to the swarm, Docker sends the secret to the swarm manager over a mutual TLS connection. The secret is stored in the Raft log, which is encrypted. The entire Raft log is replicated across the other managers, ensuring the same high availability guarantees for secrets as for the rest of the swarm management data.
When you grant a newly-created or running service access to a secret, the
decrypted secret is mounted into the container in an in-memory filesystem. The
location of the mount point within the container defaults to
/run/secrets/<secret_name>
in Linux containers, or
C:\ProgramData\Docker\secrets
in Windows containers. You can also specify a
custom location.
You can update a service to grant it access to additional secrets or revoke its access to a given secret at any time.
A node only has access to (encrypted) secrets if the node is a swarm manager or if it is running service tasks which have been granted access to the secret. When a container task stops running, the decrypted secrets shared to it are unmounted from the in-memory filesystem for that container and flushed from the node's memory.
If a node loses connectivity to the swarm while it is running a task container with access to a secret, the task container still has access to its secrets, but cannot receive updates until the node reconnects to the swarm.
You can add or inspect an individual secret at any time, or list all secrets. You cannot remove a secret that a running service is using. See Rotate a secret for a way to remove a secret without disrupting running services.
To update or roll back secrets more easily, consider adding a version number or date to the secret name. This is made easier by the ability to control the mount point of the secret within a given container.
Read more about docker secret
commands
Use these links to read about specific commands, or continue to the example about using secrets with a service.
docker secret create
docker secret inspect
docker secret ls
docker secret rm
--secret
flag fordocker service create
--secret-add
and--secret-rm
flags fordocker service update
Examples
This section includes three graduated examples which illustrate how to use Docker secrets. The images used in these examples have been updated to make it easier to use Docker secrets. To find out how to modify your own images in a similar way, see Build support for Docker Secrets into your images.
Note
These examples use a single-Engine swarm and unscaled services for simplicity. The examples use Linux containers, but Windows containers also support secrets. See Windows support.
Defining and using secrets in compose files
Both the docker-compose
and docker stack
commands support defining secrets
in a compose file. See
the Compose file reference for details.
Simple example: Get started with secrets
This simple example shows how secrets work in just a few commands. For a real-world example, continue to Intermediate example: Use secrets with a Nginx service.
Add a secret to Docker. The
docker secret create
command reads standard input because the last argument, which represents the file to read the secret from, is set to-
.$ printf ""This is a secret"" | docker secret create my_secret_data -
Create a
redis
service and grant it access to the secret. By default, the container can access the secret at/run/secrets/<secret_name>
, but you can customize the file name on the container using thetarget
option.$ docker service create --name redis --secret my_secret_data redis:alpine
Verify that the task is running without issues using
docker service ps
. If everything is working, the output looks similar to this:$ docker service ps redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS bkna6bpn8r1a redis.1 redis:alpine ip-172-31-46-109 Running Running 8 seconds ago
If there were an error, and the task were failing and repeatedly restarting, you would see something like this:
$ docker service ps redis NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS redis.1.siftice35gla redis:alpine moby Running Running 4 seconds ago \_ redis.1.whum5b7gu13e redis:alpine moby Shutdown Failed 20 seconds ago ""task: non-zero exit (1)"" \_ redis.1.2s6yorvd9zow redis:alpine moby Shutdown Failed 56 seconds ago ""task: non-zero exit (1)"" \_ redis.1.ulfzrcyaf6pg redis:alpine moby Shutdown Failed about a minute ago ""task: non-zero exit (1)"" \_ redis.1.wrny5v4xyps6 redis:alpine moby Shutdown Failed 2 minutes ago ""task: non-zero exit (1)""
Get the ID of the
redis
service task container usingdocker ps
, so that you can usedocker container exec
to connect to the container and read the contents of the secret data file, which defaults to being readable by all and has the same name as the name of the secret. The first command below illustrates how to find the container ID, and the second and third commands use shell completion to do this automatically.$ docker ps --filter name=redis -q 5cb1c2348a59 $ docker container exec $(docker ps --filter name=redis -q) ls -l /run/secrets total 4 -r--r--r-- 1 root root 17 Dec 13 22:48 my_secret_data $ docker container exec $(docker ps --filter name=redis -q) cat /run/secrets/my_secret_data This is a secret
Verify that the secret is not available if you commit the container.
$ docker commit $(docker ps --filter name=redis -q) committed_redis $ docker run --rm -it committed_redis cat /run/secrets/my_secret_data cat: can't open '/run/secrets/my_secret_data': No such file or directory
Try removing the secret. The removal fails because the
redis
service is running and has access to the secret.$ docker secret ls ID NAME CREATED UPDATED wwwrxza8sxy025bas86593fqs my_secret_data 4 hours ago 4 hours ago $ docker secret rm my_secret_data Error response from daemon: rpc error: code = 3 desc = secret 'my_secret_data' is in use by the following service: redis
Remove access to the secret from the running
redis
service by updating the service.$ docker service update --secret-rm my_secret_data redis
Repeat steps 3 and 4 again, verifying that the service no longer has access to the secret. The container ID is different, because the
service update
command redeploys the service.$ docker container exec -it $(docker ps --filter name=redis -q) cat /run/secrets/my_secret_data cat: can't open '/run/secrets/my_secret_data': No such file or directory
Stop and remove the service, and remove the secret from Docker.
$ docker service rm redis $ docker secret rm my_secret_data
Simple example: Use secrets in a Windows service
This is a very simple example which shows how to use secrets with a Microsoft IIS service running on Docker for Windows running Windows containers on Microsoft Windows 10. It is a naive example that stores the webpage in a secret.
This example assumes that you have PowerShell installed.
Save the following into a new file
index.html
.<html lang=""en""> <head><title>Hello Docker</title></head> <body> <p>Hello Docker! You have deployed a HTML page.</p> </body> </html>
If you have not already done so, initialize or join the swarm.
> docker swarm init
Save the
index.html
file as a swarm secret namedhomepage
.> docker secret create homepage index.html
Create an IIS service and grant it access to the
homepage
secret.> docker service create ` --name my-iis ` --publish published=8000,target=8000 ` --secret src=homepage,target=""\inetpub\wwwroot\index.html"" ` microsoft/iis:nanoserver
Note
There is technically no reason to use secrets for this example; configs are a better fit. This example is for illustration only.
Access the IIS service at
http://localhost:8000/
. It should serve the HTML content from the first step.Remove the service and the secret.
> docker service rm my-iis > docker secret rm homepage > docker image remove secret-test
Intermediate example: Use secrets with a Nginx service
This example is divided into two parts. The first part is all about generating the site certificate and does not directly involve Docker secrets at all, but it sets up the second part, where you store and use the site certificate and Nginx configuration as secrets.
Generate the site certificate
Generate a root CA and TLS certificate and key for your site. For production
sites, you may want to use a service such as Let’s Encrypt
to generate the
TLS certificate and key, but this example uses command-line tools. This step
is a little complicated, but is only a set-up step so that you have
something to store as a Docker secret. If you want to skip these sub-steps,
you can
use Let's Encrypt to
generate the site key and certificate, name the files site.key
and
site.crt
, and skip to
Configure the Nginx container.
Generate a root key.
$ openssl genrsa -out ""root-ca.key"" 4096
Generate a CSR using the root key.
$ openssl req \ -new -key ""root-ca.key"" \ -out ""root-ca.csr"" -sha256 \ -subj '/C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA'
Configure the root CA. Edit a new file called
root-ca.cnf
and paste the following contents into it. This constrains the root CA to signing leaf certificates and not intermediate CAs.[root_ca] basicConstraints = critical,CA:TRUE,pathlen:1 keyUsage = critical, nonRepudiation, cRLSign, keyCertSign subjectKeyIdentifier=hash
Sign the certificate.
$ openssl x509 -req -days 3650 -in ""root-ca.csr"" \ -signkey ""root-ca.key"" -sha256 -out ""root-ca.crt"" \ -extfile ""root-ca.cnf"" -extensions \ root_ca
Generate the site key.
$ openssl genrsa -out ""site.key"" 4096
Generate the site certificate and sign it with the site key.
$ openssl req -new -key ""site.key"" -out ""site.csr"" -sha256 \ -subj '/C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost'
Configure the site certificate. Edit a new file called
site.cnf
and paste the following contents into it. This constrains the site certificate so that it can only be used to authenticate a server and can't be used to sign certificates.[server] authorityKeyIdentifier=keyid,issuer basicConstraints = critical,CA:FALSE extendedKeyUsage=serverAuth keyUsage = critical, digitalSignature, keyEncipherment subjectAltName = DNS:localhost, IP:127.0.0.1 subjectKeyIdentifier=hash
Sign the site certificate.
$ openssl x509 -req -days 750 -in ""site.csr"" -sha256 \ -CA ""root-ca.crt"" -CAkey ""root-ca.key"" -CAcreateserial \ -out ""site.crt"" -extfile ""site.cnf"" -extensions server
The
site.csr
andsite.cnf
files are not needed by the Nginx service, but you need them if you want to generate a new site certificate. Protect theroot-ca.key
file.
Configure the Nginx container
Produce a very basic Nginx configuration that serves static files over HTTPS. The TLS certificate and key are stored as Docker secrets so that they can be rotated easily.
In the current directory, create a new file called
site.conf
with the following contents:server { listen 443 ssl; server_name localhost; ssl_certificate /run/secrets/site.crt; ssl_certificate_key /run/secrets/site.key; location / { root /usr/share/nginx/html; index index.html index.htm; } }
Create three secrets, representing the key, the certificate, and the
site.conf
. You can store any file as a secret as long as it is smaller than 500 KB. This allows you to decouple the key, certificate, and configuration from the services that use them. In each of these commands, the last argument represents the path to the file to read the secret from on the host machine's filesystem. In these examples, the secret name and the file name are the same.$ docker secret create site.key site.key $ docker secret create site.crt site.crt $ docker secret create site.conf site.conf
$ docker secret ls ID NAME CREATED UPDATED 2hvoi9mnnaof7olr3z5g3g7fp site.key 58 seconds ago 58 seconds ago aya1dh363719pkiuoldpter4b site.crt 24 seconds ago 24 seconds ago zoa5df26f7vpcoz42qf2csth8 site.conf 11 seconds ago 11 seconds ago
Create a service that runs Nginx and has access to the three secrets. The last part of the
docker service create
command creates a symbolic link from the location of thesite.conf
secret to/etc/nginx.conf.d/
, where Nginx looks for extra configuration files. This step happens before Nginx actually starts, so you don't need to rebuild your image if you change the Nginx configuration.Note
Normally you would create a Dockerfile which copies the
site.conf
into place, build the image, and run a container using your custom image. This example does not require a custom image. It puts thesite.conf
into place and runs the container all in one step.Secrets are located within the
/run/secrets/
directory in the container by default, which may require extra steps in the container to make the secret available in a different path. The example below creates a symbolic link to the true location of thesite.conf
file so that Nginx can read it:$ docker service create \ --name nginx \ --secret site.key \ --secret site.crt \ --secret site.conf \ --publish published=3000,target=443 \ nginx:latest \ sh -c ""ln -s /run/secrets/site.conf /etc/nginx/conf.d/site.conf && exec nginx -g 'daemon off;'""
Instead of creating symlinks, secrets allow you to specify a custom location using the
target
option. The example below illustrates how thesite.conf
secret is made available at/etc/nginx/conf.d/site.conf
inside the container without the use of symbolic links:$ docker service create \ --name nginx \ --secret site.key \ --secret site.crt \ --secret source=site.conf,target=/etc/nginx/conf.d/site.conf \ --publish published=3000,target=443 \ nginx:latest \ sh -c ""exec nginx -g 'daemon off;'""
The
site.key
andsite.crt
secrets use the short-hand syntax, without a customtarget
location set. The short syntax mounts the secrets in `/run/secrets/ with the same name as the secret. Within the running containers, the following three files now exist:/run/secrets/site.key
/run/secrets/site.crt
/etc/nginx/conf.d/site.conf
Verify that the Nginx service is running.
$ docker service ls ID NAME MODE REPLICAS IMAGE zeskcec62q24 nginx replicated 1/1 nginx:latest $ docker service ps nginx NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS nginx.1.9ls3yo9ugcls nginx:latest moby Running Running 3 minutes ago
Verify that the service is operational: you can reach the Nginx server, and that the correct TLS certificate is being used.
$ curl --cacert root-ca.crt https://localhost:3000 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support. refer to <a href=""https://nginx.org"">nginx.org</a>.<br/> Commercial support is available at <a href=""https://www.nginx.com"">nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html>
$ openssl s_client -connect localhost:3000 -CAfile root-ca.crt CONNECTED(00000003) depth=1 /C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA verify return:1 depth=0 /C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost verify return:1 --- Certificate chain 0 s:/C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost i:/C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA --- Server certificate -----BEGIN CERTIFICATE----- … -----END CERTIFICATE----- subject=/C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost issuer=/C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA --- No client certificate CA names sent --- SSL handshake has read 1663 bytes and written 712 bytes --- New, TLSv1/SSLv3, Cipher is AES256-SHA Server public key is 4096 bit Secure Renegotiation IS supported Compression: NONE Expansion: NONE SSL-Session: Protocol : TLSv1 Cipher : AES256-SHA Session-ID: A1A8BF35549C5715648A12FD7B7E3D861539316B03440187D9DA6C2E48822853 Session-ID-ctx: Master-Key: F39D1B12274BA16D3A906F390A61438221E381952E9E1E05D3DD784F0135FB81353DA38C6D5C021CB926E844DFC49FC4 Key-Arg : None Start Time: 1481685096 Timeout : 300 (sec) Verify return code: 0 (ok)
To clean up after running this example, remove the
nginx
service and the stored secrets.$ docker service rm nginx $ docker secret rm site.crt site.key site.conf
Advanced example: Use secrets with a WordPress service
In this example, you create a single-node MySQL service with a custom root password, add the credentials as secrets, and create a single-node WordPress service which uses these credentials to connect to MySQL. The next example builds on this one and shows you how to rotate the MySQL password and update the services so that the WordPress service can still connect to MySQL.
This example illustrates some techniques to use Docker secrets to avoid saving sensitive credentials within your image or passing them directly on the command line.
Note
This example uses a single-Engine swarm for simplicity, and uses a single-node MySQL service because a single MySQL server instance cannot be scaled by simply using a replicated service, and setting up a MySQL cluster is beyond the scope of this example.
Also, changing a MySQL root passphrase isn’t as simple as changing a file on disk. You must use a query or a
mysqladmin
command to change the password in MySQL.
Generate a random alphanumeric password for MySQL and store it as a Docker secret with the name
mysql_password
using thedocker secret create
command. To make the password shorter or longer, adjust the last argument of theopenssl
command. This is just one way to create a relatively random password. You can use another command to generate the password if you choose.Note
After you create a secret, you cannot update it. You can only remove and re-create it, and you cannot remove a secret that a service is using. However, you can grant or revoke a running service's access to secrets using
docker service update
. If you need the ability to update a secret, consider adding a version component to the secret name, so that you can later add a new version, update the service to use it, then remove the old version.The last argument is set to
-
, which indicates that the input is read from standard input.$ openssl rand -base64 20 | docker secret create mysql_password - l1vinzevzhj4goakjap5ya409
The value returned is not the password, but the ID of the secret. In the remainder of this tutorial, the ID output is omitted.
Generate a second secret for the MySQL
root
user. This secret isn't shared with the WordPress service created later. It's only needed to bootstrap themysql
service.$ openssl rand -base64 20 | docker secret create mysql_root_password -
List the secrets managed by Docker using
docker secret ls
:$ docker secret ls ID NAME CREATED UPDATED l1vinzevzhj4goakjap5ya409 mysql_password 41 seconds ago 41 seconds ago yvsczlx9votfw3l0nz5rlidig mysql_root_password 12 seconds ago 12 seconds ago
The secrets are stored in the encrypted Raft logs for the swarm.
Create a user-defined overlay network which is used for communication between the MySQL and WordPress services. There is no need to expose the MySQL service to any external host or container.
$ docker network create -d overlay mysql_private
Create the MySQL service. The MySQL service has the following characteristics:
Because the scale is set to
1
, only a single MySQL task runs. Load-balancing MySQL is left as an exercise to the reader and involves more than just scaling the service.Only reachable by other containers on the
mysql_private
network.Uses the volume
mydata
to store the MySQL data, so that it persists across restarts to themysql
service.The secrets are each mounted in a
tmpfs
filesystem at/run/secrets/mysql_password
and/run/secrets/mysql_root_password
. They are never exposed as environment variables, nor can they be committed to an image if thedocker commit
command is run. Themysql_password
secret is the one used by the non-privileged WordPress container to connect to MySQL.Sets the environment variables
MYSQL_PASSWORD_FILE
andMYSQL_ROOT_PASSWORD_FILE
to point to the files/run/secrets/mysql_password
and/run/secrets/mysql_root_password
. Themysql
image reads the password strings from those files when initializing the system database for the first time. Afterward, the passwords are stored in the MySQL system database itself.Sets environment variables
MYSQL_USER
andMYSQL_DATABASE
. A new database calledwordpress
is created when the container starts, and thewordpress
user has full permissions for this database only. This user cannot create or drop databases or change the MySQL configuration.$ docker service create \ --name mysql \ --replicas 1 \ --network mysql_private \ --mount type=volume,source=mydata,destination=/var/lib/mysql \ --secret source=mysql_root_password,target=mysql_root_password \ --secret source=mysql_password,target=mysql_password \ -e MYSQL_ROOT_PASSWORD_FILE=""/run/secrets/mysql_root_password"" \ -e MYSQL_PASSWORD_FILE=""/run/secrets/mysql_password"" \ -e MYSQL_USER=""wordpress"" \ -e MYSQL_DATABASE=""wordpress"" \ mysql:latest
Verify that the
mysql
container is running using thedocker service ls
command.$ docker service ls ID NAME MODE REPLICAS IMAGE wvnh0siktqr3 mysql replicated 1/1 mysql:latest
Now that MySQL is set up, create a WordPress service that connects to the MySQL service. The WordPress service has the following characteristics:
- Because the scale is set to
1
, only a single WordPress task runs. Load-balancing WordPress is left as an exercise to the reader, because of limitations with storing WordPress session data on the container filesystem. - Exposes WordPress on port 30000 of the host machine, so that you can access it from external hosts. You can expose port 80 instead if you do not have a web server running on port 80 of the host machine.
- Connects to the
mysql_private
network so it can communicate with themysql
container, and also publishes port 80 to port 30000 on all swarm nodes. - Has access to the
mysql_password
secret, but specifies a different target file name within the container. The WordPress container uses the mount point/run/secrets/wp_db_password
. - Sets the environment variable
WORDPRESS_DB_PASSWORD_FILE
to the file path where the secret is mounted. The WordPress service reads the MySQL password string from that file and add it to thewp-config.php
configuration file. - Connects to the MySQL container using the username
wordpress
and the password in/run/secrets/wp_db_password
and creates thewordpress
database if it does not yet exist. - Stores its data, such as themes and plugins, in a volume called
wpdata
so these files persist when the service restarts.
$ docker service create \ --name wordpress \ --replicas 1 \ --network mysql_private \ --publish published=30000,target=80 \ --mount type=volume,source=wpdata,destination=/var/www/html \ --secret source=mysql_password,target=wp_db_password \ -e WORDPRESS_DB_USER=""wordpress"" \ -e WORDPRESS_DB_PASSWORD_FILE=""/run/secrets/wp_db_password"" \ -e WORDPRESS_DB_HOST=""mysql:3306"" \ -e WORDPRESS_DB_NAME=""wordpress"" \ wordpress:latest
- Because the scale is set to
Verify the service is running using
docker service ls
anddocker service ps
commands.$ docker service ls ID NAME MODE REPLICAS IMAGE wvnh0siktqr3 mysql replicated 1/1 mysql:latest nzt5xzae4n62 wordpress replicated 1/1 wordpress:latest
$ docker service ps wordpress ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS aukx6hgs9gwc wordpress.1 wordpress:latest moby Running Running 52 seconds ago
At this point, you could actually revoke the WordPress service's access to the
mysql_password
secret, because WordPress has copied the secret to its configuration filewp-config.php
. Don't do that for now, because we use it later to facilitate rotating the MySQL password.Access
http://localhost:30000/
from any swarm node and set up WordPress using the web-based wizard. All of these settings are stored in the MySQLwordpress
database. WordPress automatically generates a password for your WordPress user, which is completely different from the password WordPress uses to access MySQL. Store this password securely, such as in a password manager. You need it to log into WordPress after rotating the secret.Go ahead and write a blog post or two and install a WordPress plugin or theme to verify that WordPress is fully operational and its state is saved across service restarts.
Do not clean up any services or secrets if you intend to proceed to the next example, which demonstrates how to rotate the MySQL root password.
Example: Rotate a secret
This example builds upon the previous one. In this scenario, you create a new
secret with a new MySQL password, update the mysql
and wordpress
services to
use it, then remove the old secret.
Note
Changing the password on a MySQL database involves running extra queries or commands, as opposed to just changing a single environment variable or a file, since the image only sets the MySQL password if the database doesn’t already exist, and MySQL stores the password within a MySQL database by default. Rotating passwords or other secrets may involve additional steps outside of Docker.
Create the new password and store it as a secret named
mysql_password_v2
.$ openssl rand -base64 20 | docker secret create mysql_password_v2 -
Update the MySQL service to give it access to both the old and new secrets. Remember that you cannot update or rename a secret, but you can revoke a secret and grant access to it using a new target filename.
$ docker service update \ --secret-rm mysql_password mysql $ docker service update \ --secret-add source=mysql_password,target=old_mysql_password \ --secret-add source=mysql_password_v2,target=mysql_password \ mysql
Updating a service causes it to restart, and when the MySQL service restarts the second time, it has access to the old secret under
/run/secrets/old_mysql_password
and the new secret under/run/secrets/mysql_password
.Even though the MySQL service has access to both the old and new secrets now, the MySQL password for the WordPress user has not yet been changed.
Note
This example does not rotate the MySQL
root
password.Now, change the MySQL password for the
wordpress
user using themysqladmin
CLI. This command reads the old and new password from the files in/run/secrets
but does not expose them on the command line or save them in the shell history.Do this quickly and move on to the next step, because WordPress loses the ability to connect to MySQL.
First, find the ID of the
mysql
container task.$ docker ps --filter name=mysql -q c7705cf6176f
Substitute the ID in the command below, or use the second variant which uses shell expansion to do it all in a single step.
$ docker container exec <CONTAINER_ID> \ bash -c 'mysqladmin --user=wordpress --password=""$(< /run/secrets/old_mysql_password)"" password ""$(< /run/secrets/mysql_password)""'
Or:
$ docker container exec $(docker ps --filter name=mysql -q) \ bash -c 'mysqladmin --user=wordpress --password=""$(< /run/secrets/old_mysql_password)"" password ""$(< /run/secrets/mysql_password)""'
Update the
wordpress
service to use the new password, keeping the target path at/run/secrets/wp_db_password
. This triggers a rolling restart of the WordPress service and the new secret is used.$ docker service update \ --secret-rm mysql_password \ --secret-add source=mysql_password_v2,target=wp_db_password \ wordpress
Verify that WordPress works by browsing to http://localhost:30000/ on any swarm node again. Use the WordPress username and password from when you ran through the WordPress wizard in the previous task.
Verify that the blog post you wrote still exists, and if you changed any configuration values, verify that they are still changed.
Revoke access to the old secret from the MySQL service and remove the old secret from Docker.
$ docker service update \ --secret-rm mysql_password \ mysql $ docker secret rm mysql_password
Run the following commands to remove the WordPress service, the MySQL container, the
mydata
andwpdata
volumes, and the Docker secrets:$ docker service rm wordpress mysql $ docker volume rm mydata wpdata $ docker secret rm mysql_password_v2 mysql_root_password
Build support for Docker Secrets into your images
If you develop a container that can be deployed as a service and requires sensitive data, such as a credential, as an environment variable, consider adapting your image to take advantage of Docker secrets. One way to do this is to ensure that each parameter you pass to the image when creating the container can also be read from a file.
Many of the Docker Official Images in the Docker library, such as the wordpress image used in the above examples, have been updated in this way.
When you start a WordPress container, you provide it with the parameters it
needs by setting them as environment variables. The WordPress image has been
updated so that the environment variables which contain important data for
WordPress, such as WORDPRESS_DB_PASSWORD
, also have variants which can read
their values from a file (WORDPRESS_DB_PASSWORD_FILE
). This strategy ensures
that backward compatibility is preserved, while allowing your container to read
the information from a Docker-managed secret instead of being passed directly.
Note
Docker secrets do not set environment variables directly. This was a conscious decision, because environment variables can unintentionally be leaked between containers (for instance, if you use
--link
).
Use Secrets in Compose
services:
db:
image: mysql:latest
volumes:
- db_data:/var/lib/mysql
environment:
MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password
MYSQL_DATABASE: wordpress
MYSQL_USER: wordpress
MYSQL_PASSWORD_FILE: /run/secrets/db_password
secrets:
- db_root_password
- db_password
wordpress:
depends_on:
- db
image: wordpress:latest
ports:
- ""8000:80""
environment:
WORDPRESS_DB_HOST: db:3306
WORDPRESS_DB_USER: wordpress
WORDPRESS_DB_PASSWORD_FILE: /run/secrets/db_password
secrets:
- db_password
secrets:
db_password:
file: db_password.txt
db_root_password:
file: db_root_password.txt
volumes:
db_data:
This example creates a simple WordPress site using two secrets in a Compose file.
The top-level element secrets
defines two secrets db_password
and
db_root_password
.
When deploying, Docker creates these two secrets and populates them with the content from the file specified in the Compose file.
The db
service uses both secrets, and wordpress
is using one.
When you deploy, Docker mounts a file under /run/secrets/<secret_name>
in the
services. These files are never persisted on disk, but are managed in memory.
Each service uses environment variables to specify where the service should look for that secret data.
More information on short and long syntax for secrets can be found in the Compose Specification.",,,
12f1a78cd91dfc19a77a854b705bc16daa2303aaa7a94bc81b051edb4f7f24a0,"Build attestations
Build attestations describe how an image was built, and what it contains. The attestations are created at build-time by BuildKit, and become attached to the final image as metadata.
The purpose of attestations is to make it possible to inspect an image and see where it comes from, who created it and how, and what it contains. This enables you to make informed decisions about how an image impacts the supply chain security of your application. It also enables the use of policy engines for validating images based on policy rules you've defined.
Two types of build annotations are available:
- Software Bill of Material (SBOM): list of software artifacts that an image contains, or that were used to build the image.
- Provenance: how an image was built.
Purpose of attestations
The use of open source and third-party packages is more widespread than ever before. Developers share and reuse code because it helps increase productivity, allowing teams to create better products, faster.
Importing and using code created elsewhere without vetting it introduces a severe security risk. Even if you do review the software that you consume, new zero-day vulnerabilities are frequently discovered, requiring development teams take action to remediate them.
Build attestations make it easier to see the contents of an image, and where it comes from. Use attestations to analyze and decide whether to use an image, or to see if images you are already using are exposed to vulnerabilities.
Creating attestations
When you build an image with docker buildx build
, you can add attestation
records to the resulting image using the --provenance
and --sbom
options.
You can opt in to add either the SBOM or provenance attestation type, or both.
$ docker buildx build --sbom=true --provenance=true .
Note
The default image store doesn't support attestations. If you're using the default image store and you build an image using the default
docker
driver, or using a different driver with the--load
flag, the attestations are lost.To make sure the attestations are preserved, you can:
- Use a
docker-container
driver with the--push
flag to push the image to a registry directly.- Enable the containerd image store.
Note
Provenance attestations are enabled by default, with the
mode=min
option. You can disable provenance attestations using the--provenance=false
flag, or by setting theBUILDX_NO_DEFAULT_ATTESTATIONS
environment variable.Using the
--provenance=true
flag attaches provenance attestations withmode=max
by default. See Provenance attestation for more details.
BuildKit generates the attestations when building the image. The attestation records are wrapped in the in-toto JSON format and attached to the image index in a manifest for the final image.
Storage
BuildKit produces attestations in the in-toto format, as defined by the in-toto framework, a standard supported by the Linux Foundation.
Attestations attach to images as a manifest in the image index. The data records of the attestations are stored as JSON blobs.
Because attestations attach to images as a manifest, it means that you can inspect the attestations for any image in a registry without having to pull the whole image.
All BuildKit exporters support attestations. The local
and tar
can't save
the attestations to an image manifest, since it's outputting a directory of
files or a tarball, not an image. Instead, these exporters write the
attestations to one or more JSON files in the root directory of the export.
Example
The following example shows a truncated in-toto JSON representation of an SBOM attestation.
{
""_type"": ""https://in-toto.io/Statement/v0.1"",
""predicateType"": ""https://spdx.dev/Document"",
""subject"": [
{
""name"": ""pkg:docker/<registry>/<image>@<tag/digest>?platform=<platform>"",
""digest"": {
""sha256"": ""e8275b2b76280af67e26f068e5d585eb905f8dfd2f1918b3229db98133cb4862""
}
}
],
""predicate"": {
""SPDXID"": ""SPDXRef-DOCUMENT"",
""creationInfo"": {
""created"": ""2022-12-15T11:47:54.546747383Z"",
""creators"": [""Organization: Anchore, Inc"", ""Tool: syft-v0.60.3""],
""licenseListVersion"": ""3.18""
},
""dataLicense"": ""CC0-1.0"",
""documentNamespace"": ""https://anchore.com/syft/dir/run/src/core-da0f600b-7f0a-4de0-8432-f83703e6bc4f"",
""name"": ""/run/src/core"",
// list of files that the image contains, e.g.:
""files"": [
{
""SPDXID"": ""SPDXRef-1ac501c94e2f9f81"",
""comment"": ""layerID: sha256:9b18e9b68314027565b90ff6189d65942c0f7986da80df008b8431276885218e"",
""fileName"": ""/bin/busybox"",
""licenseConcluded"": ""NOASSERTION""
}
],
// list of packages that were identified for this image:
""packages"": [
{
""name"": ""busybox"",
""originator"": ""Person: Sören Tempel <soeren+alpine@soeren-tempel.net>"",
""sourceInfo"": ""acquired package info from APK DB: lib/apk/db/installed"",
""versionInfo"": ""1.35.0-r17"",
""SPDXID"": ""SPDXRef-980737451f148c56"",
""description"": ""Size optimized toolbox of many common UNIX utilities"",
""downloadLocation"": ""https://busybox.net/"",
""licenseConcluded"": ""GPL-2.0-only"",
""licenseDeclared"": ""GPL-2.0-only""
// ...
}
],
// files-packages relationship
""relationships"": [
{
""relatedSpdxElement"": ""SPDXRef-1ac501c94e2f9f81"",
""relationshipType"": ""CONTAINS"",
""spdxElementId"": ""SPDXRef-980737451f148c56""
},
...
],
""spdxVersion"": ""SPDX-2.2""
}
}
To deep-dive into the specifics about how attestations are stored, see Image Attestation Storage (BuildKit).
Attestation manifest format
Attestations are stored as manifests, referenced by the image's index. Each attestation manifest refers to a single image manifest (one platform-variant of the image). Attestation manifests contain a single layer, the ""value"" of the attestation.
The following example shows the structure of an attestation manifest:
{
""schemaVersion"": 2,
""mediaType"": ""application/vnd.oci.image.manifest.v1+json"",
""config"": {
""mediaType"": ""application/vnd.oci.image.config.v1+json"",
""size"": 167,
""digest"": ""sha256:916d7437a36dd0e258e64d9c5a373ca5c9618eeb1555e79bd82066e593f9afae""
},
""layers"": [
{
""mediaType"": ""application/vnd.in-toto+json"",
""size"": 1833349,
""digest"": ""sha256:3138024b98ed5aa8e3008285a458cd25a987202f2500ce1a9d07d8e1420f5491"",
""annotations"": {
""in-toto.io/predicate-type"": ""https://spdx.dev/Document""
}
}
]
}
Attestations as OCI artifacts
You can configure the format of the attestation manifest using the
oci-artifact
option
for the image
and registry
exporters. If set to true
, the structure of
the attestation manifest changes as follows:
- An
artifactType
field is added to the attestation manifest, with a value ofapplication/vnd.docker.attestation.manifest.v1+json
. - The
config
field is an empty descriptor instead of a ""dummy"" config. - A
subject
field is also added, pointing to the image manifest that the attestation refers to.
The following example shows an attestation with the OCI artifact format:
{
""schemaVersion"": 2,
""mediaType"": ""application/vnd.oci.image.manifest.v1+json"",
""artifactType"": ""application/vnd.docker.attestation.manifest.v1+json"",
""config"": {
""mediaType"": ""application/vnd.oci.empty.v1+json"",
""size"": 2,
""digest"": ""sha256:44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a"",
""data"": ""e30=""
},
""layers"": [
{
""mediaType"": ""application/vnd.in-toto+json"",
""size"": 2208,
""digest"": ""sha256:6d2f2c714a6bee3cf9e4d3cb9a966b629efea2dd8556ed81f19bd597b3325286"",
""annotations"": {
""in-toto.io/predicate-type"": ""https://slsa.dev/provenance/v0.2""
}
}
],
""subject"": {
""mediaType"": ""application/vnd.oci.image.manifest.v1+json"",
""size"": 1054,
""digest"": ""sha256:bc2046336420a2852ecf915786c20f73c4c1b50d7803aae1fd30c971a7d1cead"",
""platform"": {
""architecture"": ""amd64"",
""os"": ""linux""
}
}
}
What's next
Learn more about the available attestation types and how to use them:",,,
91483e5d777c14b4417824994e8995b39f2e28b933d108042fb0c9d145812256,"Protect the Docker daemon socket
By default, Docker runs through a non-networked UNIX socket. It can also optionally communicate using SSH or a TLS (HTTPS) socket.
Use SSH to protect the Docker daemon socket
Note
The given
USERNAME
must have permissions to access the docker socket on the remote machine. Refer to manage Docker as a non-root user to learn how to give a non-root user access to the docker socket.
The following example creates a
docker context
to connect with a remote dockerd
daemon on host1.example.com
using SSH, and
as the docker-user
user on the remote machine:
$ docker context create \
--docker host=ssh://docker-user@host1.example.com \
--description=""Remote engine"" \
my-remote-engine
my-remote-engine
Successfully created context ""my-remote-engine""
After creating the context, use docker context use
to switch the docker
CLI
to use it, and to connect to the remote engine:
$ docker context use my-remote-engine
my-remote-engine
Current context is now ""my-remote-engine""
$ docker info
<prints output of the remote engine>
Use the default
context to switch back to the default (local) daemon:
$ docker context use default
default
Current context is now ""default""
Alternatively, use the DOCKER_HOST
environment variable to temporarily switch
the docker
CLI to connect to the remote host using SSH. This does not require
creating a context, and can be useful to create an ad-hoc connection with a different
engine:
$ export DOCKER_HOST=ssh://docker-user@host1.example.com
$ docker info
<prints output of the remote engine>
SSH Tips
For the best user experience with SSH, configure ~/.ssh/config
as follows to allow
reusing a SSH connection for multiple invocations of the docker
CLI:
ControlMaster auto
ControlPath ~/.ssh/control-%C
ControlPersist yes
Use TLS (HTTPS) to protect the Docker daemon socket
If you need Docker to be reachable through HTTP rather than SSH in a safe manner,
you can enable TLS (HTTPS) by specifying the tlsverify
flag and pointing Docker's
tlscacert
flag to a trusted CA certificate.
In the daemon mode, it only allows connections from clients authenticated by a certificate signed by that CA. In the client mode, it only connects to servers with a certificate signed by that CA.
Important
Using TLS and managing a CA is an advanced topic. Familiarize yourself with OpenSSL, x509, and TLS before using it in production.
Create a CA, server and client keys with OpenSSL
Note
Replace all instances of
$HOST
in the following example with the DNS name of your Docker daemon's host.
First, on the Docker daemon's host machine, generate CA private and public keys:
$ openssl genrsa -aes256 -out ca-key.pem 4096
Generating RSA private key, 4096 bit long modulus
..............................................................................++
........++
e is 65537 (0x10001)
Enter pass phrase for ca-key.pem:
Verifying - Enter pass phrase for ca-key.pem:
$ openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem
Enter pass phrase for ca-key.pem:
You are about to be asked to enter information that will be incorporated
into your certificate request.
What you are about to enter is what is called a Distinguished Name or a DN.
There are quite a few fields but you can leave some blank
For some fields there will be a default value,
If you enter '.', the field will be left blank.
-----
Country Name (2 letter code) [AU]:
State or Province Name (full name) [Some-State]:Queensland
Locality Name (eg, city) []:Brisbane
Organization Name (eg, company) [Internet Widgits Pty Ltd]:Docker Inc
Organizational Unit Name (eg, section) []:Sales
Common Name (e.g. server FQDN or YOUR name) []:$HOST
Email Address []:Sven@home.org.au
Now that you have a CA, you can create a server key and certificate signing request (CSR). Make sure that ""Common Name"" matches the hostname you use to connect to Docker:
Note
Replace all instances of
$HOST
in the following example with the DNS name of your Docker daemon's host.
$ openssl genrsa -out server-key.pem 4096
Generating RSA private key, 4096 bit long modulus
.....................................................................++
.................................................................................................++
e is 65537 (0x10001)
$ openssl req -subj ""/CN=$HOST"" -sha256 -new -key server-key.pem -out server.csr
Next, we're going to sign the public key with our CA:
Since TLS connections can be made through IP address as well as DNS name, the IP addresses
need to be specified when creating the certificate. For example, to allow connections
using 10.10.10.20
and 127.0.0.1
:
$ echo subjectAltName = DNS:$HOST,IP:10.10.10.20,IP:127.0.0.1 >> extfile.cnf
Set the Docker daemon key's extended usage attributes to be used only for server authentication:
$ echo extendedKeyUsage = serverAuth >> extfile.cnf
Now, generate the signed certificate:
$ openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem \
-CAcreateserial -out server-cert.pem -extfile extfile.cnf
Signature ok
subject=/CN=your.host.com
Getting CA Private Key
Enter pass phrase for ca-key.pem:
Authorization plugins offer more fine-grained control to supplement authentication from mutual TLS. In addition to other information described in the above document, authorization plugins running on a Docker daemon receive the certificate information for connecting Docker clients.
For client authentication, create a client key and certificate signing request:
Note
For simplicity of the next couple of steps, you may perform this step on the Docker daemon's host machine as well.
$ openssl genrsa -out key.pem 4096
Generating RSA private key, 4096 bit long modulus
.........................................................++
................++
e is 65537 (0x10001)
$ openssl req -subj '/CN=client' -new -key key.pem -out client.csr
To make the key suitable for client authentication, create a new extensions config file:
$ echo extendedKeyUsage = clientAuth > extfile-client.cnf
Now, generate the signed certificate:
$ openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem \
-CAcreateserial -out cert.pem -extfile extfile-client.cnf
Signature ok
subject=/CN=client
Getting CA Private Key
Enter pass phrase for ca-key.pem:
After generating cert.pem
and server-cert.pem
you can safely remove the
two certificate signing requests and extensions config files:
$ rm -v client.csr server.csr extfile.cnf extfile-client.cnf
With a default umask
of 022, your secret keys are world-readable and
writable for you and your group.
To protect your keys from accidental damage, remove their write permissions. To make them only readable by you, change file modes as follows:
$ chmod -v 0400 ca-key.pem key.pem server-key.pem
Certificates can be world-readable, but you might want to remove write access to prevent accidental damage:
$ chmod -v 0444 ca.pem server-cert.pem cert.pem
Now you can make the Docker daemon only accept connections from clients providing a certificate trusted by your CA:
$ dockerd \
--tlsverify \
--tlscacert=ca.pem \
--tlscert=server-cert.pem \
--tlskey=server-key.pem \
-H=0.0.0.0:2376
To connect to Docker and validate its certificate, provide your client keys, certificates and trusted CA:
Tip
This step should be run on your Docker client machine. As such, you need to copy your CA certificate, your server certificate, and your client certificate to that machine.
Note
Replace all instances of
$HOST
in the following example with the DNS name of your Docker daemon's host.
$ docker --tlsverify \
--tlscacert=ca.pem \
--tlscert=cert.pem \
--tlskey=key.pem \
-H=$HOST:2376 version
Note
Docker over TLS should run on TCP port 2376.
Warning
As shown in the example above, you don't need to run the
docker
client withsudo
or thedocker
group when you use certificate authentication. That means anyone with the keys can give any instructions to your Docker daemon, giving them root access to the machine hosting the daemon. Guard these keys as you would a root password!
Secure by default
If you want to secure your Docker client connections by default, you can move
the files to the .docker
directory in your home directory --- and set the
DOCKER_HOST
and DOCKER_TLS_VERIFY
variables as well (instead of passing
-H=tcp://$HOST:2376
and --tlsverify
on every call).
$ mkdir -pv ~/.docker
$ cp -v {ca,cert,key}.pem ~/.docker
$ export DOCKER_HOST=tcp://$HOST:2376 DOCKER_TLS_VERIFY=1
Docker now connects securely by default:
$ docker ps
Other modes
If you don't want to have complete two-way authentication, you can run Docker in various other modes by mixing the flags.
Daemon modes
tlsverify
,tlscacert
,tlscert
,tlskey
set: Authenticate clientstls
,tlscert
,tlskey
: Do not authenticate clients
Client modes
tls
: Authenticate server based on public/default CA pooltlsverify
,tlscacert
: Authenticate server based on given CAtls
,tlscert
,tlskey
: Authenticate with client certificate, do not authenticate server based on given CAtlsverify
,tlscacert
,tlscert
,tlskey
: Authenticate with client certificate and authenticate server based on given CA
If found, the client sends its client certificate, so you just need
to drop your keys into ~/.docker/{ca,cert,key}.pem
. Alternatively,
if you want to store your keys in another location, you can specify that
location using the environment variable DOCKER_CERT_PATH
.
$ export DOCKER_CERT_PATH=~/.docker/zone1/
$ docker --tlsverify ps
Connecting to the secure Docker port using curl
To use curl
to make test API requests, you need to use three extra command line
flags:
$ curl https://$HOST:2376/images/json \
--cert ~/.docker/cert.pem \
--key ~/.docker/key.pem \
--cacert ~/.docker/ca.pem",,,
66b32c052ae952ee33c0322d87af435821f650de1cfbbee9ae231fbbdcf09f43,"Build multi-arch extensions
It is highly recommended that, at a minimum, your extension is supported for the following architectures:
linux/amd64
linux/arm64
Docker Desktop retrieves the extension image according to the user’s system architecture. If the extension does not provide an image that matches the user’s system architecture, Docker Desktop is not able to install the extension. As a result, users can’t run the extension in Docker Desktop.
Build and push for multiple architectures
If you created an extension from the docker extension init
command, the
Makefile
at the root of the directory includes a target with name
push-extension
.
You can run make push-extension
to build your extension against both
linux/amd64
and linux/arm64
platforms, and push them to Docker Hub.
For example:
$ make push-extension
Alternatively, if you started from an empty directory, use the command below to build your extension for multiple architectures:
$ docker buildx build --push --platform=linux/amd64,linux/arm64 --tag=username/my-extension:0.0.1 .
You can then check the image manifest to see if the image is available for both
architectures using the
docker buildx imagetools
command:
$ docker buildx imagetools inspect username/my-extension:0.0.1
Name: docker.io/username/my-extension:0.0.1
MediaType: application/vnd.docker.distribution.manifest.list.v2+json
Digest: sha256:f3b552e65508d9203b46db507bb121f1b644e53a22f851185d8e53d873417c48
Manifests:
Name: docker.io/username/my-extension:0.0.1@sha256:71d7ecf3cd12d9a99e73ef448bf63ae12751fe3a436a007cb0969f0dc4184c8c
MediaType: application/vnd.docker.distribution.manifest.v2+json
Platform: linux/amd64
Name: docker.io/username/my-extension:0.0.1@sha256:5ba4ceea65579fdd1181dfa103cc437d8e19d87239683cf5040e633211387ccf
MediaType: application/vnd.docker.distribution.manifest.v2+json
Platform: linux/arm64
Tip
If you're having trouble pushing the image, make sure you're signed in to Docker Hub. Otherwise, run
docker login
to authenticate.
For more information, see Multi-platform images page.
Adding multi-arch binaries
If your extension includes some binaries that deploy to the host, it’s important that they also have the right architecture when building the extension against multiple architectures.
Currently, Docker does not provide a way to explicitly specify multiple binaries for every architecture in the metadata.json
file. However, you can add architecture-specific binaries depending on the TARGETARCH
in the extension’s Dockerfile
.
The following example shows an extension that uses a binary as part of its operations. The extension needs to run both in Docker Desktop for Mac and Windows.
In the Dockerfile
, download the binary depending on the target architecture:
#syntax=docker/dockerfile:1.3-labs
FROM alpine AS dl
WORKDIR /tmp
RUN apk add --no-cache curl tar
ARG TARGETARCH
RUN <<EOT ash
mkdir -p /out/darwin
curl -fSsLo /out/darwin/kubectl ""https://dl.k8s.io/release/$(curl -Ls https://dl.k8s.io/release/stable.txt)/bin/darwin/${TARGETARCH}/kubectl""
chmod a+x /out/darwin/kubectl
EOT
RUN <<EOT ash
if [ ""amd64"" = ""$TARGETARCH"" ]; then
mkdir -p /out/windows
curl -fSsLo /out/windows/kubectl.exe ""https://dl.k8s.io/release/$(curl -Ls https://dl.k8s.io/release/stable.txt)/bin/windows/amd64/kubectl.exe""
fi
EOT
FROM alpine
LABEL org.opencontainers.image.title=""example-extension"" \
org.opencontainers.image.description=""My Example Extension"" \
org.opencontainers.image.vendor=""Docker Inc."" \
com.docker.desktop.extension.api.version="">= 0.3.3""
COPY --from=dl /out /
In the metadata.json
file, specify the path for every binary on every platform:
{
""icon"": ""docker.svg"",
""ui"": {
""dashboard-tab"": {
""title"": ""Example Extension"",
""src"": ""index.html"",
""root"": ""ui""
}
},
""host"": {
""binaries"": [
{
""darwin"": [
{
""path"": ""/darwin/kubectl""
}
],
""windows"": [
{
""path"": ""/windows/kubectl.exe""
}
]
}
]
}
}
As a result, when TARGETARCH
equals:
arm64
, thekubectl
binary fetched corresponds to thearm64
architecture, and is copied to/darwin/kubectl
in the final stage.amd64
, twokubectl
binaries are fetched. One for Darwin and another for Windows. They are copied to/darwin/kubectl
and/windows/kubectl.exe
respectively, in the final stage.
Note
The binary destination path for Darwin is
darwin/kubectl
in both cases. The only change is the architecture-specific binary that is downloaded.
When the extension is installed, the extension framework copies the binaries from the extension image at /darwin/kubectl
for Darwin, or /windows/kubectl.exe
for Windows, to a specific location in the user’s host filesystem.
Can I develop extensions that run Windows containers?
Although Docker Extensions is supported on Docker Desktop for Windows, Mac, and Linux, the extension framework only supports Linux containers. Therefore, you must target linux
as the OS when you build your extension image.",,,
61d954842285cf52a30ee14a10b542855aee3adbb8742e77052b323c3bd10960,"Docker build cache
When you build the same Docker image multiple times, knowing how to optimize the build cache is a great tool for making sure the builds run fast.
How the build cache works
Understanding Docker's build cache helps you write better Dockerfiles that result in faster builds.
The following example shows a small Dockerfile for a program written in C.
# syntax=docker/dockerfile:1
FROM ubuntu:latest
RUN apt-get update && apt-get install -y build-essentials
COPY main.c Makefile /src/
WORKDIR /src/
RUN make build
Each instruction in this Dockerfile translates to a layer in your final image. You can think of image layers as a stack, with each layer adding more content on top of the layers that came before it:
Whenever a layer changes, that layer will need to be re-built. For example,
suppose you make a change to your program in the main.c
file. After this
change, the COPY
command will have to run again in order for those changes to
appear in the image. In other words, Docker will invalidate the cache for this
layer.
If a layer changes, all other layers that come after it are also affected. When
the layer with the COPY
command gets invalidated, all layers that follow will
need to run again, too:
And that's the Docker build cache in a nutshell. Once a layer changes, then all downstream layers need to be rebuilt as well. Even if they wouldn't build anything differently, they still need to re-run.
Other resources
For more information on using cache to do efficient builds, see:",,,
b757c4e66e8e58f6feaa3f764726112e877f29dbbdd97750de1489c047cf5e93,"Seccomp security profiles for Docker
Secure computing mode (seccomp
) is a Linux kernel feature. You can use it to
restrict the actions available within the container. The seccomp()
system
call operates on the seccomp state of the calling process. You can use this
feature to restrict your application's access.
This feature is available only if Docker has been built with seccomp
and the
kernel is configured with CONFIG_SECCOMP
enabled. To check if your kernel
supports seccomp
:
$ grep CONFIG_SECCOMP= /boot/config-$(uname -r)
CONFIG_SECCOMP=y
Pass a profile for a container
The default seccomp
profile provides a sane default for running containers with
seccomp and disables around 44 system calls out of 300+. It is moderately
protective while providing wide application compatibility. The default Docker
profile can be found
here.
In effect, the profile is an allowlist that denies access to system calls by
default and then allows specific system calls. The profile works by defining a
defaultAction
of SCMP_ACT_ERRNO
and overriding that action only for specific
system calls. The effect of SCMP_ACT_ERRNO
is to cause a Permission Denied
error. Next, the profile defines a specific list of system calls which are fully
allowed, because their action
is overridden to be SCMP_ACT_ALLOW
. Finally,
some specific rules are for individual system calls such as personality
, and others,
to allow variants of those system calls with specific arguments.
seccomp
is instrumental for running Docker containers with least privilege. It
is not recommended to change the default seccomp
profile.
When you run a container, it uses the default profile unless you override it
with the --security-opt
option. For example, the following explicitly
specifies a policy:
$ docker run --rm \
-it \
--security-opt seccomp=/path/to/seccomp/profile.json \
hello-world
Significant syscalls blocked by the default profile
Docker's default seccomp profile is an allowlist which specifies the calls that are allowed. The table below lists the significant (but not all) syscalls that are effectively blocked because they are not on the allowlist. The table includes the reason each syscall is blocked rather than white-listed.
| Syscall | Description |
|---|---|
acct | Accounting syscall which could let containers disable their own resource limits or process accounting. Also gated by CAP_SYS_PACCT . |
add_key | Prevent containers from using the kernel keyring, which is not namespaced. |
bpf | Deny loading potentially persistent BPF programs into kernel, already gated by CAP_SYS_ADMIN . |
clock_adjtime | Time/date is not namespaced. Also gated by CAP_SYS_TIME . |
clock_settime | Time/date is not namespaced. Also gated by CAP_SYS_TIME . |
clone | Deny cloning new namespaces. Also gated by CAP_SYS_ADMIN for CLONE_* flags, except CLONE_NEWUSER . |
create_module | Deny manipulation and functions on kernel modules. Obsolete. Also gated by CAP_SYS_MODULE . |
delete_module | Deny manipulation and functions on kernel modules. Also gated by CAP_SYS_MODULE . |
finit_module | Deny manipulation and functions on kernel modules. Also gated by CAP_SYS_MODULE . |
get_kernel_syms | Deny retrieval of exported kernel and module symbols. Obsolete. |
get_mempolicy | Syscall that modifies kernel memory and NUMA settings. Already gated by CAP_SYS_NICE . |
init_module | Deny manipulation and functions on kernel modules. Also gated by CAP_SYS_MODULE . |
ioperm | Prevent containers from modifying kernel I/O privilege levels. Already gated by CAP_SYS_RAWIO . |
iopl | Prevent containers from modifying kernel I/O privilege levels. Already gated by CAP_SYS_RAWIO . |
kcmp | Restrict process inspection capabilities, already blocked by dropping CAP_SYS_PTRACE . |
kexec_file_load | Sister syscall of kexec_load that does the same thing, slightly different arguments. Also gated by CAP_SYS_BOOT . |
kexec_load | Deny loading a new kernel for later execution. Also gated by CAP_SYS_BOOT . |
keyctl | Prevent containers from using the kernel keyring, which is not namespaced. |
lookup_dcookie | Tracing/profiling syscall, which could leak a lot of information on the host. Also gated by CAP_SYS_ADMIN . |
mbind | Syscall that modifies kernel memory and NUMA settings. Already gated by CAP_SYS_NICE . |
mount | Deny mounting, already gated by CAP_SYS_ADMIN . |
move_pages | Syscall that modifies kernel memory and NUMA settings. |
nfsservctl | Deny interaction with the kernel NFS daemon. Obsolete since Linux 3.1. |
open_by_handle_at | Cause of an old container breakout. Also gated by CAP_DAC_READ_SEARCH . |
perf_event_open | Tracing/profiling syscall, which could leak a lot of information on the host. |
personality | Prevent container from enabling BSD emulation. Not inherently dangerous, but poorly tested, potential for a lot of kernel vulnerabilities. |
pivot_root | Deny pivot_root , should be privileged operation. |
process_vm_readv | Restrict process inspection capabilities, already blocked by dropping CAP_SYS_PTRACE . |
process_vm_writev | Restrict process inspection capabilities, already blocked by dropping CAP_SYS_PTRACE . |
ptrace | Tracing/profiling syscall. Blocked in Linux kernel versions before 4.8 to avoid seccomp bypass. Tracing/profiling arbitrary processes is already blocked by dropping CAP_SYS_PTRACE , because it could leak a lot of information on the host. |
query_module | Deny manipulation and functions on kernel modules. Obsolete. |
quotactl | Quota syscall which could let containers disable their own resource limits or process accounting. Also gated by CAP_SYS_ADMIN . |
reboot | Don't let containers reboot the host. Also gated by CAP_SYS_BOOT . |
request_key | Prevent containers from using the kernel keyring, which is not namespaced. |
set_mempolicy | Syscall that modifies kernel memory and NUMA settings. Already gated by CAP_SYS_NICE . |
setns | Deny associating a thread with a namespace. Also gated by CAP_SYS_ADMIN . |
settimeofday | Time/date is not namespaced. Also gated by CAP_SYS_TIME . |
stime | Time/date is not namespaced. Also gated by CAP_SYS_TIME . |
swapon | Deny start/stop swapping to file/device. Also gated by CAP_SYS_ADMIN . |
swapoff | Deny start/stop swapping to file/device. Also gated by CAP_SYS_ADMIN . |
sysfs | Obsolete syscall. |
_sysctl | Obsolete, replaced by /proc/sys. |
umount | Should be a privileged operation. Also gated by CAP_SYS_ADMIN . |
umount2 | Should be a privileged operation. Also gated by CAP_SYS_ADMIN . |
unshare | Deny cloning new namespaces for processes. Also gated by CAP_SYS_ADMIN , with the exception of unshare --user . |
uselib | Older syscall related to shared libraries, unused for a long time. |
userfaultfd | Userspace page fault handling, largely needed for process migration. |
ustat | Obsolete syscall. |
vm86 | In kernel x86 real mode virtual machine. Also gated by CAP_SYS_ADMIN . |
vm86old | In kernel x86 real mode virtual machine. Also gated by CAP_SYS_ADMIN . |
Run without the default seccomp profile
You can pass unconfined
to run a container without the default seccomp
profile.
$ docker run --rm -it --security-opt seccomp=unconfined debian:latest \
unshare --map-root-user --user sh -c whoami",,,
591744f9d6e7d4f9efb39dd9cd44ec4e470b1c4adb7cfb84adcbaf9219890fc6,"Extension security
Extension capabilities
An extension can have the following optional parts:
- A user interface in HTML or JavaScript, displayed in Docker Desktop Dashboard
- A backend part that runs as a container
- Executables deployed on the host machine.
Extensions are executed with the same permissions as the Docker Desktop user. Extension capabilities include running any Docker commands (including running containers and mounting folders), running extension binaries, and accessing files on your machine that are accessible by the user running Docker Desktop.
The Extensions SDK provides a set of JavaScript APIs to invoke commands or invoke these binaries from the extension UI code. Extensions can also provide a backend part that starts a long-lived running container in the background.
Important
Make sure you trust the publisher or author of the extension when you install it, as the extension has the same access rights as the user running Docker Desktop.",,,
6fe60d3ed90ecbc90e2b90a3c036345d87c1f7a22402798224b754c468ad2b65,"Using the Docker Desktop CLI
Table of contents
Availability:
Beta
Requires:
Docker Desktop
4.37 and later
The Docker Desktop CLI lets you perform key operations such as starting, stopping, restarting, and checking the status of Docker Desktop directly from the command line.
The Docker Desktop CLI provides:
- Enhanced automation and CI/CD integration: Perform Docker Desktop operations directly in CI/CD pipelines for better workflow automation.
- An improved developer experience: Restart, quit, or reset Docker Desktop from the command line, reducing dependency on the Docker Desktop Dashboard and improving flexibility and efficiency.
Usage
docker desktop COMMAND [OPTIONS]
Commands
| Command | Description |
|---|---|
start | Starts Docker Desktop |
stop | Stops Docker Desktop |
restart | Restarts Docker Desktop |
status | Displays whether Docker Desktop is running or stopped. |
engine ls | Lists available engines (Windows only) |
engine use | Switch between Linux and Windows containers (Windows only) |
update | Manage Docker Desktop updates. Available for Mac only and with Docker Desktop version 4.38 and later. |
For more details on each command, see the Docker Desktop CLI reference.",,,
cb790e4962c04034cc17d7ec0106e51bae5de4a802cd226c21370a8fbf80aeca,"Image security insights
Strengthen the security of your Docker images with Docker Hub's image security insights. Docker Hub lets you perform either point-in-time static vulnerability scanning or always up-to-date image analysis using Docker Scout.
Docker Scout image analysis
After turning on Docker Scout image analysis, Docker Scout automatically analyzes images in your Docker Hub repository.
Image analysis extracts the Software Bill of Material (SBOM) and other image metadata, and evaluates it against vulnerability data from security advisories.
The following sections describe how to turn on or off Docker Scout image analysis for a Docker Hub repository. For more details about the image analysis, see Docker Scout.
Turn on Docker Scout image analysis
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears.
Select the Settings tab.
Under Image security insight settings, select Docker Scout image analysis.
Select Save.
Turn off Docker Scout image analysis
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears.
Select the Settings tab.
Under Image security insight settings, select None.
Select Save.
Static vulnerability scanning
Note
Docker Hub static vulnerability scanning requires a Docker Pro, Team, or Business subscription.
When you push an image to a Docker Hub repository after turning on static scanning, Docker Hub automatically scans the image to identify vulnerabilities. The scan results shows the security state of your images at the time when the scan was run.
Scan results include:
- The source of the vulnerability, such as Operating System (OS) packages and libraries
- The version in which it was introduced
- A recommended fixed version, if available, to remediate the vulnerabilities discovered.
Changes to static scanning in Docker Hub
From February 27th, 2023, Docker changed the technology that supports the Docker Hub static scanning feature. The static scanning is now powered natively by Docker, instead of a third-party.
As a result of this change, scanning now detects vulnerabilities at a more granular level than before. This in turn means that vulnerability reports may show a higher number of vulnerabilities. If you used vulnerability scanning before February 27th, 2023, you may see that new vulnerability reports list a higher number of vulnerabilities, due to a more thorough analysis.
There is no action required on your part. Scans continue to run as usual with no interruption or changes to pricing. Historical data continues to be available.
Turn on static vulnerability scanning
Repository owners and administrators can enable static vulnerability scanning on a repository. If you are a member of a Team or a Business subscription, ensure the repository you would like to enable scanning on is part of the Team or a Business tier.
When scanning is active on a repository, anyone with push access can trigger a scan by pushing an image to Docker Hub.
To enable static vulnerability scanning:
Note
Static vulnerability scanning supports scanning images which are of AMD64 architecture, Linux OS, and are less than 10 GB in size.
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears.
Select the Settings tab.
Under Image security insight settings, select Static scanning.
Select Save.
Scan an image
To scan an image for vulnerabilities, push the image to Docker Hub, to the repository for which you have turned on scanning.
View the vulnerability report
To view the vulnerability report:
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears. It may take a couple of minutes for the vulnerability report to appear in your repository.
Select the Tags tab, then Digest, then Vulnerabilities to view the detailed scan report.
The scan report displays vulnerabilities identified by the scan, sorting them according to their severity, with highest severity listed at the top. It displays information about the package that contains the vulnerability, the version in which it was introduced, and whether the vulnerability is fixed in a later version.
For more information on this view, see Image details view.
Inspect vulnerabilities
The vulnerability report sorts vulnerabilities based on their severity. It displays information about the package that contains the vulnerability, the version in which it was introduced, and whether the vulnerability has been fixed in a later version.
The vulnerability scan report also allows development teams and security leads to compare the vulnerability counts across tags to see whether the vulnerabilities are decreasing or increasing over time.
Fix vulnerabilities
Once a list of vulnerabilities have been identified, there are a couple of actions you can take to remediate the vulnerabilities. For example, you can:
- Specify an updated base image in the Dockerfile, check your application-level dependencies, rebuild the Docker image, and then push the new image to Docker Hub.
- Rebuild the Docker image, run an update command on the OS packages, and push a newer version of image to Docker Hub.
- Edit the Dockerfile to manually remove or update specific libraries that contain vulnerabilities, rebuild the image, and push the new image to Docker Hub
Docker Scout can provide you with concrete and contextual remediation steps for improving image security. For more information, see Docker Scout.
Turn off static vulnerability scanning
Repository owners and administrators can disable static vulnerability scanning on a repository. To disable scanning:
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears.
Select the Settings tab.
Under Image security insight settings, select None.
Select Save.",,,
a16dbf683b9103f5ab75c132e0de6342f19abce7e7eed70f0c52a30e3234b83d,"UI styling overview for Docker extensions
Our Design System is a constantly evolving set of specifications that aim to ensure visual consistency across Docker products, and meet level AA accessibility standards. We've opened parts of it to extension authors, documenting basic styles (color, typography) and components. See: Docker Extensions Styleguide.
We require extensions to match the wider Docker Desktop UI to a certain degree, and reserve the right to make this stricter in the future.
To get started on your UI, follow the steps below.
Step one: Choose your framework
Recommended: React+MUI, using our theme
Docker Desktop's UI is written in React and
MUI (using Material UI to specific). This is the only officially supported framework for building extensions, and the one that our init
command automatically configures for you. Using it brings significant benefits to authors:
- You can use our Material UI theme to automatically replicate Docker Desktop's look and feel.
- In future, we'll release utilities and components specifically targeting this combination (e.g. custom MUI components, or React hooks for interacting with Docker).
Read our MUI best practices guide to learn future-proof ways to use MUI with Docker Desktop.
Not recommended: Some other framework
You may prefer to use another framework, perhaps because you or your team are more familiar with it or because you have existing assets you want to reuse. This is possible, but highly discouraged. It means that:
- You'll need to manually replicate the look and feel of Docker Desktop. This takes a lot of effort, and if you don't match our theme closely enough, users will find your extension jarring and we may ask you to make changes during a review process.
- You'll have a higher maintenance burden. Whenever Docker Desktop's theme changes (which could happen in any release), you'll need to manually change your extension to match it.
- If your extension is open-source, deliberately avoiding common conventions will make it harder for the community to contribute to it.
Step two: Follow the below recommendations
Follow our MUI best practices (if applicable)
See our MUI best practices article.
Only use colors from our palette
With minor exceptions, displaying your logo for example, you should only use colors from our palette. These can be found in our style guide document, and will also soon be available in our MUI theme and via CSS variables.
Use counterpart colors in light/dark mode
Our colors have been chosen so that the counterpart colors in each variant of the palette should have the same essential characteristics. Anywhere you use red-300
in light mode, you should use red-300
in dark mode too.
What's next?
- Take a look at our MUI best practices.
- Learn how to publish your extension.",,,
98e04050823eceb55947d7a45ab1dc730fe2a199f0a4ef9dc0e176a7445ca0e9,"Update Docker Hub description with GitHub Actions
You can update the Docker Hub repository description using a third party action called Docker Hub Description with this action:
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up QEMU
uses: docker/setup-qemu-action@v3
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build and push
uses: docker/build-push-action@v6
with:
push: true
tags: user/app:latest
- name: Update repo description
uses: peter-evans/dockerhub-description@e98e4d1628a5f3be2be7c231e50981aee98723ae # v4.0.0
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
repository: user/app",,,
79d0185f1c3b588026f51135ee35998209c6d62a544bd79f85f56bf481453ecf,"Swarm mode key concepts
This topic introduces some of the concepts unique to the cluster management and orchestration features of Docker Engine 1.12.
What is a swarm?
The cluster management and orchestration features embedded in Docker Engine are built using swarmkit. Swarmkit is a separate project which implements Docker's orchestration layer and is used directly within Docker.
A swarm consists of multiple Docker hosts which run in Swarm mode and act as managers, to manage membership and delegation, and workers, which run swarm services. A given Docker host can be a manager, a worker, or perform both roles. When you create a service, you define its optimal state - number of replicas, network and storage resources available to it, ports the service exposes to the outside world, and more. Docker works to maintain that desired state. For instance, if a worker node becomes unavailable, Docker schedules that node's tasks on other nodes. A task is a running container which is part of a swarm service and is managed by a swarm manager, as opposed to a standalone container.
One of the key advantages of swarm services over standalone containers is that you can modify a service's configuration, including the networks and volumes it is connected to, without the need to manually restart the service. Docker will update the configuration, stop the service tasks with out of date configuration, and create new ones matching the desired configuration.
When Docker is running in Swarm mode, you can still run standalone containers on any of the Docker hosts participating in the swarm, as well as swarm services. A key difference between standalone containers and swarm services is that only swarm managers can manage a swarm, while standalone containers can be started on any daemon. Docker daemons can participate in a swarm as managers, workers, or both.
In the same way that you can use Docker Compose to define and run containers, you can define and run Swarm service stacks.
Keep reading for details about concepts related to Docker swarm services, including nodes, services, tasks, and load balancing.
Nodes
A node is an instance of the Docker engine participating in the swarm. You can also think of this as a Docker node. You can run one or more nodes on a single physical computer or cloud server, but production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.
To deploy your application to a swarm, you submit a service definition to a manager node. The manager node dispatches units of work called tasks to worker nodes.
Manager nodes also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. Manager nodes select a single leader to conduct orchestration tasks.
Worker nodes receive and execute tasks dispatched from manager nodes. By default manager nodes also run services as worker nodes, but you can configure them to run manager tasks exclusively and be manager-only nodes. An agent runs on each worker node and reports on the tasks assigned to it. The worker node notifies the manager node of the current state of its assigned tasks so that the manager can maintain the desired state of each worker.
Services and tasks
A service is the definition of the tasks to execute on the manager or worker nodes. It is the central structure of the swarm system and the primary root of user interaction with the swarm.
When you create a service, you specify which container image to use and which commands to execute inside running containers.
In the replicated services model, the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.
For global services, the swarm runs one task for the service on every available node in the cluster.
A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. Manager nodes assign tasks to worker nodes according to the number of replicas set in the service scale. Once a task is assigned to a node, it cannot move to another node. It can only run on the assigned node or fail.
Load balancing
The swarm manager uses ingress load balancing to expose the services you want to make available externally to the swarm. The swarm manager can automatically assign the service a published port or you can configure a published port for the service. You can specify any unused port. If you do not specify a port, the swarm manager assigns the service a port in the 30000-32767 range.
External components, such as cloud load balancers, can access the service on the published port of any node in the cluster whether or not the node is currently running the task for the service. All nodes in the swarm route ingress connections to a running task instance.
Swarm mode has an internal DNS component that automatically assigns each service in the swarm a DNS entry. The swarm manager uses internal load balancing to distribute requests among services within the cluster based upon the DNS name of the service.
What's next?
- Read the Swarm mode overview.
- Get started with the Swarm mode tutorial.",,,
650e486c2a9f8496a1ece176713484bc4e8938007307d571cb1c5465defbd90a,"Raft consensus in swarm mode
When Docker Engine runs in Swarm mode, manager nodes implement the Raft Consensus Algorithm to manage the global cluster state.
The reason why Swarm mode is using a consensus algorithm is to make sure that all the manager nodes that are in charge of managing and scheduling tasks in the cluster are storing the same consistent state.
Having the same consistent state across the cluster means that in case of a failure, any Manager node can pick up the tasks and restore the services to a stable state. For example, if the Leader Manager which is responsible for scheduling tasks in the cluster dies unexpectedly, any other Manager can pick up the task of scheduling and re-balance tasks to match the desired state.
Systems using consensus algorithms to replicate logs in a distributed systems do require special care. They ensure that the cluster state stays consistent in the presence of failures by requiring a majority of nodes to agree on values.
Raft tolerates up to (N-1)/2
failures and requires a majority or quorum of
(N/2)+1
members to agree on values proposed to the cluster. This means that in
a cluster of 5 Managers running Raft, if 3 nodes are unavailable, the system
cannot process any more requests to schedule additional tasks. The existing
tasks keep running but the scheduler cannot rebalance tasks to
cope with failures if the manager set is not healthy.
The implementation of the consensus algorithm in Swarm mode means it features the properties inherent to distributed systems:
- Agreement on values in a fault tolerant system. (Refer to FLP impossibility theorem and the Raft Consensus Algorithm paper)
- Mutual exclusion through the leader election process
- Cluster membership management
- Globally consistent object sequencing and CAS (compare-and-swap) primitives",,,
d7e47b1e7e186a8ea1cca424125232ac73036475f5ac4cd314f0f23eb236516f,"BuildKit
Overview
BuildKit is an improved backend to replace the legacy builder. BuildKit is the default builder for users on Docker Desktop, and Docker Engine as of version 23.0.
BuildKit provides new functionality and improves your builds' performance. It also introduces support for handling more complex scenarios:
- Detect and skip executing unused build stages
- Parallelize building independent build stages
- Incrementally transfer only the changed files in your build context between builds
- Detect and skip transferring unused files in your build context
- Use Dockerfile frontend implementations with many new features
- Avoid side effects with rest of the API (intermediate images and containers)
- Prioritize your build cache for automatic pruning
Apart from many new features, the main areas BuildKit improves on the current experience are performance, storage management, and extensibility. From the performance side, a significant update is a new fully concurrent build graph solver. It can run build steps in parallel when possible and optimize out commands that don't have an impact on the final result. We have also optimized the access to the local source files. By tracking only the updates made to these files between repeated build invocations, there is no need to wait for local files to be read or uploaded before the work can begin.
LLB
At the core of BuildKit is a Low-Level Build (LLB) definition format. LLB is an intermediate binary format that allows developers to extend BuildKit. LLB defines a content-addressable dependency graph that can be used to put together very complex build definitions. It also supports features not exposed in Dockerfiles, like direct data mounting and nested invocation.
Everything about execution and caching of your builds is defined in LLB. The caching model is entirely rewritten compared to the legacy builder. Rather than using heuristics to compare images, LLB directly tracks the checksums of build graphs and content mounted to specific operations. This makes it much faster, more precise, and portable. The build cache can even be exported to a registry, where it can be pulled on-demand by subsequent invocations on any host.
LLB can be generated directly using a golang client package that allows defining the relationships between your build operations using Go language primitives. This gives you full power to run anything you can imagine, but will probably not be how most people will define their builds. Instead, most users would use a frontend component, or LLB nested invocation, to run a prepared set of build steps.
Frontend
A frontend is a component that takes a human-readable build format and converts it to LLB so BuildKit can execute it. Frontends can be distributed as images, and the user can target a specific version of a frontend that is guaranteed to work for the features used by their definition.
For example, to build a Dockerfile with BuildKit, you would use an external Dockerfile frontend.
Getting started
BuildKit is the default builder for users on Docker Desktop and Docker Engine v23.0 and later.
If you have installed Docker Desktop, you don't need to enable BuildKit. If you are running a version of Docker Engine version earlier than 23.0, you can enable BuildKit either by setting an environment variable, or by making BuildKit the default setting in the daemon configuration.
To set the BuildKit environment variable when running the docker build
command, run:
$ DOCKER_BUILDKIT=1 docker build .
Note
Buildx always uses BuildKit.
To use Docker BuildKit by default, edit the Docker daemon configuration in
/etc/docker/daemon.json
as follows, and restart the daemon.
{
""features"": {
""buildkit"": true
}
}
If the /etc/docker/daemon.json
file doesn't exist, create new file called
daemon.json
and then add the following to the file. And restart the Docker
daemon.
BuildKit on Windows
Warning
BuildKit only fully supports building Linux containers. Windows container support is experimental.
BuildKit has experimental support for Windows containers (WCOW) as of version 0.13.
This section walks you through the steps for trying it out.
We appreciate any feedback you submit by
opening an issue here, especially buildkitd.exe
.
Known limitations
For information about open bugs and limitations related to BuildKit on Windows, see GitHub issues.
Prerequisites
- Architecture:
amd64
,arm64
(binaries available but not officially tested yet). - Supported OS: Windows Server 2019, Windows Server 2022, Windows 11.
- Base images:
ServerCore:ltsc2019
,ServerCore:ltsc2022
,NanoServer:ltsc2022
. See the compatibility map here. - Docker Desktop version 4.29 or later
Steps
Note
The following commands require administrator (elevated) privileges in a PowerShell terminal.
Enable the Hyper-V and Containers Windows features.
> Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V, Containers -All
If you see
RestartNeeded
asTrue
, restart your machine and re-open a PowerShell terminal as an administrator. Otherwise, continue with the next step.Switch to Windows containers in Docker Desktop.
Select the Docker icon in the taskbar, and then Switch to Windows containers....
Install containerd version 1.7.7 or later following the setup instructions here.
Download and extract the latest BuildKit release.
$version = ""v0.13.1"" # specify the release version, v0.13+ $arch = ""amd64"" # arm64 binary available too curl.exe -LO https://github.com/moby/buildkit/releases/download/$version/buildkit-$version.windows-$arch.tar.gz # there could be another `.\bin` directory from containerd instructions # you can move those mv bin bin2 tar.exe xvf .\buildkit-$version.windows-$arch.tar.gz ## x bin/ ## x bin/buildctl.exe ## x bin/buildkitd.exe
Install BuildKit binaries on
PATH
.# after the binaries are extracted in the bin directory # move them to an appropriate path in your $Env:PATH directories or: Copy-Item -Path "".\bin"" -Destination ""$Env:ProgramFiles\buildkit"" -Recurse -Force # add `buildkitd.exe` and `buildctl.exe` binaries in the $Env:PATH $Path = [Environment]::GetEnvironmentVariable(""PATH"", ""Machine"") + ` [IO.Path]::PathSeparator + ""$Env:ProgramFiles\buildkit"" [Environment]::SetEnvironmentVariable( ""Path"", $Path, ""Machine"") $Env:Path = [System.Environment]::GetEnvironmentVariable(""Path"",""Machine"") + "";"" + ` [System.Environment]::GetEnvironmentVariable(""Path"",""User"")
Start the BuildKit daemon.
> buildkitd.exe
In another terminal with administrator privileges, create a remote builder that uses the local BuildKit daemon.
Note
This requires Docker Desktop version 4.29 or later.
> docker buildx create --name buildkit-exp --use --driver=remote npipe:////./pipe/buildkitd buildkit-exp
Verify the builder connection by running
docker buildx inspect
.> docker buildx inspect
The output should indicate that the builder platform is Windows, and that the endpoint of the builder is a named pipe.
Name: buildkit-exp Driver: remote Last Activity: 2024-04-15 17:51:58 +0000 UTC Nodes: Name: buildkit-exp0 Endpoint: npipe:////./pipe/buildkitd Status: running BuildKit version: v0.13.1 Platforms: windows/amd64 ...
Create a Dockerfile and build a
hello-buildkit
image.> mkdir sample_dockerfile > cd sample_dockerfile > Set-Content Dockerfile @"" FROM mcr.microsoft.com/windows/nanoserver:ltsc2022 USER ContainerAdministrator COPY hello.txt C:/ RUN echo ""Goodbye!"" >> hello.txt CMD [""cmd"", ""/C"", ""type C:\\hello.txt""] ""@ Set-Content hello.txt @"" Hello from BuildKit! This message shows that your installation appears to be working correctly. ""@
Build and push the image to a registry.
> docker buildx build --push -t <username>/hello-buildkit .
After pushing to the registry, run the image with
docker run
.> docker run <username>/hello-buildkit",,,
61eb0841cc03e4187c12a0f09db73867f76d61e3f0d87cc1646080a150e18691,"Webhooks
Table of contents
You can use webhooks to cause an action in another service in response to a push event in the repository. Webhooks are POST requests sent to a URL you define in Docker Hub.
Create a webhook
To create a webhook:
- In your chosen repository, select the Webhooks tab.
- Provide a name for the webhook.
- Provide a destination webhook URL. This is where webhook POST requests are delivered.
- Select Create.
View webhook delivery history
To view the history of the webhook:
- Hover over your webhook under the Current Webhooks section.
- Select the Menu options icon.
- Select View History.
You can then view the delivery history, and whether delivering the POST request was successful or not.
Example webhook payload
Webhook payloads have the following JSON format:
{
""callback_url"": ""https://registry.hub.docker.com/u/svendowideit/testhook/hook/2141b5bi5i5b02bec211i4eeih0242eg11000a/"",
""push_data"": {
""pushed_at"": 1417566161,
""pusher"": ""trustedbuilder"",
""tag"": ""latest""
},
""repository"": {
""comment_count"": 0,
""date_created"": 1417494799,
""description"": """",
""dockerfile"": ""#\n# BUILD\u0009\u0009docker build -t svendowideit/apt-cacher .\n# RUN\u0009\u0009docker run -d -p 3142:3142 -name apt-cacher-run apt-cacher\n#\n# and then you can run containers with:\n# \u0009\u0009docker run -t -i -rm -e http_proxy http://192.168.1.2:3142/ debian bash\n#\nFROM\u0009\u0009ubuntu\n\n\nVOLUME\u0009\u0009[/var/cache/apt-cacher-ng]\nRUN\u0009\u0009apt-get update ; apt-get install -yq apt-cacher-ng\n\nEXPOSE \u0009\u00093142\nCMD\u0009\u0009chmod 777 /var/cache/apt-cacher-ng ; /etc/init.d/apt-cacher-ng start ; tail -f /var/log/apt-cacher-ng/*\n"",
""full_description"": ""Docker Hub based automated build from a GitHub repo"",
""is_official"": false,
""is_private"": true,
""is_trusted"": true,
""name"": ""testhook"",
""namespace"": ""svendowideit"",
""owner"": ""svendowideit"",
""repo_name"": ""svendowideit/testhook"",
""repo_url"": ""https://registry.hub.docker.com/u/svendowideit/testhook/"",
""star_count"": 0,
""status"": ""Active""
}
}",,,
3b0729e551061a87b38ec25ce7b4c69065816c806e80e279e0916289910fd3f4,"Overview of Compose Bridge
Compose Bridge lets you transform your Compose configuration file into configuration files for different platforms, primarily focusing on Kubernetes. The default transformation generates Kubernetes manifests and a Kustomize overlay which are designed for deployment on Docker Desktop with Kubernetes enabled.
It's a flexible tool that lets you either take advantage of the default transformation or create a custom transformation to suit specific project needs and requirements.
Compose Bridge significantly simplifies the transition from Docker Compose to Kubernetes, making it easier for you to leverage the power of Kubernetes while maintaining the simplicity and efficiency of Docker Compose.
How it works
Compose Bridge uses transformations to let you convert a Compose model into another form.
A transformation is packaged as a Docker image that receives the fully resolved Compose model as /in/compose.yaml
and can produce any target format file under /out
.
Compose Bridge provides its own transformation for Kubernetes using Go templates, so that it is easy to extend for customization by replacing or appending your own templates.
For more detailed information on how these transformations work and how you can customize them for your projects, see Customize.
Setup
To get started with Compose Bridge, you need to:
- Download and install Docker Desktop version 4.33 and later.
- Sign in to your Docker account.
- Navigate to the Features in development tab in Settings.
- From the Experimental features tab, select Enable Compose Bridge.
Feedback
To give feedback, report bugs, or receive support, email desktop-preview@docker.com
. There is also a dedicated Slack channel. To join, simply send an email to the provided address.",,,
9a7b80121245a38ed992296cc4f679df756da8c814788e9016fc1947fb2e94eb,"Troubleshoot your autobuilds
Note
Automated builds require a Docker Pro, Team, or Business subscription.
Failing builds
If a build fails, a Retry icon appears next to the build report line on the General and Builds tabs. The Build report page and Timeline logs also display a Retry button.
Note
If you are viewing the build details for a repository that belongs to an organization, the Cancel and Retry buttons only appear if you have
Read & Write
access to the repository.
Automated builds have a 4-hour execution time limit. If a build reaches this time limit, it's automatically cancelled, and the build logs display the following message:
2022-11-02T17:42:27Z The build was cancelled or exceeded the maximum execution time.
This log message is the same as when you actively cancel a build. To identify whether a build was automatically cancelled, check the build duration.
Build repositories with linked private submodules
Docker Hub sets up a deploy key in your source code repository that allows it to clone the repository and build it. This key only works for a single, specific code repository. If your source code repository uses private Git submodules, or requires that you clone other private repositories to build, Docker Hub cannot access these additional repositories, your build cannot complete, and an error is logged in your build timeline.
To work around this, you can set up your automated build using the SSH_PRIVATE
environment variable to override the deployment key and grant Docker Hub's build
system access to the repositories.
Note
If you are using autobuild for teams, use the process below instead, and configure a service user for your source code provider. You can also do this for an individual account to limit Docker Hub's access to your source repositories.
Generate a SSH keypair that you use for builds only, and add the public key to your source code provider account.
This step is optional, but allows you to revoke the build-only keypair without removing other access.
Copy the private half of the keypair to your clipboard.
In Docker Hub, navigate to the build page for the repository that has linked private submodules. (If necessary, follow the steps here to configure the automated build.)
At the bottom of the screen, select the plus icon next to Build Environment variables.
Enter
SSH_PRIVATE
as the name for the new environment variable.Paste the private half of the keypair into the Value field.
Select Save, or Save and Build to validate that the build now completes.
Note
You must configure your private git submodules using git clone over SSH (
git@submodule.tld:some-submodule.git
) rather than HTTPS.",,,
78395940a50c99a72fa23450de04648a4bc0220be384040bea469021f838917f,"docker
| Description | The base command for the Docker CLI. |
|---|
Description
Depending on your Docker system configuration, you may be required to preface
each docker
command with sudo
. To avoid having to use sudo
with the
docker
command, your system administrator can create a Unix group called
docker
and add users to it.
For more information about installing Docker or sudo
configuration, refer to
the
installation instructions for your operating system.
Display help text
To list the help on any command just execute the command, followed by the
--help
option.
$ docker run --help
Usage: docker run [OPTIONS] IMAGE [COMMAND] [ARG...]
Create and run a new container from an image
Options:
--add-host value Add a custom host-to-IP mapping (host:ip) (default [])
-a, --attach value Attach to STDIN, STDOUT or STDERR (default [])
<...>
Environment variables
The following list of environment variables are supported by the docker
command
line:
| Variable | Description |
|---|---|
DOCKER_API_VERSION | Override the negotiated API version to use for debugging (e.g. 1.19 ) |
DOCKER_CERT_PATH | Location of your authentication keys. This variable is used both by the docker CLI and the
dockerd daemon |
DOCKER_CONFIG | The location of your client configuration files. |
DOCKER_CONTENT_TRUST_SERVER | The URL of the Notary server to use. Defaults to the same URL as the registry. |
DOCKER_CONTENT_TRUST | When set Docker uses notary to sign and verify images. Equates to --disable-content-trust=false for build, create, pull, push, run. |
DOCKER_CONTEXT | Name of the docker context to use (overrides DOCKER_HOST env var and default context set with docker context use ) |
DOCKER_CUSTOM_HEADERS | (Experimental) Configure
custom HTTP headers to be sent by the client. Headers must be provided as a comma-separated list of name=value pairs. This is the equivalent to the HttpHeaders field in the configuration file. |
DOCKER_DEFAULT_PLATFORM | Default platform for commands that take the --platform flag. |
DOCKER_HIDE_LEGACY_COMMANDS | When set, Docker hides ""legacy"" top-level commands (such as docker rm , and docker pull ) in docker help output, and only Management commands per object-type (e.g., docker container ) are printed. This may become the default in a future release. |
DOCKER_HOST | Daemon socket to connect to. |
DOCKER_TLS | Enable TLS for connections made by the docker CLI (equivalent of the --tls command-line option). Set to a non-empty value to enable TLS. Note that TLS is enabled automatically if any of the other TLS options are set. |
DOCKER_TLS_VERIFY | When set Docker uses TLS and verifies the remote. This variable is used both by the docker CLI and the
dockerd daemon |
BUILDKIT_PROGRESS | Set type of progress output (auto , plain , tty , rawjson ) when
building with
BuildKit backend. Use plain to show container output (default auto ). |
Because Docker is developed using Go, you can also use any environment variables used by the Go runtime. In particular, you may find these useful:
| Variable | Description |
|---|---|
HTTP_PROXY | Proxy URL for HTTP requests unless overridden by NoProxy. |
HTTPS_PROXY | Proxy URL for HTTPS requests unless overridden by NoProxy. |
NO_PROXY | Comma-separated values specifying hosts that should be excluded from proxying. |
See the Go specification for details on these variables.
Option types
Single character command line options can be combined, so rather than
typing docker run -i -t --name test busybox sh
,
you can write docker run -it --name test busybox sh
.
Boolean
Boolean options take the form -d=false
. The value you see in the help text is
the default value which is set if you do not specify that flag. If you
specify a Boolean flag without a value, this will set the flag to true
,
irrespective of the default value.
For example, running docker run -d
will set the value to true
, so your
container will run in ""detached"" mode, in the background.
Options which default to true
(e.g., docker build --rm=true
) can only be
set to the non-default value by explicitly setting them to false
:
$ docker build --rm=false .
Multi
You can specify options like -a=[]
multiple times in a single command line,
for example in these commands:
$ docker run -a stdin -a stdout -i -t ubuntu /bin/bash
$ docker run -a stdin -a stdout -a stderr ubuntu /bin/ls
Sometimes, multiple options can call for a more complex value string as for
-v
:
$ docker run -v /host:/container example/mysql
Note
Do not use the
-t
and-a stderr
options together due to limitations in thepty
implementation. Allstderr
inpty
mode simply goes tostdout
.
Strings and Integers
Options like --name=""""
expect a string, and they
can only be specified once. Options like -c=0
expect an integer, and they can only be specified once.
Configuration files
By default, the Docker command line stores its configuration files in a
directory called .docker
within your $HOME
directory.
Docker manages most of the files in the configuration directory
and you shouldn't modify them. However, you can modify the
config.json
file to control certain aspects of how the docker
command behaves.
You can modify the docker
command behavior using environment
variables or command-line options. You can also use options within
config.json
to modify some of the same behavior. If an environment variable
and the --config
flag are set, the flag takes precedent over the environment
variable. Command line options override environment variables and environment
variables override properties you specify in a config.json
file.
Change the .docker
directory
To specify a different directory, use the DOCKER_CONFIG
environment variable or the --config
command line option. If both are
specified, then the --config
option overrides the DOCKER_CONFIG
environment
variable. The example below overrides the docker ps
command using a
config.json
file located in the ~/testconfigs/
directory.
$ docker --config ~/testconfigs/ ps
This flag only applies to whatever command is being ran. For persistent
configuration, you can set the DOCKER_CONFIG
environment variable in your
shell (e.g. ~/.profile
or ~/.bashrc
). The example below sets the new
directory to be HOME/newdir/.docker
.
$ echo export DOCKER_CONFIG=$HOME/newdir/.docker > ~/.profile
Docker CLI configuration file (config.json
) properties
Use the Docker CLI configuration to customize settings for the docker
CLI. The
configuration file uses JSON formatting, and properties:
By default, configuration file is stored in ~/.docker/config.json
. Refer to the
change the .docker
directory section to use a
different location.
Warning
The configuration file and other files inside the
~/.docker
configuration directory may contain sensitive information, such as authentication information for proxies or, depending on your credential store, credentials for your image registries. Review your configuration file's content before sharing with others, and prevent committing the file to version control.
Customize the default output format for commands
These fields lets you customize the default output format for some commands
if no --format
flag is provided.
| Property | Description |
|---|---|
configFormat | Custom default format for docker config ls output. See
docker config ls for a list of supported formatting directives. |
imagesFormat | Custom default format for docker images / docker image ls output. See
docker images for a list of supported formatting directives. |
networksFormat | Custom default format for docker network ls output. See
docker network ls for a list of supported formatting directives. |
nodesFormat | Custom default format for docker node ls output. See
docker node ls for a list of supported formatting directives. |
pluginsFormat | Custom default format for docker plugin ls output. See
docker plugin ls for a list of supported formatting directives. |
psFormat | Custom default format for docker ps / docker container ps output. See
docker ps for a list of supported formatting directives. |
secretFormat | Custom default format for docker secret ls output. See
docker secret ls for a list of supported formatting directives. |
serviceInspectFormat | Custom default format for docker service inspect output. See
docker service inspect for a list of supported formatting directives. |
servicesFormat | Custom default format for docker service ls output. See
docker service ls for a list of supported formatting directives. |
statsFormat | Custom default format for docker stats output. See
docker stats for a list of supported formatting directives. |
tasksFormat | Custom default format for docker stack ps output. See
docker stack ps for a list of supported formatting directives. |
volumesFormat | Custom default format for docker volume ls output. See
docker volume ls for a list of supported formatting directives. |
Custom HTTP headers
The property HttpHeaders
specifies a set of headers to include in all messages
sent from the Docker client to the daemon. Docker doesn't try to interpret or
understand these headers; it simply puts them into the messages. Docker does
not allow these headers to change any headers it sets for itself.
Alternatively, use the DOCKER_CUSTOM_HEADERS
environment variable,
which is available in v27.1 and higher. This environment-variable is experimental,
and its exact behavior may change.
Credential store options
The property credsStore
specifies an external binary to serve as the default
credential store. When this property is set, docker login
will attempt to
store credentials in the binary specified by docker-credential-<value>
which
is visible on $PATH
. If this property isn't set, credentials are stored
in the auths
property of the CLI configuration file. For more information,
see the
Credential stores section in the docker login
documentation
The property credHelpers
specifies a set of credential helpers to use
preferentially over credsStore
or auths
when storing and retrieving
credentials for specific registries. If this property is set, the binary
docker-credential-<value>
will be used when storing or retrieving credentials
for a specific registry. For more information, see the
Credential helpers section in the docker login
documentation
Automatic proxy configuration for containers
The property proxies
specifies proxy environment variables to be automatically
set on containers, and set as --build-arg
on containers used during docker build
.
A ""default""
set of proxies can be configured, and will be used for any Docker
daemon that the client connects to, or a configuration per host (Docker daemon),
for example, https://docker-daemon1.example.com
. The following properties can
be set for each environment:
| Property | Description |
|---|---|
httpProxy | Default value of HTTP_PROXY and http_proxy for containers, and as --build-arg on docker build |
httpsProxy | Default value of HTTPS_PROXY and https_proxy for containers, and as --build-arg on docker build |
ftpProxy | Default value of FTP_PROXY and ftp_proxy for containers, and as --build-arg on docker build |
noProxy | Default value of NO_PROXY and no_proxy for containers, and as --build-arg on docker build |
allProxy | Default value of ALL_PROXY and all_proxy for containers, and as --build-arg on docker build |
These settings are used to configure proxy settings for containers only, and not
used as proxy settings for the docker
CLI or the dockerd
daemon. Refer to the
environment variables and
HTTP/HTTPS proxy
sections for configuring proxy settings for the CLI and daemon.
Warning
Proxy settings may contain sensitive information (for example, if the proxy requires authentication). Environment variables are stored as plain text in the container's configuration, and as such can be inspected through the remote API or committed to an image when using
docker commit
.
Default key-sequence to detach from containers
Once attached to a container, users detach from it and leave it running using
the using CTRL-p CTRL-q
key sequence. This detach key sequence is customizable
using the detachKeys
property. Specify a <sequence>
value for the
property. The format of the <sequence>
is a comma-separated list of either
a letter [a-Z], or the ctrl-
combined with any of the following:
a-z
(a single lowercase alpha character )@
(at sign)[
(left bracket)\\
(two backward slashes)_
(underscore)^
(caret)
Your customization applies to all containers started in with your Docker client.
Users can override your custom or the default key sequence on a per-container
basis. To do this, the user specifies the --detach-keys
flag with the docker attach
, docker exec
, docker run
or docker start
command.
CLI plugin options
The property plugins
contains settings specific to CLI plugins. The
key is the plugin name, while the value is a further map of options,
which are specific to that plugin.
Sample configuration file
Following is a sample config.json
file to illustrate the format used for
various fields:
{
""HttpHeaders"": {
""MyHeader"": ""MyValue""
},
""psFormat"": ""table {{.ID}}\\t{{.Image}}\\t{{.Command}}\\t{{.Labels}}"",
""imagesFormat"": ""table {{.ID}}\\t{{.Repository}}\\t{{.Tag}}\\t{{.CreatedAt}}"",
""pluginsFormat"": ""table {{.ID}}\t{{.Name}}\t{{.Enabled}}"",
""statsFormat"": ""table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}"",
""servicesFormat"": ""table {{.ID}}\t{{.Name}}\t{{.Mode}}"",
""secretFormat"": ""table {{.ID}}\t{{.Name}}\t{{.CreatedAt}}\t{{.UpdatedAt}}"",
""configFormat"": ""table {{.ID}}\t{{.Name}}\t{{.CreatedAt}}\t{{.UpdatedAt}}"",
""serviceInspectFormat"": ""pretty"",
""nodesFormat"": ""table {{.ID}}\t{{.Hostname}}\t{{.Availability}}"",
""detachKeys"": ""ctrl-e,e"",
""credsStore"": ""secretservice"",
""credHelpers"": {
""awesomereg.example.org"": ""hip-star"",
""unicorn.example.com"": ""vcbait""
},
""plugins"": {
""plugin1"": {
""option"": ""value""
},
""plugin2"": {
""anotheroption"": ""anothervalue"",
""athirdoption"": ""athirdvalue""
}
},
""proxies"": {
""default"": {
""httpProxy"": ""http://user:pass@example.com:3128"",
""httpsProxy"": ""https://my-proxy.example.com:3129"",
""noProxy"": ""intra.mycorp.example.com"",
""ftpProxy"": ""http://user:pass@example.com:3128"",
""allProxy"": ""socks://example.com:1234""
},
""https://manager1.mycorp.example.com:2377"": {
""httpProxy"": ""http://user:pass@example.com:3128"",
""httpsProxy"": ""https://my-proxy.example.com:3129""
}
}
}
Experimental features
Experimental features provide early access to future product functionality. These features are intended for testing and feedback, and they may change between releases without warning or can be removed from a future release.
Starting with Docker 20.10, experimental CLI features are enabled by default, and require no configuration to enable them.
Notary
If using your own notary server and a self-signed certificate or an internal
Certificate Authority, you need to place the certificate at
tls/<registry_url>/ca.crt
in your Docker config directory.
Alternatively you can trust the certificate globally by adding it to your system's list of root Certificate Authorities.
Options
| Option | Default | Description |
|---|---|---|
--config | /root/.docker | Location of client config files |
-c, --context | Name of the context to use to connect to the daemon (overrides DOCKER_HOST env var and default context set with docker context use ) | |
-D, --debug | Enable debug mode | |
-H, --host | Daemon socket to connect to | |
-l, --log-level | info | Set the logging level (debug , info , warn , error , fatal ) |
--tls | Use TLS; implied by --tlsverify | |
--tlscacert | /root/.docker/ca.pem | Trust certs signed only by this CA |
--tlscert | /root/.docker/cert.pem | Path to TLS certificate file |
--tlskey | /root/.docker/key.pem | Path to TLS key file |
--tlsverify | Use TLS and verify the remote |
Examples
Specify daemon host (-H, --host)
You can use the -H
, --host
flag to specify a socket to use when you invoke
a docker
command. You can use the following protocols:
| Scheme | Description | Example |
|---|---|---|
unix://[<path>] | Unix socket (Linux only) | unix:///var/run/docker.sock |
tcp://[<IP or host>[:port]] | TCP connection | tcp://174.17.0.1:2376 |
ssh://[username@]<IP or host>[:port] | SSH connection | ssh://user@192.168.64.5 |
npipe://[<name>] | Named pipe (Windows only) | npipe:////./pipe/docker_engine |
If you don't specify the -H
flag, and you're not using a custom
context,
commands use the following default sockets:
unix:///var/run/docker.sock
on macOS and Linuxnpipe:////./pipe/docker_engine
on Windows
To achieve a similar effect without having to specify the -H
flag for every
command, you could also
create a context,
or alternatively, use the
DOCKER_HOST
environment variable.
For more information about the -H
flag, see
Daemon socket option.
Using TCP sockets
The following example shows how to invoke docker ps
over TCP, to a remote
daemon with IP address 174.17.0.1
, listening on port 2376
:
$ docker -H tcp://174.17.0.1:2376 ps
Note
By convention, the Docker daemon uses port
2376
for secure TLS connections, and port2375
for insecure, non-TLS connections.
Using SSH sockets
When you use SSH invoke a command on a remote daemon, the request gets forwarded
to the /var/run/docker.sock
Unix socket on the SSH host.
$ docker -H ssh://user@192.168.64.5 ps
You can optionally specify the location of the socket by appending a path component to the end of the SSH address.
$ docker -H ssh://user@192.168.64.5/var/run/docker.sock ps
Subcommands
| Command | Description |
|---|---|
docker build (legacy builder) | Build an image from a Dockerfile |
docker builder | Manage builds |
docker buildx | Docker Buildx |
docker checkpoint | Manage checkpoints |
docker compose | Docker Compose |
docker config | Manage Swarm configs |
docker container | Manage containers |
docker context | Manage contexts |
docker debug | Get a shell into any container or image. An alternative to debugging with `docker exec`. |
docker desktop (Beta) | Docker Desktop |
docker image | Manage images |
docker init | Creates Docker-related starter files for your project |
docker inspect | Return low-level information on Docker objects |
docker login | Authenticate to a registry |
docker logout | Log out from a registry |
docker manifest | Manage Docker image manifests and manifest lists |
docker network | Manage networks |
docker node | Manage Swarm nodes |
docker plugin | Manage plugins |
docker scout | Command line tool for Docker Scout |
docker search | Search Docker Hub for images |
docker secret | Manage Swarm secrets |
docker service | Manage Swarm services |
docker stack | Manage Swarm stacks |
docker swarm | Manage Swarm |
docker system | Manage Docker |
docker trust | Manage trust on Docker images |
docker version | Show the Docker version information |
docker volume | Manage volumes |",,,
a0c67db89724c58f47bc0b029902331e4e1ae7e45b9369e2d22598db6ff096ad,"Verify repository client with certificates
In Running Docker with HTTPS, you learned that, by default, Docker runs via a non-networked Unix socket and TLS must be enabled in order to have the Docker client and the daemon communicate securely over HTTPS. TLS ensures authenticity of the registry endpoint and that traffic to/from registry is encrypted.
This article demonstrates how to ensure the traffic between the Docker registry server and the Docker daemon (a client of the registry server) is encrypted and properly authenticated using certificate-based client-server authentication.
We show you how to install a Certificate Authority (CA) root certificate for the registry and how to set the client TLS certificate for verification.
Understand the configuration
A custom certificate is configured by creating a directory under
/etc/docker/certs.d
using the same name as the registry's hostname, such as
localhost
. All *.crt
files are added to this directory as CA roots.
Note
On Linux any root certificates authorities are merged with the system defaults, including the host's root CA set. If you are running Docker on Windows Server, or Docker Desktop for Windows with Windows containers, the system default certificates are only used when no custom root certificates are configured.
The presence of one or more <filename>.key/cert
pairs indicates to Docker
that there are custom certificates required for access to the desired
repository.
Note
If multiple certificates exist, each is tried in alphabetical order. If there is a 4xx-level or 5xx-level authentication error, Docker continues to try with the next certificate.
The following illustrates a configuration with custom certificates:
/etc/docker/certs.d/ <-- Certificate directory
└── localhost:5000 <-- Hostname:port
├── client.cert <-- Client certificate
├── client.key <-- Client key
└── ca.crt <-- Root CA that signed
the registry certificate, in PEM
The preceding example is operating-system specific and is for illustrative purposes only. You should consult your operating system documentation for creating an os-provided bundled certificate chain.
Create the client certificates
Use OpenSSL's genrsa
and req
commands to first generate an RSA
key and then use the key to create the certificate.
$ openssl genrsa -out client.key 4096
$ openssl req -new -x509 -text -key client.key -out client.cert
Note
These TLS commands only generate a working set of certificates on Linux. The version of OpenSSL in macOS is incompatible with the type of certificate Docker requires.
Troubleshooting tips
The Docker daemon interprets .crt
files as CA certificates and .cert
files
as client certificates. If a CA certificate is accidentally given the extension
.cert
instead of the correct .crt
extension, the Docker daemon logs the
following error message:
Missing key KEY_NAME for client certificate CERT_NAME. CA certificates should use the extension .crt.
If the Docker registry is accessed without a port number, do not add the port to the directory name. The following shows the configuration for a registry on default port 443 which is accessed with docker login my-https.registry.example.com
:
/etc/docker/certs.d/
└── my-https.registry.example.com <-- Hostname without port
├── client.cert
├── client.key
└── ca.crt",,,
5e07ffb76e36a73ec7280ce97e10d4400b60d23ea5e234be8eaaab035f2a8387,"Docker Plugin API
Docker plugins are out-of-process extensions which add capabilities to the Docker Engine.
This document describes the Docker Engine plugin API. To view information on plugins managed by Docker Engine, refer to Docker Engine plugin system.
This page is intended for people who want to develop their own Docker plugin. If you just want to learn about or use Docker plugins, look here.
What plugins are
A plugin is a process running on the same or a different host as the Docker daemon, which registers itself by placing a file on the daemon host in one of the plugin directories described in Plugin discovery.
Plugins have human-readable names, which are short, lowercase strings. For
example, flocker
or weave
.
Plugins can run inside or outside containers. Currently running them outside containers is recommended.
Plugin discovery
Docker discovers plugins by looking for them in the plugin directory whenever a user or container tries to use one by name.
There are three types of files which can be put in the plugin directory.
.sock
files are Unix domain sockets..spec
files are text files containing a URL, such asunix:///other.sock
ortcp://localhost:8080
..json
files are text files containing a full json specification for the plugin.
Plugins with Unix domain socket files must run on the same host as the Docker daemon.
Plugins with .spec
or .json
files can run on a different host if you specify a remote URL.
Unix domain socket files must be located under /run/docker/plugins
, whereas
spec files can be located either under /etc/docker/plugins
or /usr/lib/docker/plugins
.
The name of the file (excluding the extension) determines the plugin name.
For example, the flocker
plugin might create a Unix socket at
/run/docker/plugins/flocker.sock
.
You can define each plugin into a separated subdirectory if you want to isolate definitions from each other.
For example, you can create the flocker
socket under /run/docker/plugins/flocker/flocker.sock
and only
mount /run/docker/plugins/flocker
inside the flocker
container.
Docker always searches for Unix sockets in /run/docker/plugins
first. It checks for spec or json files under
/etc/docker/plugins
and /usr/lib/docker/plugins
if the socket doesn't exist. The directory scan stops as
soon as it finds the first plugin definition with the given name.
JSON specification
This is the JSON format for a plugin:
{
""Name"": ""plugin-example"",
""Addr"": ""https://example.com/docker/plugin"",
""TLSConfig"": {
""InsecureSkipVerify"": false,
""CAFile"": ""/usr/shared/docker/certs/example-ca.pem"",
""CertFile"": ""/usr/shared/docker/certs/example-cert.pem"",
""KeyFile"": ""/usr/shared/docker/certs/example-key.pem""
}
}
The TLSConfig
field is optional and TLS will only be verified if this configuration is present.
Plugin lifecycle
Plugins should be started before Docker, and stopped after Docker. For
example, when packaging a plugin for a platform which supports systemd
, you
might use
systemd
dependencies to
manage startup and shutdown order.
When upgrading a plugin, you should first stop the Docker daemon, upgrade the plugin, then start Docker again.
Plugin activation
When a plugin is first referred to -- either by a user referring to it by name
(e.g. docker run --volume-driver=foo
) or a container already configured to
use a plugin being started -- Docker looks for the named plugin in the plugin
directory and activates it with a handshake. See Handshake API below.
Plugins are not activated automatically at Docker daemon startup. Rather, they are activated only lazily, or on-demand, when they are needed.
Systemd socket activation
Plugins may also be socket activated by systemd
. The official
Plugins helpers
natively supports socket activation. In order for a plugin to be socket activated it needs
a service
file and a socket
file.
The service
file (for example /lib/systemd/system/your-plugin.service
):
[Unit]
Description=Your plugin
Before=docker.service
After=network.target your-plugin.socket
Requires=your-plugin.socket docker.service
[Service]
ExecStart=/usr/lib/docker/your-plugin
[Install]
WantedBy=multi-user.target
The socket
file (for example /lib/systemd/system/your-plugin.socket
):
[Unit]
Description=Your plugin
[Socket]
ListenStream=/run/docker/plugins/your-plugin.sock
[Install]
WantedBy=sockets.target
This will allow plugins to be actually started when the Docker daemon connects to the sockets they're listening on (for instance the first time the daemon uses them or if one of the plugin goes down accidentally).
API design
The Plugin API is RPC-style JSON over HTTP, much like webhooks.
Requests flow from the Docker daemon to the plugin. The plugin needs to implement an HTTP server and bind this to the Unix socket mentioned in the ""plugin discovery"" section.
All requests are HTTP POST
requests.
The API is versioned via an Accept header, which currently is always set to
application/vnd.docker.plugins.v1+json
.
Handshake API
Plugins are activated via the following ""handshake"" API call.
/Plugin.Activate
Request: empty body
Response:
{
""Implements"": [""VolumeDriver""]
}
Responds with a list of Docker subsystems which this plugin implements. After activation, the plugin will then be sent events from this subsystem.
Possible values are:
Plugin retries
Attempts to call a method on a plugin are retried with an exponential backoff for up to 30 seconds. This may help when packaging plugins as containers, since it gives plugin containers a chance to start up before failing any user containers which depend on them.
Plugins helpers
To ease plugins development, we're providing an sdk
for each kind of plugins
currently supported by Docker at
docker/go-plugins-helpers.",,,
aae1edf1ecacebc6578b216750e981e812c5763b0cb21d83af78051b82bfd3a4,"SCIM provisioning
System for Cross-domain Identity Management (SCIM) is available for Docker Business customers. This guide provides an overview of SCIM provisioning.
How SCIM works
SCIM offers automated user provisioning and de-provisioning for Docker through your identity provider (IdP). Once SCIM is enabled, users assigned to the Docker application in your IdP are automatically provisioned and added to your Docker organization. If a user is unassigned, they are removed from Docker.
SCIM also syncs user profile updates, such as name changes, made in your IdP. SCIM can be used with Docker’s default Just-in-Time (JIT) provisioning configuration, or on its own with JIT disabled.
SCIM supports the automation of:
- Creating users
- Updating user profiles
- Removing and deactivating users
- Re-activating users
- Group mapping
Supported attributes
Important
Docker uses JIT provisioning by default for SSO configurations. If you enable SCIM, JIT values still overwrite the attribute values set by SCIM provisioning. To avoid conflicts, your JIT attribute values must match your SCIM attribute values. To avoid conflicts between SCIM and JIT, you can also disable JIT provisioning. See Just-in-Time for more information.
Attributes are pieces of user information, such as name and email, that are synchronized between your IdP and Docker when using SCIM. Proper mapping of these attributes is essential for seamless user provisioning and to prevent duplicate entries when using SSO.
The following table lists the supported attributes for SCIM:
| Attribute | Description |
|---|---|
| userName | User’s primary email address, used as the unique identifier |
| name.givenName | User’s first name |
| name.familyName | User’s surname |
| active | Indicates if a user is enabled or disabled, set to “false” to de-provision a user |
For additional details about supported attributes and SCIM, see Docker Hub API SCIM reference.
Enable SCIM in Docker
You must configure SSO before you enable SCIM. Enforcing SSO isn't required to use SCIM.
- Sign in to the Admin Console.
- Select your organization or company in the left navigation drop-down menu, and then select SSO and SCIM.
- In the SSO connections table, select the Actions icon and Setup SCIM.
- Copy the SCIM Base URL and API Token and paste the values into your IdP.
- Sign in to Docker Hub.
- Navigate to the SSO settings page for your organization or company.
- Organization: Select Organizations, your organization, Settings, and then Security.
- Company: Select Organizations, your company, and then Settings.
- In the SSO connections table, select the Actions icon and Setup SCIM.
- Copy the SCIM Base URL and API Token and paste the values into your IdP.
Enable SCIM in your IdP
The user interface for your IdP may differ slightly from the following steps. You can refer to the documentation for your IdP to verify. For additional details, see the documentation for your IdP:
Enable SCIM
- Sign in to Okta and select Admin to open the admin portal.
- Open the application you created when you configured your SSO connection.
- On the application page, select the General tab, then Edit App Settings.
- Enable SCIM provisioning, then select Save.
- Now you can access the Provisioning tab in Okta. Navigate to this tab, then select Edit SCIM Connection.
- To configure SCIM in Okta, set up your connection using the following values and settings:
- SCIM Base URL: SCIM connector base URL (copied from Docker Hub)
- Unique identifier field for users:
email
- Supported provisioning actions: Push New Users and Push Profile Updates
- Authentication Mode: HTTP Header
- SCIM Bearer Token: HTTP Header Authorization Bearer Token (copied from Docker Hub)
- Select Test Connector Configuration.
- Review the test results and select Save.
Enable synchronization
- In Okta, select Provisioning.
- Select To App, then Edit.
- Enable Create Users, Update User Attributes, and Deactivate Users.
- Select Save.
- Remove unnecessary mappings. The necessary mappings are:
- Username
- Given name
- Family name
- In the Azure admin portal, go to Enterprise Applications, then select the Docker application you created when you set up your SSO connection.
- Select Provisioning, then Get Started.
- Select Automatic provisioning mode.
- Enter the SCIM Base URL and API Token from Docker into the Admin Credentials form.
- Test the connection, then select Save.
- Go to Mappings, then select Provision Azure Active Directory Groups.
- Set the Enabled value to No.
- Select Provision Azure Active Directory Users.
- Remove all unsupported attributes.
- Select Save.
- Set the provisioning status to On.
Set up role mapping
You can assign roles to members in your organization in your IdP. To set up a role, you can use optional user-level attributes for the person you want to assign a role. In addition to roles, you can set an organization or team to override the default provisioning values set by the SSO connection.
Note
Role mappings are supported for both SCIM and JIT provisioning. With JIT provisioning, role mapping only applies when a user is initially provisioned to the organization.
The following table lists the supported optional user-level attributes.
| Attribute | Possible values | Considerations |
|---|---|---|
dockerRole | member , editor , or owner , for a list of permissions for each role, see
Roles and permissions | If you don't assign a role in the IdP, the value of the dockerRole attribute defaults to member . When you set the attribute, this overrides the default value. |
dockerOrg | organizationName , for example, an organization named ""moby"" would be moby | Setting this attribute overrides the default organization configured by the SSO connection. Also, this won't add the user to the default team. If this attribute isn't set, the user is provisioned to the default organization and the default team. If set and dockerTeam is also set, this provisions the user to the team within that organization. |
dockerTeam | teamName , for example, a team named ""developers"" would be developers | Setting this attribute provisions the user to the default organization and to the specified team, instead of the SSO connection's default team. This also creates the team if it doesn't exist. You can still use group mapping to provision users to teams in multiple organizations. See Group mapping for more details. |
After you set the role in the IdP, you must initiate a sync in your IdP to push the changes to Docker.
The external namespace to use to set up these attributes is urn:ietf:params:scim:schemas:extension:docker:2.0:User
.
Set up role mapping in Okta
- Setup SSO and SCIM first.
- In the Okta admin portal, go to Directory, select Profile Editor, and then User (Default).
- Select Add Attribute and configure the values for the role, organization, or team you want to add. Exact naming isn't required.
- Return to the Profile Editor and select your application.
- Select Add Attribute and enter the required values. The External Name and External Namespace must be exact. The external name values for organization/team/role mapping are
dockerOrg
,dockerTeam
, anddockerRole
respectively, as listed in the previous table. The external namespace is the same for all of them:urn:ietf:params:scim:schemas:extension:docker:2.0:User
. - After creating the attributes, navigate to the top of the page and select Mappings, then Okta User to YOUR APP.
- Go to the newly created attributes and map the variable names to the external names, then select Save Mappings. If you’re using JIT provisioning, continue to the following steps.
- Navigate to Applications and select YOUR APP.
- Select General, then SAML Settings, and Edit.
- Select Step 2 and configure the mapping from the user attribute to the Docker variables.
Assign roles by user
- In the Okta admin portal, select Directory, then People.
- Select Profile, then Edit.
- Select Attributes and update the attributes to the desired values.
Assign roles by group
- In the Okta admin portal, select Directory, then People.
- Select YOUR GROUP, then Applications.
- Open YOUR APPLICATION and select the Edit icon.
- Update the attributes to the desired values.
If a user doesn't already have attributes set up, users who are added to the group will inherit these attributes upon provisioning.
Set up role mapping in Azure AD
- Setup SSO and SCIM first.
- In the Azure AD admin portal, open Enterprise Apps and select YOUR APP.
- Select Provisioning, then Mappings, and Provision Azure Active Directory Users.
- To set up the new mapping, check Show advanced options, then select Edit attribute options.
- Create new entries with the desired mapping for role, organization, or group (for example,
urn:ietf:params:scim:schemas:extension:docker:2.0:User:dockerRole
) as a string type. - Navigate back to Attribute Mapping for users and select Add new mapping.
Expression mapping
This implementation works best for roles, but can't be used along with organization and team mapping using the same method. With this approach, you can assign attributes at a group level, which members can inherit. This is the recommended approach for role mapping.
In the Edit Attribute view, select the Expression mapping type.
If you can create app roles named as the role directly (for example,
owner
oreditor
), in the Expression field, you can useSingleAppRoleAssignment([appRoleAssignments])
.Alternatively, if you’re restricted to using app roles you have already defined (for example,
My Corp Administrators
) you’ll need to setup a switch for these roles. For example:Switch(SingleAppRoleAssignment([appRoleAssignments]), ""member"", ""My Corp Administrator"", ""owner"", ""My Corp Editor"", ""editor"")`
Set the following fields:
- Target attribute:
urn:ietf:params:scim:schemas:extension:docker:2.0:User:dockerRole
- Match objects using this attribute: No
- Apply this mapping: Always
- Target attribute:
Save your configuration.
Direct mapping
Direct mapping is an alternative to expression mapping. This implementation works for all three mapping types at the same time. In order to assign users, you'll need to use the Microsoft Graph API.
In the Edit Attribute view, select the Direct mapping type.
Set the following fields:
- Source attribute: choose one of the allowed extension attributes in Entra (for example,
extensionAttribute1
) - Target attribute:
urn:ietf:params:scim:schemas:extension:docker:2.0:User:dockerRole
- Match objects using this attribute: No
- Apply this mapping: Always
If you're setting more than one attribute, for example role and organization, you need to choose a different extension attribute for each one.
- Source attribute: choose one of the allowed extension attributes in Entra (for example,
Save your configuration.
Assign users
If you used expression mapping in the previous step, navigate to App registrations, select YOUR APP, and App Roles. Create an app role for each Docker role. If possible, create it with a display name that is directly equivalent to the role in Docker, for example, owner
instead of Owner
. If set up this way, then you can use expression mapping to SingleAppRoleAssignment([appRoleAssignments])
. Otherwise, a custom switch will have to be used. See
Expression mapping.
To add a user:
- Select YOUR APP, then Users and groups.
- Select Add user/groups, select the user you want to add, then Select their desired role.
To add a group:
- Select YOUR APP, then Users and groups.
- Select Add user/groups, select the user you want to add, then Select their desired role.
If you used direct mapping in the previous step, go to Microsoft Graph Explorer and sign in to your tenant. You need to be a tenant admin to use this feature. Use the Microsoft Graph API to assign the extension attribute to the user with the value that corresponds to what the attribute was mapped to. See the Microsoft Graph API documentation on adding or updating data in extension attributes.
See the documentation for your IdP for additional details:
Disable SCIM
If SCIM is disabled, any user provisioned through SCIM will remain in the organization. Future changes for your users will not sync from your IdP. User de-provisioning is only possible when manually removing the user from the organization.
- Sign in to the Admin Console.
- Select your organization or company in the left navigation drop-down menu, and then select SSO and SCIM.
- In the SSO connections table, select the Actions icon.
- Select Disable SCIM.
- Sign in to Docker Hub.
- Navigate to the SSO settings page for your organization or company.
- Organization: Select Organizations, your organization, Settings, and then Security.
- Company: Select Organizations, your company, and then Settings.
- In the SSO connections table, select the Actions icon.
- Select Disable SCIM.
More resources
The following videos demonstrate how to configure SCIM for your IdP:",,,
db49c3f509ebc63b2f11416c14093da84a982b2c4e9bb0f9da73785a974bad1f,"ETW logging driver
The Event Tracing for Windows (ETW) logging driver forwards container logs as ETW events. ETW stands for Event Tracing in Windows, and is the common framework for tracing applications in Windows. Each ETW event contains a message with both the log and its context information. A client can then create an ETW listener to listen to these events.
The ETW provider that this logging driver registers with Windows, has the
GUID identifier of: {a3693192-9ed6-46d2-a981-f8226c8363bd}
. A client creates an
ETW listener and registers to listen to events from the logging driver's provider.
It doesn't matter the order in which the provider and listener are created.
A client can create their ETW listener and start listening for events from the provider,
before the provider has been registered with the system.
Usage
Here is an example of how to listen to these events using the logman utility program included in most installations of Windows:
logman start -ets DockerContainerLogs -p {a3693192-9ed6-46d2-a981-f8226c8363bd} 0 0 -o trace.etl
- Run your container(s) with the etwlogs driver, by adding
--log-driver=etwlogs
to the Docker run command, and generate log messages. logman stop -ets DockerContainerLogs
- This generates an etl file that contains the events. One way to convert this
file into human-readable form is to run:
tracerpt -y trace.etl
.
Each ETW event contains a structured message string in this format:
container_name: %s, image_name: %s, container_id: %s, image_id: %s, source: [stdout | stderr], log: %s
Details on each item in the message can be found below:
| Field | Description |
|---|---|
container_name | The container name at the time it was started. |
image_name | The name of the container's image. |
container_id | The full 64-character container ID. |
image_id | The full ID of the container's image. |
source | stdout or stderr . |
log | The container log message. |
Here is an example event message (output formatted for readability):
container_name: backstabbing_spence,
image_name: windowsservercore,
container_id: f14bb55aa862d7596b03a33251c1be7dbbec8056bbdead1da8ec5ecebbe29731,
image_id: sha256:2f9e19bd998d3565b4f345ac9aaf6e3fc555406239a4fb1b1ba879673713824b,
source: stdout,
log: Hello world!
A client can parse this message string to get both the log message, as well as its context information. The timestamp is also available within the ETW event.
Note
This ETW provider only emits a message string, and not a specially structured ETW event. Therefore, you don't have to register a manifest file with the system to read and interpret its ETW events.",,,
ee891cbad59b20a8a12b1618291c9838724b0e215e4f0965601754b9681a0d3a,"Azure Blob Storage cache
Table of contents
Availability:
Experimental
The azblob
cache store uploads your resulting build cache to
Azure's blob storage service.
This cache storage backend is not supported with the default docker
driver.
To use this feature, create a new builder using a different driver. See
Build drivers for more information.
Synopsis
$ docker buildx build --push -t <registry>/<image> \
--cache-to type=azblob,name=<cache-image>[,parameters...] \
--cache-from type=azblob,name=<cache-image>[,parameters...] .
The following table describes the available CSV parameters that you can pass to
--cache-to
and --cache-from
.
| Name | Option | Type | Default | Description |
|---|---|---|---|---|
name | cache-to ,cache-from | String | Required. The name of the cache image. | |
account_url | cache-to ,cache-from | String | Base URL of the storage account. | |
secret_access_key | cache-to ,cache-from | String | Blob storage account key, see authentication. | |
mode | cache-to | min ,max | min | Cache layers to export, see cache mode. |
ignore-error | cache-to | Boolean | false | Ignore errors caused by failed cache exports. |
Authentication
The secret_access_key
, if left unspecified, is read from environment variables
on the BuildKit server following the scheme for the
Azure Go SDK.
The environment variables are read from the server, not the Buildx client.
Further reading
For an introduction to caching see Docker build cache.
For more information on the azblob
cache backend, see the
BuildKit README.",,,
5541ce2373b81fd8fbb59b20c20f7dd8215d3887b00eade5c25c5894e3e2d2c0,"Multi-platform image with GitHub Actions
You can build
multi-platform images using
the platforms
option, as shown in the following example:
Note
- For a list of available platforms, see the Docker Setup Buildx action.
- If you want support for more platforms, you can use QEMU with the Docker Setup QEMU action.
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up QEMU
uses: docker/setup-qemu-action@v3
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build and push
uses: docker/build-push-action@v6
with:
platforms: linux/amd64,linux/arm64
push: true
tags: user/app:latest
Build and load multi-platform images
The default Docker setup for GitHub Actions runners does not support loading multi-platform images to the local image store of the runner after building them. To load a multi-platform image, you need to enable the containerd image store option for the Docker Engine.
There is no way to configure the default Docker setup in the GitHub Actions
runners directly, but you can use docker/setup-docker-action
to customize the
Docker Engine and CLI settings for a job.
The following example workflow enables the containerd image store, builds a multi-platform image, and loads the results into the GitHub runner's local image store.
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Set up Docker
uses: docker/setup-docker-action@v4
with:
daemon-config: |
{
""debug"": true,
""features"": {
""containerd-snapshotter"": true
}
}
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up QEMU
uses: docker/setup-qemu-action@v3
- name: Build and push
uses: docker/build-push-action@v6
with:
platforms: linux/amd64,linux/arm64
load: true
tags: user/app:latest
Distribute build across multiple runners
In the previous example, each platform is built on the same runner which can take a long time depending on the number of platforms and your Dockerfile.
To solve this issue you can use a matrix strategy to distribute the build for
each platform across multiple runners and create manifest list using the
buildx imagetools create
command.
The following workflow will build the image for each platform on a dedicated
runner using a matrix strategy and push by digest. Then, the merge
job will
create manifest lists and push them to two registries:
- Docker Hub:
docker.io/docker-user/my-app
- GitHub Container Registry:
ghcr.io/gh-user/my-app
This example also uses the
metadata
action
to set tags and labels.
name: ci
on:
push:
env:
REGISTRY_IMAGE: user/app
jobs:
build:
runs-on: ubuntu-latest
strategy:
fail-fast: false
matrix:
platform:
- linux/amd64
- linux/arm64
steps:
- name: Prepare
run: |
platform=${{ matrix.platform }}
echo ""PLATFORM_PAIR=${platform//\//-}"" >> $GITHUB_ENV
- name: Docker meta
id: meta
uses: docker/metadata-action@v5
with:
images: ${{ env.REGISTRY_IMAGE }}
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up QEMU
uses: docker/setup-qemu-action@v3
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build and push by digest
id: build
uses: docker/build-push-action@v6
with:
platforms: ${{ matrix.platform }}
labels: ${{ steps.meta.outputs.labels }}
tags: ${{ env.REGISTRY_IMAGE }}
outputs: type=image,push-by-digest=true,name-canonical=true,push=true
- name: Export digest
run: |
mkdir -p ${{ runner.temp }}/digests
digest=""${{ steps.build.outputs.digest }}""
touch ""${{ runner.temp }}/digests/${digest#sha256:}""
- name: Upload digest
uses: actions/upload-artifact@v4
with:
name: digests-${{ env.PLATFORM_PAIR }}
path: ${{ runner.temp }}/digests/*
if-no-files-found: error
retention-days: 1
merge:
runs-on: ubuntu-latest
needs:
- build
steps:
- name: Download digests
uses: actions/download-artifact@v4
with:
path: ${{ runner.temp }}/digests
pattern: digests-*
merge-multiple: true
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Docker meta
id: meta
uses: docker/metadata-action@v5
with:
images: ${{ env.REGISTRY_IMAGE }}
tags: |
type=ref,event=branch
type=ref,event=pr
type=semver,pattern={{version}}
type=semver,pattern={{major}}.{{minor}}
- name: Create manifest list and push
working-directory: ${{ runner.temp }}/digests
run: |
docker buildx imagetools create $(jq -cr '.tags | map(""-t "" + .) | join("" "")' <<< ""$DOCKER_METADATA_OUTPUT_JSON"") \
$(printf '${{ env.REGISTRY_IMAGE }}@sha256:%s ' *)
- name: Inspect image
run: |
docker buildx imagetools inspect ${{ env.REGISTRY_IMAGE }}:${{ steps.meta.outputs.version }}
With Bake
It's also possible to build on multiple runners using Bake, with the bake action.
You can find a live example in this GitHub repository.
The following example achieves the same results as described in the previous section.
variable ""DEFAULT_TAG"" {
default = ""app:local""
}
// Special target: https://github.com/docker/metadata-action#bake-definition
target ""docker-metadata-action"" {
tags = [""${DEFAULT_TAG}""]
}
// Default target if none specified
group ""default"" {
targets = [""image-local""]
}
target ""image"" {
inherits = [""docker-metadata-action""]
}
target ""image-local"" {
inherits = [""image""]
output = [""type=docker""]
}
target ""image-all"" {
inherits = [""image""]
platforms = [
""linux/amd64"",
""linux/arm/v6"",
""linux/arm/v7"",
""linux/arm64""
]
}
name: ci
on:
push:
env:
REGISTRY_IMAGE: user/app
jobs:
prepare:
runs-on: ubuntu-latest
outputs:
matrix: ${{ steps.platforms.outputs.matrix }}
steps:
- name: Checkout
uses: actions/checkout@v4
- name: Create matrix
id: platforms
run: |
echo ""matrix=$(docker buildx bake image-all --print | jq -cr '.target.""image-all"".platforms')"" >>${GITHUB_OUTPUT}
- name: Show matrix
run: |
echo ${{ steps.platforms.outputs.matrix }}
- name: Docker meta
id: meta
uses: docker/metadata-action@v5
with:
images: ${{ env.REGISTRY_IMAGE }}
- name: Rename meta bake definition file
run: |
mv ""${{ steps.meta.outputs.bake-file }}"" ""${{ runner.temp }}/bake-meta.json""
- name: Upload meta bake definition
uses: actions/upload-artifact@v4
with:
name: bake-meta
path: ${{ runner.temp }}/bake-meta.json
if-no-files-found: error
retention-days: 1
build:
runs-on: ubuntu-latest
needs:
- prepare
strategy:
fail-fast: false
matrix:
platform: ${{ fromJson(needs.prepare.outputs.matrix) }}
steps:
- name: Prepare
run: |
platform=${{ matrix.platform }}
echo ""PLATFORM_PAIR=${platform//\//-}"" >> $GITHUB_ENV
- name: Download meta bake definition
uses: actions/download-artifact@v4
with:
name: bake-meta
path: ${{ runner.temp }}
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up QEMU
uses: docker/setup-qemu-action@v3
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build
id: bake
uses: docker/bake-action@v6
with:
files: |
./docker-bake.hcl
cwd://${{ runner.temp }}/bake-meta.json
targets: image
set: |
*.tags=${{ env.REGISTRY_IMAGE }}
*.platform=${{ matrix.platform }}
*.output=type=image,push-by-digest=true,name-canonical=true,push=true
- name: Export digest
run: |
mkdir -p ${{ runner.temp }}/digests
digest=""${{ fromJSON(steps.bake.outputs.metadata).image['containerimage.digest'] }}""
touch ""${{ runner.temp }}/digests/${digest#sha256:}""
- name: Upload digest
uses: actions/upload-artifact@v4
with:
name: digests-${{ env.PLATFORM_PAIR }}
path: ${{ runner.temp }}/digests/*
if-no-files-found: error
retention-days: 1
merge:
runs-on: ubuntu-latest
needs:
- build
steps:
- name: Download meta bake definition
uses: actions/download-artifact@v4
with:
name: bake-meta
path: ${{ runner.temp }}
- name: Download digests
uses: actions/download-artifact@v4
with:
path: ${{ runner.temp }}/digests
pattern: digests-*
merge-multiple: true
- name: Login to DockerHub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Create manifest list and push
working-directory: ${{ runner.temp }}/digests
run: |
docker buildx imagetools create $(jq -cr '.target.""docker-metadata-action"".tags | map(select(startswith(""${{ env.REGISTRY_IMAGE }}"")) | ""-t "" + .) | join("" "")' ${{ runner.temp }}/bake-meta.json) \
$(printf '${{ env.REGISTRY_IMAGE }}@sha256:%s ' *)
- name: Inspect image
run: |
docker buildx imagetools inspect ${{ env.REGISTRY_IMAGE }}:$(jq -r '.target.""docker-metadata-action"".args.DOCKER_META_VERSION' ${{ runner.temp }}/bake-meta.json)",,,
9ffe558a8b5f5beb9da498f57fed1bc469164aea903acdbb77fb26f4ecd7ddad,"Install Docker Desktop on Ubuntu
Docker Desktop terms
Commercial use of Docker Desktop in larger enterprises (more than 250 employees OR more than $10 million USD in annual revenue) requires a paid subscription.
This page contains information on how to install, launch and upgrade Docker Desktop on an Ubuntu distribution.
Prerequisites
To install Docker Desktop successfully, you must:
- Meet the general system requirements.
- Have an x86-64 system with Ubuntu 22.04, 24.04, or the latest non-LTS version.
- For non-Gnome Desktop environments,
gnome-terminal
must be installed:$ sudo apt install gnome-terminal
Install Docker Desktop
Recommended approach to install Docker Desktop on Ubuntu:
Set up Docker's package repository. See step one of Install using the
apt
repository.Download the latest DEB package. For checksums, see the Release notes.
Install the package with apt as follows:
$ sudo apt-get update $ sudo apt-get install ./docker-desktop-amd64.deb
Note
At the end of the installation process,
apt
displays an error due to installing a downloaded package. You can ignore this error message.N: Download is performed unsandboxed as root, as file '/home/user/Downloads/docker-desktop.deb' couldn't be accessed by user '_apt'. - pkgAcquire::Run (13: Permission denied)
By default, Docker Desktop is installed at
/opt/docker-desktop
.
There are a few post-install configuration steps done through the post-install script contained in the deb package.
The post-install script:
- Sets the capability on the Docker Desktop binary to map privileged ports and set resource limits.
- Adds a DNS name for Kubernetes to
/etc/hosts
. - Creates a symlink from
/usr/local/bin/com.docker.cli
to/usr/bin/docker
. This is because the classic Docker CLI is installed at/usr/bin/docker
. The Docker Desktop installer also installs a Docker CLI binary that includes cloud-integration capabilities and is essentially a wrapper for the Compose CLI, at/usr/local/bin/com.docker.cli
. The symlink ensures that the wrapper can access the classic Docker CLI.
Launch Docker Desktop
To start Docker Desktop for Linux:
Navigate to the Docker Desktop application in your Gnome/KDE Desktop.
Select Docker Desktop to start Docker.
The Docker Subscription Service Agreement displays.
Select Accept to continue. Docker Desktop starts after you accept the terms.
Note that Docker Desktop won't run if you do not agree to the terms. You can choose to accept the terms at a later date by opening Docker Desktop.
For more information, see Docker Desktop Subscription Service Agreement. It is recommended that you also read the FAQs.
Alternatively, open a terminal and run:
$ systemctl --user start docker-desktop
When Docker Desktop starts, it creates a dedicated context that the Docker CLI can use as a target and sets it as the current context in use. This is to avoid a clash with a local Docker Engine that may be running on the Linux host and using the default context. On shutdown, Docker Desktop resets the current context to the previous one.
The Docker Desktop installer updates Docker Compose and the Docker CLI binaries
on the host. It installs Docker Compose V2 and gives users the choice to
link it as docker-compose from the Settings panel. Docker Desktop installs
the new Docker CLI binary that includes cloud-integration capabilities in /usr/local/bin/com.docker.cli
and creates a symlink to the classic Docker CLI at /usr/local/bin
.
After you’ve successfully installed Docker Desktop, you can check the versions of these binaries by running the following commands:
$ docker compose version
Docker Compose version v2.29.1
$ docker --version
Docker version 27.1.1, build 6312585
$ docker version
Client:
Version: 23.0.5
API version: 1.42
Go version: go1.21.12
<...>
To enable Docker Desktop to start on sign in, from the Docker menu, select Settings > General > Start Docker Desktop when you sign in to your computer.
Alternatively, open a terminal and run:
$ systemctl --user enable docker-desktop
To stop Docker Desktop, select the Docker menu icon to open the Docker menu and select Quit Docker Desktop.
Alternatively, open a terminal and run:
$ systemctl --user stop docker-desktop
Upgrade Docker Desktop
Once a new version for Docker Desktop is released, the Docker UI shows a notification. You need to download the new package each time you want to upgrade Docker Desktop and run:
$ sudo apt-get install ./docker-desktop-amd64.deb
Next steps
- Explore Docker's subscriptions to see what Docker can offer you.
- Take a look at the Docker workshop to learn how to build an image and run it as a containerized application.
- Explore Docker Desktop and all its features.
- Troubleshooting describes common problems, workarounds, how to run and submit diagnostics, and submit issues.
- FAQs provide answers to frequently asked questions.
- Release notes lists component updates, new features, and improvements associated with Docker Desktop releases.
- Back up and restore data provides instructions on backing up and restoring data related to Docker.",,,
3bf726fc9638d15ed326cada1179eee54055aa80b96ec0a3959f38e352b6c430,"Docker Desktop release notes
This page contains information about the new features, improvements, known issues, and bug fixes in Docker Desktop releases.
Releases are gradually rolled out to ensure quality control. If the latest version is not yet available to you, allow some time — updates typically become available within a week of the release date.
Docker Desktop versions older than 6 months from the latest release are not available for download. Previous release notes are available in our documentation repository.
For more frequently asked questions, see the FAQs.
Warning
If you're experiencing malware detection issues on Mac, follow the steps documented in docker/for-mac#7527.
4.38.0
2025-01-30Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
New
- Installing Docker Desktop via the PKG installer is now generally available.
- Enforcing sign-in via configuration profiles is now generally available.
- Docker Compose, Docker Scout, the Docker CLI, and Ask Gordon can now be updated independently of Docker Desktop and without a full restart (Beta).
- The new
update
command has been added to the Docker Desktop CLI (Mac only). - Bake is now generally available, with support for entitlements and composable attributes.
- You can now create multi-node Kubernetes clusters in Docker Desktop.
- Ask Gordon is more widely available. It is still in Beta.
Upgrades
- containerd v1.7.24
- Docker Buildx v0.20.1
- Docker Compose v2.32.4
- Docker Engine v27.5.1
- Docker Scout CLI v1.16.1
- Runc v1.2.2
- NVIDIA Container Toolkit v1.17.4
- Kubernetes v1.31.4
- Docker Debug
v0.0.38
Bug fixes and enhancements
For all platforms
- Fixed a bug where access tokens generated by the
docker login
web flow could not be refreshed by Docker Desktop. - Fixed a bug where container creation via the Docker API using
curl
failed when Enhanced Container Isolation was enabled. - Fixed a bug where the RAM policy was not refreshed after the refresh period had elapsed.
- Fixed a bug in Enhanced Container Isolation when mounting the Docker socket into a container, and then creating Docker containers with bind-mounts from within that container.
- Fixed an issue that caused a discrepancy between the GUI and the CLI, the former forcing the
0.0.0.0
HostIP in port-mappings. This caused default binding IPs configured through Engine'sip
flag, or through the bridge optioncom.docker.network.bridge.host_binding_ipv4
, to not be used. - Fixed a bug where the
pac
setting was ignored inadmin-settings.json
. - Build UI:
- Added a progress status when importing a build.
- Fixed a bug where users were unable to import builds.
- Fixed a bug where some builders using SSH endpoints were not skipped.
For Mac
- Fixed a bug in Docker VMM where bind-mounts from non-root volumes would weren't working as expected.
- Fixed an issue that caused startup failures on systems without IPv6. Fixes docker/for-mac#14298.
- Fixed a bug that caused Docker Desktop to hang. See docker/for-mac#7493.
- Fixed an issue where the uninstaller would fail if the settings file is missing.
- Fixed a bug where config profiles deployed via Workspace One were ignored.
For Windows
- The Docker Desktop installer will now present a UAC prompt when launched.
- Fixed an issue where Docker Desktop would fail to start for data disks created with old WSL versions that shared the same identifier as other WSL distros.
- Docker Desktop now restarts when WSL integration settings are changed. This ensures proper setup of WSL integration when using Enhanced Container Isolation.
For Linux
- Added support for gvisor networking. Users with an incompatible version of qemu (8.x) will stay on qemu networking, and others will be migrated automatically.
Deprecation
For all platforms
- Deprecated
com.docker.diagnose check|check-dot|check-hypervisordetect-host-hypervisor
.
4.37.2
2025-01-09Download Docker Desktop
Mac with Apple chip (checksum) | Mac with Intel chip (checksum)
Bug fixes and enhancements
For Mac
- Prevents a bug that caused Docker Desktop to not update
com.docker.vmnetd
orcom.docker.socket
to newer versions.
Known issues
For Mac
- If you’re seeing a security popup about malware on
com.docker.vmnetd
orcom.docker.socket
, follow the steps documented in docker/for-mac#7527.
4.37.1
2024-12-17Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
Bug fixes and enhancements
For all platforms
- Fixed an issue that caused the AI Catalog in Docker Hub to be unavailable in Docker Desktop.
- Fixed an issue that caused Docker Desktop to panic with
index out of range [0] with length 0
when using Enhanced Container Isolation.
Known issues
For Mac
- If you’re seeing a security popup about a malware on
com.docker.vmnetd
orcom.docker.socket
, follow the steps documented in docker/for-mac#7527.
4.37.0
2024-12-12Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
New
- You can now perform key operations such as starting, stopping, restarting, and checking the status of Docker Desktop directly from the command line (Beta).
- The AI Catalog in Docker Hub is available directly through Docker Desktop.
Upgrades
- Docker Buildx v0.19.2
- Docker Compose v2.31.0
- Docker Engine v27.4.0
- Docker Scout CLI v1.15.1
- NVIDIA Container Toolkit v1.17.2
Bug fixes and enhancements
For all platforms
- The default disk usage limit for Docker Engine in new installations is now 1TB.
- Fixed an issue where containers could not establish loopback
AF_VSOCK
connections. - Fixed a bug where resetting default settings would also reset the CLI context.
- Fixed a bug where the Docker Desktop Dashboard would get out of sync with the Docker daemon after restarting the engine while in Resource Saver mode (Windows with WSL2 backend only) or after switching engines (macOS).
- Fixed a bug where Resource Saver mode would fail to re-engage after restarting the engine while in Resource Saver mode.
- Build UI:
- Fixed a bug where the source file could not be found for some builds.
- Fixed a bug where error logs were not displayed in the Source tab.
- Fixed a bug where users had to scroll to the bottom for error logs in Source tab.
- Fixed a bug where timestamps would be broken in the Logs tab.
For Mac
- Fixed a bug that would create certain user directories with root permission when running the uninstaller binary twice with
sudo
.
For Windows
- Added support for Windows on ARM using WSL 2 version 2.3.24 and later to single distribution mode on WSL 2.
- Fixed an issue where Docker Desktop would fail to start. Fixes docker/for-win#14453
Known issues
For all platforms
- Kubernetes cluster may not start if Registry Access Manager is enabled. As a workaround, add
registry.k8s.io
and<geo>-docker.pkg.dev
to Registry Access Management policies.
For Mac
- If you’re seeing a security popup about a malware on
com.docker.vmnetd
orcom.docker.socket
, follow the steps documented in docker/for-mac#7527.
Deprecation
For Mac
- QEMU (Legacy) as a VMM on Apple Silicon will be removed in a future version. It is recommended that you switch to the Apple Virtualization Framework for increased performance and stability. If you encounter an issue, contact Docker Support or file a GitHub issue.
- osxfs (Legacy) will be removed in a future version. It is recommended that you switch to VirtioFS for increased performance. If you encounter an issue, contact Docker Support or file a GitHub issue.
4.36.1
2025-01-09Download Docker Desktop
Mac with Apple chip (checksum) | Mac with Intel chip (checksum)
Bug fixes and enhancements
For Mac
- Prevents a bug that caused Docker Desktop to not update
com.docker.vmnetd
orcom.docker.socket
to newer versions.
Known issues
For Mac
- If you’re seeing a security popup about malware on
com.docker.vmnetd
orcom.docker.socket
, follow the steps documented in docker/for-mac#7527.
4.36.0
2024-11-18Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
New
- Existing Docker Desktop installations using the WSL2 engine on Windows are now automatically migrated to a unified single-distribution architecture for enhanced consistency and performance.
- Administrators can now:
- Enforce sign-in with macOS configuration profiles (Early Access).
- Enforce sign-in for more than one organization at a time (Early Access).
- Deploy Docker Desktop for Mac in bulk with the PKG installer (Early Access).
- Use Desktop Settings Management to manage and enforce defaults via admin.docker.com (Early Access).
- Enhance Container Isolation (ECI) has been improved to:
- Allow admins to turn off Docker socket mount restrictions.
- Support wildcard tags when using the
allowedDerivedImages
setting.
Upgrades
- Docker Buildx v0.18.0
- Docker Compose v2.30.3
- Kubernetes v1.30.5
- NVIDIA Container Toolkit v1.17.0
- Docker Scout CLI v1.15.0
- Docker Init v1.4.0
- Linux kernel
v6.10.13
Bug fixes and enhancements
For all platforms
- Fixed a bug where the
docker events
command would not terminate after streaming the events. - Docker Init: Improved Dockerfile caching for PHP applications that don't use Docker Compose.
- Synchronized file shares now respects the
filesharingAllowedDirectories
setting inadmin-settings.json
. - Fixed an issue where if Docker Desktop is configured to use a proxy, it fails to start due to an internal timeout while fetching authentication tokens.
- Added a recovery banner to retry an update if the download failed.
- Fixed an issue where if the
umask
is set to577
it would causerpmbuild
failure. Fixes docker/for-mac#6511. - Fixed a bug that restricted containers using
--network=host
to 18 open host ports. - Fixed bind mount ownership for non-root containers. Fixes docker/for-mac#6243.
- Docker Desktop will not unpause automatically after a manual pause. The system will stay paused until you manually resume the Docker engine. This fixes a bug where other software would accidentally trigger a resume by running a CLI command in the background. Fixes for-mac/#6908
- Build UI:
- The Source tab now supports multiple source files.
- Links for image dependencies in the Info tab now support other well-known registries such as GitHub, Google, and GitLab.
- Disabled the Delete button if only cloud builds are selected.
- Fixed an issue where users were unable to delete builds.
- Fixed malformed Jaeger traces that were missing events and links.
- Fixed missing export attributes when building with the cloud driver.
For Mac
- Fixed a bug in Docker VMM that prevented MySQL and other databases containers to start. Fixes reports from docker/for-mac#7464.
- The minimum memory requirement is now automatically adjusted for Docker VMM, improving the user experience and addressing reports from docker/for-mac#7464, docker/for-mac#7482.
- Fixed a bug where the advanced option Allowed privileged port mapping was not working as expected. Fixes docker/for-mac#7460.
- Docker Desktop can now automatically configure shell completion scripts for zsh, bash and fish inside the install wizard and settings screen.
- Fixed a bug where the in-app update would fail if Docker Desktop was installed by a non-admin user or if the current user was previously an administrator. Fixes for-mac/#7403 and for-mac/#6920
For Windows
- Fixed a bug preventing UDP port 53 to be bound.
- Fixed a bug where Windows daemon options were overwritten at startup.
4.35.2
2025-01-09Download Docker Desktop
Mac with Apple chip (checksum) | Mac with Intel chip (checksum)
Bug fixes and enhancements
For Mac
- Prevents a bug that caused Docker Desktop to not update
com.docker.vmnetd
orcom.docker.socket
to newer versions.
Known issues
For Mac
- If you’re seeing a security popup about malware on
com.docker.vmnetd
orcom.docker.socket
, follow the steps documented in docker/for-mac#7527.
4.35.1
2024-10-30Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
For all platforms
- Fixed a bug where Docker Desktop would incorrectly bind to port
8888
. Fixes docker/for-win#14389 and docker/for-mac#7468
4.35.0
2024-10-24Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
New
- Support for Docker Desktop on Red Hat Enterprise Linux is now generally available.
- Volume Backup and Share is now generally available and can be found in the Volumes view.
- Terminal support within Docker Desktop using system shells is now generally available.
- Beta release of Docker VMM - the more performant alternative to Apple Virtualization Framework on macOS (requires Apple Silicon and macOS 12.5 or later).
Upgrades
- containerd v1.7.21
- Docker Buildx v0.17.1
- Docker Compose v2.29.7
- Docker Engine v27.3.1
- Docker Scout CLI v1.14.0
- Docker Debug
v0.0.37
- Linux kernel
v6.10.9
Bug fixes and enhancements
For all platforms
- Fixed a bug where proxy settings in
daemon.json
would override proxies set in Docker Desktop settings. - Fixed a bug where some Docker subnet ranges were not able to be used.
- Removed
docker-index as it is now deprecated, you can use
docker scout cves fs://<path to binary>
instead. - Fixed a bug where images couldn't be sorted or filtered by tag. Fixes docker/for-win#14297.
- Fixed a bug where the
docker
CLI did not work as expected when theregistry.json
file was malformed. - Fixed a bug where the Push to Docker Hub action in the Images view would result in an
invalid tag format
error. Fixes docker/for-win#14258. - Fixed an issue where Docker Desktop startup failed when ICMPv6 setup was not successful.
- Added drivers that allow USB/IP to work.
- Fixed a bug in Enhanced Container Isolation (ECI) Docker socket mount permissions for derived images where it was incorrectly denying Docker socket mounts for some images when Docker Desktop uses the containerd image store.
- Enable
NFT_NUMGEN
,NFT_FIB_IPV4
andNFT_FIB_IPV6
kernel modules. - Build UI:
- Highlight build check warnings in the Completed builds list.
- Improve visualization for the build time charts.
- Image tags added to Build results section under the Info tab.
- Improved efficiency of host-side disk utilization for fresh installations on Mac and Linux.
- Fixed a bug that prevented the Sign in enforcement popup to be triggered when token expires.
- Fixed a bug where containers would not be displayed in the GUI immediately after signing in when using enforced sign-in.
settings.json
has been renamed tosettings-store.json
- The host networking feature no longer requires users to be signed-in in order to use it.
For Mac
- Fixed a bug where auto-start containers could be misconfigured after changing filesharing type in settings.
- Fixed a bug that would cause
~/.docker/cli-plugins
to not be populated on start-up. - Fixed a bug that prevented php composer or postgres to start as non root user. Fixes docker/for-mac#7415.
- Fixed a bug that could cause file changed on the host to appear truncated. Fixes docker/for-mac#7438.
For Windows
- New installations of Docker Desktop for Windows now require a Windows version of 19045 or later.
- Fixed an issue that caused a start failure if IPv6 is disabled either in the kernel config or via the kernel command-line in WSL. Fixes docker/for-win#14240
- Fixed the Clean / Purge data button on Windows. Fixes docker/for-win#12650.
- Disk usage statistics is now displayed in the Dashboard footer installations.
- Improved recovery for WSL distribution issues.
For Linux
- Ubuntu 24.04 is now supported on Docker Desktop.
Known issues
For Mac
- Since version 4.34.0, the toggle ""Allow privileged port mapping"" in the Advanced settings does not work. For more information, see docker/for-mac#7460.
For Windows
- Users with versions 4.14.0 and earlier could encounter issues using the in-app update. To update to the latest version, download and install the latest Docker Desktop from this page.
4.34.4
2025-01-09Download Docker Desktop
Mac with Apple chip (checksum) | Mac with Intel chip (checksum)
Bug fixes and enhancements
For Mac
- Prevents a bug that caused Docker Desktop to not update
com.docker.vmnetd
orcom.docker.socket
to newer versions.
Known issues
For Mac
- If you’re seeing a security popup about malware on
com.docker.vmnetd
orcom.docker.socket
, follow the steps documented in docker/for-mac#7527.
4.34.3
2024-10-09Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
Upgrades
Security
- Fixed CVE-2024-9348 which allows RCE via image build details source information
- Fixed NVIDIA Container Toolkit CVE-2024-0132
- Fixed NVIDIA Container Toolkit CVE-2024-0133
4.34.2
2024-09-12Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
Bug fixes and enhancements
For all platforms
- Fixed a bug where
docker compose up
would become unresponsive while in Resource Saver mode.
Security
- Fixed CVE-2024-8695 which allows RCE via crafted extension description/changelog which could be abused by a malicious extension.
- Fixed CVE-2024-8696 which allows RCE via crafted extension publisher-url/additional-urls which could be abused by a malicious extension.
4.34.1
2024-09-05Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) |
Bug fixes and enhancements
For Windows
- Fixed a bug where Docker Desktop failed to start (often on first boot) incorrectly believing another instance of the application is running. ( docker/for-win#14294 and docker/for-win#14034).
4.34.0
2024-08-29Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
New
- Host networking support on Docker Desktop is now generally available.
- If you authenticate via the CLI, you can now authenticate through a browser-based flow, removing the need for manual PAT generation.
- Windows now supports automatic reclamation of disk space in Docker Desktop for WSL2 installations using a managed virtual hard disk.
- Deploying Docker Desktop via the MSI installer is now generally available.
- Two new methods to
enforce sign-in (windows registry key and
.plist
file) are now generally available. - Fresh installations of Docker Desktop now use the containerd image store by default.
- Compose Bridge (Experimental) is now available from the Compose file viewer. Easily convert and deploy your Compose project to a Kubernetes cluster.
Upgrades
- Docker Engine v27.2.0
- Docker Compose v2.29.2
- containerd v1.7.20
- Docker Scout CLI v1.13.0
- Docker Buildx v0.16.2
- Linux kernel
v6.10.1
Bug fixes and enhancements
For all platforms
- Fixed a bug that caused the CLI to become idle when a container was started with AutoRemove (
--rm
) but whose port bindings would be rejected by Docker Desktop at start-up. - Fixed a bug where diagnostics collection would fail sporadically on the Support screen.
- Fixed a bug where folders wouldn't expand in a container's File tab. Fixes docker/for-win#14204.
- In-app updates now respect the proxy settings.
- Extended the ECI Docker socket mount permissions feature to optionally child images derived from allowed images. This allows ECI to work with buildpacks (e.g., Paketo) that create ephemeral local images that use Docker socket mounts.
- Fixed a bug that caused the Containers view to flash when using certain proxy settings. Fixes docker/for-win#13972.
- Improved the output of
docker image list
to show multi-platform-related image information.
For Mac
- Fixed a bug where a
Partial repair error
would occasionally appear when triggering the Configuration integrity check feature. - Configuration integrity check feature now shows information on why the Docker socket is mis-configured.
- Fixed an issue where the Configuration integrity check feature would report the system path instead of the user path if Docker Desktop is installed as
User
. - Fixed a bug where applications trying to read extended attributes from bind mounted volumes could experience failures. Fixes docker/for-mac#7377.
For Windows
- Fixed a bug where Docker Desktop would reset docker's
credsStore
todesktop
when the user's intention is to keep it empty. Fixes docker/for-win#9843. - Fixed a bug that would cause Docker Desktop to not start in the WSL2 engine docker/for-win#14034.
- Fixed a bug that caused WSL distribution to terminate abruptly. Fixes docker/for-win/14230.
- Fixed an issue that caused WSL to update in each startup. Fixes docker/for-win/13868, docker/for-win/13806.
Known issues
- Compose Bridge does not work automatically when you enable it within the Experimental settings tab. It takes a few minutes before you are notified that you must 'repair' Docker Desktop which then installs the
compose-bridge
binary. - The Convert and Deploy button in the Compose file viewer might be disabled even when Kubernetes is running and Compose Bridge is enabled. The workaround for this is to disable Compose Bridge in the Experimental settings tab, apply the change with Apply & restart, then re-enable and select Apply & restart again.
- There is a known issue when authenticating against a registry in the Docker CLI (
docker login [registry address]
) where, if the provided registry address includes a repository/image name (such asdocker login index.docker.io/docker/welcome-to-docker
), the repository part (docker/welcome-to-docker
) is not normalized and results in credentials being stored incorrectly, which causes subsequent pulls from the registry (docker pull index.docker.io/docker/welcome-to-docker
) to not be authenticated. To prevent this, don't include any extraneous suffix in the registry address when runningdocker login
.Note
Using
docker login
with an address that includes URL path segments is not a documented use case and is considered unsupported. The recommended usage is to specify only a registry hostname, and optionally a port, as the address fordocker login
. - When running
docker compose up
and Docker Desktop is in the Resource Saver mode, the command is unresponsive. As a workaround, manually exit the Resource Saving mode and Docker Compose becomes responsive again. - When Enhanced Container Isolation (ECI) is enabled, Docker Desktop may not enter Resource Saver mode. This will be fixed in a future Docker Desktop release.
- The new ECI Docker socket mount permissions for derived images feature does not yet work when Docker Desktop is configured with the Use containerd for pulling and storing images. This will be fixed in the next Docker Desktop release.
4.33.2
2025-01-09Download Docker Desktop
Mac with Apple chip (checksum) | Mac with Intel chip (checksum)
Bug fixes and enhancements
For Mac
- Prevents a bug that caused Docker Desktop to not update
com.docker.vmnetd
orcom.docker.socket
to newer versions.
Known issues
For Mac
- If you’re seeing a security popup about malware on
com.docker.vmnetd
orcom.docker.socket
, follow the steps documented in docker/for-mac#7527.
4.33.1
2024-07-31Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) |
Bug fixes and enhancements
For Windows
- Added support for WSL2 2.3.11 and above, which includes loadable kernel modules. Fixes docker/for-win#14222
4.33.0
2024-07-25Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
New
- Docker Debug is now generally available.
- BuildKit now evaluates Dockerfile rules to inform you of potential issues.
- Resource Allocation settings can now be accessed directly from the resource usage data displayed in the Dashboard footer.
- New and improved experience for troubleshooting.
Upgrades
- Docker Compose v2.29.1
- Docker Engine v27.1.1
- containerd v1.7.19
- NVIDIA Container Toolkit v1.16.0
- Docker Scout CLI v1.11.0
- Kubernetes v1.30.2
- Linux kernel
v6.10
Bug fixes and enhancements
For all platforms
- Fixed an issue that caused containers started with
--net=host
and listening on an IPv6 address to be accessible from the host. - Improved the UX for enabling the containerd image store in the Settings tab.
- Fixed an issue that caused a deadlock seen while using the
grpcfuse
filesharing option under heavy load. - Fixed a bug where Mac-specific admin settings were impacting other platforms.
- IPv6 address blocks can now be specified in Docker Engine's
default-address-pools
. - Fixed an issue with the validation of the Docker Engine's
bip
,fixed-cidr
andfixed-cidr-v6
. Fixes docker/for-mac#7104. - Docker Engine's
default-network-opts
parameter is now properly validated. - VirtioFS performance improvements include increasing directory cache timeout, handling change notifications from the host, removing extra FUSE operations for security.capability attributes, optimizing host event detection, and providing an API to clean caches after container termination.
- Docker Desktop now notifies when there is a port conflict in a host networking container.
- Compose Bridge command line option is now available via Experimental features. When enabled, run
compose-bridge
to convert your Compose configuration to Kubernetes resources. - Builds view:
- Added build checks to the build details' Source tab.
- Added build tags to the build details' Info tab under the Source details section.
- Newly imported builds are now highlighted.
- Improved performance of error message handling.
- Fixed a connection issue to the builder which prevented build records from displaying.
- Fixed the navigation when opening builds through the CLI.
For Mac
- The Configuration integrity check feature now provides more context around what has changed with your Docker Desktop configuration. For more information, see the FAQs.
- The Configuration integrity check feature shows an error when it fails to repair Docker Desktop.
- Fixed a bug where the IPv6 TCP was set to
host.docker.internal
. Fixes docker/for-mac#7332. - Fixed an issue where the
docker-compose
symlink pointed to an empty location. Fixes docker/for-mac#7345.
For Linux
- Fixed an issue where some
wincred
values were persisted after uninstall. Reported by Javier Yong @Javiery3889. - Fixed an issue where the notification Another application changed your Desktop configurations is incorrectly triggered.
Security
For all platforms
- Includes a fix for AuthZ Plugin Bypass Regression in Docker Engine. For more information, see CVE-2024-41110.
For Windows
- Fixed an issue where some
wincred
values were persisted after uninstall. Reported by Javier Yong @Javiery3889.
Known Issues
For Windows
- Docker Desktop fails to start with WSL pre-releases
v2.3.11.0
andv2.3.12.0
, which is included in Windows 11 Insider. To fix this ensure WSLv2.2.4.0
is installed. For more information, see microsoft/WSL#11794. This affects Docker Desktop 4.33.0 and earlier.
4.32.1
2025-01-09Download Docker Desktop
Mac with Apple chip (checksum) | Mac with Intel chip (checksum)
Bug fixes and enhancements
For Mac
- Prevents a bug that caused Docker Desktop to not update
com.docker.vmnetd
orcom.docker.socket
to newer versions.
Known issues
For Mac
- If you’re seeing a security popup about malware on
com.docker.vmnetd
orcom.docker.socket
, follow the steps documented in docker/for-mac#7527.
4.32.0
2024-07-04Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
New
- Docker Engine and CLI updated to version 27.0.
- Docker Desktop now supports moving data to a different drive on macOS and Windows with WSL2 backend. See docker/for-win#13384.
- You can now schedule backups for volume exports in the Volumes tab (Beta).
- Access a terminal shell directly from Docker Desktop (Beta).
Upgrades
- Docker Buildx v0.15.1
- Docker Compose v2.28.1
- Docker Scout CLI v1.10.0
- Docker Engine v27.0.3
- Docker Init v1.3.0
Bug fixes and enhancements
For all platforms
- Improved instructions for
watch
in the Compose File Viewer - Added support for Golang projects that don't have dependencies in Docker Init. Addresses docker/roadmap#611
- Settings Management now lets admins set the default value to
ProxyEnableKerberosNTLM
. - Removed a temporary compatibility fix for older versions of Visual Studio Code.
- Builds view:
- Changed icon for imported build record to a ""files"" icon.
- Improved the error message when trying to connect to an already connected Docker Build Cloud builder.
- Fixed an issue where build records would disappear unexpectedly.
- Fixed an issue that prevented users from being able to re-open an imported build.
- Fixed an issue where build details were not displayed when a build's state had changed from running to completed.
- Fixed malformed build source link in build details.
- Fixed missing build stats for named contexts.
- Fixed image index/manifest not being displayed anymore in build results.
- Fixed an issue where build traces exported from the UI would appear as a single, flattened list when imported to Jaeger
- Fixed truncated digest/sha in build details.
- Fixed final status animation of active builds.
For Windows
- Fixed an issue on the WSL 2 engine where Docker Desktop would not detect the existence of the
docker-desktop-data
distribution if it had been manually moved by the user. - The Windows on ARM installer and the privileged service are now built for ARM64.
For Mac
- Re-added
CONFIG_DM_CRYPT
kernel module. - Re-added
CONFIG_PSI
kernel module. - Re-added
CONFIG_GTP
kernel module. - Re-added
CONFIG_NFT_BRIDGE_META
kernel module. - Fixed a regression where the Another application changed your Desktop configuration warning message appeared whenever
/var/run/docker.socket
was pointing to an unexpected path. - Changed the Configuration Check menu entry and banner to a notification.
- Improved the performance of read and write operations on bind mounts.
- Fixed fatal errors with some
AMD64
Java images. Fixes docker/for-mac/7286 and docker/for-mac/7006. - Fixed an issue that caused Docker Desktop to remove
Docker.app
when installing from/Applications
. - Fixed an issue that caused bind mounts to fail. Fixes docker/for-mac#7274.
Known issues
For all platforms
- The Manage Synchronized File Shares with Compose setting is automatically enabled for all users who opt into Access experimental features. This converts all bind mounts into synchronized file shares. To disable this behavior, deselect Access experimental features. Then, manually delete any file shares by going to the File sharing tab within Resources, navigating to the Synchronized file shares section, selecting the file shares you want to remove, and selecting Delete.
For Mac
- When running
docker-compose
after an update, it will returncommand not found
. As a workaround, you can create the following symlink:sudo ln -sf /Applications/Docker.app/Contents/Resources/cli-plugins/docker-compose /usr/local/bin/docker-compose
4.31.1
2024-06-10Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) |
Bug fixes and enhancements
For Windows
- Fixed a bug where containers, images and volumes created before the update were potentially invisible for users. Fixes docker/for-win#14118.
4.31.0
2024-06-06Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
New
- Air-Gapped Containers is now generally available.
- Docker Compose File Viewer shows your Compose YAML with syntax highlighting and contextual links to relevant docs (Beta, progressive rollout).
- New Sidebar user experience.
Upgrades
- Docker Engine and CLI v26.1.4.
- Docker Scout CLI v1.9.1
- Docker Compose v2.27.1
- Docker Buildx v0.14.1
- Containerd v1.6.33
- Credential Helpers v0.8.2
- NVIDIA Container Toolkit v1.15.0
- Go 1.22.4
- Linux kernel
v6.6.31
Bug fixes and enhancements
For all platforms
- Newer releases are now displayed in the Software updates settings tab when an update has already been downloaded.
- Added
proxyEnableKerberosNTLM
config tosettings.json
to enable fallback to basic proxy authentication if Kerberos/NTLM environment is not properly set up. - Fixed a bug where Docker Debug was not working properly with Enhanced Container Isolation enabled.
- Fixed a bug where UDP responses were not truncated properly.
- Fixed a bug where the Update screen was hidden when using Settings Management.
- Fixed a bug where proxy settings defined in
admin-settings.json
were not applied correctly on startup. - Fixed a bug where the Manage Synchronized file shares with Compose toggle did not correctly reflect the value with the feature.
- Fixed a bug where a bind mounted file modified on host is not updated after the container restarts, when gRPC FUSE file sharing is used on macOS and on Windows with Hyper-V. Fixes docker/for-mac#7274, docker/for-win#14060.
- Builds view:
- New Import builds feature that lets you import build records for builds by other people, or builds in a CI environment.
- Fixed missing OpenTelemetry traces in build results for failed builds.
- Fixed
default-load
appearing as invalid driver-opt for the container driver. - Fixed deep link to build details.
For Windows
- Changed the
--allowed-org
installer flag to write a policy registry key instead of to theregistry.json
.
For Mac
- Moved the setting Automatically check configuration from Advanced settings to General settings.
- Improved VirtioFS caching by implementing longer attributes timeout and invalidation.
For Linux
- Added Linux headers to the VM, to ease the compilation of custom kernel modules.
Security
For all platforms
- Fixed a security bug in Enhanced Container Isolation (ECI) mode where a user could create Docker volumes sourced from restricted directories inside the Docker Desktop VM and mount them into containers, thereby giving the container access to such restricted VM directories.
- By default, only extensions listed in the marketplace can be installed in Docker Desktop. This can be changed in Docker Desktop's settings. Extension developers will need to change this option in order to test their extensions.
For Windows
- Fixed
CVE-2024-5652 in which a user in the
docker-users
group can cause a Windows Denial-of-Service through theexec-path
Docker daemon config option in Windows containers mode. This vulnerability was discovered by Hashim Jawad ( @ihack4falafel) working with Trend Micro Zero Day Initiative.
Deprecation
For all platforms
- The CLI binary that used to be shipped as
com.docker.cli
is now shipped simply asdocker
. This release leaves the CLI binary ascom.docker.cli
, but it will be removed next release.
For Windows
- Removed support for legacy version packs from the WSL2 engine.
Known Issues
For Windows
- When upgrading to Docker Desktop 4.31.0, existing containers, images and volumes become invisible for users that created those containers and images using Docker Desktop 4.8.0 or lower, on Windows hosts with WSL only. The data is not lost, it just becomes invisible to Docker Desktop 4.31.0. If impacted, downgrade to version 4.30 or earlier. For more information see: docker/for-win#14118.
For Linux
- Ubuntu 24.04 LTS is not yet supported, Docker Desktop will fail to start. Due to a change in how the latest Ubuntu release restricts the unprivileged namespaces,
sudo sysctl -w kernel.apparmor_restrict_unprivileged_userns=0
needs to be ran at least once. Refer to the Ubuntu Blog.
4.30.0
2024-05-06Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
New
For all platforms
- Docker Desktop now supports SOCKS5 proxies. Requires a Business subscription.
- Added a new setting to manage the onboarding survey in Settings Management.
For Windows
- Added support for Kerberos and NTLM proxy authentication on Windows. Requires a Business subscription.
Upgrades
- Docker Compose v2.27.0
- Docker Engine v26.1.1
- Wasm runtimes:
- Updated
runwasi
shims tov0.4.0
- Updated
deislabs
shims tov0.11.1
- Updated
spin
shim tov0.13.1
- Updated
- Docker Scout CLI v1.8.0
- Docker Debug
v0.0.29
- Linux kernel
v6.6.26
- Go 1.22.2
Bug fixes and enhancements
For all platforms
- Improved Enhanced Container Isolation (ECI) security when running
docker build
commands in rootless containers. - Fixed a bug where
docker events
exited withUnexpected EOF
when Docker Desktop entered/exited Resource Saver mode. - Fixed a bug where
docker stats --no-stream
hung when Docker Desktop was in Resource Saver mode. - Fixed a bug in the self-diagnose CLI that incorrectly showed the VM had not started. Fixes docker/for-mac#7241.
- Fixed a bug where high-throughput port forward transfers could stall. Fixes docker/for-mac#7207.
- Fixed CLI-plugin symlinks not being removed when CLI apps were removed.
- Fixed a bug in the shared ports drawer to show the right message for local engines.
- Dev Environments is being sunset and has moved to the Beta tab in Features in development.
- Builds view:
- Better bulk delete for build records.
- Added action to open the relevant web page for container images and Git sources in build dependencies.
- Added action to download Provenance and OpenTelemetry traces in Jaeger or OTLP format.
- Fixed source details for remote build invocations.
- Fixed a bug where multi-platform builds would show up as separate records when using a cloud builder.
For Mac
- Fixed a bug where a segmentation fault was triggered with Virtualization Framework, on post-2019 Macs. See docker/for-mac#6824.
- Enabled
CONFIG_SECURITY=y
kernel config, for example for Tetragon. Fixes docker/for-mac#7250. - Re-added support for
SQUASHFS
compression. Fixes docker/for-mac#7260. - Fixed a bug that caused a new version of Docker Desktop to be marked as damaged.
- Increased network MTU when using qemu on Apple Silicon.
- Fixed a bug preventing Docker Desktop to start if Rosetta was not installed. Fixes docker/for-mac#7243.
For Windows
- Added a simplified provisioning mode for WSL2 that avoids the need for the ancillary
docker-desktop-data
WSL distribution (experimental). - Fixed bash completions for the Docker CLI in a WSL environment.
- Fixed a regression in Docker Desktop 4.28 that caused host files bind-mounted into containers to not show up properly inside the container, when using Docker-in-Docker (via mounts of
/var/run/docker.sock
) on WSL. - Fixed a bug that would cause the following error
merging settings: integratedWslDistros type mismatch
.
Known issues
For all platforms
- If you have enabled a feature in Docker Desktop that requires you to be signed in, such as Host networking you must remain signed in to use Docker Desktop. To continue using Docker Desktop or to modify these settings, ensure you are signed in.
- To enable or disable Manage Synchronized file shares with Compose, Access experimental features and Manage Synchronized file shares with Compose have to be checked or unchecked at the same time.
- The Docker CLI will sometimes hang when running a container with the autoremove option (
--rm
) if the container fails to start (e.g.:docker run --rm alpine invalidcommand
). In this case, the CLI process may need to be manually killed.
For Windows
- When starting Docker Desktop as a non-admin user, the following error connect
ENOENT \\.\pipe\errorReporter
might be triggered if the user is not a member of the docker-users group. This can be resolved by adding the user to the docker-users group. Before starting Docker Desktop, make sure to sign out and then sign back in and unregisterdocker-desktop
distribution if that was created, usingwsl --unregister docker-desktop
.
For Linux
- Ubuntu 24.04 LTS is not yet supported, Docker Desktop will fail to start. Due to a change in how the latest Ubuntu release restricts the unprivileged namespaces,
sudo sysctl -w kernel.apparmor_restrict_unprivileged_userns=0
needs to be ran at least once. Refer to the Ubuntu Blog for more details.
4.29.0
2024-04-08Download Docker Desktop
Windows (checksum) | Windows ARM Beta (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
New
- You can now enforce Rosetta usage via Settings Management.
- Docker socket mount restrictions with ECI is now generally available.
- Docker Engine and CLI updated to Moby 26.0. This includes Buildkit 0.13, sub volumes mounts, networking updates, and improvements to the containerd multi-platform image store UX.
- New and improved Docker Desktop error screens: swift troubleshooting, easy diagnostics uploads, and actionable remediation.
- Compose supports Synchronized file shares (experimental).
- New interactive Compose CLI (experimental).
- Beta release of:
- Air-Gapped Containers with Settings Management.
- Host networking in Docker Desktop.
- Docker Debug for running containers.
- Volumes Backup & Share extension functionality available in the Volumes tab.
Upgrades
- Docker Compose v2.26.1
- Docker Scout CLI v1.6.3
- Docker Engine v26.0.0
- Buildx v0.13.1
- Kubernetes v1.29.2
- cri-dockerd v0.3.11
- Docker Debug v0.0.27
Bug fixes and enhancements
For all platforms
- Fixed an issue with dropdown menu opening beyond the application window.
- Docker Init:
- Updated the formatting of CLI output to improve legibility.
- Fixed an issue with
.dockerignore
to avoid ignoring application files that start with ""compose"". - Improved how Java applications are started based on Spring Boot version. Fixes docker/for-mac#7171.
- Removed non-official Docker image used for Rust cross-compilation.
- The maximum number of files per Synchronized file share now exceeds 2 million.
- Fixed an issue that caused the warning: ""The value provided to Autocomplete is invalid."" when selecting the Export to local image field.
- Run Cloud can now be accessed from the Docker Desktop Dashboard.
- Opting out from sending analytics will now also disable collecting data for bug reports.
- You can now share and unshare a port to the Cloud Engine in the Containers view.
- Shared cloud can now be accessed from the footer in the right-hand side of the Dashboard.
- Added beta support for host networking on macOS, Windows and Docker Desktop for Linux docker#238.
- Added a timestamp to new unread notifications.
- Fixed typo in the virtualization support error message. Fixes docker/desktop-linux#197.
- Docker Desktop now allows connections to
host.docker.internal
to be blocked by a rule in a PAC file. - Fixed the placement of the secondary menu in the Images and Containers lists.
- Fixed a race condition that occurred when starting Docker Desktop with QEMU.
- Improved the error message when an image pull is blocked by Registry Access Management policy.
- Re-add
CONFIG_BONDING=y
in the kernel config.
For Mac
- Fixed Kubernetes not starting successfully. Fixes docker/for-mac#7136 and docker/for-mac#7031.
- Fixed a bug when the browser was not able to send back authentication information to Docker Desktop. Fixes docker/for-mac/issues#7160.
For Windows
- Fixed a bug where
docker run -v
would fail after switching between WSL 2 and Hyper-V. - Fixed a bug where Docker Desktop was not stopping its WSL distributions (
docker-desktop
anddocker-desktop-data
) when it was shutdown. Fixes docker/for-win/issues/13443 and docker/for-win/issues/13938.
For Linux
- Fixed an issue that caused the list of available experimental features in the UI to become out-of-sync with the backend data.
Security
- Disabled Electron
runAsNode
fuse to improve security hardening. For more info, see Electron's documentation.. - Fixed CVE-2024-6222 which allows an attacker who has gained access to the Docker Desktop VM through a container breakout to further escape to the host by passing extensions and dashboard related IPC messages. Reported by Billy Jheng Bing-Jhong, Đỗ Minh Tuấn, Muhammad Alifa Ramdhan working with Trend Micro Zero Day Initiative.
Known issues
For Mac
- Docker Desktop on Apple Silicon doesn't start if Rosetta is not installed. This will be fixed in future releases. See docker/for-mac#7243.
4.28.0
2024-02-26New
- Settings Management now allows admins to set the default file-sharing implementation and specify which paths developer can add file shares to.
- Added support for
socks5://
HTTP and HTTPS proxy URLs when theSOCKS
proxy support beta feature is enabled. - Users can now filter volumes to see which ones are in use in the Volumes tab.
Upgrades
- Compose v2.24.6
- Docker Engine v25.0.3
- Docker Scout CLI v1.5.0
- Qemu 8.1.5
- Wasm runtimes:
- Updated runwasi shims to
v0.4.0
, including:- wasmtime
v17.0
, with initial support for WASI preview 2 - wasmedge
v0.13.5
- wasmer
v4.1.2
- wasmtime
- Updated deislabs shims to
v0.11.1
, including:- lunatic
v0.13.2
- slight
v0.5.1
- spin
v2.2.0
- wws
v1.7.0
- lunatic
- Updated runwasi shims to
Bug fixes and enhancements
For all platforms
- Fixed
postgis
withQemu
. Fixes docker/for-mac#7172. - Re added
CONFIG_BLK_DEV_DM
kernel config forkpartx
. Fixes docker/for-mac#7197. - Allow
SOCKS
proxies to be set via a proxy autoconfigpac file
. - Re added
CONFIG_AUDIT
kernel config. - Fixed a bug with the Rust build on
virtiofs
. See rust-lang/docker-rust#161. - Fixed an issue that caused the
missing registry authentication
error when pulling Kubernetes images. - Fixed an issue that caused Docker Compose commands to hang.
- Fixed a bug in
docker build
that caused Docker Desktop to crash. Fixes docker/for-win#13885, docker/for-win#13896, docker/for-win#13899, docker/for-mac#7164, docker/for-mac#7169 - Docker Init:
- Improved how Java applications are started based on Spring Boot version. Fixes docker/for-mac#7171.
- Removed non-official Docker image used for Rust cross-compilation
- Builds view:
- Active and completed builds can be found in dedicated tabs.
- Build details now displays build duration and cache steps.
- OpenTelemetry traces are now displayed in the build results.
- Fixed an issue where context builders events were not always triggered.
- Restyle the empty state view to make the dashboard clearer.
For Mac
- Fix
httpd
issue with Rosetta. docker/for-mac#7182 - Fixed a bug that caused a crash on the
virtualization.framework
. Fixes docker/for-mac#7024
For Windows
- Fixed an issue with DNS timeouts on Windows.
- Added support for Enhanced Container Isolation Docker socket mount permission on WSL user distributions.
- Fixed an issue that caused the
failed to get console mode
error when redirecting output from the CLI. - Fixed an issue with the engine socket permissions when mounted inside containers. Fixes docker/for-win#13898
Known Issues
For Windows
- In dark mode, the Disk image location in Resources>Advanced settings is not visible. As a workaround, change to light mode.
4.27.2
2024-02-08Upgrades
- Compose v2.24.5
- Docker Scout CLI v1.4.1
- Docker Debug v0.0.24
Bug fixes and enhancements
For all platforms
- Fixed a bug where the diagnostics ID would not print correctly when uploading diagnostics from the terminal.
- Fixed a bug where the default settings values were being reset to default on startup, when using Settings Management.
- Fixed a bug with the dashboard being shown at startup even though the Open Docker Dashboard when Docker Desktop starts option was disabled. Fixes docker/for-win#13887.
- Fixed a bug in the build backend service that caused Docker Desktop to crash. Fixes docker/for-win#13885, docker/for-win#13896, docker/for-win#13899, docker/for-mac#7164, docker/for-mac#7169.
- Fixed the Docker Engine socket permissions when mounted inside containers. Fixes docker/for-win#13898.
- Docker Scout:
- Updated dependencies to address Leaky Vessels series of CVEs ( CVE-2024-21626, CVE-2024-24557)
- Added initial VEX document to document false positive CVE-2020-8911 and CVE-2020-8912
- Added support for cosign SBOM attestations
- Added support for VEX in-toto attestations
- Docker Debug:
- Fixed a bug when pulling the image behind resource accesses management
- Fixed connection issues
For Mac
- Re-added kernel modules needed by
Istio
. Fixes docker/for-mac#7148. - Node now uses all the cores available under Rosetta.
- Fixed an issue with
php-fpm
. Fixes docker/for-mac#7037.
4.27.1
2024-02-01Upgrades
- Docker Engine v25.0.2 which contains a fix for CVE-2024-24557, CVE-2024-23650, CVE-2024-23651, CVE-2024-23652 and CVE-2024-23653
- Containerd v1.6.28
- Runc v1.1.12 which contains a fix for CVE-2024-21626
Bug fixes and enhancements
For Mac
- Fixed a bug that caused Docker Desktop to hang when applying an update.
4.27.0
2024-01-25New
- Docker init now supports Java and is generally available to all users.
- Synchronized File Shares provides fast and flexible host-to-VM file sharing within Docker Desktop. Utilizing the technology behind Docker’s acquisition of Mutagen, this feature provides an alternative to virtual bind mounts that uses synchronized filesystem caches, improving performance for developers working with large codebases.
- Organization admins can now configure Docker socket mount permissions when ECI is enabled.
- Containerd Image Store support is now generally available to all users.
- Get a debug shell into any container or image with the new
docker debug
command (Beta). - Organization admins, with a Docker Business subscription, can now configure a custom list of extensions with Private Extensions Marketplace enabled (Beta)
Upgrades
- Amazon ECR Credential Helper v0.7.1
- Buildx v0.12.1
- Containerd v1.6.27
- Compose v2.24.3
- Docker Credential Helpers v0.8.1
- Runc v1.1.11
- Docker Engine v25.0.0
- Kubernetes v1.29.1
- Docker Scout v1.3.0
Bug fixes and enhancements
For all platforms
- The
docker scan
command has been removed. To continue learning about the vulnerabilities of your images, and many other features, use thedocker scout
command. - Fixed a bug where automatic updates would not download when the Always download updates checkbox was selected.
- Fixed typo in the dashboard tooltip. Fixes docker/for-mac#7132
- Improved signal handling behavior (e.g. when pressing Ctrl-C in the terminal while running a
docker
command). - Re-added kernel modules required by
minikube start --cni=cilium
. - Fixed a bug that caused the installation screen to appear again when admin controls are enabled after sign in.
- Fixed a bug where Docker would not start if a shared folder is no longer present.
- Fixed the number of available CPUs displayed in the Containers section of the Dashboard.
- Re-added kernel modules for
btrfs
,xfs
,vfat
,exfat
,ntfs3
,f2fs
,squashfs
,udf
,9p
andautofs
. - Container usage charts have been moved to a vertical Resource usage side panel to allow for more space in the containers list. Accessing the usage charts remains the same via the Show charts button.
- Fixed a bug where selecting Close Application at sign-in was leaving behind a hung backend process.
- Fixed a bug which caused Docker Desktop to become unresponsive when analytics is disabled through Settings Management.
- Docker init:
- Added support for containerizing a Java server
- Various fixes on Windows
- Builder settings:
- You can now refresh storage data for your builder at any point in time.
- You can now delete the build history for a builder.
- Builds view:
- An error message is now shown when a build record cannot be removed.
- Fixed an issue where a cloud builder could not be created in rootless mode on macOS.
- Inline cache and Git source are now properly handled in the Build timing section of the Info tab.
- The Builder used and the author invoking the build is now displayed in past builds on the History tab.
- Several improvements made to better link past builds on the History tab.
- Several improvements to make the build name more accurate.
- Fixed stuck builds in the Active builds list when a builder cannot be reached.
- Fixed an issue preventing the build record from being deleted in some circumstances.
- Fixed an issue where build names could be empty.
- Fixed a general issue with the Builds view when Resource saver mode is enabled.
For Mac
- Enabled
Huge Pages
and fixed PHP segmentation fault with Rosetta. Fixes docker/for-mac#7117. - Fixed
xvfb
under Rosetta. Fixes docker/for-mac#7122 - Fixed
ERR_WORKER_INVALID_EXEC_ARGV
error under Rosetta. docker/for-mac#6998. - Fixed a bug where Docker Desktop could deadlock if
admin-settings.json
was syntactically invalid.
For Windows
- Fixed a bug that prevented UTF-16 strings from being encoded to UTF-8 for some locales. Fixes docker/for-win#13868.
- Fixed a bug where the credentials store configuration would reset on app restart with the WSL integration. Fixes docker/for-win#13529.
- Fixed an issue that prevented the correct WSL engine errors from propagating to the user.
- Fixed an issue that would cause Docker Desktop to hang when quitting from Windows Containers mode.
Security
For Windows
- Mitigated several DLL side-loading vulnerabilities in the Docker Desktop installer on Windows, reported by Suman Kumar Chakraborty ( @Hijack-Everything)
Known issues
For all platforms
- When using Setting Management, the settings that are not set in the
admin-settings.json
will be reset to default when Docker Desktop starts.
For Mac
- Updating to 4.27.0 from the Software updates sometimes hangs. As a workaround, use the 4.27.0 installer from this page.
4.26.1
2023-12-14Bug fixes and enhancements
For all platforms
- Updated feedback links inside Docker Desktop to ensure they continue to work correctly
For Windows
- Switch the CLI binaries to a version compatible with older versions of glibc, such as used in Ubuntu 20.04 fixes docker/for-win#13824
4.26.0
2023-12-04New
- Administrators can now control access to beta and experimental features in the Features in development tab with Settings Management.
- Introduced four new version update states in the footer.
docker init
(Beta) now supports PHP with Apache + Composer.- The Builds view is now GA. You can now inspect builds, troubleshoot errors, and optimize build speed.
Upgrades
- Compose v2.23.3
- Docker Scout CLI v1.2.0.
- Buildx v0.12.0
- Wasm runtimes:
- wasmtime, wasmedge and wasmer
v0.3.1
. - lunatic, slight, spin, and wws
v0.10.0
. - Wasmtime is now based on wasmtime
v14.0
and supports wasi preview-2 components - Wasmedge is now based on WasmEdge
v0.13.5
- Spin is now based on Spin
v2.0.1
- wws is now based on wws
v1.7.0
- wasmtime, wasmedge and wasmer
- Docker Engine v24.0.7
- Containerd v1.6.25
- runc v1.1.10
Bug fixes and enhancements
For all platforms
- You can now provide feedback from the commandline by using
docker feedback
. - Improved the text and position of the startup options in the General settings tab.
- Redesigned the dashboard's header bar, added links to other Docker resources, improved display of account information.
- Fixed a bug where enabling the containerd image store and Wasm simultaneously would not enable Wasm.
- containerd integration:
- Fixed
docker push/pull
authentication not being sent to non-DockerHub registries in cases whereServerAddress
is not provided. - Fixed
docker history
reporting wrong IDs and tags. - Fixed
docker tag
not preserving internal metadata. - Fixed
docker commit
when the daemon configured with--userns-remap
. - Fixed
docker image list
to show real image creation date. - Added support for
-a
flag todocker pull
(pull all remote repository tags). - Added support for
--group-add
flag todocker run
(append extra groups). - Adjusted some errors reported by
docker push/pull
.
- Fixed
- Docker Init:
- Improved cross-compilation in Dockerfiles for Golang and Rust.
- Improved caching in Dockerfile for ASP.NET Core.
- Docker Desktop now gives more detailed information about pending updates in the dashboard footer.
- Fixed a bug in Enhanced Container Isolation mode where
docker run --init
was failing. - Fixed a bug where a notification prompting the user to download a new version of Docker Desktop remained visible after the user started downloading the new version.
- Added a notification that indicates when Docker Desktop is installing a new version.
- Fixed a bug where the cursor changed to a pointer when the user hovered over a notification that has no call to action.
For Mac
- Fixed an issue where Rosetta would not work with PHP. Fixes docker/for-mac#6773 and docker/for-mac#7037.
- Fixed several issues related to Rosetta not working. Fixed [ docker/for-mac#6973, [ docker/for-mac#7009, [ docker/for-mac#7068 and [ docker/for-mac#7075
- Improved the performance of NodeJS under Rosetta.
- Fixed the Unable to open /proc/self/exe Rosetta errors.
- Fixed a bug were the setting Start Docker Desktop when you sign in would not work. Fixes docker/for-mac#7052.
- You can now enable the use of Kernel networking path for UDP through the UI. Fixes docker/for-mac#7008.
- Fixed a regression where the
uninstall
CLI tool was missing. - Addressed an issue which caused Docker Desktop to become unresponsive when analytics were disabled with Settings Management.
For Windows
- Added support for WSL mirrored mode networking (requires WSL
v2.0.4
and up). - Added missing signatures on DLL and VBS files.
Known issues
For Windows
- Docker CLI doesn’t work when using WSL 2 integration on an older Linux distribution (for example, Ubuntu 20.04) which uses a
glibc
version older than2.32
. This will be fixed in future releases. See docker/for-win#13824.
4.25.2
2023-11-21Bug fixes and enhancements
For all platforms
- Fixed a bug where a blank UI would appear after submitting a response in the Welcome Survey.
For Windows
- Fixed a bug where Docker Desktop on WSL 2 would shut down dockerd unexpectedly when idle. Fixes docker/for-win#13789
4.25.1
2023-11-13Bug fixes and enhancements
For all platforms
- Fixed a regression in 4.25 where Docker would not start if the swap file was corrupt. Corrupt swap files will be re-created on next boot.
- Fixed a bug when swap is disabled. Fixes docker/for-mac#7045, docker/for-mac#7044 and docker/for-win#13779.
- The
sysctl vm.max_map_count
is now set to 262144. See docker/for-mac#7047
For Windows
- Fixed an issue where Switch to Windows Containers would not appear on the tray menu for some users. See docker/for-win#13761.
- Fixed a bug where the WSL integration would not work for users using a shell other than
sh
. See docker/for-win#13764. - Re-added
DockerCli.exe
.
4.25.0
2023-10-26New
- Rosetta is now Generally Available for all users on macOS 13 or later. It provides faster emulation of Intel-based images on Apple Silicon. To use Rosetta, see Settings. Rosetta is enabled by default on macOS 14.1 and later.
- Docker Desktop now detects if a WSL version is out of date. If an out dated version of WSL is detected, you can allow Docker Desktop to automatically update the installation or you can manually update WSL outside of Docker Desktop.
- New installations of Docker Desktop for Windows now require a Windows version of 19044 or later.
- Administrators now have the ability to control Docker Scout image analysis in Settings Management.
Upgrades
Bug fixes and enhancements
For all platforms
- Fixed a spacing problem in the
Accept License
pop-up. - Fixed a bug where the Notifications drawer changed size when navigating between Notifications list and Notification details view.
- containerd integration:
docker push
now supportsLayer already exists
andMounted from
progress statuses.docker save
is now able to export images from all tags of the repository.- Hide push upload progress of manifests, configs and indexes (small json blobs) to match the original push behavior.
- Fixed
docker diff
containing extra differences. - Fixed
docker history
not showing intermediate image IDs for images built with the classic builder. - Fixed
docker load
not being able to load images from compressed tar archives. - Fixed registry mirrors not working.
- Fixed
docker diff
not working correctly when called multiple times concurrently for the same container. - Fixed
docker push
not reusing layers when pushing layers to different repositories on the same registry.
- Docker Init:
- Fixed outdated links to Docker documentation included in generated files
- Add support for ASP.NET Core 8 (in addition to 6 and 7)
- Fixed a bug that caused a failure when installing Wasm shims.
- Fixed a bug where Docker Desktop exits the Resource Saver mode every 15 minutes, or, if the timer is set above 15 minutes, the resource saver mode never kicks in.
- Promoted the Enable background SBOM indexing option to General settings.
For Mac
- Minimum OS version to install or update Docker Desktop on macOS is now macOS Monterey (version 12) or later.
- Enhanced error messaging when an update cannot be completed if the user doesn't match the owner of
Docker.app
. Fixes docker/for-mac#7000. - Fixed a bug where Re-apply configuration might not work when
/var/run/docker.sock
is mis-configured. - Docker Desktop doesn't overwrite
ECRCredentialHelper
if already present in/usr/local/bin
.
For Windows
- Fixed an issue where Switch to Windows Containers would show in the tray menu on Windows Home Editions. Fixes docker/for-win#13715
For Linux
- Fixed a bug in
docker login
. Fixes docker/docker-credential-helpers#299
Known Issues
For Mac
- Upgrading to MacOS 14 can cause Docker Desktop to also update to a latest version even if the auto update option is disabled.
- Uninstalling Docker Desktop from the command line is not available. As a workaround, you can uninstall Docker Desktop from the Dashboard.
For Windows
- Switch to Windows containers option in the tray menu may not show up on Windows. As a workaround, edit the
settings.json
file and set""displaySwitchWinLinContainers"": true
.
For all platforms
- Docker operations, such as pulling images or logging in, fail with 'connection refused' or 'timeout' errors if the Swap file size is set to 0MB. As a workaround, configure the swap file size to a non-zero value in the Resources tab in Settings.
4.24.2
2023-10-12Bug fixes and enhancements
For all platforms
- Fixed a bug where Docker Desktop would send multiple requests to
notify.bugsnag.com
. Fixes docker/for-win#13722. - Fixed a performance regression for PyTorch.
4.24.1
2023-10-04Download Docker Desktop
Bug fixes and enhancements
For Windows
- Fixed a bug on Docker Desktop for Windows where the Docker Desktop Dashboard wouldn't display container logs correctly. Fixes docker/for-win#13714.
4.24.0
2023-09-28New
- The new Notification center is now available to all users so you can be notified of new releases, installation progress updates, and more. Select the bell icon in the bottom-right corner of the Docker Desktop Dashboard to access the notification center.
- Compose Watch is now available to all users. For more information, see Use Compose Watch.
- Resource Saver is now available to all users and is enabled by default. To configure this feature, navigate to the Resources tab in Settings. For more information see Docker Desktop's Resource Saver mode.
- You can now view and manage the Docker Engine state, with pause, stop, and resume, directly from the Docker Desktop Dashboard.
Upgrades
- Compose v2.22.0
- Go 1.21.1
- Wasm runtimes:
- wasmtime, wasmedge
v0.2.0
. - lunatic, slight, spin, and wws
v0.9.1
. - Added wasmer wasm shims.
- wasmtime, wasmedge
Bug fixes and enhancements
For all platforms
- Docker Init:
- Fixed an issue formatting Dockerfile file paths for ASP.NET projects on Windows.
- Improved performance on language detection for large directories with lots of files.
- Added a timeout to polling for resource usage stats used by the Containers view. Fixes docker/for-mac#6962.
- containerd integration:
- Implemented push/pull/save image events.
- Implemented pulling legacy schema1 images.
- Implemented
docker push --all-tags
. - Implemented counting containers using a specific image (visible for example in
docker system df -v
). - Validated pulled image names are not reserved.
- Handle
userns-remap
daemon setting. - Fixed legacy builder build errors when multiple COPY/ADD instructions are used.
- Fixed
docker load
causing pool corruption which could some subsequent image related operations. - Fixed not being able to reference images via truncated digest with a
sha256:
prefix. - Fixed
docker images
(without--all
) showing intermediate layers (created by the legacy classic builder). - Fixed
docker diff
containing extra differences. - Changed
docker pull
output to match the output with containerd integration disabled.
- Fixed a grammatical error in Kubernetes status message. See docker/for-mac#6971.
- Docker containers now use all host CPU cores by default.
- Improved inter-process security in dashboard UI.
For Mac
- Fixed a kernel panic on Apple silicon Macs with macOS version below 12.5. Fixes docker/for-mac#6975.
- Fixed a bug where Docker Desktop failed to start if invalid directories were included in
filesharingDirectories
. Fixes docker/for-mac#6980. - Fixed a bug where installer is creating root-owned directories. Fixes docker/for-mac#6984.
- Fixed a bug where installer is failing on setting up the docker socket when missing
/Library/LaunchDaemons
. Fixes docker/for-mac#6967. - Fixed a permission denied error when binding a privileged port to a non-localhost IP on macOS. Fixes docker/for-mac#697.
- Fixed a resource leak introduced in 4.23. Related to docker/for-mac#6953.
For Windows
- Fixed a bug where a ""Docker Desktop service not running"" popup appeared when service is already running. See docker/for-win#13679.
- Fixed a bug that caused Docker Desktop fail to start on Windows hosts. Fixes docker/for-win#13662.
- Modified the Docker Desktop resource saver feature to skip reducing kernel memory on WSL when no containers are running, as this was causing timeouts in some cases. Instead, users are encouraged to enable ""autoMemoryReclaim"" on WSL directly via the .wslconfig file (available since WSL 1.3.10).
Known issues
For Mac
- Creating a container with the port 53 fails with the error address
already in use
. As a workaround, deactivate network acceleration by adding""kernelForUDP"": false
, in thesettings.json
file located at~/Library/Group Containers/group.com.docker/settings.json
.
4.23.0
2023-09-11Upgrades
- Compose v2.21.0
- Docker Engine v24.0.6
- Docker Scout CLI v0.24.1.
- Wasm runtimes:
- wasmtime, wasmedge revision
d0a1a1cd
. - slight and spin wasm
v0.9.0
.
- wasmtime, wasmedge revision
New
- Added support for new Wasm runtimes: wws and lunatic.
docker init
now supports ASP.NET- Increased performance of exposed ports on macOS, for example with
docker run -p
.
Removed
- Removed Compose V1 from Docker Desktop as it has stopped receiving updates. Compose V2 has replaced it and is now integrated into all current Docker Desktop versions. For more information, see Migrate to Compose V2.
Bug fixes and enhancements
For all platforms
- With
Docker Scout, you can now:
- Manage temporary and cached files with
docker scout cache
. - Manage environments with
docker scout environment
. - Configure the default organization with
docker scout config
. - List packages of an image with their vulnerabilities with
docker scout cves --format only-packages
. - Enroll an organization with Docker scout with
docker scout enroll
. - Stop, analyze, and compare local file systems with
docker scout cves --type fs
.
- Manage temporary and cached files with
- Fixed a bug where
docker stats
would hang when Docker Desktop was in Resource Saver mode. - Fixed a bug where turning off experimental features via Settings in the Docker Desktop Dashboard would not fully turn off Resource Saver mode.
- Fixed a bug where the Containers list action button was clipped.
- containerd image store:
- Fixed
failed to read config content
error when interacting with some images. - Fixed building Dockerfiles with
FROM scratch
instruction when using the legacy classic builder (DOCKER_BUILDKIT=0
). - Fixed
mismatched image rootfs errors
when building images with legacy classic builder (DOCKER_BUILDKIT=0
). - Fixed
ONBUILD
andMAINTAINER
Dockerfile instruction - Fixed healthchecks.
- Fixed
For Mac
- All users on macOS 12.5 or greater now have VirtioFS turned on by default. You can revert this in Settings in the General tab.
- Improved single-stream TCP throughput.
- Reinstated the health check for macOS that notifies you if there has been a change on your system which might cause problems running Docker binaries.
For Linux
- Fixed a bug where the GUI is killed when opening the Docker Desktop app twice. See docker/desktop-linux#148.
For Windows
- Fixed a bug where non-admin users would get prompted for credentials when switching to Windows Containers or after disabling WSL and switching to the Hyper-V engine. This issue would occur after an OS restart, or on a cold start of Docker Desktop.
Security
For all platforms
- Fixed CVE-2023-5165 which allows Enhanced Container Isolation bypass via debug shell. The affected functionality is available for Docker Business customers only and assumes an environment where users are not granted local root or Administrator privileges.
- Fixed CVE-2023-5166 which allows Access Token theft via a crafted extension icon URL.
Known Issues
- Binding a privileged port on Docker Desktop does not work on macOS. As a workaround you can expose the port on all interfaces (using
0.0.0.0
) or using localhost (using127.0.0.1
).
4.22.1
2023-08-24Bug fixes and enhancements
For all platforms
- Mitigated several issues impacting Docker Desktop startup and Resource Saver mode. docker/for-mac#6933
For Windows
- Fixed
Clean / Purge data
troubleshoot option on Windows. docker/for-win#13630
4.22.0
2023-08-03Upgrades
Note
In this release, the bundled Docker Compose and Buildx binaries show a different version string. This relates to our efforts to test new features without causing backwards compatibility issues.
For example,
docker buildx version
outputsbuildx v0.11.2-desktop.1
.
New
- Resource Usage has moved from experimental to GA.
- You can now split large Compose projects into multiple sub-projects with
include
.
Bug fixes and enhancements
For all platforms
- Settings Management now lets you turn off Docker Extensions for your organisation.
- Fixed a bug where turning on Kubernetes from the UI failed when the system was paused.
- Fixed a bug where turning on Wasm from the UI failed when the system was paused.
- Bind mounts are now shown when you inspect a container.
- You can now download Wasm runtimes when the containerd image store is enabled.
- With
Quick Search, you can now:
- Find any container or Compose app residing on your local system. In addition, you can access environment variables and perform essential actions such as starting, stopping, or deleting containers.
- Find public Docker Hub images, local images, or images from remote repositories.
- Discover more about specific extensions and install them.
- Navigate through your volumes and gain insights about the associated containers.
- Search and access Docker's documentation.
For Mac
- Fixed a bug that prevented Docker Desktop from starting. docker/for-mac#6890
- Resource Saver is now available on Mac. It optimises Docker Desktop's usage of your system resources when no containers are running. To access this feature, make sure you have turned on access to experimental features in settings.
For Windows
- Fixed a bug where the self-diagnose tool showed a false-positive failure when vpnkit is expected to be not running. Fixes docker/for-win#13479.
- Fixed a bug where an invalid regular expression in the search bar caused an error. Fixes docker/for-win#13592.
- Resource Saver is now available on Windows Hyper-V. It optimises Docker Desktop's usage of your system resources when no containers are running. To access this feature, make sure you have turned on access to experimental features in settings.
4.21.1
2023-07-03For all platforms
- Fixed connection leak for Docker contexts using SSH ( docker/for-mac#6834 and docker/for-win#13564)
For Mac
- Removed configuration health check for further investigation and addressing specific setups.
4.21.0
2023-06-29New
- Added support for new Wasm runtimes: slight, spin, and wasmtime. Users can download Wasm runtimes on demand when the containerd image store is enabled.
- Added Rust server support to Docker init.
- Beta release of the Builds view that lets you inspect builds and manage builders. This can be found in the Features in Development tab in Settings.
Upgrades
- Buildx v0.11.0
- Compose v2.19.0
- Kubernetes v1.27.2
- cri-tools v1.27.0
- cri-dockerd v0.3.2
- coredns v1.10.1
- cni v1.2.0
- etcd v3.5.7
Bug fixes and enhancements
For all platforms
- Docker Desktop now automatically pauses the Docker Engine when it is not in use and wakes up again on demand.
- VirtioFS is now the default file sharing implementation for new installations of Docker Desktop on macOS 12.5 and higher.
- Improved product usage reporting using OpenTelemetry (experimental).
- Fixed Docker socket permissions. Fixes docker/for-win#13447 and docker/for-mac#6823.
- Fixed an issue which caused Docker Desktop to hang when quitting the application whilst paused.
- Fixed a bug which caused the Logs and Terminal tab content in the Container view to be covered by a fixed toolbar docker/for-mac#6814.
- Fixed a bug which caused input labels to overlap with input values on the container run dialog. Fixes docker/for-win#13304.
- Fixed a bug which meant users couldn't select the Docker Extension menu. Fixes docker/for-mac#6840 and docker/for-mac#6855
For Mac
- Added a health check for macOS that notifies users if there has been a change on their system which might cause problems running Docker binaries.
For Windows
- Fixed a bug on WSL 2 where if Desktop is paused, killed, and then restarted, the startup hangs unless WSL is shut down first with
wsl --shutdown
. - Fixed the WSL engine in cases where wsl.exe is not on the PATH docker/for-win#13547.
- Fixed the WSL engine's ability to detect cases where one of the Docker Desktop distributions' drive is missing docker/for-win#13554.
- A slow or unresponsive WSL integration no longer prevents Docker Desktop from starting. Fixes docker/for-win#13549.
- Fixed a bug that caused Docker Desktop to crash on startup docker/for-win#6890.
- Added the following installer flags:
--hyper-v-default-data-root
which specifies the default location for Hyper-V VM disk.--windows-containers-default-data-root
which specifies the default data root for Windows Containers.--wsl-default-data-root
which specifies the default location for WSL distribution disks.
4.20.1
2023-06-05Bug fixes and enhancements
For all platforms
- containerd image store: Fixed a bug that caused
docker load
to fail when loading an image that contains attestations. - containerd image store: Fixed the default image exporter during build.
For Windows
- Fixed a bug that made it difficult to parse the WSL version on the host in non-western locales. Fixes docker/for-win#13518 and docker/for-win#13524.
4.20.0
2023-05-30Upgrades
Bug fixes and enhancements
For all platforms
- Docker Scout CLI now finds the most recently built image if it is not provided as an argument.
- Improved the
Docker Scout CLI
compare
command. - Added a warning about the
retirement of Docker Compose ECS/ACS integrations in November 2023. Can be suppressed with
COMPOSE_CLOUD_EOL_SILENT=1
. - Fixed an HTTP proxy bug where an HTTP 1.0 client could receive an HTTP 1.1 response.
- Enabled Docker Desktop's Enhanced Container Isolation (ECI) feature on WSL-2. This is available with a Docker Business subscription.
- Fixed a bug on the Containers table where previously hidden columns were displayed again after a fresh installation of Docker Desktop.
For Mac
- You can now reclaim disk space more quickly when files are deleted in containers. Related to docker/for-mac#371.
- Fixed a bug that prevented containers accessing 169.254.0.0/16 IPs. Fixes docker/for-mac#6825.
- Fixed a bug in
com.docker.diagnose check
where it would complain about a missing vpnkit even when vpnkit is not expected to be running. Related to docker/for-mac#6825.
For Windows
- Fixed a bug that meant WSL data could not be moved to a different disk. Fixes docker/for-win#13269.
- Fixed a bug where Docker Desktop was not stopping its WSL distributions (docker-desktop and docker-desktop-data) when it was shutdown, consuming host memory unnecessarily.
- Added a new setting that allows the Windows Docker daemon to use Docker Desktop's internal proxy when running Windows containers. See Windows proxy settings.
For Linux
- Fixed an issue with the Docker Compose V1/V2 compatibility setting.
4.19.0
2023-04-27New
- Docker Engine and CLI updated to Moby 23.0.
- The Learning Center now supports in-product walkthroughs.
- Docker init (Beta) now supports Node.js and Python.
- Faster networking between VM and host on macOS.
- You can now inspect and analyze remote images from Docker Desktop without pulling them.
- Usability and performance improvements to the Artifactory images view.
Removed
- Removed
docker scan
command. To continue learning about the vulnerabilities of your images, and many other features, use the newdocker scout
command. Rundocker scout --help
, or read the docs to learn more.
Upgrades
Bug fixes and enhancements
For all platforms
- Improved
docker scout compare
command to compare two images, now also aliased underdocker scout diff
. - Added more details to dashboard errors when a
docker-compose
action fails ( docker/for-win#13378). - Added support for setting HTTP proxy configuration during installation. This can be done via the
--proxy-http-mode
,--overrider-proxy-http
,--override-proxy-https
and--override-proxy-exclude
installer flags in the case of installation from the CLI on Mac and Windows, or alternatively by setting the values in theinstall-settings.json
file. - Docker Desktop now stops overriding .docker/config.json
credsStore
keys on application start. Note that if you use a custom credential helper then the CLIdocker login
anddocker logout
does not affect whether the UI is signed in to Docker or not. In general, it is better to sign into Docker via the UI since the UI supports multi-factor authentication. - Added a warning about the
forthcoming removal of Compose V1 from Docker Desktop. Can be suppressed with
COMPOSE_V1_EOL_SILENT=1
. - In the Compose config, boolean fields in YAML should be either
true
orfalse
. Deprecated YAML 1.1 values such as “on” or “no” now produce a warning. - Improved UI for image table, allowing rows to use more available space.
- Fixed various bugs in port-forwarding.
- Fixed a HTTP proxy bug where an HTTP request without a Server Name Indication record would be rejected with an error.
For Windows
- Reverted to fully patching etc/hosts on Windows (includes
host.docker.internal
andgateway.docker.internal
again). For WSL, this behavior is controlled by a new setting in the General tab. Fixes docker/for-win#13388 and docker/for-win#13398. - Fixed a spurious
courgette.log
file appearing on the Desktop when updating Docker Desktop. Fixes docker/for-win#12468. - Fixed the ""zoom in"" shortcut (ctrl+=). Fixes docker/for-win#13392.
- Fixed a bug where the tray menu would not correctly update after second container type switch. Fixes docker/for-win#13379.
For Mac
- Increased the performance of VM networking when using the Virtualization framework on macOS Ventura and above. Docker Desktop for Mac now uses gVisor instead of VPNKit. To continue using VPNKit, add
""networkType"":""vpnkit""
to yoursettings.json
file located at~/Library/Group Containers/group.com.docker/settings.json
. - Fixed a bug where an error window is displayed on uninstall.
- Fixed a bug where the setting
deprecatedCgroupv1
was ignored. Fixes docker/for-mac#6801. - Fixed cases where
docker pull
would returnEOF
.
For Linux
- Fixed a bug where the VM networking crashes after 24h. Fixes docker/desktop-linux#131.
Security
For all platforms
- Fixed a security issue allowing users to bypass Image Access Management (IAM) restrictions configured by their organisation by avoiding
registry.json
enforced login via deleting thecredsStore
key from their Docker CLI configuration file. Only affects Docker Business customers. - Fixed CVE-2023-24532.
- Fixed CVE-2023-25809.
- Fixed CVE-2023-27561.
- Fixed CVE-2023-28642.
- Fixed CVE-2023-28840.
- Fixed CVE-2023-28841.
- Fixed CVE-2023-28842.
4.18.0
2023-04-03New
- Initial beta release of
docker init
as per the roadmap. - Added a new Learning Center tab to help users get started with Docker.
- Added an experimental file-watch command to Docker Compose that automatically updates your running Compose services as you edit and save your code.
Upgrades
- Buildx v0.10.4
- Compose 2.17.2
- Containerd v1.6.18, which includes fixes for CVE-2023-25153 and CVE-2023-25173.
- Docker Engine v20.10.24, which contains fixes for CVE-2023-28841, CVE-2023-28840, and CVE-2023-28842.
Bug fixes and enhancements
For all platforms
- Docker Scout CLI can now compare two images and display packages and vulnerabilities differences. This command is in Early Access and might change in the future.
- Docker Scout CLI now displays base image update and remediation recommendations using
docker scout recommendations
. It also displays a short overview of an image usingdocker scout quickview
commands. - You can now search for extensions direct from the Marketplace, as well as using Global Search.
- Fixed a bug where
docker buildx
container builders would lose access to the network after 24hrs. - Reduced how often users are prompted for feedback on Docker Desktop.
- Removed minimum VM swap size.
- Added support for subdomain match, CIDR match,
.
and_.
in HTTP proxy exclude lists. - Fixed a bug in the transparent TLS proxy when the Server Name Indication field is not set.
- Fixed a grammatical error in Docker Desktop engine status message.
For Windows
- Fixed a bug where
docker run --gpus=all
hangs. Fixes docker/for-win#13324. - Fixed a bug where Registry Access Management policy updates were not downloaded.
- Docker Desktop now allows Windows containers to work when BitLocker is enabled on
C:
. - Docker Desktop with the WSL backend no longer requires the
com.docker.service
privileged service to run permanently. For more information see Permission requirements for Windows.
For Mac
- Fixed a performance issue where attributes stored on the host would not be cached for VirtioFS users.
- The first time Docker Desktop for Mac is launched, the user is presented with an installation window to confirm or adjust the configuration that requires privileged access. For more information see Permission requirements for Mac.
- Added the Advanced tab in Settings, where users can adjust the settings which require privileged access.
For Linux
- Fixed a bug where the VM networking crashes after 24h. docker/for-linux#131
Security
For all platforms
- Fixed
CVE-2023-1802 where a security issue with the Artifactory Integration would cause it to fall back to sending registry credentials over plain HTTP if HTTPS check failed. Only users who have
Access experimental features
enabled are affected. Fixes docker/for-win#13344.
For Mac
- Removed the
com.apple.security.cs.allow-dyld-environment-variables
andcom.apple.security.cs.disable-library-validation
entitlements which allow an arbitrary dynamic library to be loaded with Docker Desktop via theDYLD_INSERT_LIBRARIES
environment variable.
Known Issues
- Uninstalling Docker Desktop on Mac from the Troubleshoot page might trigger an unexpected fatal error popup.
4.17.1
2023-03-20Bug fixes and enhancements
For Windows
- Docker Desktop now allows Windows containers to work when BitLocker is enabled on C:
- Fixed a bug where
docker buildx
container builders would lose access to the network after 24hrs. - Fixed a bug where Registry Access Management policy updates were not downloaded.
- Improved debug information to better characterise failures under WSL 2.
Known Issues
- Running containers with
--gpus
on Windows with the WSL 2 backend does not work. This will be fixed in future releases. See docker/for-win/13324.
4.17.0
2023-02-27New
- Docker Desktop now ships with Docker Scout. Pull and view analysis for images from Docker Hub and Artifactory repositories, get base image updates and recommended tags and digests, and filter your images on vulnerability information. To learn more, see Docker Scout.
docker scan
has been replaced bydocker scout
. See Docker Scout CLI, for more information.- You can now discover extensions that have been autonomously published in the Extensions Marketplace. For more information on self-published extensions, see Marketplace Extensions.
- Container File Explorer is available as an experimental feature. Debug the filesystem within your containers straight from the GUI.
- You can now search for volumes in Global Search.
Upgrades
- Containerd v1.6.18, which includes fixes for CVE-2023-25153 and CVE-2023-25173.
- Docker Engine v20.10.23.
- Go 1.19.5
Bug fixes and enhancements
For all platforms
- Fixed a bug where diagnostic gathering could hang waiting for a subprocess to exit.
- Prevented the transparent HTTP proxy from mangling requests too much. Fixes Tailscale extension login, see tailscale/docker-extension#49.
- Fixed a bug in the transparent TLS proxy where the Server Name Indication field is not set.
- Added support for subdomain match, CIDR match,
.
and*.
in HTTP proxy exclude lists. - Ensured HTTP proxy settings are respected when uploading diagnostics.
- Fixed fatal error when fetching credentials from the credential helper.
- Fixed fatal error related to concurrent logging.
- Improved the UI for Extension actions in the Marketplace.
- Added new filters in the Extensions Marketplace. You can now filter extensions by category and reviewed status.
- Added a way to report a malicious extension to Docker.
- Updated Dev Environments to v0.2.2 with initial set up reliability & security fixes.
- Added a whalecome survey for new users only.
- The confirmation dialogs on the troubleshooting page are now consistent in style with other similar dialogs.
- Fixed fatal error caused by resetting the Kubernetes cluster before it has started.
- Implemented
docker import
for the containerd integration. - Fixed image tagging with an existing tag with the containerd integration.
- Implemented the dangling filter on images for the containerd integration.
- Fixed
docker ps
failing with containers whose images are no longer present with the containerd integration.
For Mac
- Fixed download of Registry Access Management policy on systems where the privileged helper tool
com.docker.vmnetd
is not installed. - Fixed a bug where
com.docker.vmnetd
could not be installed if/Library/PrivilegedHelperTools
does not exist. - Fixed a bug where the ""system"" proxy would not handle ""autoproxy"" / ""pac file"" configurations.
- Fixed a bug where vmnetd installation fails to read
Info.Plist
on case-sensitive file systems. The actual filename isInfo.plist
. Fixes docker/for-mac#6677. - Fixed a bug where user is prompted to create the docker socket symlink on every startup. Fixes docker/for-mac#6634.
- Fixed a bug that caused the Start Docker Desktop when you log in setting not to work. Fixes docker/for-mac#6723.
- Fixed UDP connection tracking and
host.docker.internal
. Fixes docker/for-mac#6699. - Improved kubectl symlink logic to respect existing binaries in
/usr/local/bin
. Fixes docker/for-mac#6328. - Docker Desktop now automatically installs Rosetta when you opt-in to use it but have not already installed it.
For Windows
- Added statical linking of WSL integration tools against
musl
so there is no need to installalpine-pkg-glibc
in user distributions. - Added support for running under cgroupv2 on WSL 2. This is activated by adding
kernelCommandLine = systemd.unified_cgroup_hierarchy=1 cgroup_no_v1=all
to your%USERPROFILE%\.wslconfig
file in the[wsl2]
section. - Fixed an issue that caused Docker Desktop to get stuck in the ""starting"" phase when in WSL 2 mode (introduced in 4.16).
- Fixed Docker Desktop failing to start the WSL 2 backend when file system compression or encryption is enabled on
%LOCALAPPDATA%
. - Fixed Docker Desktop failing to report a missing or outdated (incapable of running WSL version 2 distributions) WSL installation when starting.
- Fixed a bug where opening in Visual Studio Code fails if the target path has a space.
- Fixed a bug that causes
~/.docker/context
corruption and the error message ""unexpected end of JSON input"". You can also remove~/.docker/context
to work around this problem. - Ensured the credential helper used in WSL 2 is properly signed. Related to docker/for-win#10247.
- Fixed an issue that caused WSL integration agents to be terminated erroneously. Related to docker/for-win#13202.
- Fixed corrupt contexts on start. Fixes docker/for-win#13180 and docker/for-win#12561.
For Linux
- Added Docker Buildx plugin for Docker Desktop for Linux.
- Changed compression algorithm to
xz
for RPM and Arch Linux distribution. - Fixed a bug that caused leftover files to be left in the root directory of the Debian package. Fixes docker/for-linux#123.
Security
For all platforms
- Fixed
CVE-2023-0628, which allows an attacker to execute an arbitrary command inside a Dev Environments container during initialization by tricking a user to open a crafted malicious
docker-desktop://
URL. - Fixed
CVE-2023-0629, which allows an unprivileged user to bypass Enhanced Container Isolation (ECI) restrictions by setting the Docker host to
docker.raw.sock
, ornpipe:////.pipe/docker_engine_linux
on Windows, via the-H
(--host
) CLI flag or theDOCKER_HOST
environment variable and launch containers without the additional hardening features provided by ECI. This does not affect already running containers, nor containers launched through the usual approach (without Docker's raw socket).
4.16.3
2023-01-30Bug fixes and enhancements
For Windows
- Fixed Docker Desktop failing to start the WSL 2 backend when file system compression or encryption is enabled on
%LOCALAPPDATA%
. Fixes docker/for-win#13184. - Fixed Docker Desktop failing to report a missing or outdated WSL installation when starting. Fixes docker/for-win#13184.
4.16.2
2023-01-19Bug fixes and enhancements
For all platforms
- Fixed an issue where
docker build
anddocker tag
commands produced animage already exists
error if the containerd integration feature is enabled. - Fixed a regression introduced with Docker Desktop 4.16 breaking networking from containers with target platform linux/386 on amd64 systems. Fixes docker/for-mac/6689.
For Mac
- Fixed the capitalization of
Info.plist
which causedvmnetd
to break on case-sensitive file systems. Fixes docker/for-mac/6677.
For Windows
- Fixed a regression introduced with Docker Desktop 4.16 causing it to get stuck in the ""starting"" phase when in WSL2 mode. Fixes docker/for-win/13165
4.16.1
2023-01-13Bug fixes and enhancements
For all platforms
- Fixed
sudo
inside a container failing with a security related error for some images. Fixes docker/for-mac/6675 and docker/for-win/13161.
4.16.0
2023-01-12New
- Extensions have moved from Beta to GA.
- Quick Search has moved from experimental to GA.
- Extensions are now included in Quick Search.
- Analyzing large images is now up to 4x faster.
- New local images view has moved from experimental to GA.
- New Beta feature for MacOS 13, Rosetta for Linux, has been added for faster emulation of Intel-based images on Apple Silicon.
Upgrades
Bug fixes and enhancements
For all platforms
- Fixed
docker build --quiet
not outputting the image identifier with thecontainerd
integration. - Fixed image inspect not showing image labels with the
containerd
integration. - Increased the contrast between running and stopped container icons to make it easier for colorblind people to scan the containers list.
- Fixed a bug where the user is prompted for new HTTP proxy credentials repeatedly until Docker Desktop is restarted.
- Added a diagnostics command
com.docker.diagnose login
to check HTTP proxy configuration. - Fixed actions on compose stack not working properly. Fixes docker/for-mac#6566.
- Fixed the Docker Desktop Dashboard trying at startup to get disk usage information and display an error banner before the engine was running.
- Added an informational banner with instructions on how to opt-out of experimental feature access next to all experimental features.
- Docker Desktop now supports downloading Kubernetes images via an HTTP proxy.
- Fixed tooltips to not block action buttons. Fixes docker/for-mac#6516.
- Fixed the blank ""An error occurred"" container list on the Container view.
For Mac
- Minimum OS version to install or update Docker Desktop on macOS is now macOS Big Sur (version 11) or later.
- Fixed the Docker engine not starting when Enhanced Container Isolation is enabled if the legacy
osxfs
implementation is used for file sharing. - Fixed files created on VirtioFS having the executable bit set. Fixes docker/for-mac#6614.
- Added back a way to uninstall Docker Desktop from the command line. Fixes docker/for-mac#6598.
- Fixed hardcoded
/usr/bin/kill
. Fixes docker/for-mac#6589. - Fixed truncation (for example with the
truncate
command) of very large files (> 38GB) shared on VirtioFS with an incorrect size. - Changed the disk image size in Settings to use the decimal system (base 10) to coincide with how Finder displays disk capacity.
- Fixed Docker crash under network load. Fixes docker/for-mac#6530.
- Fixed an issue causing Docker to prompt the user to install the
/var/run/docker.sock
symlink after every reboot. - Ensured the Login Item which installs the
/var/run/docker.sock
symlink is signed. - Fixed bug where
$HOME/.docker
was removed on factory reset.
For Windows
- Fixed
docker build
hanging while printing ""load metadata for"". Fixes docker/for-win#10247. - Fixed typo in diagnose.exe output Fixes docker/for-win#13107.
- Added support for running under cgroupv2 on WSL 2. This is activated by adding
kernelCommandLine = systemd.unified_cgroup_hierarchy=1 cgroup_no_v1=all
to your%USERPROFILE%\.wslconfig
file in the[wsl2]
section.
Known Issues
- Calling
sudo
inside a container fails with a security related error for some images. See docker/for-mac/6675 and docker/for-win/13161.
4.15.0
2022-12-01New
- Substantial performance improvements for macOS users with the option of enabling the new VirtioFS file sharing technology. Available for macOS 12.5 and above.
- Docker Desktop for Mac no longer needs to install the privileged helper process
com.docker.vmnetd
on install or on the first run. For more information see Permission requirements for Mac. - Added WebAssembly capabilities. Use with the containerd integration.
- Improved the descriptions for beta and experimental settings to clearly explain the differences and how people can access them.
- Available disk space of VM now displays in the footer of Docker Desktop Dashboard for Mac and Linux.
- A disk space warning now displays in the footer if available space is below 3GB.
- Changes to Docker Desktop's interface as we become more ADA accessible and visually unified.
- Added a Build tab inside Extensions which contains all the necessary resources to build an extension.
- Added the ability to share extensions more easily, either with
docker extension share
CLI or with the share button in the extensions Manage tab. - Extensions in the Marketplace now display the number of installs. You can also sort extensions by the number of installs.
- Dev Environments allow cloning a Git repository to a local bind mount, so you can use any local editor or IDE.
- More Dev Environments improvements: custom names, better private repo support, improved port handling.
Upgrades
Bug fixes and enhancements
For all platforms
- Containers are now restored on restart with the containerd integration.
- Fixed listing multi-platform images with the containerd integration.
- Better handling of dangling images with the containerd integration.
- Implement ""reference"" filter for images with the containerd integration.
- Added support for selecting upstream HTTP/HTTPS proxies automatically via
proxy.pac
in containers,docker pull
etc. - Fixed regressions when parsing image references on pull. Fixes docker/for-win#13053, docker/for-mac#6560, and docker/for-mac#6540.
For Mac
- Improved the performance of
docker pull
.
For Windows
- Fixed an issue where the system HTTP proxies were not used when Docker starts and the developer logs in.
- When Docker Desktop is using ""system"" proxies and if the Windows settings change, Docker Desktop now uses the new Windows settings without a restart.
For Linux
- Fixed hot-reload issue on Linux. Fixes docker/desktop-linux#30.
- Disabled tray icon animations on Linux which fixes crashes for some users.
4.14.1
2022-11-17Bug fixes and enhancements
For all platforms
- Fixed container DNS lookups when using Registry Access Management.
For Mac
- Fixed an issue preventing the Analyze Image button on the Images tab from working.
- Fixed a bug causing symlinks to not be created for the user if
/usr/local/lib
doesn't already exist. Fixes docker/for-mac#6569
4.14.0
2022-11-10New
- Set Virtualization framework as the default hypervisor for macOS >= 12.5.
- Migrate previous install to Virtualization framework hypervisor for macOS >= 12.5.
- The Enhanced Container Isolation feature, available to Docker Business users, can now be enabled from the General Settings.
Updates
- Docker Engine v20.10.21,
which contains mitigations against a Git vulnerability, tracked in
CVE-2022-39253,
and updates the handling of
image:tag@digest
image references. - Docker Compose v2.12.2
- Containerd v1.6.9
- Go 1.19.3
Bug fixes and enhancements
For all platforms
- Docker Desktop now requires an internal network subnet of size /24. If you were previously using a /28, it is automatically expanded to /24. If you experience networking issues, check to see if you have a clash between the Docker subnet and your infrastructure. Fixes docker/for-win#13025.
- Fixed an issue that prevents users from creating Dev Environments when the Git URL has upper-case characters.
- Fix the
vpnkit.exe is not running
error reported in diagnostics. - Reverted qemu to 6.2.0 to fix errors like
PR_SET_CHILD_SUBREAPER is unavailable
when running emulated amd64 code. - Enabled contextIsolation and sandbox mode inside Extensions. Now Extensions run in a separate context and this limits the harm that malicious code can cause by limiting access to most system resources.
- Included
unpigz
to allow parallel decompression of pulled images. - Fixed issues related to performing actions on selected containers. Fixes https://github.com/docker/for-win/issues/13005
- Added functionality that allows you to display timestamps for your container or project view.
- Fixed a possible segfault when interrupting
docker pull
with Control+C. - Increased the default DHCP lease time to avoid the VM's network glitching and dropping connections every two hours.
- Removed the infinite spinner on the containers list. Fixes https://github.com/docker/for-mac/issues/6486
- Fixed bug which showed incorrect values on used space in Settings.
- Fixed a bug that caused Kubernetes not to start with the containerd integration.
- Fixed a bug that caused
kind
not to start with the containerd integration. - Fixed a bug that caused Dev Environments to not work with the containerd integration.
- Implemented
docker diff
in the containerd integration. - Implemented
docker run —-platform
in the containerd integration. - Fixed a bug that caused insecure registries not to work with the containerd integration.
For Mac
- Fixed a startup failure for users of Virtualization framework.
- Re-added the
/var/run/docker.sock
on Mac by default, to increase compatibility with tooling liketilt
anddocker-py.
- Fixed an issue that prevented the creation of Dev Environments on new Mac installs (error ""Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"").
For Windows
- Re-added
DockerCli.exe -SharedDrives
. Fixes docker/for-win#5625. - Docker Desktop now allows Docker to function on machines where PowerShell is disabled.
- Fixed an issue where Compose v2 was not always enabled by default on Windows.
- Docker Desktop now deletes the
C:\Program Files\Docker
folder at uninstall.
Known Issues
- For some users on Mac OS there is a known issue with the installer that prevents the installation of a new helper tool needed for the experimental vulnerability and package discovery feature in Docker Desktop. To fix this, a symlink is needed that can be created with the following command:
sudo ln -s /Applications/Docker.app/Contents/Resources/bin/docker-index /usr/local/bin/docker-index
4.13.1
2022-10-31Updates
Bug fixes and enhancements
For all platforms
- Fixed a possible segfault when interrupting
docker pull
withControl+C
orCMD+C
. - Increased the default DHCP lease time to avoid the VM's network glitching and dropping connections every two hours.
- Reverted
Qemu
to6.2.0
to fix errors likePR_SET_CHILD_SUBREAPER is unavailable
when running emulated amd64 code.
For Mac
- Added back the
/var/run/docker.sock
symlink on Mac by default, to increase compatibility with tooling liketilt
anddocker-py
. Fixes docker/for-mac#6529. - Fixed an issue preventing the creation of Dev Environments on new Mac installs and causing
error ""Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"")
For Windows
- Docker Desktop now functions on machines where PowerShell is disabled.
4.13.0
2022-10-19New
- Two new security features have been introduced for Docker Business users, Settings Management and Enhanced Container Isolation. Read more about Docker Desktop’s new Hardened Docker Desktop security model.
- Added the new Dev Environments CLI
docker dev
, so you can create, list, and run Dev Envs via command line. Now it's easier to integrate Dev Envs into custom scripts. - Docker Desktop can now be installed to any drive and folder using the
--installation-dir
. Partially addresses docker/roadmap#94.
Updates
- Docker Scan v0.21.0
- Go 1.19.2 to address CVE-2022-2879, CVE-2022-2880 and CVE-2022-41715
- Updated Docker Engine and Docker CLI to
v20.10.20,
which contain mitigations against a Git vulnerability, tracked in
CVE-2022-39253,
and updated handling of
image:tag@digest
image references, as well as a fix for CVE-2022-36109. - Docker Credential Helpers v0.7.0
- Docker Compose v2.12.0
- Kubernetes v1.25.2
- Qemu 7.0.0 used for cpu emulation, inside the Docker Desktop VM.
- Linux kernel 5.15.49
Bug fixes and enhancements
For all platforms
- Docker Desktop now allows the use of TLS when talking to HTTP and HTTPS proxies to encrypt proxy usernames and passwords.
- Docker Desktop now stores HTTP and HTTPS proxy passwords in the OS credential store.
- If Docker Desktop detects that the HTTP or HTTPS proxy password has changed then it will prompt developers for the new password.
- The Bypass proxy settings for these hosts and domains setting now handles domain names correctly for HTTPS.
- The Remote Repositories view and Tip of the Day now works with HTTP and HTTPS proxies which require authentication
- We’ve introduced dark launch for features that are in early stages of the product development lifecycle. Users that are opted in can opt out at any time in the settings under the “beta features” section.
- Added categories to the Extensions Marketplace.
- Added an indicator in the whale menu and on the Extension tab on when extension updates are available.
- Fixed failing uninstalls of extensions with image names that do not have a namespace, as in 'my-extension'.
- Show port mapping explicitly in the Container tab.
- Changed the refresh rate for disk usage information for images to happen automatically once a day.
- Made the tab style consistent for the Container and Volume tabs.
- Fixed Grpcfuse filesharing mode enablement in Settings. Fixes docker/for-mac#6467
- Virtualization Framework and VirtioFS are disabled for users running macOS < 12.5.
- Ports on the Containers tab are now clickable.
- The Extensions SDK now allows
ddClient.extension.vm.cli.exec
,ddClient.extension.host.cli.exec
,ddClient.docker.cli.exec
to accept a different working directory and pass environment variables through the options parameters. - Added a small improvement to navigate to the Extensions Marketplace when clicking on Extensions in the sidebar.
- Added a badge to identify new extensions in the Marketplace.
- Fixed kubernetes not starting with the containerd integration.
- Fixed
kind
not starting with the containerd integration. - Fixed dev environments not working with the containerd integration.
- Implemented
docker diff
in the containerd integration. - Implemented
docker run —-platform
in the containerd integration. - Fixed insecure registries not working with the containerd integration.
- Fixed a bug that showed incorrect values on used space in Settings.
- Docker Desktop now installs credential helpers from Github releases. See docker/for-win#10247, docker/for-win#12995.
- Fixed an issue where users were logged out of Docker Desktop after 7 days.
For Mac
- Added Hide, Hide others, Show all menu items for Docker Desktop. See docker/for-mac#6446.
- Fixed a bug which caused the application to be deleted when running the install utility from the installed application. Fixes docker/for-mac#6442.
- By default Docker will not create the /var/run/docker.sock symlink on the host and use the docker-desktop CLI context instead.
For Linux
- Fixed a bug that prevented pushing images from the Dashboard
4.12.0
2022-09-01New
- Added the ability to use containerd for pulling and storing images. This is an experimental feature.
- Docker Desktop now runs untagged images. Fixes docker/for-mac#6425.
- Added search capabilities to Docker Extension's Marketplace. Fixes docker/roadmap#346.
- Added the ability to zoom in, out or set Docker Desktop to Actual Size. This is done by using keyboard shortcuts ⌘ + / CTRL +, ⌘ - / CTRL -, ⌘ 0 / CTRL 0 on Mac and Windows respectively, or through the View menu on Mac.
- Added compose stop button if any related container is stoppable.
- Individual compose containers are now deletable from the Container view.
- Removed the workaround for virtiofsd <-> qemu protocol mismatch on Fedora 35, as it is no longer needed. Fedora 35 users should upgrade the qemu package to the most recent version (qemu-6.1.0-15.fc35 as of the time of writing).
- Implemented an integrated terminal for containers.
- Added a tooltip to display the link address for all external links by default.
Updates
- Docker Compose v2.10.2
- Docker Scan v0.19.0
- Kubernetes v1.25.0
- Go 1.19
- cri-dockerd v0.2.5
- Buildx v0.9.1
- containerd v1.6.8
- containerd v1.6.7
- runc v1.1.4
- runc v1.1.3
Security
For all platforms
- Fixed CVE-2023-0626 which allows RCE via query parameters in the message-box route in the Electron client.
- Fixed CVE-2023-0625 which allows RCE via extension description/changelog which could be abused by a malicious extension.
For Windows
- Fixed
CVE-2023-0627 which allows to bypass for the
--no-windows-containers
installation flag which was introduced in version 4.11. This flag allows administrators to disable the use of Windows containers. - Fixed CVE-2023-0633 in which an argument injection to the Docker Desktop installer which may result in local privilege escalation.
Bug fixes and minor enhancements
For all platforms
- Compose V2 is now enabled after factory reset.
- Compose V2 is now enabled by default on new installations of Docker Desktop.
- Precedence order of environment variables in Compose is more consistent, and clearly documented.
- Upgraded kernel to 5.10.124.
- Improved overall performance issues caused by calculating disk size. Related to docker/for-win#9401.
- Docker Desktop now prevents users on ARM macs without Rosetta installed from switching back to Compose V1, which has only intel binaries.
- Changed the default sort order to descending for volume size and the Created column, along with the container's Started column.
- Re-organized container row actions by keeping only the start/stop and delete actions visible at all times, while allowing access to the rest via the row menu item.
- The Quickstart guide now runs every command immediately.
- Defined the sort order for container/compose Status column to running > some running > paused > some paused > exited > some exited > created.
- Fixed issues with the image list appearing empty in Docker Desktop even though there are images. Related to docker/for-win#12693 and docker/for-mac#6347.
- Defined what images are ""in use"" based on whether or not system containers are displayed. If system containers related to Kubernetes and Extensions are not displayed, the related images are not defined as ""in use.""
- Fixed a bug that made Docker clients in some languages hang on
docker exec
. Fixes https://github.com/apocas/dockerode/issues/534. - A failed spawned command when building an extension no longer causes Docker Desktop to unexpectedly quit.
- Fixed a bug that caused extensions to be displayed as disabled in the left menu when they are not.
- Fixed
docker login
to private registries when Registry Access Management is enabled and access to Docker Hub is blocked. - Fixed a bug where Docker Desktop fails to start the Kubernetes cluster if the current cluster metadata is not stored in the
.kube/config
file. - Updated the tooltips in Docker Desktop and MUI theme package to align with the overall system design.
- Copied terminal contents do not contain non-breaking spaces anymore.
For Mac
- Minimum version to install or update Docker Desktop on macOS is now 10.15. Fixes docker/for-mac#6007.
- Fixed a bug where the Tray menu incorrectly displays ""Download will start soon..."" after downloading the update. Fixes some issue reported in for-mac/issues#5677
- Fixed a bug that didn't restart Docker Desktop after applying an update.
- Fixed a bug that caused the connection to Docker to be lost when the computer sleeps if a user is using virtualization.framework and restrictive firewall software.
- Fixed a bug that caused Docker Desktop to run in the background even after a user had quit the application. Fixes docker/for-mac##6440
- Disabled both Virtualization Framework and VirtioFS for users running macOS < 12.5
For Windows
- Fixed a bug where versions displayed during an update could be incorrect. Fixes for-win/issues#12822.
4.11.1
2022-08-05Bug fixes and enhancements
For all platforms
- Fixed regression preventing VM system locations (e.g. /var/lib/docker) from being bind mounted for-mac/issues#6433
For Windows
- Fixed
docker login
to private registries from WSL2 distribution docker/for-win#12871
4.11.0
2022-07-28New
- Docker Desktop is now fully supported for Docker Business customers inside VMware ESXi and Azure VMs. For more information, see Run Docker Desktop inside a VM or VDI environment
- Added two new extensions ( vcluster and PGAdmin4) to the Extensions Marketplace.
- The ability to sort extensions has been added to the Extensions Marketplace.
- Fixed a bug that caused some users to be asked for feedback too frequently. You'll now only be asked for feedback twice a year.
- Added custom theme settings for Docker Desktop. This allows you to specify dark or light mode for Docker Desktop independent of your device settings. Fixes docker/for-win#12747
- Added a new flag for Windows installer.
--no-windows-containers
disables the Windows containers integration. - Added a new flag for Mac install command.
--user <username>
sets up Docker Desktop for a specific user, preventing them from needing an admin password on first run.
Updates
Bug fixes and enhancements
For all platforms
- Added the Container / Compose icon as well as the exposed port(s) / exit code to the Containers screen.
- Updated the Docker theme palette colour values to match our design system.
- Improved an error message from
docker login
if Registry Access Management is blocking the Docker engine's access to Docker Hub. - Increased throughput between the Host and Docker. For example increasing performance of
docker cp
. - Collecting diagnostics takes less time to complete.
- Selecting or deselecting a compose app on the containers overview now selects/deselects all its containers.
- Tag names on the container overview image column are visible.
- Added search decorations to the terminal's scrollbar so that matches outside the viewport are visible.
- Fixed an issue with search which doesn't work well on containers page docker/for-win#12828.
- Fixed an issue which caused infinite loading on the Volume screen docker/for-win#12789.
- Fixed a problem in the Container UI where resizing or hiding columns didn't work. Fixes docker/for-mac#6391.
- Fixed a bug where the state of installing, updating, or uninstalling multiple extensions at once was lost when leaving the Marketplace screen.
- Fixed an issue where the compose version in the about page would only get updated from v2 to v1 after restarting Docker Desktop.
- Fixed an issue where users cannot see the log view because their underlying hardware didn't support WebGL2 rendering. Fixes docker/for-win#12825.
- Fixed a bug where the UI for Containers and Images got out of sync.
- Fixed a startup race when the experimental virtualization framework is enabled.
For Mac
- Fixed an issue executing Compose commands from the UI. Fixes docker/for-mac#6400.
For Windows
- Fixed horizontal resizing issue. Fixes docker/for-win#12816.
- If an HTTP/HTTPS proxy is configured in the UI, then it automatically sends traffic from image builds and running containers to the proxy. This avoids the need to separately configure environment variables in each container or build.
- Added the
--backend=windows
installer option to set Windows containers as the default backend.
For Linux
- Fixed bug related to setting up file shares with spaces in their path.
4.10.1
2022-07-05Bug fixes and enhancements
For Windows
- Fixed a bug where actions in the UI failed with Compose apps that were created from WSL. Fixes docker/for-win#12806.
For Mac
- Fixed a bug where the install command failed because paths were not initialized. Fixes docker/for-mac#6384.
4.10.0
2022-06-30New
- You can now add environment variables before running an image in Docker Desktop.
- Added features to make it easier to work with a container's logs, such as regular expression search and the ability to clear container logs while the container is still running.
- Implemented feedback on the containers table. Added ports and separated container and image names.
- Added two new extensions, Ddosify and Lacework, to the Extensions Marketplace.
Removed
- Removed Homepage while working on a new design. You can provide feedback here.
Updates
- Docker Engine v20.10.17
- Docker Compose v2.6.1
- Kubernetes v1.24.1
- cri-dockerd to v0.2.1
- CNI plugins to v1.1.1
- containerd to v1.6.6
- runc to v1.1.2
- Go 1.18.3
Bug fixes and enhancements
For all platforms
- Added additional bulk actions for starting/pausing/stopping selected containers in the Containers tab.
- Added pause and restart actions for compose projects in the Containers tab.
- Added icons and exposed ports or exit code information in the Containers tab.
- External URLs can now refer to extension details in the Extension Marketplace using links such as
docker-desktop://extensions/marketplace?extensionId=docker/logs-explorer-extension
. - The expanded or collapsed state of the Compose apps is now persisted.
docker extension
CLI commands are available with Docker Desktop by default.- Increased the size of the screenshots displayed in the Extension marketplace.
- Fixed a bug where a Docker extension fails to load if its backend container(s) are stopped. Fixes docker/extensions-sdk#16.
- Fixed a bug where the image search field is cleared without a reason. Fixes docker/for-win#12738.
- Fixed a bug where the license agreement does not display and silently blocks Docker Desktop startup.
- Fixed the displayed image and tag for unpublished extensions to actually display the ones from the installed unpublished extension.
- Fixed the duplicate footer on the Support screen.
- Dev Environments can be created from a subdirectory in a GitHub repository.
- Removed the error message if the tips of the day cannot be loaded when using Docker Desktop offline. Fixes docker/for-mac#6366.
For Mac
- Fixed a bug with location of bash completion files on macOS. Fixes docker/for-mac#6343.
- Fixed a bug where Docker Desktop does not start if the username is longer than 25 characters. Fixes docker/for-mac#6122.
- Fixed a bug where Docker Desktop was not starting due to invalid system proxy configuration. Fixes some issues reported in docker/for-mac#6289.
- Fixed a bug where Docker Desktop failed to start when the experimental virtualization framework is enabled.
- Fixed a bug where the tray icon still displayed after uninstalling Docker Desktop.
For Windows
- Fixed a bug which caused high CPU usage on Hyper-V. Fixes docker/for-win#12780.
- Fixed a bug where Docker Desktop for Windows would fail to start. Fixes docker/for-win#12784.
- Fixed the
--backend=wsl-2
installer flag which did not set the backend to WSL 2. Fixes docker/for-win#12746.
For Linux
- Fixed a bug when settings cannot be applied more than once.
- Fixed Compose version displayed in the
About
screen.
Known Issues
- Occasionally the Docker engine will restart during a
docker system prune
. This is a known issue in the version of buildkit used in the current engine and will be fixed in future releases.
4.9.1
2022-06-16Download Docker Desktop
Windows (checksum) | Mac with Apple chip (checksum) | Mac with Intel chip (checksum) | Debian - RPM - Arch (checksum)
Bug fixes and enhancements
For all platforms
- Fixed blank dashboard screen. Fixes docker/for-win#12759.
4.9.0
2022-06-02New
- Added additional guides on the homepage for: Elasticsearch, MariaDB, Memcached, MySQL, RabbitMQ and Ubuntu.
- Added a footer to the Docker Desktop Dashboard with general information about the Docker Desktop update status and Docker Engine statistics
- Re-designed the containers table, adding:
- A button to copy a container ID to the clipboard
- A pause button for each container
- Column resizing for the containers table
- Persistence of sorting and resizing for the containers table
- Bulk deletion for the containers table
Updates
Bug fixes and enhancements
For all platforms
- Fixed an issue which caused Docker Desktop to hang if you quit the app whilst Docker Desktop was paused.
- Fixed the Kubernetes cluster not resetting properly after the PKI expires.
- Fixed an issue where the Extensions Marketplace was not using the defined http proxies.
- Improved the logs search functionality in Docker Desktop Dashboard to allow spaces.
- Middle-button mouse clicks on buttons in the Dashboard now behave as a left-button click instead of opening a blank window.
For Mac
- Fixed an issue to avoid creating
/opt/containerd/bin
and/opt/containerd/lib
on the host if/opt
has been added to the file sharing directories list.
For Windows
- Fixed a bug in the WSL 2 integration where if a file or directory is bind-mounted to a container, and the container exits, then the file or directory is replaced with the other type of object with the same name. For example, if a file is replaced with a directory or a directory with a file, any attempts to bind-mount the new object fails.
- Fixed a bug where the Tray icon and Dashboard UI didn't show up and Docker Desktop didn't fully start. Fixes docker/for-win#12622.
Known issues
For Linux
- Changing ownership rights for files in bind mounts fails. This is due to the way we have implemented file sharing between the host and VM within which the Docker Engine runs. We aim to resolve this issue in the next release.
4.8.2
2022-05-18Updates
Bug fixes and minor enahancements
- Fixed an issue with manual proxy settings which caused problems when pulling images. Fixes docker/for-win#12714 and docker/for-mac#6315.
- Fixed high CPU usage when extensions are disabled. Fixes docker/for-mac#6310.
- Docker Desktop now redacts HTTP proxy passwords in log files and diagnostics.
Known issues
For Linux
- Changing ownership rights for files in bind mounts fails. This is due to the way we have implemented file sharing between the host and VM within which the Docker Engine runs. We aim to resolve this issue in the next release.
4.8.1
2022-05-09New
- Released Docker Desktop for Linux.
- Beta release of Docker Extensions and Extensions SDK.
- Created a Docker Homepage where you can run popular images and discover how to use them.
- Compose V2 is now GA
Bug fixes and enhancements
- Fixed a bug that caused the Kubernetes cluster to be deleted when updating Docker Desktop.
Known issues
For Linux
- Changing ownership rights for files in bind mounts fails. This is due to the way we have implemented file sharing between the host and VM within which the Docker Engine runs. We aim to resolve this issue in the next release.
4.8.0
2022-05-06New
- Released Docker Desktop for Linux.
- Beta release of Docker Extensions and Extensions SDK.
- Created a Docker Homepage where you can run popular images and discover how to use them.
- Compose V2 is now GA
Updates
Bug fixes and minor enhancements
For all platforms
- Introduced reading system proxy. You no longer need to manually configure proxies unless it differs from your OS level proxy.
- Fixed a bug that showed Remote Repositories in the Dashboard when running behind a proxy.
- Fixed vpnkit establishing and blocking the client connection even if the server is gone. See docker/for-mac#6235
- Made improvements on the Volume tab in Docker Desktop:
- Volume size is displayed.
- Columns can be resized, hidden and reordered.
- A columns sort order and hidden state is persisted, even after Docker Desktop restarts.
- Row selection is persisted when switching between tabs, even after Docker Desktop restarts.
- Fixed a bug in the Dev Environments tab that did not add a scroll when more items were added to the screen.
- Standardised the header title and action in the Dashboard.
- Added support for downloading Registry Access Management policies through HTTP proxies.
- Fixed an issue related to empty remote repositories when the machine is in sleep mode for an extended period of time.
- Fixed a bug where dangling images were not selected in the cleanup process if their name was not marked as ""<none>"" but their tag is.
- Improved the error message when
docker pull
fails because an HTTP proxy is required. - Added the ability to clear the search bar easily in Docker Desktop.
- Renamed the ""Containers / Apps"" tab to ""Containers"".
- Fixed a silent crash in the Docker Desktop installer when
C:\ProgramData\DockerDesktop
is a file or a symlink. - Fixed a bug where an image with no namespace, for example
docker pull <private registry>/image
, would be erroneously blocked by Registry Access Management unless access to Docker Hub was enabled in settings.
For Mac
- Docker Desktop's icon now matches Big Sur Style guide. See docker/for-mac#5536
- Fixed a problem with duplicate Dock icons and Dock icon not working as expected. Fixes docker/for-mac#6189.
- Improved support for the
Cmd+Q
shortcut.
For Windows
- Improved support for the
Ctrl+W
shortcut.
Known issues
For all platforms
- Currently, if you are running a Kubernetes cluster, it will be deleted when you upgrade to Docker Desktop 4.8.0. We aim to fix this in the next release.
For Linux
- Changing ownership rights for files in bind mounts fails. This is due to the way we have implemented file sharing between the host and VM within which the Docker Engine runs. We aim to resolve this issue in the next release.
4.7.1
2022-04-19Bug fixes and enhancements
For all platforms
- Fixed a crash on the Quick Start Guide final screen.
For Windows
- Fixed a bug where update was failing with a symlink error. Fixes docker/for-win#12650.
- Fixed a bug that prevented using Windows container mode. Fixes docker/for-win#12652.
4.7.0
2022-04-07New
- IT Administrators can now install Docker Desktop remotely using the command line.
- Add the Docker Software Bill of Materials (SBOM) CLI plugin. The new CLI plugin enables users to generate SBOMs for Docker images.
- Use
cri-dockerd for new Kubernetes clusters instead of
dockershim
. The change is transparent from the user's point of view and Kubernetes containers run on the Docker Engine as before.cri-dockerd
allows Kubernetes to manage Docker containers using the standard Container Runtime Interface, the same interface used to control other container runtimes. For more information, see The Future of Dockershim is cri-dockerd.
Updates
Security
- Update Docker Engine to v20.10.14 to address CVE-2022-24769
- Update containerd to v1.5.11 to address CVE-2022-24769
Bug fixes and enhancements
For all platforms
- Fixed a bug where the Registry Access Management policy was never refreshed after a failure.
- Logs and terminals in the UI now respect your OS theme in light and dark mode.
- Easily clean up many volumes at once via multi-select checkboxes.
- Improved login feedback.
For Mac
- Fixed an issue that sometimes caused Docker Desktop to display a blank white screen. Fixes docker/for-mac#6134.
- Fixed a problem where gettimeofday() performance drops after waking from sleep when using Hyperkit. Fixes docker/for-mac#3455.
- Fixed an issue that caused Docker Desktop to become unresponsive during startup when
osxfs
is used for file sharing.
For Windows
- Fixed volume title. Fixes docker/for-win#12616.
- Fixed a bug in the WSL 2 integration that caused Docker commands to stop working after restarting Docker Desktop or after switching to Windows containers.
4.6.1
2022-03-22Updates
Bug fixes and enhancements
- Prevented spinning in vpnkit-forwarder filling the logs with error messages.
- Fixed diagnostics upload when there is no HTTP proxy set. Fixes docker/for-mac#6234.
- Removed a false positive ""vm is not running"" error from self-diagnose. Fixes docker/for-mac#6233.
4.6.0
2022-03-14New
For all platforms
- The Docker Desktop Dashboard Volume Management feature now offers the ability to efficiently clean up volumes using multi-select checkboxes.
For Mac
- Docker Desktop 4.6.0 gives macOS users the option of enabling a new experimental file sharing technology called VirtioFS. During testing VirtioFS has been shown to drastically reduce the time taken to sync changes between the host and VM, leading to substantial performance improvements. For more information, see VirtioFS.
Updates
For all platforms
- Docker Engine v20.10.13
- Compose v2.3.3
- Buildx 0.8.0
- containerd v1.4.13
- runc v1.0.3
- Go 1.17.8
- Linux kernel 5.10.104
Security
For all platforms
- Fixed
CVE-2022-0847, aka “Dirty Pipe”, an issue that could enable attackers to modify files in container images on the host, from inside a container.
If using the WSL 2 backend, you must update WSL 2 by running
wsl --update
.
For Windows
- Fixed CVE-2022-26659, which could allow an attacker to overwrite any administrator writable file on the system during the installation or the update of Docker Desktop.
For Mac
Bug fixes and enhancements
For all platforms
- Fixed uploading diagnostics when an HTTPS proxy is set.
- Made checking for updates from the systray menu open the Software updates settings section.
For Mac
- Fixed the systray menu not displaying all menu items after starting Docker Desktop. Fixes docker/for-mac#6192.
- Fixed a regression about Docker Desktop not starting in background anymore. Fixes docker/for-mac#6167.
- Fixed missing Docker Desktop Dock icon. Fixes docker/for-mac#6173.
- Used speed up block device access when using the experimental
virtualization.framework
. See benchmarks. - Increased default VM memory allocation to half of physical memory (min 2 GB, max 8 GB) for better out-of-the-box performances.
For Windows
- Fixed the UI stuck in
starting
state forever although Docker Desktop is working fine from the command line. - Fixed missing Docker Desktop systray icon docker/for-win#12573
- Fixed Registry Access Management under WSL 2 with latest 5.10.60.1 kernel.
- Fixed a UI crash when selecting the containers of a Compose application started from a WSL 2 environment. Fixes docker/for-win#12567.
- Fixed copying text from terminal in Quick Start Guide. Fixes docker/for-win#12444.
Known issues
For Mac
- After enabling VirtioFS, containers with processes running with different Unix user IDs may experience caching issues. For example if a process running as
root
queries a file and another process running as usernginx
tries to access the same file immediately, thenginx
process will get a ""Permission Denied"" error.
4.5.1
2022-02-15Bug fixes and enhancements
For Windows
- Fixed an issue that caused new installations to default to the Hyper-V backend instead of WSL 2.
- Fixed a crash in the Docker Desktop Dashboard which would make the systray menu disappear.
If you are running Docker Desktop on Windows Home, installing 4.5.1 will switch it back to WSL 2 automatically. If you are running another version of Windows, and you want Docker Desktop to use the WSL 2 backend, you must manually switch by enabling the Use the WSL 2 based engine option in the Settings > General section.
Alternatively, you can edit the Docker Desktop settings file located at %APPDATA%\Docker\settings.json
and manually switch the value of the wslEngineEnabled
field to true
.
4.5.0
2022-02-10New
- Docker Desktop 4.5.0 introduces a new version of the Docker menu which creates a consistent user experience across all operating systems. For more information, see the blog post New Docker Menu & Improved Release Highlights with Docker Desktop 4.5
- The 'docker version' output now displays the version of Docker Desktop installed on the machine.
Updates
Security
For Mac
- Fixed CVE-2021-44719 where Docker Desktop could be used to access any user file on the host from a container, bypassing the allowed list of shared folders.
For Windows
- Fixed CVE-2022-23774 where Docker Desktop allows attackers to move arbitrary files.
Bug fixes and enhancements
For all platforms
- Fixed an issue where Docker Desktop incorrectly prompted users to sign in after they quit Docker Desktop and start the application.
- Increased the filesystem watch (inotify) limits by setting
fs.inotify.max_user_watches=1048576
andfs.inotify.max_user_instances=8192
in Linux. Fixes docker/for-mac#6071.
For Mac
- Fixed an issue that caused the VM to become unresponsive during startup when using
osxfs
and when no host directories are shared with the VM. - Fixed an issue that didn't allow users to stop a Docker Compose application using Docker Desktop Dashboard if the application was started in a different version of Docker Compose. For example, if the user started a Docker Compose application in V1 and then switched to Docker Compose V2, attempts to stop the Docker Compose application would fail.
- Fixed an issue where Docker Desktop incorrectly prompted users to sign in after they quit Docker Desktop and start the application.
- Fixed an issue where the About Docker Desktop window wasn't working anymore.
- Limit the number of CPUs to 8 on Mac M1 to fix the startup problem. Fixes docker/for-mac#6063.
For Windows
- Fixed an issue related to compose app started with version 2, but the dashboard only deals with version 1
Known issues
For Windows
Installing Docker Desktop 4.5.0 from scratch has a bug which defaults Docker Desktop to use the Hyper-V backend instead of WSL 2. This means, Windows Home users will not be able to start Docker Desktop as WSL 2 is the only supported backend. To work around this issue, you must uninstall 4.5.0 from your machine and then download and install Docker Desktop 4.5.1 or a higher version. Alternatively, you can edit the Docker Desktop settings.json file located at %APPDATA%\Docker\settings.json
and manually switch the value of the wslEngineEnabled
field to true
.
4.4.4
2022-01-24Bug fixes and enhancements
For Windows
- Fixed logging in from WSL 2. Fixes docker/for-win#12500.
Known issues
For Windows
- Clicking Proceed to Desktop after signing in through the browser, sometimes does not bring the Dashboard to the front.
- After logging in, when the Dashboard receives focus, it sometimes stays in the foreground even when clicking a background window. As a workaround you need to click the Dashboard before clicking another application window.
- The tips of the week show on top of the mandatory login dialog when an organization restriction is enabled via a
registry.json
file.
4.4.3
2022-01-14Bug fixes and enhancements
For Windows
- Disabled Dashboard shortcuts to prevent capturing them even when minimized or un-focussed. Fixes docker/for-win#12495.
Known issues
For Windows
- Clicking Proceed to Desktop after signing in through the browser, sometimes does not bring the Dashboard to the front.
- After logging in, when the Dashboard receives focus, it sometimes stays in the foreground even when clicking a background window. As a workaround you need to click the Dashboard before clicking another application window.
- The tips of the week show on top of the mandatory login dialog when an organization restriction is enabled via a
registry.json
file.
4.4.2
22-01-13New
- Easy, Secure sign in with Auth0 and Single Sign-on
- Single Sign-on: Users with a Docker Business subscription can now configure SSO to authenticate using their identity providers (IdPs) to access Docker. For more information, see Single Sign-on.
- Signing in to Docker Desktop now takes you through the browser so that you get all the benefits of auto-filling from password managers.
Upgrades
Security
- Fixed CVE-2021-45449 that affects users currently on Docker Desktop version 4.3.0 or 4.3.1.
Docker Desktop version 4.3.0 and 4.3.1 has a bug that may log sensitive information (access token or password) on the user's machine during login. This only affects users if they are on Docker Desktop 4.3.0, 4.3.1 and the user has logged in while on 4.3.0, 4.3.1. Gaining access to this data would require having access to the user’s local files.
Bug fixes and enhancements
For all platforms
- Docker Desktop displays an error if
registry.json
contains more than one organization in theallowedOrgs
field. If you are using multiple organizations for different groups of developers, you must provision a separateregistry.json
file for each group. - Fixed a regression in Compose that reverted the container name separator from
-
to_
. Fixes docker/compose-switch.
For Mac
- Fixed the memory statistics for containers in the Dashboard. Fixes docker/for-mac/#4774.
- Added a deprecated option to
settings.json
:""deprecatedCgroupv1"": true
, which switches the Linux environment back to cgroups v1. If your software requires cgroups v1, you should update it to be compatible with cgroups v2. Although cgroups v1 should continue to work, it is likely that some future features will depend on cgroups v2. It is also possible that some Linux kernel bugs will only be fixed with cgroups v2. - Fixed an issue where putting the machine to Sleep mode after pausing Docker Desktop results in Docker Desktop not being able to resume from pause after the machine comes out of Sleep mode. Fixes for-mac#6058.
For Windows
- Doing a
Reset to factory defaults
no longer shuts down Docker Desktop.
Known issues
For all platforms
- The tips of the week show on top of the mandatory login dialog when an organization restriction is enabled via a
registry.json
file.
For Windows
- Clicking Proceed to Desktop after logging in in the browser, sometimes does not bring the Dashboard to the front.
- After logging in, when the Dashboard receives focus, it sometimes stays in the foreground even when clicking a background window. As a workaround you need to click the Dashboard before clicking another application window.
- When the Dashboard is open, even if it does not have focus or is minimized, it will still catch keyboard shortcuts (e.g. ctrl-r for Restart)
4.3.2
2021-12-21Security
- Fixed CVE-2021-45449 that affects users currently on Docker Desktop version 4.3.0 or 4.3.1.
Docker Desktop version 4.3.0 and 4.3.1 has a bug that may log sensitive information (access token or password) on the user's machine during login. This only affects users if they are on Docker Desktop 4.3.0, 4.3.1 and the user has logged in while on 4.3.0, 4.3.1. Gaining access to this data would require having access to the user’s local files.
Upgrades
Security
Log4j 2 CVE-2021-44228: We have updated the docker scan
CLI plugin.
This new version of docker scan
is able to detect
Log4j 2
CVE-2021-44228 and
Log4j 2
CVE-2021-45046
For more information, read the blog post Apache Log4j 2 CVE-2021-44228.
4.3.1
2021-12-11Upgrades
Security
Log4j 2 CVE-2021-44228: We have updated the docker scan
CLI plugin for you.
Older versions of docker scan
in Docker Desktop 4.3.0 and earlier versions are
not able to detect
Log4j 2
CVE-2021-44228.
For more information, read the blog post Apache Log4j 2 CVE-2021-44228.
4.3.0
2021-12-02Upgrades
- Docker Engine v20.10.11
- containerd v1.4.12
- Buildx 0.7.1
- Compose v2.2.1
- Kubernetes 1.22.4
- Docker Hub Tool v0.4.4
- Go 1.17.3
Bug fixes and minor changes
For all platforms
- Added a self-diagnose warning if the host lacks Internet connectivity.
- Fixed an issue which prevented users from saving files from a volume using the Save As option in the Volumes UI. Fixes docker/for-win#12407.
- Docker Desktop now uses cgroupv2. If you need to run
systemd
in a container then:- Ensure your version of
systemd
supports cgroupv2. It must be at leastsystemd
247. Consider upgrading anycentos:7
images tocentos:8
. - Containers running
systemd
need the following options:--privileged --cgroupns=host -v /sys/fs/cgroup:/sys/fs/cgroup:rw
.
- Ensure your version of
For Mac
- Docker Desktop on Apple silicon no longer requires Rosetta 2, with the exception of three optional command line tools.
For Windows
- Fixed an issue that caused Docker Desktop to fail during startup if the home directory path contains a character used in regular expressions. Fixes docker/for-win#12374.
Known issue
Docker Desktop Dashboard incorrectly displays the container memory usage as zero on
Hyper-V based machines.
You can use the
docker stats
command on the command line as a workaround to view the
actual memory usage. See
docker/for-mac#6076.
Deprecation
- The following internal DNS names are deprecated and will be removed from a future release:
docker-for-desktop
,docker-desktop
,docker.for.mac.host.internal
,docker.for.mac.localhost
,docker.for.mac.gateway.internal
. You must now usehost.docker.internal
,vm.docker.internal
, andgateway.docker.internal
. - Removed: Custom RBAC rules have been removed from Docker Desktop as it gives
cluster-admin
privileges to all Service Accounts. Fixes docker/for-mac/#4774.
4.2.0
2021-11-09New
Pause/Resume: You can now pause your Docker Desktop session when you are not actively using it and save CPU resources on your machine.
Software Updates: The option to turn off automatic check for updates is now available for users on all Docker subscriptions, including Docker Personal and Docker Pro. All update-related settings have been moved to the Software Updates section.
Window management: The Docker Desktop Dashboard window size and position persists when you close and reopen Docker Desktop.
Upgrades
Bug fixes and minor changes
For all platforms
- Improved: Self-diagnose now also checks for overlap between host IPs and
docker networks
. - Fixed the position of the indicator that displays the availability of an update on the Docker Desktop Dashboard.
For Mac
- Fixed an issue that caused Docker Desktop to stop responding upon clicking Exit on the fatal error dialog.
- Fixed a rare startup failure affecting users having a
docker volume
bind-mounted on top of a directory from the host. If existing, this fix will also remove manually user addedDENY DELETE
ACL entries on the corresponding host directory. - Fixed a bug where a
Docker.qcow2
file would be ignored on upgrade and a freshDocker.raw
used instead, resulting in containers and images disappearing. Note that if a system has both files (due to the previous bug) then the most recently modified file will be used, to avoid recent containers and images disappearing again. To force the use of the oldDocker.qcow2
, delete the newerDocker.raw
file. Fixes docker/for-mac#5998. - Fixed a bug where subprocesses could fail unexpectedly during shutdown, triggering an unexpected fatal error popup. Fixes docker/for-mac#5834.
For Windows
- Fixed Docker Desktop sometimes hanging when clicking Exit in the fatal error dialog.
- Fixed an issue that frequently displayed the Download update popup when an update has been downloaded but hasn't been applied yet docker/for-win#12188.
- Fixed installing a new update killing the application before it has time to shut down.
- Fixed: Installation of Docker Desktop now works even with group policies preventing users to start prerequisite services (e.g. LanmanServer) docker/for-win#12291.
4.1.1
2021-10-12Bug fixes and minor changes
For Mac
When upgrading from 4.1.0, the Docker menu does not change to Update and restart so you can just wait for the download to complete (icon changes) and then select Restart. This bug is fixed in 4.1.1, for future upgrades.
- Fixed a bug where a
Docker.qcow2
file would be ignored on upgrade and a freshDocker.raw
used instead, resulting in containers and images disappearing. If a system has both files (due to the previous bug), then the most recently modified file will be used to avoid recent containers and images disappearing again. To force the use of the oldDocker.qcow2
, delete the newerDocker.raw
file. Fixes docker/for-mac#5998. - Fixed the update notification overlay sometimes getting out of sync between the Settings button and the Software update button in the Docker Desktop Dashboard.
- Fixed the menu entry to install a newly downloaded Docker Desktop update. When an update is ready to install, the Restart option changes to Update and restart.
For Windows
- Fixed a regression in WSL 2 integrations for some distributions (e.g. Arch or Alpine). Fixes docker/for-win#12229
- Fixed update notification overlay sometimes getting out of sync between the Settings button and the Software update button in the Dashboard.
4.1.0
2021-09-30New
- Software Updates: The Settings tab now includes a new section to help you manage Docker Desktop updates. The Software Updates section notifies you whenever there's a new update and allows you to download the update or view information on what's included in the newer version.
- Compose V2 You can now specify whether to use Docker Compose V2 in the General settings.
- Volume Management: Volume management is now available for users on any subscription, including Docker Personal. Ships Docker Public Roadmap#215
Upgrades
- Compose V2
- Buildx 0.6.3
- Kubernetes 1.21.5
- Go 1.17.1
- Alpine 3.14
- Qemu 6.1.0
- Base distribution to debian:bullseye
Bug fixes and minor changes
For Windows
- Fixed a bug related to anti-malware software triggering, self-diagnose avoids calling the
net.exe
utility. - Fixed filesystem corruption in the WSL 2 Linux VM in self-diagnose. This can be caused by microsoft/WSL#5895.
- Fixed
SeSecurityPrivilege
requirement issue. See docker/for-win#12037. - Fixed CLI context switch sync with UI. See docker/for-win#11721.
- Added the key
vpnKitMaxPortIdleTime
tosettings.json
to allow the idle network connection timeout to be disabled or extended. - Fixed a crash on exit. See docker/for-win#12128.
- Fixed a bug where the CLI tools would not be available in WSL 2 distributions.
- Fixed switching from Linux to Windows containers that was stuck because access rights on panic.log. See for-win#11899.
Known Issues
For Windows
Docker Desktop may fail to start when upgrading to 4.1.0 on some WSL-based distributions such as ArchWSL. See docker/for-win#12229
4.0.1
2021-09-13Upgrades
- Compose V2 RC3
- Compose v2 is now hosted on github.com/docker/compose.
- Fixed go panic on downscale using
compose up --scale
. - Fixed a race condition in
compose run --rm
while capturing exit code.
Bug fixes and minor changes
For all platforms
- Fixed a bug where copy-paste was not available in the Docker Desktop Dashboard.
For Windows
- Fixed a bug where Docker Desktop would not start correctly with the Hyper-V engine. See docker/for-win#11963
4.0.0
2021-08-31New
Docker has announced updates and extensions to the product subscriptions to increase productivity, collaboration, and added security for our developers and businesses.
The updated Docker Subscription Service Agreement includes a change to the terms for Docker Desktop.
- Docker Desktop remains free for small businesses (fewer than 250 employees AND less than $10 million in annual revenue), personal use, education, and non-commercial open source projects.
- It requires a paid subscription (Pro, Team, or Business), for as little as $5 a month, for professional use in larger enterprises.
- The effective date of these terms is August 31, 2021. There is a grace period until January 31, 2022 for those that will require a paid subscription to use Docker Desktop.
- The Docker Pro and Docker Team subscriptions now include commercial use of Docker Desktop.
- The existing Docker Free subscription has been renamed Docker Personal.
- No changes to Docker Engine or any other upstream open source Docker or Moby project.
To understand how these changes affect you, read the FAQs. For more information, see Docker subscription overview.
Upgrades
- Compose V2 RC2
- Fixed project name to be case-insensitive for
compose down
. See docker/compose-cli#2023 - Fixed non-normalized project name.
- Fixed port merging on partial reference.
- Fixed project name to be case-insensitive for
- Kubernetes 1.21.4
Bug fixes and minor changes
For Mac
- Fixed a bug where SSH was not available for builds from git URL. Fixes for-mac#5902
For Windows
- Fixed a bug where the CLI tools would not be available in WSL 2 distributions.
- Fixed a bug when switching from Linux to Windows containers due to access rights on
panic.log
. for-win#11899",,,
6ba65cb7bcf70eeafc377d4edaf9bb025495ccb33d4ea71f6815fdc46769c3b9,"Key features and benefits
Linux user namespace on all containers
With Enhanced Container Isolation, all user containers leverage the Linux user namespace for extra isolation. This means that the root user in the container maps to an unprivileged user in the Docker Desktop Linux VM.
For example:
$ docker run -it --rm --name=first alpine
/ # cat /proc/self/uid_map
0 100000 65536
The output 0 100000 65536
is the signature of the Linux user namespace. It
means that the root user (0) in the container is mapped to unprivileged user
100000 in the Docker Desktop Linux VM, and the mapping extends for a continuous
range of 64K user IDs. The same applies to group IDs.
Each container gets an exclusive range of mappings, managed by Sysbox. For example, if a second container is launched the mapping range is different:
$ docker run -it --rm --name=second alpine
/ # cat /proc/self/uid_map
0 165536 65536
In contrast, without Enhanced Container Isolation, the container's root user is in fact root on the host (aka ""true root"") and this applies to all containers:
$ docker run -it --rm alpine
/ # cat /proc/self/uid_map
0 0 4294967295
By virtue of using the Linux user namespace, Enhanced Container Isolation ensures the container processes never run as user ID 0 (true root) in the Linux VM. In fact they never run with any valid user-ID in the Linux VM. Thus, their Linux capabilities are constrained to resources within the container only, increasing isolation significantly compared to regular containers, both container-to-host and cross-container isolation.
Privileged containers are also secured
Privileged containers docker run --privileged ...
are insecure because they
give the container full access to the Linux kernel. That is, the container runs
as true root with all capabilities enabled, seccomp and AppArmor restrictions
are disabled, all hardware devices are exposed, for example.
Organizations aiming to secure Docker Desktop on developers' machines face challenges with privileged containers. These containers, whether running benign or malicious workloads, can gain control of the Linux kernel within the Docker Desktop VM, potentially altering security related settings, for example registry access management, and network proxies.
With Enhanced Container Isolation, privileged containers can no longer do this. The combination of the Linux user namespace and other security techniques used by Sysbox ensures that processes inside a privileged container can only access resources assigned to the container.
Note
Enhanced Container Isolation does not prevent users from launching privileged containers, but rather runs them securely by ensuring that they can only modify resources associated with the container. Privileged workloads that modify global kernel settings, for example loading a kernel module or changing Berkeley Packet Filters (BPF) settings will not work properly as they will receive ""permission denied"" error when attempting such operations.
For example, Enhanced Container Isolation ensures privileged containers can't access Docker Desktop network settings in the Linux VM configured via BPF:
$ docker run --privileged djs55/bpftool map show
Error: can't get next map: Operation not permitted
In contrast, without Enhanced Container Isolation, privileged containers can easily do this:
$ docker run --privileged djs55/bpftool map show
17: ringbuf name blocked_packets flags 0x0
key 0B value 0B max_entries 16777216 memlock 0B
18: hash name allowed_map flags 0x0
key 4B value 4B max_entries 10000 memlock 81920B
20: lpm_trie name allowed_trie flags 0x1
key 8B value 8B max_entries 1024 memlock 16384B
Note that some advanced container workloads require privileged containers, for example Docker-in-Docker, Kubernetes-in-Docker, etc. With Enhanced Container Isolation you can still run such workloads but do so much more securely than before.
Containers can't share namespaces with the Linux VM
When Enhanced Container Isolation is enabled, containers can't share Linux namespaces with the host (e.g., PID, network, uts, etc.) as that essentially breaks isolation.
For example, sharing the PID namespace fails:
$ docker run -it --rm --pid=host alpine
docker: Error response from daemon: failed to create shim task: OCI runtime create failed: error in the container spec: invalid or unsupported container spec: sysbox containers can't share namespaces [pid] with the host (because they use the linux user-namespace for isolation): unknown.
Similarly sharing the network namespace fails:
$ docker run -it --rm --network=host alpine
docker: Error response from daemon: failed to create shim task: OCI runtime create failed: error in the container spec: invalid or unsupported container spec: sysbox containers can't share a network namespace with the host (because they use the linux user-namespace for isolation): unknown.
In addition, the --userns=host
flag, used to disable the user namespace on the
container, is ignored:
$ docker run -it --rm --userns=host alpine
/ # cat /proc/self/uid_map
0 100000 65536
Finally, Docker build --network=host
and Docker buildx entitlements
(network.host
, security.insecure
) are not allowed. Builds that require these
won't work properly.
Bind mount restrictions
When Enhanced Container Isolation is enabled, Docker Desktop users can continue to bind mount host directories into containers as configured via Settings > Resources > File sharing, but they are no longer allowed to bind mount arbitrary Linux VM directories into containers.
This prevents containers from modifying sensitive files inside the Docker Desktop Linux VM, files that can hold configurations for registry access management, proxies, Docker Engine configurations, and more.
For example, the following bind mount of the Docker Engine's configuration file
(/etc/docker/daemon.json
inside the Linux VM) into a container is restricted
and therefore fails:
$ docker run -it --rm -v /etc/docker/daemon.json:/mnt/daemon.json alpine
docker: Error response from daemon: failed to create shim task: OCI runtime create failed: error in the container spec: can't mount /etc/docker/daemon.json because it's configured as a restricted host mount: unknown
In contrast, without Enhanced Container Isolation this mount works and gives the container full read and write access to the Docker Engine's configuration.
Of course, bind mounts of host files continue to work as usual. For example,
assuming a user configures Docker Desktop to file share her $HOME
directory,
she can bind mount it into the container:
$ docker run -it --rm -v $HOME:/mnt alpine
/ #
Note
By default, Enhanced Container Isolation won't allow bind mounting the Docker Engine socket (
/var/run/docker.sock
) into a container, as doing so essentially grants the container control of Docker Engine, thus breaking container isolation. However, as some legitimate use cases require this, it's possible to relax this restriction for trusted container images. See Docker socket mount permissions.
Vetting sensitive system calls
Another feature of Enhanced Container Isolation is that it intercepts and vets a
few highly sensitive system calls inside containers, such as mount
and
umount
. This ensures that processes that have capabilities to execute these
system calls can't use them to breach the container.
For example, a container that has CAP_SYS_ADMIN
(required to execute the
mount
system call) can't use that capability to change a read-only bind mount
into a read-write mount:
$ docker run -it --rm --cap-add SYS_ADMIN -v $HOME:/mnt:ro alpine
/ # mount -o remount,rw /mnt /mnt
mount: permission denied (are you root?)
Since the $HOME
directory was mounted into the container's /mnt
directory as
read-only, it can't be changed from within the container to read-write, even if the container process has the capability to do so. This
ensures container processes can't use mount
, or umount
, to breach the container's
root filesystem.
Note however that in the previous example the container can still create mounts within the container, and mount them read-only or read-write as needed. Those mounts are allowed since they occur within the container, and therefore don't breach it's root filesystem:
/ # mkdir /root/tmpfs
/ # mount -t tmpfs tmpfs /root/tmpfs
/ # mount -o remount,ro /root/tmpfs /root/tmpfs
/ # findmnt | grep tmpfs
├─/root/tmpfs tmpfs tmpfs ro,relatime,uid=100000,gid=100000
/ # mount -o remount,rw /root/tmpfs /root/tmpfs
/ # findmnt | grep tmpfs
├─/root/tmpfs tmpfs tmpfs rw,relatime,uid=100000,gid=100000
This feature, together with the user-namespace, ensures that even if a container process has all Linux capabilities they can't be used to breach the container.
Finally, Enhanced Container Isolation does system call vetting in such a way that it does not affect the performance of containers in the great majority of cases. It intercepts control-path system calls that are rarely used in most container workloads but data-path system calls are not intercepted.
Filesystem user-ID mappings
As mentioned, ECI enables the Linux user namespace on all containers. This ensures that the container's user-ID range (0->64K) maps to an unprivileged range of ""real"" user-IDs in the Docker Desktop Linux VM (e.g., 100000->165535).
Moreover, each container gets an exclusive range of real user-IDs in the Linux VM (e.g., container 0 could get mapped to 100000->165535, container 2 to 165536->231071, container 3 to 231072->296607, and so on). The same applies to group-IDs. In addition, if a container is stopped and restarted, there is no guarantee it will receive the same mapping as before. This is by design and further improves security.
However this presents a problem when mounting Docker volumes into containers. Files written to such volumes have the real user/group-IDs and therefore won't be accessible across a container's start/stop/restart, or between containers due to the different real user-ID/group-ID of each container.
To solve this problem, Sysbox uses ""filesystem user-ID remapping"" via the Linux
Kernel's ID-mapped mounts feature (added in 2021) or an alternative shiftsfs
module. These technologies map filesystem accesses from the container's
real user-ID (e.g., range 100000->165535) to the range (0->65535) inside Docker
Desktop's Linux VM. This way, volumes can now be mounted or shared across
containers, even if each container uses an exclusive range of user-IDs. Users
need not worry about the container's real user-IDs.
Although filesystem user-ID remapping may cause containers to access Linux VM files mounted into the container with real user-ID 0, the restricted mounts feature ensures that sensitive Linux VM files can't be mounted into the container.
Procfs & sysfs emulation
Another feature of Enhanced Container Isolation is that inside each container,
the /proc
and /sys
filesystems are partially emulated. This
serves several purposes, such as hiding sensitive host information inside the
container and namespacing host kernel resources that are not yet namespaced by
the Linux kernel itself.
As a simple example, when Enhanced Container Isolation is enabled the
/proc/uptime
file shows the uptime of the container itself, not that of the
Docker Desktop Linux VM:
$ docker run -it --rm alpine
/ # cat /proc/uptime
5.86 5.86
In contrast, without Enhanced Container Isolation you see the uptime of the Docker Desktop Linux VM. Though this is a trivial example, it shows how Enhanced Container Isolation aims to prevent the Linux VM's configuration and information from leaking into the container so as to make it more difficult to breach the VM.
In addition several other resources under /proc/sys
that are not namespaced by
the Linux Kernel are also emulated inside the container. Each container
sees a separate view of each such resource and Sysbox reconciles the values
across the containers when programming the corresponding Linux kernel setting.
This has the advantage of enabling container workloads that would otherwise require truly privileged containers to access such non-namespaced kernel resources to run with Enhanced Container Isolation enabled, thereby improving security.",,,
663c4ace83ac846df2777627571188b0db6a3e5a42a8b3ca2dd5d5f83659ba49,"Inline cache
The inline
cache storage backend is the simplest way to get an external cache
and is easy to get started using if you're already building and pushing an
image.
The downside of inline cache is that it doesn't scale with multi-stage builds as well as the other drivers do. It also doesn't offer separation between your output artifacts and your cache output. This means that if you're using a particularly complex build flow, or not exporting your images directly to a registry, then you may want to consider the registry cache.
Synopsis
$ docker buildx build --push -t <registry>/<image> \
--cache-to type=inline \
--cache-from type=registry,ref=<registry>/<image> .
No additional parameters are supported for the inline
cache.
To export cache using inline
storage, pass type=inline
to the --cache-to
option:
$ docker buildx build --push -t <registry>/<image> \
--cache-to type=inline .
Alternatively, you can also export inline cache by setting the build argument
BUILDKIT_INLINE_CACHE=1
, instead of using the --cache-to
flag:
$ docker buildx build --push -t <registry>/<image> \
--build-arg BUILDKIT_INLINE_CACHE=1 .
To import the resulting cache on a future build, pass type=registry
to
--cache-from
which lets you extract the cache from inside a Docker image in
the specified registry:
$ docker buildx build --push -t <registry>/<image> \
--cache-from type=registry,ref=<registry>/<image> .
Further reading
For an introduction to caching see Docker build cache.
For more information on the inline
cache backend, see the
BuildKit README.",,,
26240c83efd430e74d860cc92190332d0b65dc36e07678e78dd467e4ad33eebe,"Use the PKG installer
The PKG package supports various MDM (Mobile Device Management) solutions, making it ideal for bulk installations and eliminating the need for manual setups by individual users. With this package, IT administrators can ensure standardized, policy-driven installations of Docker Desktop, enhancing efficiency and software management across their organizations.
Install interactively
- In the Docker Admin Console, navigate to your organization.
- Under Docker Desktop, select the Deploy page.
- From the macOS tab, select the Download PKG installer button.
- Once downloaded, double-click
Docker.pkg
to run the installer. - Follow the instructions on the installation wizard to authorize the installer and proceed with the install.
- Introduction: Select
Continue
. - License: Review the license agreement and select
Agree
. - Destination Select: This step is optional. It is recommended that you don't change the default installation destination (usually
Macintosh HD
). SelectContinue
. - Installation Type: Select
Install
. - Installation: Authenticate using your administrator password or Touch ID.
- Summary: After the installation completes, select
Close
.
- Introduction: Select
Note
When installing Docker Desktop with the PKG, in-app updates are automatically disabled. This feature ensures your organization maintains the required Docker Desktop version. For Docker Desktop installed with the .dmg installer, in-app updates remain supported.
Docker Desktop notifies you when an update is available. To update Docker Desktop, download the latest installer from the Docker Admin Console. Navigate to the Deploy page > under Docker Desktop.
To keep up to date with new releases, check the release notes page.
Install from the command line
In the Docker Admin Console, navigate to your organization.
Under Security and access, select the Deploy Docker Desktop page.
From the macOS tab, select the Download PKG installer button.
From your terminal, run the following command:
$ sudo installer -pkg ""/path/to/Docker.pkg"" -target /Applications
Additional resources
- See how you can deploy Docker Desktop for Mac via Intune or Jamf Pro
- Explore how to Enforce sign-in for your users.",,,
d4957853f728ee0ecd79810d0b128a2700e24088d195c8d929c79300d3c428d6,"Splunk logging driver
The splunk
logging driver sends container logs to
HTTP Event Collector
in Splunk Enterprise and Splunk Cloud.
Usage
You can configure Docker logging to use the splunk
driver by default or on a
per-container basis.
To use the splunk
driver as the default logging driver, set the keys
log-driver
and log-opts
to appropriate values in the daemon.json
configuration file and restart Docker. For example:
{
""log-driver"": ""splunk"",
""log-opts"": {
""splunk-token"": """",
""splunk-url"": """",
...
}
}
The daemon.json file is located in /etc/docker/
on Linux hosts or
C:\ProgramData\docker\config\daemon.json
on Windows Server. For more about
configuring Docker using daemon.json
, see
daemon.json.
Note
log-opts
configuration options in thedaemon.json
configuration file must be provided as strings. Boolean and numeric values (such as the value forsplunk-gzip
orsplunk-gzip-level
) must therefore be enclosed in quotes (""
).
To use the splunk
driver for a specific container, use the commandline flags
--log-driver
and log-opt
with docker run
:
$ docker run --log-driver=splunk --log-opt splunk-token=VALUE --log-opt splunk-url=VALUE ...
Splunk options
The following properties let you configure the Splunk logging driver.
- To configure the
splunk
driver across the Docker environment, editdaemon.json
with the key,""log-opts"": {""NAME"": ""VALUE"", ...}
. - To configure the
splunk
driver for an individual container, usedocker run
with the flag,--log-opt NAME=VALUE ...
.
| Option | Required | Description |
|---|---|---|
splunk-token | required | Splunk HTTP Event Collector token. |
splunk-url | required | Path to your Splunk Enterprise, self-service Splunk Cloud instance, or Splunk Cloud managed cluster (including port and scheme used by HTTP Event Collector) in one of the following formats: https://your_splunk_instance:8088 , https://input-prd-p-XXXXXXX.cloud.splunk.com:8088 , or https://http-inputs-XXXXXXXX.splunkcloud.com . |
splunk-source | optional | Event source. |
splunk-sourcetype | optional | Event source type. |
splunk-index | optional | Event index. |
splunk-capath | optional | Path to root certificate. |
splunk-caname | optional | Name to use for validating server certificate; by default the hostname of the splunk-url is used. |
splunk-insecureskipverify | optional | Ignore server certificate validation. |
splunk-format | optional | Message format. Can be inline , json or raw . Defaults to inline . |
splunk-verify-connection | optional | Verify on start, that Docker can connect to Splunk server. Defaults to true. |
splunk-gzip | optional | Enable/disable gzip compression to send events to Splunk Enterprise or Splunk Cloud instance. Defaults to false. |
splunk-gzip-level | optional | Set compression level for gzip. Valid values are -1 (default), 0 (no compression), 1 (best speed) ... 9 (best compression). Defaults to DefaultCompression. |
tag | optional | Specify tag for message, which interpret some markup. Default value is {{.ID}} (12 characters of the container ID). Refer to the
log tag option documentation for customizing the log tag format. |
labels | optional | Comma-separated list of keys of labels, which should be included in message, if these labels are specified for container. |
labels-regex | optional | Similar to and compatible with labels . A regular expression to match logging-related labels. Used for advanced
log tag options. |
env | optional | Comma-separated list of keys of environment variables, which should be included in message, if these variables are specified for container. |
env-regex | optional | Similar to and compatible with env . A regular expression to match logging-related environment variables. Used for advanced
log tag options. |
If there is collision between the label
and env
keys, the value of the env
takes precedence. Both options add additional fields to the attributes of a
logging message.
Below is an example of the logging options specified for the Splunk Enterprise instance. The instance is installed locally on the same machine on which the Docker daemon is running.
The path to the root certificate and Common Name is specified using an HTTPS
scheme. This is used for verification. The SplunkServerDefaultCert
is
automatically generated by Splunk certificates.
$ docker run \
--log-driver=splunk \
--log-opt splunk-token=176FCEBF-4CF5-4EDF-91BC-703796522D20 \
--log-opt splunk-url=https://splunkhost:8088 \
--log-opt splunk-capath=/path/to/cert/cacert.pem \
--log-opt splunk-caname=SplunkServerDefaultCert \
--log-opt tag=""{{.Name}}/{{.FullID}}"" \
--log-opt labels=location \
--log-opt env=TEST \
--env ""TEST=false"" \
--label location=west \
your/application
The splunk-url
for Splunk instances hosted on Splunk Cloud is in a format
like https://http-inputs-XXXXXXXX.splunkcloud.com
and does not include a
port specifier.
Message formats
There are three logging driver messaging formats: inline
(default), json
,
and raw
.
The default format is inline
where each log message is embedded as a string.
For example:
{
""attrs"": {
""env1"": ""val1"",
""label1"": ""label1""
},
""tag"": ""MyImage/MyContainer"",
""source"": ""stdout"",
""line"": ""my message""
}
{
""attrs"": {
""env1"": ""val1"",
""label1"": ""label1""
},
""tag"": ""MyImage/MyContainer"",
""source"": ""stdout"",
""line"": ""{\""foo\"": \""bar\""}""
}
To format messages as json
objects, set --log-opt splunk-format=json
. The
driver attempts to parse every line as a JSON object and send it as an embedded
object. If it can't parse the message, it's sent inline
. For example:
{
""attrs"": {
""env1"": ""val1"",
""label1"": ""label1""
},
""tag"": ""MyImage/MyContainer"",
""source"": ""stdout"",
""line"": ""my message""
}
{
""attrs"": {
""env1"": ""val1"",
""label1"": ""label1""
},
""tag"": ""MyImage/MyContainer"",
""source"": ""stdout"",
""line"": {
""foo"": ""bar""
}
}
To format messages as raw
, set --log-opt splunk-format=raw
. Attributes
(environment variables and labels) and tags are prefixed to the message. For
example:
MyImage/MyContainer env1=val1 label1=label1 my message
MyImage/MyContainer env1=val1 label1=label1 {""foo"": ""bar""}
Advanced options
The Splunk logging driver lets you configure a few advanced options by setting environment variables for the Docker daemon.
| Environment variable name | Default value | Description |
|---|---|---|
SPLUNK_LOGGING_DRIVER_POST_MESSAGES_FREQUENCY | 5s | The time to wait for more messages to batch. |
SPLUNK_LOGGING_DRIVER_POST_MESSAGES_BATCH_SIZE | 1000 | The number of messages that should accumulate before sending them in one batch. |
SPLUNK_LOGGING_DRIVER_BUFFER_MAX | 10 * 1000 | The maximum number of messages held in buffer for retries. |
SPLUNK_LOGGING_DRIVER_CHANNEL_SIZE | 4 * 1000 | The maximum number of pending messages that can be in the channel used to send messages to background logger worker, which batches them. |",,,
57a9d9033215e8aa7da687b8c382018876a9f51c84bb604eb2a0b874e8829b0d,"Automation with content trust
It is very common for Docker Content Trust to be built into existing automation systems. To allow tools to wrap Docker and push trusted content, there are environment variables that can be passed through to the client.
This guide follows the steps as described in Signing images with Docker Content Trust. Make sure you understand and follow the prerequisites.
When working directly with the Notary client, it uses its own set of environment variables.
Add a delegation private key
To automate importing a delegation private key to the local Docker trust store, we need to pass a passphrase for the new key. This passphrase will be required everytime that delegation signs a tag.
$ export DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE=""mypassphrase123""
$ docker trust key load delegation.key --name jeff
Loading key from ""delegation.key""...
Successfully imported key from delegation.key
Add a delegation public key
If you initialize a repository at the same time as adding a delegation public key, then you will need to use the local Notary Canonical Root Key's passphrase to create the repositories trust data. If the repository has already been initiated then you only need the repositories passphrase.
# Export the Local Root Key Passphrase if required.
$ export DOCKER_CONTENT_TRUST_ROOT_PASSPHRASE=""rootpassphrase123""
# Export the Repository Passphrase
$ export DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE=""repopassphrase123""
# Initialize Repo and Push Delegation
$ docker trust signer add --key delegation.crt jeff registry.example.com/admin/demo
Adding signer ""jeff"" to registry.example.com/admin/demo...
Initializing signed repository for registry.example.com/admin/demo...
Successfully initialized ""registry.example.com/admin/demo""
Successfully added signer: registry.example.com/admin/demo
Sign an image
Finally when signing an image, we will need to export the passphrase of the
signing key. This was created when the key was loaded into the local Docker
trust store with $ docker trust key load
.
$ export DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE=""mypassphrase123""
$ docker trust sign registry.example.com/admin/demo:1
Signing and pushing trust data for local image registry.example.com/admin/demo:1, may overwrite remote trust data
The push refers to repository [registry.example.com/admin/demo]
428c97da766c: Layer already exists
2: digest: sha256:1a6fd470b9ce10849be79e99529a88371dff60c60aab424c077007f6979b4812 size: 524
Signing and pushing trust metadata
Successfully signed registry.example.com/admin/demo:1
Build with content trust
You can also build with content trust. Before running the docker build
command,
you should set the environment variable DOCKER_CONTENT_TRUST
either manually or
in a scripted fashion. Consider the simple Dockerfile below.
# syntax=docker/dockerfile:1
FROM docker/trusttest:latest
RUN echo
The FROM
tag is pulling a signed image. You cannot build an image that has a
FROM
that is not either present locally or signed. Given that content trust
data exists for the tag latest
, the following build should succeed:
$ docker build -t docker/trusttest:testing .
Using default tag: latest
latest: Pulling from docker/trusttest
b3dbab3810fc: Pull complete
a9539b34a6ab: Pull complete
Digest: sha256:d149ab53f871
If content trust is enabled, building from a Dockerfile that relies on tag without trust data, causes the build command to fail:
$ docker build -t docker/trusttest:testing .
unable to process Dockerfile: No trust data for notrust",,,
dcdd1a002750bdc6baaaa6d6aa93e939126c4c6b72b9a29604062653b7dcc716,"Docker Engine 18.02 release notes
Table of contents
18.02.0-ce
2018-02-07
Builder
- Gitutils: fix checking out submodules moby/moby#35737
Client
- Attach: Ensure attach exit code matches container's docker/cli#696
- Added support for tmpfs-mode in compose file docker/cli#808
- Adds a new compose file version 3.6 docker/cli#808
- Fix issue of filter in
docker ps
wherehealth=starting
returns nothing moby/moby#35940
- Improve presentation of published port ranges docker/cli#581
- Bump Go to 1.9.3 docker/cli#827
- Fix broken Kubernetes stack flags docker/cli#831
- Annotate ""stack"" commands to be ""swarm"" and ""kubernetes"" docker/cli#804
Experimental
- Add manifest command docker/cli#138
- LCOW remotefs - return error in Read() implementation moby/moby#36051
- LCOW: Coalesce daemon stores, allow dual LCOW and WCOW mode moby/moby#34859
- LCOW: Fix OpenFile parameters moby/moby#36043
- LCOW: Raise minimum requirement to Windows RS3 RTM build (16299) moby/moby#36065
Logging
- Improve daemon config reload; log active configuration moby/moby#36019
- Fixed error detection using IsErrNotFound and IsErrNotImplemented for the ContainerLogs method moby/moby#36000
- Add journald tag as SYSLOG_IDENTIFIER moby/moby#35570
- Splunk: limit the reader size on error responses moby/moby#35509
Networking
- Disable service on release network results in zero-downtime deployments with rolling upgrades moby/moby#35960
- Fix services failing to start if multiple networks with the same name exist in different spaces moby/moby#30897
- Fix duplicate networks being added with
docker service update --network-add
docker/cli#780 - Fixing ingress network when upgrading from 17.09 to 17.12. moby/moby#36003
- Fix ndots configuration docker/libnetwork#1995
- Fix IPV6 networking being deconfigured if live-restore is enabled docker/libnetwork#2043
- Add support for MX type DNS queries in the embedded DNS server docker/libnetwork#2041
Packaging
- Added packaging for Fedora 26, Fedora 27, and Centos 7 on aarch64 docker/docker-ce-packaging#71
- Removed support for Ubuntu Zesty docker/docker-ce-packaging#73
- Removed support for Fedora 25 docker/docker-ce-packaging#72
Runtime
- Fixes unexpected Docker Daemon shutdown based on pipe error moby/moby#35968
- Fix some occurrences of hcsshim::ImportLayer failed in Win32: The system cannot find the path specified moby/moby#35924
- Windows: increase the maximum layer size during build to 127GB moby/moby#35925
- Fix Devicemapper: Error running DeleteDevice dm_task_run failed moby/moby#35919
- Introduce « exec_die » event moby/moby#35744
- Update API to version 1.36 moby/moby#35744
- Fix
docker update
not updating cpu quota, and cpu-period of a running container moby/moby#36030
- Make container shm parent unbindable moby/moby#35830
- Make image (layer) downloads faster by using pigz moby/moby#35697
- Protect the daemon from volume plugins that are slow or deadlocked moby/moby#35441
- Fix
DOCKER_RAMDISK
environment variable not being honoured moby/moby#35957
- Bump containerd to 1.0.1 (9b55aab90508bd389d7654c4baf173a981477d55) moby/moby#35986
- Update runc to fix hang during start and exec moby/moby#36097
- Fix ""--node-generic-resource"" singular/plural moby/moby#36125",,,
c7d411c7b699590b5be0834fe0c1918266d61cd750e4f8a7338adc1f9e579290,"Docker Engine prior releases
1.13.1 (2017-02-08)
Important
On Linux distributions where
devicemapper
was the default storage driver, theoverlay2
, oroverlay
is now used by default (if the kernel supports it). To use devicemapper, you can manually configure the storage driver to use through the--storage-driver
daemon option, or by setting ""storage-driver"" in thedaemon.json
configuration file.
Important
In Docker 1.13, the managed plugin api changed, as compared to the experimental version introduced in Docker 1.12. You must uninstall plugins which you installed with Docker 1.12 before upgrading to Docker 1.13. You can uninstall plugins using the
docker plugin rm
command.
If you have already upgraded to Docker 1.13 without uninstalling previously-installed plugins, you may see this message when the Docker daemon starts:
Error starting daemon: json: cannot unmarshal string into Go value of type types.PluginEnv
To manually remove all plugins and resolve this problem, take the following steps:
- Remove plugins.json from:
/var/lib/docker/plugins/
. - Restart Docker. Verify that the Docker daemon starts with no errors.
- Reinstall your plugins.
Contrib
Remote API (v1.26) & Client
- Support secrets in docker stack deploy with compose file #30144
Runtime
- Fix size issue in
docker system df
#30378 - Fix error on
docker inspect
when Swarm certificates were expired. #29246 - Fix deadlock on v1 plugin with activate error #30408
- Fix SELinux regression #30649
Plugins
- Support global scoped network plugins (v2) in swarm mode #30332
- Add
docker plugin upgrade
#29414
Windows
1.13.0 (2017-01-18)
Important
On Linux distributions where
devicemapper
was the default storage driver, theoverlay2
, oroverlay
is now used by default (if the kernel supports it). To use devicemapper, you can manually configure the storage driver to use through the--storage-driver
daemon option, or by setting ""storage-driver"" in thedaemon.json
configuration file.
Important
In Docker 1.13, the managed plugin api changed, as compared to the experimental version introduced in Docker 1.12. You must uninstall plugins which you installed with Docker 1.12 before upgrading to Docker 1.13. You can uninstall plugins using the
docker plugin rm
command.
If you have already upgraded to Docker 1.13 without uninstalling previously-installed plugins, you may see this message when the Docker daemon starts:
Error starting daemon: json: cannot unmarshal string into Go value of type types.PluginEnv
To manually remove all plugins and resolve this problem, take the following steps:
- Remove plugins.json from:
/var/lib/docker/plugins/
. - Restart Docker. Verify that the Docker daemon starts with no errors.
- Reinstall your plugins.
Builder
- Add capability to specify images used as a cache source on build. These images do not need to have local parent chain and can be pulled from other registries #26839
- (experimental) Add option to squash image layers to the FROM image after successful builds #22641
- Fix dockerfile parser with empty line after escape #24725
- Add step number on
docker build
#24978
- Add support for compressing build context during image build #25837
- add
--network
todocker build
#27702
- Fix inconsistent behavior between
--label
flag ondocker build
anddocker run
#26027 - Fix image layer inconsistencies when using the overlay storage driver #27209
- Unused build-args are now allowed. A warning is presented instead of an error and failed build #27412
- Fix builder cache issue on Windows #27805
Contrib
- Add support for building docker debs for Ubuntu 16.04 Xenial on PPC64LE #23438
- Add support for building docker debs for Ubuntu 16.04 Xenial on s390x #26104
- Add support for building docker debs for Ubuntu 16.10 Yakkety Yak on PPC64LE #28046
- Add RPM builder for VMWare Photon OS #24116
- Add shell completions to tgz #27735
- Update the install script to allow using the mirror in China #27005
- Add DEB builder for Ubuntu 16.10 Yakkety Yak #27993
- Add RPM builder for Fedora 25 #28222
- Add
make deb
support for aarch64 #27625
Distribution
- Update notary dependency to 0.4.2 (full changelogs
here)
#27074
- Support for compilation on windows docker/notary#970
- Improved error messages for client authentication errors docker/notary#972
- Support for finding keys that are anywhere in the
~/.docker/trust/private
directory, not just under~/.docker/trust/private/root_keys
or~/.docker/trust/private/tuf_keys
docker/notary#981 - Previously, on any error updating, the client would fall back on the cache. Now we only do so if there is a network error or if the server is unavailable or missing the TUF data. Invalid TUF data will cause the update to fail - for example if there was an invalid root rotation. docker/notary#982
- Improve root validation and yubikey debug logging docker/notary#858 docker/notary#891
- Warn if certificates for root or delegations are near expiry docker/notary#802
- Warn if role metadata is near expiry docker/notary#786
- Fix passphrase retrieval attempt counting and terminal detection docker/notary#906
- Avoid unnecessary blob uploads when different users push same layers to authenticated registry #26564
- Allow external storage for registry credentials #26354
Logging
- Standardize the default logging tag value in all logging drivers #22911
- Improve performance and memory use when logging of long log lines #22982
- Enable syslog driver for windows #25736
- Add Logentries Driver #27471
- Update of AWS log driver to support tags #27707
- Unix socket support for fluentd #26088
- Enable fluentd logging driver on Windows #28189
- Sanitize docker labels when used as journald field names #23725
- Fix an issue where
docker logs --tail
returned less lines than expected #28203 - Splunk Logging Driver: performance and reliability improvements #26207
- Splunk Logging Driver: configurable formats and skip for verifying connection #25786
Networking
- Add
--attachable
network support to enabledocker run
to work in swarm-mode overlay network #25962 - Add support for host port PublishMode in services using the
--publish
option indocker service create
#27917 and #28943 - Add support for Windows server 2016 overlay network driver (requires upcoming ws2016 update) #28182
- Change the default
FORWARD
policy toDROP
#28257
- Add support for specifying static IP addresses for predefined network on windows #22208
- Fix
--publish
flag ondocker run
not working with IPv6 addresses #27860 - Fix inspect network show gateway with mask #25564
- Fix an issue where multiple addresses in a bridge may cause
--fixed-cidr
to not have the correct addresses #26659
- Add creation timestamp to
docker network inspect
#26130
- Show peer nodes in
docker network inspect
for swarm overlay networks #28078 - Enable ping for service VIP address #28019
Plugins
- Add support for dynamically reloading authorization plugins #22770
- Add description in
docker plugin ls
#25556 - Add
-f
/--format
todocker plugin inspect
#25990 - Add
docker plugin create
command #28164
- Send request's TLS peer certificates to authorization plugins #27383
- Support for global-scoped network and ipam plugins in swarm-mode #27287
- Split
docker plugin install
into two API call/privileges
and/pull
#28963
Remote API (v1.25) & Client
- Support
docker stack deploy
from a Compose file #27998 - (experimental) Implement checkpoint and restore #22049
- Add
--format
flag todocker info
#23808
- Remove
--name
fromdocker volume create
#23830
- Add
docker stack ls
#23886 - Add a new
is-task
ps filter #24411 - Add
--env-file
flag todocker service create
#24844 - Add
--format
ondocker stats
#24987 - Make
docker node ps
default toself
in swarm node #25214 - Add
--group
indocker service create
#25317 - Add
--no-trunc
to service/node/stack ps output #25337 - Add Logs to
ContainerAttachOptions
so go clients can request to retrieve container logs as part of the attach process #26718 - Allow client to talk to an older server #27745
- Inform user client-side that a container removal is in progress #26074
- Do not allow more than one mode be requested at once in the services endpoint #26643
- Add capability to /containers/create API to specify mounts in a more granular and safer way #22373
- Add
--format
flag tonetwork ls
andvolume ls
#23475
- Allow the top-level
docker inspect
command to inspect any kind of resource #23614
- Add --cpus flag to control cpu resources for
docker run
anddocker create
, and addNanoCPUs
toHostConfig
#27958
- Allow unsetting the
--entrypoint
indocker run
ordocker create
#23718
- Restructure CLI commands by adding
docker image
anddocker container
commands for more consistency #26025
- Remove
COMMAND
column fromservice ls
output #28029
- Add
--format
todocker events
#26268
- Allow specifying multiple nodes on
docker node ps
#26299 - Restrict fractional digits to 2 decimals in
docker images
output #26303
- Add
--dns-option
todocker run
#28186 - Add Image ID to container commit event #28128
- Add external binaries version to docker info #27955
- Add information for
Manager Addresses
in the output ofdocker info
#28042 - Add a new reference filter for
docker images
#27872
Runtime
- Add
--experimental
daemon flag to enable experimental features, instead of shipping them in a separate build #27223 - Add a
--shutdown-timeout
daemon flag to specify the default timeout (in seconds) to stop containers gracefully before daemon exit #23036 - Add
--stop-timeout
to specify the timeout value (in seconds) for individual containers to stop #22566 - Add a new daemon flag
--userland-proxy-path
to allow configuring the userland proxy instead of using the hardcodeddocker-proxy
from$PATH
#26882 - Add boolean flag
--init
ondockerd
and ondocker run
to use tini a zombie-reaping init process as PID 1 #26061 #28037 - Add a new daemon flag
--init-path
to allow configuring the path to thedocker-init
binary #26941 - Add support for live reloading insecure registry in configuration #22337
- Add support for storage-opt size on Windows daemons #23391
- Improve reliability of
docker run --rm
by moving it from the client to the daemon #20848
- Add support for
--cpu-rt-period
and--cpu-rt-runtime
flags, allowing containers to run real-time threads whenCONFIG_RT_GROUP_SCHED
is enabled in the kernel #23430
- Fix partial/full filter issue in
service tasks --filter
#24850 - Allow engine to run inside a user namespace #25672
- Fix a race condition between device deferred removal and resume device, when using the devicemapper graphdriver #23497
- Add
docker stats
support in Windows #25737 - Allow using
--pid=host
and--net=host
when--userns=host
#25771
- (experimental) Add metrics (Prometheus) output for basic
container
,image
, anddaemon
operations #25820
- Fix issue in
docker stats
withNetworkDisabled=true
#25905
- Add
docker top
support in Windows #25891 - Record pid of exec'd process #27470
- Add support for looking up user/groups via
getent
#27599 - Add new
docker system
command withdf
andprune
subcommands for system resource management, as well asdocker {container,image,volume,network} prune
subcommands #26108 #27525 / #27525
- Fix an issue where containers could not be stopped or killed by setting xfs max_retries to 0 upon ENOSPC with devicemapper #26212
- Fix
docker cp
failing to copy to a container's volume dir on CentOS with devicemapper #28047
- Promote overlay(2) graphdriver #27932
- Add
--seccomp-profile
daemon flag to specify a path to a seccomp profile that overrides the default #26276
- Fix ulimits in
docker inspect
when--default-ulimit
is set on daemon #26405 - Add workaround for overlay issues during build in older kernels #28138
- Add
TERM
environment variable ondocker exec -t
#26461
- Honor a container’s
--stop-signal
setting upondocker kill
#26464
Swarm Mode
- Add secret management #27794
- Add support for templating service options (hostname, mounts, and environment variables) #28025
- Display the endpoint mode in the output of
docker service inspect --pretty
#26906 - Make
docker service ps
output more bearable by shortening service IDs in task names #28088 - Make
docker node ps
default to the current node #25214
- Add
--dns
, --dns-opt
, and--dns-search
to service create. #27567 - Add
--force
todocker service update
#27596 - Add
--health-*
and--no-healthcheck
flags todocker service create
anddocker service update
#27369 - Add
-q
todocker service ps
#27654
- Display number of global services in
docker service ls
#27710
- Remove
--name
flag fromdocker service update
. This flag is only functional ondocker service create
, so was removed from theupdate
command #26988 - Fix worker nodes failing to recover because of transient networking issues #26646
- Add support for health aware load balancing and DNS records #27279
- Add
--hostname
todocker service create
#27857 - Add
--host
todocker service create
, and--host-add
,--host-rm
todocker service update
#28031 - Add
--tty
flag todocker service create
/update
#28076
- Autodetect, store, and expose node IP address as seen by the manager #27910
- Encryption at rest of manager keys and raft data #27967
- Add
--update-max-failure-ratio
,--update-monitor
and--rollback
flags todocker service update
#26421
- Fix an issue with address autodiscovery on
docker swarm init
running inside a container #26457
- (experimental) Add
docker service logs
command to view logs for a service #28089 - Pin images by digest for
docker service create
andupdate
#28173
- Add short (
-f
) flag fordocker node rm --force
anddocker swarm leave --force
#28196
- Add options to customize Raft snapshots (
--max-snapshots
,--snapshot-interval
) #27997
- Don't repull image if pinned by digest #28265
- Swarm-mode support for Windows #27838
- Allow hostname to be updated on service #28771
- Support v2 plugins #29433
- Add content trust for services #29469
Volume
- Add a
--force
flag indocker volume rm
to forcefully purge the data of the volume that has already been deleted #23436 - Enhance
docker volume inspect
to show all options used when creating the volume #26671 - Add support for local NFS volumes to resolve hostnames #27329
Security
- Fix selinux labeling of volumes shared in a container #23024
- Prohibit
/sys/firmware/**
from being accessed with apparmor #26618
Deprecation
- Marked the
docker daemon
command as deprecated. The daemon is moved to a separate binary (dockerd
), and should be used instead #26834 - Deprecate unversioned API endpoints #28208
- Remove Ubuntu 15.10 (Wily Werewolf) as supported platform. Ubuntu 15.10 is EOL, and no longer receives updates #27042
- Remove Fedora 22 as supported platform. Fedora 22 is EOL, and no longer receives updates #27432
- Remove Fedora 23 as supported platform. Fedora 23 is EOL, and no longer receives updates #29455
- Deprecate the
repo:shortid
syntax ondocker pull
#27207 - Deprecate backing filesystem without
d_type
for overlay and overlay2 storage drivers #27433 - Deprecate
MAINTAINER
in Dockerfile #25466 - Deprecate
filter
param for endpoint/images/json
#27872 - Deprecate setting duplicate engine labels #24533
- Deprecate ""top-level"" network information in
NetworkSettings
#28437
1.12.6 (2017-01-10)
Important
Docker 1.12 ships with an updated systemd unit file for rpm based installs (which includes RHEL, Fedora, CentOS, and Oracle Linux 7). When upgrading from an older version of Docker, the upgrade process may not automatically install the updated version of the unit file, or fail to start the
docker service
if;
- the systemd unit file (
/usr/lib/systemd/system/docker.service
) contains local changes, or- a systemd drop-in file is present, and contains
-H fd://
in theExecStart
directive
Starting the docker service
will produce an error:
Failed to start docker.service: Unit docker.socket failed to load: No such file or directory.
or
no sockets found via socket activation: make sure the service was started by systemd.
To resolve this:
- Backup the current version of the unit file, and replace the file with the version that ships with docker 1.12
- Remove the
Requires=docker.socket
directive from the/usr/lib/systemd/system/docker.service
file if present - Remove
-H fd://
from theExecStart
directive (both in the main unit file, and in any drop-in files present).
After making those changes, run sudo systemctl daemon-reload
, and sudo systemctl restart docker
to reload changes and (re)start the docker daemon.
Note
Docker 1.12.5 will correctly validate that either an IPv6 subnet is provided or that the IPAM driver can provide one when you specify the
--ipv6
option.
If you are currently using the --ipv6
option without specifying the
--fixed-cidr-v6
option, the Docker daemon will refuse to start with the
following message:
Error starting daemon: Error initializing network controller: Error creating
default ""bridge"" network: failed to parse pool request
for address space ""LocalDefault"" pool "" subpool "":
could not find an available, non-overlapping IPv6 address
pool among the defaults to assign to the network
To resolve this error, either remove the --ipv6
flag (to preserve the same
behavior as in Docker 1.12.3 and earlier), or provide an IPv6 subnet as the
value of the --fixed-cidr-v6
flag.
In a similar way, if you specify the --ipv6
flag when creating a network
with the default IPAM driver, without providing an IPv6 --subnet
, network
creation will fail with the following message:
Error response from daemon: failed to parse pool request for address space
""LocalDefault"" pool """" subpool """": could not find an
available, non-overlapping IPv6 address pool among
the defaults to assign to the network
To resolve this, either remove the --ipv6
flag (to preserve the same behavior
as in Docker 1.12.3 and earlier), or provide an IPv6 subnet as the value of the
--subnet
flag.
The network creation will instead succeed if you use an external IPAM driver which supports automatic allocation of IPv6 subnets.
Runtime
- Fix runC privilege escalation (CVE-2016-9962)
1.12.5 (2016-12-15)
Important
Docker 1.12 ships with an updated systemd unit file for rpm based installs (which includes RHEL, Fedora, CentOS, and Oracle Linux 7). When upgrading from an older version of Docker, the upgrade process may not automatically install the updated version of the unit file, or fail to start the
docker service
if;
- the systemd unit file (
/usr/lib/systemd/system/docker.service
) contains local changes, or- a systemd drop-in file is present, and contains
-H fd://
in theExecStart
directive
Starting the docker service
will produce an error:
Failed to start docker.service: Unit docker.socket failed to load: No such file or directory.
or
no sockets found via socket activation: make sure the service was started by systemd.
To resolve this:
- Backup the current version of the unit file, and replace the file with the version that ships with docker 1.12
- Remove the
Requires=docker.socket
directive from the/usr/lib/systemd/system/docker.service
file if present - Remove
-H fd://
from theExecStart
directive (both in the main unit file, and in any drop-in files present).
After making those changes, run sudo systemctl daemon-reload
, and sudo systemctl restart docker
to reload changes and (re)start the docker daemon.
Note
Docker 1.12.5 will correctly validate that either an IPv6 subnet is provided or that the IPAM driver can provide one when you specify the
--ipv6
option.
If you are currently using the --ipv6
option without specifying the
--fixed-cidr-v6
option, the Docker daemon will refuse to start with the
following message:
Error starting daemon: Error initializing network controller: Error creating
default ""bridge"" network: failed to parse pool request
for address space ""LocalDefault"" pool "" subpool "":
could not find an available, non-overlapping IPv6 address
pool among the defaults to assign to the network
To resolve this error, either remove the --ipv6
flag (to preserve the same
behavior as in Docker 1.12.3 and earlier), or provide an IPv6 subnet as the
value of the --fixed-cidr-v6
flag.
In a similar way, if you specify the --ipv6
flag when creating a network
with the default IPAM driver, without providing an IPv6 --subnet
, network
creation will fail with the following message:
Error response from daemon: failed to parse pool request for address space
""LocalDefault"" pool """" subpool """": could not find an
available, non-overlapping IPv6 address pool among
the defaults to assign to the network
To resolve this, either remove the --ipv6
flag (to preserve the same behavior
as in Docker 1.12.3 and earlier), or provide an IPv6 subnet as the value of the
--subnet
flag.
The network network creation will instead succeed if you use an external IPAM driver which supports automatic allocation of IPv6 subnets.
Runtime
- Fix race on sending stdin close event #29424
Networking
- Fix panic in docker network ls when a network was created with
--ipv6
and no ipv6--subnet
in older docker versions #29416
Contrib
- Fix compilation on Darwin #29370
1.12.4 (2016-12-12)
Important
Docker 1.12 ships with an updated systemd unit file for rpm based installs (which includes RHEL, Fedora, CentOS, and Oracle Linux 7). When upgrading from an older version of Docker, the upgrade process may not automatically install the updated version of the unit file, or fail to start the
docker service
if;
- the systemd unit file (
/usr/lib/systemd/system/docker.service
) contains local changes, or- a systemd drop-in file is present, and contains
-H fd://
in theExecStart
directive
Starting the docker service
will produce an error:
Failed to start docker.service: Unit docker.socket failed to load: No such file or directory.
or
no sockets found via socket activation: make sure the service was started by systemd.
To resolve this:
- Backup the current version of the unit file, and replace the file with the version that ships with docker 1.12
- Remove the
Requires=docker.socket
directive from the/usr/lib/systemd/system/docker.service
file if present - Remove
-H fd://
from theExecStart
directive (both in the main unit file, and in any drop-in files present).
After making those changes, run sudo systemctl daemon-reload
, and sudo systemctl restart docker
to reload changes and (re)start the docker daemon.
Runtime
- Fix issue where volume metadata was not removed #29083
- Asynchronously close streams to prevent holding container lock #29050
- Fix selinux labels for newly created container volumes #29050
- Remove hostname validation #28990
- Fix deadlocks caused by IO races #29095 #29141
- Return an empty stats if the container is restarting #29150
- Fix volume store locking #29151
- Ensure consistent status code in API #29150
- Fix incorrect opaque directory permission in overlay2 #29093
- Detect plugin content and error out on
docker pull
#29297
Swarm Mode
- Update Swarmkit
#29047
- orchestrator/global: Fix deadlock on updates docker/swarmkit#1760
- on leader switchover preserve the vxlan id for existing networks docker/swarmkit#1773
- Refuse swarm spec not named ""default"" #29152
Networking
- Update libnetwork
#29004
#29146
- Fix panic in embedded DNS docker/libnetwork#1561
- Fix unmarhalling panic when passing --link-local-ip on global scope network docker/libnetwork#1564
- Fix panic when network plugin returns nil StaticRoutes docker/libnetwork#1563
- Fix panic in
osl.(*networkNamespace).DeleteNeighbor
docker/libnetwork#1555 - Fix panic in swarm networking concurrent map read/write docker/libnetwork#1570
- Allow encrypted networks when running docker inside a container docker/libnetwork#1502
- Do not block autoallocation of IPv6 pool docker/libnetwork#1538
- Set timeout for netlink calls docker/libnetwork#1557
- Increase networking local store timeout to one minute docker/libkv#140
- Fix a panic in
libnetwork.(*sandbox).execFunc
docker/libnetwork#1556 - Honor icc=false for internal networks docker/libnetwork#1525
Logging
- Update syslog log driver #29150
Contrib
- Run ""dnf upgrade"" before installing in fedora #29150
- Add build-date back to RPM packages #29150
- deb package filename changed to include distribution to distinguish between distribution code names #27829
1.12.3 (2016-10-26)
Important
Docker 1.12 ships with an updated systemd unit file for rpm based installs (which includes RHEL, Fedora, CentOS, and Oracle Linux 7). When upgrading from an older version of Docker, the upgrade process may not automatically install the updated version of the unit file, or fail to start the Docker service if;
- the systemd unit file (
/usr/lib/systemd/system/docker.service
) contains local changes, or- a systemd drop-in file is present, and contains
-H fd://
in theExecStart
directive
Starting the docker service
will produce an error:
Failed to start docker.service: Unit docker.socket failed to load: No such file or directory.
or
no sockets found via socket activation: make sure the service was started by systemd.
To resolve this:
- Backup the current version of the unit file, and replace the file with the version that ships with docker 1.12
- Remove the
Requires=docker.socket
directive from the/usr/lib/systemd/system/docker.service
file if present - Remove
-H fd://
from theExecStart
directive (both in the main unit file, and in any drop-in files present).
After making those changes, run sudo systemctl daemon-reload
, and sudo systemctl restart docker
to reload changes and (re)start the docker daemon.
Runtime
- Fix ambient capability usage in containers (CVE-2016-8867) #27610
- Prevent a deadlock in libcontainerd for Windows #27136
- Fix error reporting in CopyFileWithTar #27075
- Reset health status to starting when a container is restarted #27387
- Properly handle shared mount propagation in storage directory #27609
Swarm Mode
- Fix conversion of restart-policy #27062
- Update Swarmkit #27554
- Avoid restarting a task that has already been restarted docker/swarmkit#1305
- Allow duplicate published ports when they use different protocols docker/swarmkit#1632
- Allow multiple randomly assigned published ports on service docker/swarmkit#1657
- Fix panic when allocations happen at init time docker/swarmkit#1651
Networking
- Update libnetwork #27559
- Fix race in serializing sandbox to string docker/libnetwork#1495
- Fix race during deletion docker/libnetwork#1503
- Reset endpoint port info on connectivity revoke in bridge driver docker/libnetwork#1504
- Fix a deadlock in networking code docker/libnetwork#1507
- Fix a race in load balancer state docker/libnetwork#1512
Logging
- Update fluent-logger-golang to v1.2.1 #27474
Contrib
1.12.2 (2016-10-11)
Important
Docker 1.12 ships with an updated systemd unit file for rpm based installs (which includes RHEL, Fedora, CentOS, and Oracle Linux 7). When upgrading from an older version of Docker, the upgrade process may not automatically install the updated version of the unit file, or fail to start the
docker service
if;
- the systemd unit file (
/usr/lib/systemd/system/docker.service
) contains local changes, or- a systemd drop-in file is present, and contains
-H fd://
in theExecStart
directive
Starting the docker service
will produce an error:
Failed to start docker.service: Unit docker.socket failed to load: No such file or directory.
or
no sockets found via socket activation: make sure the service was started by systemd.
To resolve this:
- Backup the current version of the unit file, and replace the file with the version that ships with docker 1.12
- Remove the
Requires=docker.socket
directive from the/usr/lib/systemd/system/docker.service
file if present - Remove
-H fd://
from theExecStart
directive (both in the main unit file, and in any drop-in files present).
After making those changes, run sudo systemctl daemon-reload
, and sudo systemctl restart docker
to reload changes and (re)start the docker daemon.
Runtime
- Fix a panic due to a race condition filtering
docker ps
#26049
- Implement retry logic to prevent ""Unable to remove filesystem"" errors when using the aufs storage driver #26536
- Prevent devicemapper from removing device symlinks if
dm.use_deferred_removal
is enabled #24740
- Fix an issue where the CLI did not return correct exit codes if a command was run with invalid options #26777
- Fix a panic due to a bug in stdout / stderr processing in health checks #26507
- Fix exec's children handling #26874
- Fix exec form of HEALTHCHECK CMD #26208
Networking
- Fix a daemon start panic on armv5 #24315
- Vendor libnetwork #26879 #26953
- Avoid returning early on agent join failures docker/libnetwork#1473
- Fix service published port cleanup issues docker/libetwork#1432 docker/libnetwork#1433
- Recover properly from transient gossip failures docker/libnetwork#1446
- Disambiguate node names known to gossip cluster to avoid node name collision docker/libnetwork#1451
- Honor user provided listen address for gossip docker/libnetwork#1460
- Allow reachability via published port across services on the same host docker/libnetwork#1398
- Change the ingress sandbox name from random id to just
ingress_sbox
docker/libnetwork#1449
- Disable service discovery in ingress network docker/libnetwork#1489
Swarm Mode
- Fix remote detection of a node's address when it joins the cluster #26211
- Vendor SwarmKit #26765
- Bounce session after failed status update docker/swarmkit#1539
- Fix possible raft deadlocks docker/swarmkit#1537
- Fix panic and endpoint leak when a service is updated with no endpoints docker/swarmkit#1481
- Produce an error if the same port is published twice on
service create
orservice update
docker/swarmkit#1495
- Fix an issue where changes to a service were not detected, resulting in the service not being updated docker/swarmkit#1497
- Do not allow service creation on ingress network docker/swarmkit#1600
Contrib
- Update the debian sysv-init script to use
dockerd
instead ofdocker daemon
#25869 - Improve stability when running the docker client on MacOS Sierra #26875
- Fix installation on debian stretch #27184
Windows
- Fix an issue where arrow-navigation did not work when running the docker client in ConEmu #25578
1.12.1 (2016-08-18)
Important
Docker 1.12 ships with an updated systemd unit file for rpm based installs (which includes RHEL, Fedora, CentOS, and Oracle Linux 7). When upgrading from an older version of Docker, the upgrade process may not automatically install the updated version of the unit file, or fail to start the
docker service
if;
- the systemd unit file (
/usr/lib/systemd/system/docker.service
) contains local changes, or- a systemd drop-in file is present, and contains
-H fd://
in theExecStart
directive
Starting the docker service
will produce an error:
Failed to start docker.service: Unit docker.socket failed to load: No such file or directory.
or
no sockets found via socket activation: make sure the service was started by systemd.
To resolve this:
- Backup the current version of the unit file, and replace the file with the version that ships with docker 1.12
- Remove the
Requires=docker.socket
directive from the/usr/lib/systemd/system/docker.service
file if present - Remove
-H fd://
from theExecStart
directive (both in the main unit file, and in any drop-in files present).
After making those changes, run sudo systemctl daemon-reload
, and sudo systemctl restart docker
to reload changes and (re)start the docker daemon.
Client
- Add
Joined at
information innode inspect --pretty
#25512
- Fix a crash on
service inspect
#25454 - Fix issue preventing
service update --env-add
to work as intended #25427 - Fix issue preventing
service update --publish-add
to work as intended #25428 - Remove
service update --network-add
andservice update --network-rm
flags because this feature is not yet implemented in 1.12, but was inadvertently added to the client in 1.12.0 #25646
Contrib
- Add selinux policy per distribution/version, fixing issue preventing successful installation on Fedora 24, and Oracle Linux #25334 #25593
Networking
- Fix issue that prevented containers to be accessed by hostname with Docker overlay driver in Swarm Mode #25603 #25648
- Fix random network issues on service with published port #25603
- Fix unreliable inter-service communication after scaling down and up #25603
- Fix issue where removing all tasks on a node and adding them back breaks connectivity with other services #25603
- Fix issue where a task that fails to start results in a race, causing a
network xxx not found
error that masks the actual error #25550 - Relax validation of SRV records for external services that use SRV records not formatted according to RFC 2782 #25739
Plugins (experimental)
- Make daemon events listen for plugin lifecycle events #24760
- Check for plugin state before enabling plugin #25033
- Remove plugin root from filesystem on
plugin rm
#25187 - Prevent deadlock when more than one plugin is installed #25384
Runtime
- Mask join tokens in daemon logs #25346
- Fix
docker ps --filter
causing the results to no longer be sorted by creation time #25387 - Fix various crashes #25053
Security
- Add
/proc/timer_list
to the masked paths list to prevent information leak from the host #25630 - Allow systemd to run with only
--cap-add SYS_ADMIN
rather than having to also add--cap-add DAC_READ_SEARCH
or disabling seccomp filtering #25567
Swarm
- Fix an issue where the swarm can get stuck electing a new leader after quorum is lost #25055
- Fix unwanted rescheduling of containers after a leader failover #25017
- Change swarm root CA key to P256 curve swarmkit#1376
- Allow forced removal of a node from a swarm #25159
- Fix connection leak when a node leaves a swarm swarmkit/#1277
- Backdate swarm certificates by one hour to tolerate more clock skew swarmkit/#1243
- Avoid high CPU use with many unschedulable tasks swarmkit/#1287
- Fix issue with global tasks not starting up swarmkit/#1295
- Garbage collect raft logs swarmkit/#1327
Volume
- Persist local volume options after a daemon restart #25316
- Fix an issue where the mount ID was not returned on volume unmount #25333
- Fix an issue where a volume mount could inadvertently create a bind mount #25309
docker service create --mount type=bind,...
now correctly validates if the source path exists, instead of creating it #25494
1.12.0 (2016-07-28)
Important
Docker 1.12.0 ships with an updated systemd unit file for rpm based installs (which includes RHEL, Fedora, CentOS, and Oracle Linux 7). When upgrading from an older version of Docker, the upgrade process may not automatically install the updated version of the unit file, or fail to start the
docker service
if;
- the systemd unit file (
/usr/lib/systemd/system/docker.service
) contains local changes, or- a systemd drop-in file is present, and contains
-H fd://
in theExecStart
directive
Starting the docker service
will produce an error:
Failed to start docker.service: Unit docker.socket failed to load: No such file or directory.
or
no sockets found via socket activation: make sure the service was started by systemd.
To resolve this:
- Backup the current version of the unit file, and replace the file with the version that ships with docker 1.12
- Remove the
Requires=docker.socket
directive from the/usr/lib/systemd/system/docker.service
file if present - Remove
-H fd://
from theExecStart
directive (both in the main unit file, and in any drop-in files present).
After making those changes, run sudo systemctl daemon-reload
, and sudo systemctl restart docker
to reload changes and (re)start the docker daemon.
Important
With Docker 1.12, a Linux
docker
installation now has two additional binaries;dockerd
, anddocker-proxy
. If you have scripts for installingdocker
, make sure to update them accordingly.
Builder
- New
HEALTHCHECK
Dockerfile instruction to support user-defined healthchecks #23218 - New
SHELL
Dockerfile instruction to specify the default shell when using the shell form for commands in a Dockerfile #22489 - Add
#escape=
Dockerfile directive to support platform-specific parsing of file paths in Dockerfile #22268 - Add support for comments in
.dockerignore
#23111
- Support for UTF-8 in Dockerfiles #23372
- Skip UTF-8 BOM bytes from
Dockerfile
and.dockerignore
if exist #23234 - Windows: support for
ARG
to match Linux #22508
- Fix error message when building using a daemon with the bridge network disabled #22932
Contrib
- Enable seccomp for Centos 7 and Oracle Linux 7 #22344
- Remove MountFlags in systemd unit to allow shared mount propagation #22806
Distribution
- Add
--max-concurrent-downloads
and--max-concurrent-uploads
daemon flags useful for situations where network connections don't support multiple downloads/uploads #22445
- Registry operations now honor the
ALL_PROXY
environment variable #22316 - Provide more information to the user on
docker load
#23377 - Always save registry digest metadata about images pushed and pulled #23996
Logging
- Syslog logging driver now supports DGRAM sockets #21613
- Add
--details
option todocker logs
to also display log tags #21889 - Enable syslog logger to have access to env and labels #21724
- An additional syslog-format option
rfc5424micro
to allow microsecond resolution in syslog timestamp #21844
- Inherit the daemon log options when creating containers #21153
- Remove
docker/
prefix from log messages tag and replace it with{{.DaemonName}}
so that users have the option of changing the prefix #22384
Networking
- Built-in Virtual-IP based internal and ingress load-balancing using IPVS #23361
- Routing Mesh using ingress overlay network #23361
- Secured multi-host overlay networking using encrypted control-plane and Data-plane #23361
- MacVlan driver is out of experimental #23524
- Add
driver
filter tonetwork ls
#22319 - Adding
network
filter todocker ps --filter
#23300 - Add
--link-local-ip
flag tocreate
,run
andnetwork connect
to specify a container's link-local address #23415 - Add network label filter support #21495
- Removed dependency on external KV-Store for Overlay networking in Swarm-Mode #23361
- Add container's short-id as default network alias #21901
run
options--dns
and--net=host
are no longer mutually exclusive #22408
- Fix DNS issue when renaming containers with generated names #22716
- Allow both
network inspect -f {{.Id}}
andnetwork inspect -f {{.ID}}
to address inconsistency with inspect output #23226
Plugins (experimental)
- New
plugin
command to manager plugins withinstall
,enable
,disable
,rm
,inspect
,set
subcommands #23446
Remote API (v1.24) & Client
- Split the binary into two:
docker
(client) anddockerd
(daemon) #20639 - Add
before
andsince
filters todocker images --filter
#22908 - Add
--limit
option todocker search
#23107 - Add
--filter
option todocker search
#22369 - Add security options to
docker info
output #21172 #23520 - Add insecure registries to
docker info
output #20410 - Extend Docker authorization with TLS user information #21556
- devicemapper: expose Minimum Thin Pool Free Space through
docker info
#21945
- API now returns a JSON object when an error occurs making it more consistent #22880
- Prevent
docker run -i --restart
from hanging on exit #22777 - Fix API/CLI discrepancy on hostname validation #21641
- Fix discrepancy in the format of sizes in
stats
from HumanSize to BytesSize #21773 - authz: when request is denied return forbidden exit code (403) #22448
- Windows: fix tty-related displaying issues #23878
Runtime
- Split the userland proxy to a separate binary (
docker-proxy
) #23312 - Add
--live-restore
daemon flag to keep containers running when daemon shuts down, and regain control on startup #23213 - Ability to add OCI-compatible runtimes (via
--add-runtime
daemon flag) and select one with--runtime
oncreate
andrun
#22983 - New
overlay2
graphdriver for Linux 4.0+ with multiple lower directory support #22126 - New load/save image events #22137
- Add support for reloading daemon configuration through systemd #22446
- Add disk quota support for btrfs #19651
- Add disk quota support for zfs #21946
- Add support for
docker run --pid=container:<id>
#22481 - Align default seccomp profile with selected capabilities #22554
- Add a
daemon reload
event when the daemon reloads its configuration #22590 - Add
trace
capability in the pprof profiler to show execution traces in binary form #22715 - Add a
detach
event #22898 - Add support for setting sysctls with
--sysctl
#19265 - Add
--storage-opt
flag tocreate
andrun
allowing to setsize
on devicemapper #19367 - Add
--oom-score-adjust
daemon flag with a default value of-500
making the daemon less likely to be killed before containers #24516
- Undeprecate the
-c
short alias of--cpu-shares
onrun
,build
,create
,update
#22621 - Prevent from using aufs and overlay graphdrivers on an eCryptfs mount #23121
- Fix issues with tmpfs mount ordering #22329
- Created containers are no longer listed on
docker ps -a -f exited=0
#21947 - Fix an issue where containers are stuck in a ""Removal In Progress"" state #22423
- Fix bug that was returning an HTTP 500 instead of a 400 when not specifying a command on run/create #22762
- Fix bug with
--detach-keys
whereby input matching a prefix of the detach key was not preserved #22943 - SELinux labeling is now disabled when using
--privileged
mode #22993 - If volume-mounted into a container,
/etc/hosts
,/etc/resolv.conf
,/etc/hostname
are no longer SELinux-relabeled #22993 - Fix inconsistency in
--tmpfs
behavior regarding mount options #22438 - Fix an issue where daemon hangs at startup #23148
- Ignore SIGPIPE events to prevent journald restarts to crash docker in some cases #22460
- Containers are not removed from stats list on error #20835
- Fix
on-failure
restart policy when daemon restarts #20853 - Fix an issue with
stats
when a container is using another container's network #21904
Swarm Mode
- New
swarm
command to manage swarms withinit
,join
,join-token
,leave
,update
subcommands #23361 #24823 - New
service
command to manage swarm-wide services withcreate
,inspect
,update
,rm
,ps
subcommands #23361 #25140 - New
node
command to manage nodes withaccept
,promote
,demote
,inspect
,update
,ps
,ls
andrm
subcommands #23361 #25140 - (experimental) New
stack
anddeploy
commands to manage and deploy multi-service applications #23522 #25140
Volume
- Add support for local and global volume scopes (analogous to network scopes) #22077
- Allow volume drivers to provide a
Status
field #21006 - Add name/driver filter support for volume #21361
- Mount/Unmount operations now receives an opaque ID to allow volume drivers to differentiate between two callers #21015
- Fix issue preventing to remove a volume in a corner case #22103
- Windows: Enable auto-creation of host-path to match Linux #22094
Deprecation
- Environment variables
DOCKER_CONTENT_TRUST_OFFLINE_PASSPHRASE
andDOCKER_CONTENT_TRUST_TAGGING_PASSPHRASE
have been renamed toDOCKER_CONTENT_TRUST_ROOT_PASSPHRASE
andDOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE
respectively #22574 - Remove deprecated
syslog-tag
,gelf-tag
,fluentd-tag
log option in favor of the more generictag
one #22620 - Remove deprecated feature of passing HostConfig at API container start #22570
- Remove deprecated
-f
/--force
flag on docker tag #23090 - Remove deprecated
/containers/<id|name>/copy
endpoint #22149 - Remove deprecated
docker ps
flags--since
and--before
#22138 - Deprecate the old 3-args form of
docker import
#23273
1.11.2 (2016-05-31)
Networking
- Fix a stale endpoint issue on overlay networks during ungraceful restart ( #23015)
- Fix an issue where the wrong port could be reported by
docker inspect/ps/port
( #22997)
Runtime
- Fix a potential panic when running
docker build
( #23032) - Fix interpretation of
--user
parameter ( #22998) - Fix a bug preventing container statistics to be correctly reported ( #22955)
- Fix an issue preventing container to be restarted after daemon restart ( #22947)
- Fix issues when running 32 bit binaries on Ubuntu 16.04 ( #22922)
- Fix a possible deadlock on image deletion and container attach ( #22918)
- Fix an issue where containers fail to start after a daemon restart if they depend on a containerized cluster store ( #22561)
- Fix an issue causing
docker ps
to hang on CentOS when using devicemapper ( #22168, #23067) - Fix a bug preventing to
docker exec
into a container when using devicemapper ( #22168, #23067)
1.11.1 (2016-04-26)
Distribution
- Fix schema2 manifest media type to be of type
application/vnd.docker.container.image.v1+json
( #21949)
Documentation
- Add missing API documentation for changes introduced with 1.11.0 ( #22048)
Builder
- Append label passed to
docker build
as arguments as an implicitLABEL
command at the end of the processedDockerfile
( #22184)
Networking
- Fix a panic that would occur when forwarding DNS query ( #22261)
- Fix an issue where OS threads could end up within an incorrect network namespace when using user defined networks ( #22261)
Runtime
- Fix a bug preventing labels configuration to be reloaded via the config file ( #22299)
- Fix a regression where container mounting
/var/run
would prevent other containers from being removed ( #22256) - Fix an issue where it would be impossible to update both
memory-swap
andmemory
value together ( #22255) - Fix a regression from 1.11.0 where the
/auth
endpoint would not initializeserveraddress
if it is not provided ( #22254) - Add missing cleanup of container temporary files when cancelling a schedule restart ( #22237)
- Remove scary error message when no restart policy is specified ( #21993)
- Fix a panic that would occur when the plugins were activated via the json spec ( #22191)
- Fix restart backoff logic to correctly reset delay if container ran for at least 10secs ( #22125)
- Remove error message when a container restart get cancelled ( #22123)
- Fix an issue where
docker
would not correctly clean up afterdocker exec
( #22121) - Fix a panic that could occur when serving concurrent
docker stats
commands ( #22120)` - Revert deprecation of non-existent host directories auto-creation ( #22065)
- Hide misleading rpc error on daemon shutdown ( #22058)
1.11.0 (2016-04-13)
Important
With Docker 1.11, a Linux Docker installation is now made of 4 binaries (
docker
,docker-containerd
,docker-containerd-shim
anddocker-runc
). If you have scripts relying ondocker
being a single static binaries, make sure to update them. Interaction with the daemon stay the same otherwise, the usage of the other binaries should be transparent. A Windows Docker installation remains a single binary,docker.exe
.
Builder
- Fix a bug where Docker would not use the correct uid/gid when processing the
WORKDIR
command ( #21033) - Fix a bug where copy operations with userns would not use the proper uid/gid ( #20782, #21162)
Client
- Usage of the
:
separator for security option has been deprecated.=
should be used instead ( #21232)
- The client user agent is now passed to the registry on
pull
,build
,push
,login
andsearch
operations ( #21306, #21373)
- Allow setting the Domainname and Hostname separately through the API ( #20200)
- Docker info will now warn users if it can not detect the kernel version or the operating system ( #21128)
- Fix an issue where
docker stats --no-stream
output could be all 0s ( #20803) - Fix a bug where some newly started container would not appear in a running
docker stats
command ( #20792)
- Post processing is no longer enabled for linux-cgo terminals ( #20587)
- Docker learned how to use a SOCKS proxy ( #20366, #18373)
- Docker now supports external credential stores ( #20107)
docker ps
now supports displaying the list of volumes mounted inside a container ( #20017)docker info
now also reports Docker's root directory location ( #19986)
- Docker now prohibits login in with an empty username (spaces are trimmed) ( #19806)
- Docker events attributes are now sorted by key ( #19761)
docker ps
no longer shows exported port for stopped containers ( #19483)
- Docker now cleans after itself if a save/export command fails ( #17849)
Distribution
- Fix a panic that occurred when pulling an image with 0 layers ( #21222)
- Fix a panic that could occur on error while pushing to a registry with a misconfigured token service ( #21212)
- All first-level delegation roles are now signed when doing a trusted push ( #21046)
- OAuth support for registries was added ( #20970)
docker login
now handles token using the implementation found in docker/distribution ( #20832)docker login
will no longer prompt for an email ( #20565)- Docker will now fallback to registry V1 if no basic auth credentials are available ( #20241)
- Docker will now try to resume layer download where it left off after a network error/timeout ( #19840)
- Fix generated manifest mediaType when pushing cross-repository ( #19509)
- Fix docker requesting additional push credentials when pulling an image if Content Trust is enabled ( #20382)
Logging
- Fix a race in the journald log driver ( #21311)
- Docker syslog driver now uses the RFC-5424 format when emitting logs ( #20121)
- Docker GELF log driver now allows to specify the compression algorithm and level via the
gelf-compression-type
andgelf-compression-level
options ( #19831) - Docker daemon learned to output uncolorized logs via the
--raw-logs
options ( #19794)
- Docker, on Windows platform, now includes an ETW (Event Tracing in Windows) logging driver named
etwlogs
( #19689)
- Journald log driver learned how to handle tags ( #19564)
- The fluentd log driver learned the following options:
fluentd-address
,fluentd-buffer-limit
,fluentd-retry-wait
,fluentd-max-retries
andfluentd-async-connect
( #19439) - Docker learned to send log to Google Cloud via the new
gcplogs
logging driver. ( #18766)
Misc
- When saving linked images together with
docker save
a subsequentdocker load
will correctly restore their parent/child relationship ( #21385) - Support for building the Docker cli for OpenBSD was added ( #21325)
- Labels can now be applied at network, volume and image creation ( #21270)
- The
dockremap
is now created as a system user ( #21266)
- Fix a few response body leaks ( #21258)
- Docker, when run as a service with systemd, will now properly manage its processes cgroups ( #20633)
docker info
now reports the value of cgroup KernelMemory or emits a warning if it is not supported ( #20863)docker info
now also reports the cgroup driver in use ( #20388)- Docker completion is now available on PowerShell ( #19894)
dockerinit
is no more ( #19490, #19851)
- Support for building Docker on arm64 was added ( #19013)
- Experimental support for building docker.exe in a native Windows Docker installation ( #18348)
Networking
- Fix panic if a node is forcibly removed from the cluster ( #21671)
- Fix ""error creating vxlan interface"" when starting a container in a Swarm cluster ( #21671)
docker network inspect
will now report all endpoints whether they have an active container or not ( #21160)
- Experimental support for the MacVlan and IPVlan network drivers has been added ( #21122)
- Output of
docker network ls
is now sorted by network name ( #20383)
- Fix a bug where Docker would allow a network to be created with the reserved
default
name ( #19431)
docker network inspect
returns whether a network is internal or not ( #19357)
- Control IPv6 via explicit option when creating a network (
docker network create --ipv6
). This shows up as a newEnableIPv6
field indocker network inspect
( #17513)
- Support for AAAA Records (aka IPv6 Service Discovery) in embedded DNS Server ( #21396)
- Fix to not forward docker domain IPv6 queries to external servers ( #21396)
- Multiple A/AAAA records from embedded DNS Server for DNS Round robin ( #21019)
- Fix endpoint count inconsistency after an ungraceful dameon restart ( #21261)
- Move the ownership of exposed ports and port-mapping options from Endpoint to Sandbox ( #21019)
- Fixed a bug which prevents docker reload when host is configured with ipv6.disable=1 ( #21019)
- Added inbuilt nil IPAM driver ( #21019)
- Fixed bug in iptables.Exists() logic #21019
- Fixed a Veth interface leak when using overlay network ( #21019)
- Fixed a bug which prevents docker reload after a network delete during shutdown ( #20214)
- Make sure iptables chains are recreated on firewalld reload ( #20419)
- Allow to pass global datastore during config reload ( #20419)
- For anonymous containers use the alias name for IP to name mapping, ie:DNS PTR record ( #21019)
- Fix a panic when deleting an entry from /etc/hosts file ( #21019)
- Source the forwarded DNS queries from the container net namespace ( #21019)
- Fix to retain the network internal mode config for bridge networks on daemon reload ([#21780] ( https://github.com/docker/docker/pull/21780))
- Fix to retain IPAM driver option configs on daemon reload ([#21914] ( https://github.com/docker/docker/pull/21914))
Plugins
- Fix a file descriptor leak that would occur every time plugins were enumerated ( #20686)
- Fix an issue where Authz plugin would corrupt the payload body when faced with a large amount of data ( #20602)
Runtime
- Fix a panic that could occur when cleanup after a container started with invalid parameters ( #21716)
- Fix a race with event timers stopping early ( #21692)
- Fix race conditions in the layer store, potentially corrupting the map and crashing the process ( #21677)
- Un-deprecate auto-creation of host directories for mounts. This feature was marked deprecated in ( #21666) Docker 1.9, but was decided to be too much of a backward-incompatible change, so it was decided to keep the feature.
- It is now possible for containers to share the NET and IPC namespaces when
userns
is enabled ( #21383) docker inspect <image-id>
will now expose the rootfs layers ( #21370)- Docker Windows gained a minimal
top
implementation ( #21354)
- Docker learned to report the faulty exe when a container cannot be started due to its condition ( #21345)
- Docker with device mapper will now refuse to run if
udev sync
is not available ( #21097)
- Fix a bug where Docker would not validate the config file upon configuration reload ( #21089)
- Fix a hang that would happen on attach if initial start was to fail ( #21048)
- Fix an issue where registry service options in the daemon configuration file were not properly taken into account ( #21045)
- Fix a race between the exec and resize operations ( #21022)
- Fix an issue where nanoseconds were not correctly taken in account when filtering Docker events ( #21013)
- Fix the handling of Docker command when passed a 64 bytes id ( #21002)
- Docker will now return a
204
(i.e http.StatusNoContent) code when it successfully deleted a network ( #20977)
- Fix a bug where the daemon would wait indefinitely in case the process it was about to killed had already exited on its own ( #20967
- The devmapper driver learned the
dm.min_free_space
option. If the mapped device free space reaches the passed value, new device creation will be prohibited. ( #20786)
- Docker can now prevent processes in container to gain new privileges via the
--security-opt=no-new-privileges
flag ( #20727)
- Starting a container with the
--device
option will now correctly resolves symlinks ( #20684)
- Docker now relies on
containerd
andrunc
to spawn containers. ( #20662)
- Fix docker configuration reloading to only alter value present in the given config file ( #20604)
- Docker now allows setting a container hostname via the
--hostname
flag when--net=host
( #20177) - Docker now allows executing privileged container while running with
--userns-remap
if both--privileged
and the new--userns=host
flag are specified ( #20111)
- Fix Docker not cleaning up correctly old containers upon restarting after a crash ( #19679)
- Docker will now error out if it doesn't recognize a configuration key within the config file ( #19517)
- Fix container loading, on daemon startup, when they depends on a plugin running within a container ( #19500)
docker update
learned how to change a container restart policy ( #19116)docker inspect
now also returns a newState
field containing the container state in a human readable way (i.e. one ofcreated
,restarting
,running
,paused
,exited
ordead
)( #18966)
- Docker learned to limit the number of active pids (i.e. processes) within the container via the
pids-limit
flags. NOTE: This requiresCGROUP_PIDS=y
to be in the kernel configuration. ( #18697)
docker load
now has a--quiet
option to suppress the load output ( #20078)- Fix a bug in neighbor discovery for IPv6 peers ( #20842)
- Fix a panic during cleanup if a container was started with invalid options ( #21802)
- Fix a situation where a container cannot be stopped if the terminal is closed ( #21840)
Security
- Object with the
pcp_pmcd_t
selinux type were given management access to/var/lib/docker(/.*)?
( #21370) restart_syscall
,copy_file_range
,mlock2
joined the list of allowed calls in the default seccomp profile ( #21117, #21262)send
,recv
andx32
were added to the list of allowed syscalls and arch in the default seccomp profile ( #19432)- Docker Content Trust now requests the server to perform snapshot signing ( #21046)
- Support for using YubiKeys for Content Trust signing has been moved out of experimental ( #21591)
Volumes
- Output of
docker volume ls
is now sorted by volume name ( #20389) - Local volumes can now accept options similar to the unix
mount
tool ( #20262)
- Fix an issue where one letter directory name could not be used as source for volumes ( #21106)
docker run -v
now accepts a new flagnocopy
. This tells the runtime not to copy the container path content into the volume (which is the default behavior) ( #21223)
1.10.3 (2016-03-10)
Runtime
- Fix Docker client exiting with an ""Unrecognized input header"" error #20706
- Fix Docker exiting if Exec is started with both
AttachStdin
andDetach
#20647
Distribution
- Fix a crash when pushing multiple images sharing the same layers to the same repository in parallel #20831
- Fix a panic when pushing images to a registry which uses a misconfigured token service #21030
Plugin system
- Fix issue preventing volume plugins to start when SELinux is enabled #20834
- Prevent Docker from exiting if a volume plugin returns a null response for Get requests #20682
- Fix plugin system leaking file descriptors if a plugin has an error #20680
Security
- Fix linux32 emulation to fail during docker build
#20672
It was due to the
personality
syscall being blocked by the default seccomp profile. - Fix Oracle XE 10g failing to start in a container
#20981
It was due to the
ipc
syscall being blocked by the default seccomp profile. - Fix user namespaces not working on Linux From Scratch #20685
- Fix issue preventing daemon to start if userns is enabled and the
subuid
orsubgid
files contain comments #20725
1.10.2 (2016-02-22)
Runtime
- Prevent systemd from deleting containers' cgroups when its configuration is reloaded #20518
- Fix SELinux issues by disregarding
--read-only
when mounting/dev/mqueue
#20333 - Fix chown permissions used during
docker cp
when userns is used #20446 - Fix configuration loading issue with all booleans defaulting to
true
#20471 - Fix occasional panic with
docker logs -f
#20522
Distribution
- Keep layer reference if deletion failed to avoid a badly inconsistent state #20513
- Handle gracefully a corner case when canceling migration #20372
- Fix docker import on compressed data #20367
- Fix tar-split files corruption during migration that later cause docker push and docker save to fail #20458
Networking
- Fix daemon crash if embedded DNS is sent garbage #20510
Volumes
- Fix issue with multiple volume references with same name #20381
Security
- Fix potential cache corruption and delegation conflict issues #20523
1.10.1 (2016-02-11)
Runtime
- Do not stop daemon on migration hard failure #20156
- Fix various issues with migration to content-addressable images #20058
- Fix ZFS permission bug with user namespaces #20045
- Do not leak /dev/mqueue from the host to all containers, keep it container-specific #19876 #20133
- Fix
docker ps --filter before=...
to not show stopped containers without providing-a
flag #20135
Security
- Fix issue preventing docker events to work properly with authorization plugin #20002
Distribution
- Add additional verifications and prevent from uploading invalid data to registries #20164
- Fix regression preventing uppercase characters in image reference hostname #20175
Networking
- Fix embedded DNS for user-defined networks in the presence of firewalld #20060
- Fix issue where removing a network during shutdown left Docker inoperable #20181 #20235
- Embedded DNS is now able to return compressed results #20181
- Fix port-mapping issue with
userland-proxy=false
#20181
Logging
- Fix bug where tcp+tls protocol would be rejected #20109
Volumes
- Fix issue whereby older volume drivers would not receive volume options #19983
Misc
- Remove TasksMax from Docker systemd service #20167
1.10.0 (2016-02-04)
Important
Docker 1.10 uses a new content-addressable storage for images and layers.
A migration is performed the first time docker
is run, and can take a significant amount of time depending on the number of images present. Refer to this page on the wiki for more information:
https://github.com/docker/docker/wiki/Engine-v1.10.0-content-addressability-migration
We also released a cool migration utility that enables you to perform the migration before updating to reduce downtime.
Engine 1.10 migrator can be found on Docker Hub:
https://hub.docker.com/r/docker/v1.10-migrator/
Runtime
- New
docker update
command that allows updating resource constraints on running containers #15078 - Add
--tmpfs
flag todocker run
to create a tmpfs mount in a container #13587 - Add
--format
flag todocker images
command #17692 - Allow to set daemon configuration in a file and hot-reload it with the
SIGHUP
signal #18587 - Updated docker events to include more meta-data and event types #18888 This change is backward compatible in the API, but not on the CLI.
- Add
--blkio-weight-device
flag todocker run
#13959 - Add
--device-read-bps
and--device-write-bps
flags todocker run
#14466 - Add
--device-read-iops
and--device-write-iops
flags todocker run
#15879 - Add
--oom-score-adj
flag todocker run
#16277 - Add
--detach-keys
flag toattach
,run
,start
andexec
commands to override the default key sequence that detaches from a container #15666 - Add
--shm-size
flag torun
,create
andbuild
to set the size of/dev/shm
#16168 - Show the number of running, stopped, and paused containers in
docker info
#19249 - Show the
OSType
andArchitecture
indocker info
#17478 - Add
--cgroup-parent
flag ondaemon
to set cgroup parent for all containers #19062 - Add
-L
flag to docker cp to follow symlinks #16613 - New
status=dead
filter fordocker ps
#17908
- Change
docker run
exit codes to distinguish between runtime and application errors #14012 - Enhance
docker events --since
and--until
to support nanoseconds and timezones #17495 - Add
--all
/-a
flag tostats
to include both running and stopped containers #16742 - Change the default cgroup-driver to
cgroupfs
#17704 - Emit a ""tag"" event when tagging an image with
build -t
#17115 - Best effort for linked containers' start order when starting the daemon #18208
- Add ability to add multiple tags on
build
#15780 - Permit
OPTIONS
request against any url, thus fixing issue with CORS #19569
- Fix the
--quiet
flag ondocker build
to actually be quiet #17428 - Fix
docker images --filter dangling=false
to now show all non-dangling images #19326 - Fix race condition causing autorestart turning off on restart #17629
- Recognize GPFS filesystems #19216
- Fix obscure bug preventing to start containers #19751
- Forbid
exec
during container restart #19722 - devicemapper: Increasing
--storage-opt dm.basesize
will now increase the base device size on daemon restart #19123
Security
- Add
--userns-remap
flag todaemon
to support user namespaces (previously in experimental) #19187 - Add support for custom seccomp profiles in
--security-opt
#17989 - Add default seccomp profile #18780
- Add
--authorization-plugin
flag todaemon
to customize ACLs #15365 - Docker Content Trust now supports the ability to read and write user delegations #18887 This is an optional, opt-in feature that requires the explicit use of the Notary command-line utility in order to be enabled. Enabling delegation support in a specific repository will break the ability of Docker 1.9 and 1.8 to pull from that repository, if content trust is enabled.
- Allow SELinux to run in a container when using the BTRFS storage driver #16452
Distribution
- Use content-addressable storage for images and layers
#17924
A migration is performed the first time docker is run; it can take a significant amount of time depending on the number of images and containers present.
Images no longer depend on the parent chain but contain a list of layer references.
docker load
/docker save
tarballs now also contain content-addressable image configurations. For more information: https://github.com/docker/docker/wiki/Engine-v1.10.0-content-addressability-migration - Add support for the new manifest format (""schema2"") #18785
- Lots of improvements for push and pull: performance++, retries on failed downloads, cancelling on client disconnect #18353, #18418, #19109, #18353
- Limit v1 protocol fallbacks #18590
- Fix issue where docker could hang indefinitely waiting for a nonexistent process to pull an image #19743
Networking
- Use DNS-based discovery instead of
/etc/hosts
#19198 - Support for network-scoped alias using
--net-alias
onrun
and--alias
onnetwork connect
#19242 - Add
--ip
and--ip6
onrun
andnetwork connect
to support custom IP addresses for a container in a network #19001 - Add
--ipam-opt
tonetwork create
for passing custom IPAM options #17316 - Add
--internal
flag tonetwork create
to restrict external access to and from the network #19276 - Add
kv.path
option to--cluster-store-opt
#19167 - Add
discovery.heartbeat
anddiscovery.ttl
options to--cluster-store-opt
to configure discovery TTL and heartbeat timer #18204 - Add
--format
flag tonetwork inspect
#17481 - Add
--link
tonetwork connect
to provide a container-local alias #19229 - Support for Capability exchange with remote IPAM plugins #18775
- Add
--force
tonetwork disconnect
to force container to be disconnected from network #19317
- Support for multi-host networking using built-in overlay driver for all engine supported kernels: 3.10+ #18775
--link
is now supported ondocker run
for containers in user-defined network #19229- Enhance
docker network rm
to allow removing multiple networks #17489 - Include container names in
network inspect
#17615 - Include auto-generated subnets for user-defined networks in
network inspect
#17316 - Add
--filter
flag tonetwork ls
to hide predefined networks #17782 - Add support for network connect/disconnect to stopped containers #18906
- Add network ID to container inspect #19323
- Fix MTU issue where Docker would not start with two or more default routes #18108
- Fix duplicate IP address for containers #18106
- Fix issue preventing sometimes docker from creating the bridge network #19338
- Do not substitute 127.0.0.1 name server when using
--net=host
#19573
Logging
- Enhance
docker logs --since
and--until
to support nanoseconds and time #17495 - Enhance AWS logs to auto-detect region #16640
Volumes
- Add support to set the mount propagation mode for a volume #17034
- Add
ls
andinspect
endpoints to volume plugin API #16534 Existing plugins need to make use of these new APIs to satisfy users' expectation For that, use the new MIME typeapplication/vnd.docker.plugins.v1.2+json
#19549
- Fix data not being copied to named volumes #19175
- Fix issues preventing volume drivers from being containerized #19500
- Fix
docker volumes ls --dangling=false
to now show all non-dangling volumes #19671 - Do not remove named volumes on container removal #19568
- Allow external volume drivers to host anonymous volumes #19190
Builder
- Add support for
**
in.dockerignore
to wildcard multiple levels of directories #17090
- Fix handling of UTF-8 characters in Dockerfiles #17055
- Fix permissions problem when reading from STDIN #19283
Client
- Add support for overriding the API version to use via an
DOCKER_API_VERSION
environment-variable #15964
- Fix a bug preventing Windows clients to log in to Docker Hub #19891
Misc
- systemd: Set TasksMax in addition to LimitNPROC in systemd service file #19391
Deprecations
- Remove LXC support. The LXC driver was deprecated in Docker 1.8, and has now been removed #17700
- Remove
--exec-driver
daemon flag, because it is no longer in use #17700 - Remove old deprecated single-dashed long CLI flags (such as
-rm
; use--rm
instead) #17724 - Deprecate HostConfig at API container start #17799
- Deprecate docker packages for newly EOL'd Linux distributions: Fedora 21 and Ubuntu 15.04 (Vivid) #18794, #18809
- Deprecate
-f
flag for docker tag #18350
1.9.1 (2015-11-21)
Runtime
- Do not prevent daemon from booting if images could not be restored (#17695)
- Force IPC mount to unmount on daemon shutdown/init (#17539)
- Turn IPC unmount errors into warnings (#17554)
- Fix
docker stats
performance regression (#17638) - Clarify cryptic error message upon
docker logs
if--log-driver=none
(#17767) - Fix seldom panics (#17639, #17634, #17703)
- Fix opq whiteouts problems for files with dot prefix (#17819)
- devicemapper: try defaulting to xfs instead of ext4 for performance reasons (#17903, #17918)
- devicemapper: fix displayed fs in docker info (#17974)
- selinux: only relabel if user requested so with the
z
option (#17450, #17834) - Do not make network calls when normalizing names (#18014)
Client
- Fix
docker login
on windows (#17738) - Fix bug with
docker inspect
output when not connected to daemon (#17715) - Fix
docker inspect -f {{.HostConfig.Dns}} somecontainer
(#17680)
Builder
- Fix regression with symlink behavior in ADD/COPY (#17710)
Networking
- Allow passing a network ID as an argument for
--net
(#17558) - Fix connect to host and prevent disconnect from host for
host
network (#17476) - Fix
--fixed-cidr
issue when gateway ip falls in ip-range and ip-range is not the first block in the network (#17853) - Restore deterministic
IPv6
generation fromMAC
address on defaultbridge
network (#17890) - Allow port-mapping only for endpoints created on docker run (#17858)
- Fixed an endpoint delete issue with a possible stale sbox (#18102)
Distribution
- Correct parent chain in v2 push when v1Compatibility files on the disk are inconsistent (#18047)
1.9.0 (2015-11-03)
Runtime
docker stats
now returns block IO metrics (#15005)docker stats
now details network stats per interface (#15786)- Add
ancestor=<image>
filter todocker ps --filter
flag to filter containers based on their ancestor images (#14570) - Add
label=<somelabel>
filter todocker ps --filter
to filter containers based on label (#16530) - Add
--kernel-memory
flag todocker run
(#14006) - Add
--message
flag todocker import
allowing to specify an optional message (#15711) - Add
--privileged
flag todocker exec
(#14113) - Add
--stop-signal
flag todocker run
allowing to replace the container process stopping signal (#15307) - Add a new
unless-stopped
restart policy (#15348) - Inspecting an image now returns tags (#13185)
- Add container size information to
docker inspect
(#15796) - Add
RepoTags
andRepoDigests
field to/images/{name:.*}/json
(#17275)
- Remove the deprecated
/container/ps
endpoint from the API (#15972) - Send and document correct HTTP codes for
/exec/<name>/start
(#16250) - Share shm and mqueue between containers sharing IPC namespace (#15862)
- Event stream now shows OOM status when
--oom-kill-disable
is set (#16235) - Ensure special network files (/etc/hosts etc.) are read-only if bind-mounted
with
ro
option (#14965) - Improve
rmi
performance (#16890) - Do not update /etc/hosts for the default bridge network, except for links (#17325)
- Fix conflict with duplicate container names (#17389)
- Fix an issue with incorrect template execution in
docker inspect
(#17284) - DEPRECATE
-c
short flag variant for--cpu-shares
in docker run (#16271)
Client
- Allow
docker import
to import from local files (#11907)
Builder
- Add a
STOPSIGNAL
Dockerfile instruction allowing to set a different stop-signal for the container process (#15307) - Add an
ARG
Dockerfile instruction and a--build-arg
flag todocker build
that allows to add build-time environment variables (#15182)
- Improve cache miss performance (#16890)
Storage
- devicemapper: Implement deferred deletion capability (#16381)
Networking
docker network
exits experimental and is part of standard release (#16645)- New network top-level concept, with associated subcommands and API (#16645) WARNING: the API is different from the experimental API
- Support for multiple isolated/micro-segmented networks (#16645)
- Built-in multihost networking using VXLAN based overlay driver (#14071)
- Support for third-party network plugins (#13424)
- Ability to dynamically connect containers to multiple networks (#16645)
- Support for user-defined IP address management via pluggable IPAM drivers (#16910)
- Add daemon flags
--cluster-store
and--cluster-advertise
for built-in nodes discovery (#16229) - Add
--cluster-store-opt
for setting up TLS settings (#16644) - Add
--dns-opt
to the daemon (#16031)
- DEPRECATE following container
NetworkSettings
fields in API v1.21:EndpointID
,Gateway
,GlobalIPv6Address
,GlobalIPv6PrefixLen
,IPAddress
,IPPrefixLen
,IPv6Gateway
andMacAddress
. Those are now specific to thebridge
network. UseNetworkSettings.Networks
to inspect the networking settings of a container per network.
Volumes
- New top-level
volume
subcommand and API (#14242)
- Move API volume driver settings to host-specific config (#15798)
- Print an error message if volume name is not unique (#16009)
- Ensure volumes created from Dockerfiles always use the local volume driver (#15507)
- DEPRECATE auto-creating missing host paths for bind mounts (#16349)
Logging
- Add
awslogs
logging driver for Amazon CloudWatch (#15495) - Add generic
tag
log option to allow customizing container/image information passed to driver (#15384)
- Implement the
docker logs
endpoint for the journald driver (#13707) - DEPRECATE driver-specific log tags (#15384)
Distribution
docker search
now works with partial names (#16509)
- Push optimization: avoid buffering to file (#15493)
- The daemon will display progress for images that were already being pulled by another client (#15489)
- Only permissions required for the current action being performed are requested (#)
- Renaming trust keys (and respective environment variables) from
offline
toroot
andtagging
torepository
(#16894)
- DEPRECATE trust key environment variables
DOCKER_CONTENT_TRUST_OFFLINE_PASSPHRASE
andDOCKER_CONTENT_TRUST_TAGGING_PASSPHRASE
(#16894)
Security
- Add SELinux profiles to the rpm package (#15832)
- Fix various issues with AppArmor profiles provided in the deb package (#14609)
- Add AppArmor policy that prevents writing to /proc (#15571)
1.8.3 (2015-10-12)
Distribution
- Fix layer IDs lead to local graph poisoning (CVE-2014-8178)
- Fix manifest validation and parsing logic errors allow pull-by-digest validation bypass (CVE-2014-8179)
- Add
--disable-legacy-registry
to prevent a daemon from using a v1 registry
1.8.2 (2015-09-10)
Distribution
- Fixes rare edge case of handling GNU LongLink and LongName entries.
- Fix ^C on docker pull.
- Fix docker pull issues on client disconnection.
- Fix issue that caused the daemon to panic when loggers weren't configured properly.
- Fix goroutine leak pulling images from registry V2.
Runtime
- Fix a bug mounting cgroups for docker daemons running inside docker containers.
- Initialize log configuration properly.
Client:
- Handle
-q
flag indocker ps
properly when there is a default format.
Networking
- Fix several corner cases with netlink.
Contrib
- Fix several issues with bash completion.
1.8.1 (2015-08-12)
Distribution
- Fix a bug where pushing multiple tags would result in invalid images
1.8.0 (2015-08-11)
Distribution
- Trusted pull, push and build, disabled by default
- Make tar layers deterministic between registries
- Don't allow deleting the image of running containers
- Check if a tag name to load is a valid digest
- Allow one character repository names
- Add a more accurate error description for invalid tag name
- Make build cache ignore mtime
Cli
- Add support for DOCKER_CONFIG/--config to specify config file dir
- Add --type flag for docker inspect command
- Add formatting options to
docker ps
with--format
- Replace
docker -d
with new subcommanddocker daemon
- Zsh completion updates and improvements
- Add some missing events to bash completion
- Support daemon urls with base paths in
docker -H
- Validate status= filter to docker ps
- Display when a container is in --net=host in docker ps
- Extend docker inspect to export image metadata related to graph driver
- Restore --default-gateway{,-v6} daemon options
- Add missing unpublished ports in docker ps
- Allow duration strings in
docker events
as --since/--until - Expose more mounts information in
docker inspect
Runtime
- Add new Fluentd logging driver
- Allow
docker import
to load from local files - Add logging driver for GELF via UDP
- Allow to copy files from host to containers with
docker cp
- Promote volume drivers from experimental to master
- Add rollover options to json-file log driver, and --log-driver-opts flag
- Add memory swappiness tuning options
- Remove cgroup read-only flag when privileged
- Make /proc, /sys, & /dev readonly for readonly containers
- Add cgroup bind mount by default
- Overlay: Export metadata for container and image in
docker inspect
- Devicemapper: external device activation
- Devicemapper: Compare uuid of base device on startup
- Remove RC4 from the list of registry cipher suites
- Add syslog-facility option
- LXC execdriver compatibility with recent LXC versions
- Mark LXC execriver as deprecated (to be removed with the migration to runc)
Plugins
- Separate plugin sockets and specs locations
- Allow TLS connections to plugins
Bug fixes
- Add missing 'Names' field to /containers/json API output
- Make
docker rmi
of dangling images safe while pulling - Devicemapper: Change default basesize to 100G
- Go Scheduler issue with sync.Mutex and gcc
- Fix issue where Search API endpoint would panic due to empty AuthConfig
- Set image canonical names correctly
- Check dockerinit only if lxc driver is used
- Fix ulimit usage of nproc
- Always attach STDIN if -i,--interactive is specified
- Show error messages when saving container state fails
- Fixed incorrect assumption on --bridge=none treated as disable network
- Check for invalid port specifications in host configuration
- Fix endpoint leave failure for --net=host mode
- Fix goroutine leak in the stats API if the container is not running
- Check for apparmor file before reading it
- Fix DOCKER_TLS_VERIFY being ignored
- Set umask to the default on startup
- Correct the message of pause and unpause a non-running container
- Adjust disallowed CpuShares in container creation
- ZFS: correctly apply selinux context
- Display empty string instead of
when IP opt is nil docker kill
returns error when container is not running- Fix COPY/ADD quoted/json form
- Fix goroutine leak on logs -f with no output
- Remove panic in nat package on invalid hostport
- Fix container linking in Fedora 22
- Fix error caused using default gateways outside of the allocated range
- Format times in inspect command with a template as RFC3339Nano
- Make registry client to accept 2xx and 3xx http status responses as successful
- Fix race issue that caused the daemon to crash with certain layer downloads failed in a specific order.
- Fix error when the docker ps format was not valid.
- Remove redundant ip forward check.
- Fix issue trying to push images to repository mirrors.
- Fix error cleaning up network entrypoints when there is an initialization issue.
1.7.1 (2015-07-14)
Runtime
- Fix default user spawning exec process with
docker exec
- Make
--bridge=none
not to configure the network bridge - Publish networking stats properly
- Fix implicit devicemapper selection with static binaries
- Fix socket connections that hung intermittently
- Fix bridge interface creation on CentOS/RHEL 6.6
- Fix local dns lookups added to resolv.conf
- Fix copy command mounting volumes
- Fix read/write privileges in volumes mounted with --volumes-from
Remote API
- Fix unmarshalling of Command and Entrypoint
- Set limit for minimum client version supported
- Validate port specification
- Return proper errors when attach/reattach fail
Distribution
- Fix pulling private images
- Fix fallback between registry V2 and V1
1.7.0 (2015-06-16)
Runtime
- Experimental feature: support for out-of-process volume plugins
- The userland proxy can be disabled in favor of hairpin NAT using the daemon’s
--userland-proxy=false
flag - The
exec
command supports the-u|--user
flag to specify the new process owner
- Default gateway for containers can be specified daemon-wide using the
--default-gateway
and--default-gateway-v6
flags - The CPU CFS (Completely Fair Scheduler) quota can be set in
docker run
using--cpu-quota
- Container block IO can be controlled in
docker run
using--blkio-weight
- ZFS support
- The
docker logs
command supports a--since
argument - UTS namespace can be shared with the host with
docker run --uts=host
Quality
- Networking stack was entirely rewritten as part of the libnetwork effort
- Engine internals refactoring
- Volumes code was entirely rewritten to support the plugins effort
- Sending SIGUSR1 to a daemon will dump all goroutines stacks without exiting
Build
- Support ${variable:-value} and ${variable:+value} syntax for environment variables
- Support resource management flags
--cgroup-parent
,--cpu-period
,--cpu-quota
,--cpuset-cpus
,--cpuset-mems
- git context changes with branches and directories
- The .dockerignore file support exclusion rules
Distribution
- Client support for v2 mirroring support for the official registry
Bugfixes
- Firewalld is now supported and will automatically be used when available
- mounting --device recursively
1.6.2 (2015-05-13)
Runtime
- Revert change prohibiting mounting into /sys
1.6.1 (2015-05-07)
Security
- Fix read/write /proc paths (CVE-2015-3630)
- Prohibit VOLUME /proc and VOLUME / (CVE-2015-3631)
- Fix opening of file-descriptor 1 (CVE-2015-3627)
- Fix symlink traversal on container respawn allowing local privilege escalation (CVE-2015-3629)
- Prohibit mount of /sys
Runtime
- Update AppArmor policy to not allow mounts
1.6.0 (2015-04-07)
Builder
- Building images from an image ID
- Build containers with resource constraints, ie
docker build --cpu-shares=100 --memory=1024m...
commit --change
to apply specified Dockerfile instructions while committing the imageimport --change
to apply specified Dockerfile instructions while importing the image- Builds no longer continue in the background when canceled with CTRL-C
Client
- Windows Support
Runtime
- Container and image Labels
--cgroup-parent
for specifying a parent cgroup to place container cgroup within- Logging drivers,
json-file
,syslog
, ornone
- Pulling images by ID
--ulimit
to set the ulimit on a container--default-ulimit
option on the daemon which applies to all created containers (and overwritten by--ulimit
on run)
1.5.0 (2015-02-10)
Builder
- Dockerfile to use for a given
docker build
can be specified with the-f
flag
- Dockerfile and .dockerignore files can be themselves excluded as part of the .dockerignore file, thus preventing modifications to these files invalidating ADD or COPY instructions cache
- ADD and COPY instructions accept relative paths
- Dockerfile
FROM scratch
instruction is now interpreted as a no-base specifier - Improve performance when exposing a large number of ports
Hack
- Allow client-side only integration tests for Windows
- Include docker-py integration tests against Docker daemon as part of our test suites
Packaging
- Support for the new version of the registry HTTP API
- Speed up
docker push
for images with a majority of already existing layers
- Fixed contacting a private registry through a proxy
Remote API
- A new endpoint will stream live container resource metrics and can be accessed with the
docker stats
command - Containers can be renamed using the new
rename
endpoint and the associateddocker rename
command
- Container
inspect
endpoint show the ID ofexec
commands running in this container - Container
inspect
endpoint show the number of times Docker auto-restarted the container - New types of event can be streamed by the
events
endpoint: ‘OOM’ (container died with out of memory), ‘exec_create’, and ‘exec_start'
- Fixed returned string fields which hold numeric characters incorrectly omitting surrounding double quotes
Runtime
- Docker daemon has full IPv6 support
- The
docker run
command can take the--pid=host
flag to use the host PID namespace, which makes it possible for example to debug host processes using containerized debugging tools - The
docker run
command can take the--read-only
flag to make the container’s root filesystem mounted as readonly, which can be used in combination with volumes to force a container’s processes to only write to locations that will be persisted - Container total memory usage can be limited for
docker run
using the--memory-swap
flag
- Major stability improvements for devicemapper storage driver
- Better integration with host system: containers will reflect changes to the host's
/etc/resolv.conf
file when restarted - Better integration with host system: per-container iptable rules are moved to the DOCKER chain
- Fixed container exiting on out of memory to return an invalid exit code
Other
- The HTTP_PROXY, HTTPS_PROXY, and NO_PROXY environment variables are properly taken into account by the client when connecting to the Docker daemon
1.4.1 (2014-12-15)
Runtime
- Fix issue with volumes-from and bind mounts not being honored after create
1.4.0 (2014-12-11)
Notable Features since 1.3.0
- Set key=value labels to the daemon (displayed in
docker info
), applied with new-label
daemon flag - Add support for
ENV
in Dockerfile of the form:ENV name=value name2=value2...
- New Overlayfs Storage Driver
docker info
now returns anID
andName
field- Filter events by event name, container, or image
docker cp
now supports copying from container volumes
- Fixed
docker tag
, so it honors--force
when overriding a tag for existing image.
1.3.3 (2014-12-11)
Security
- Fix path traversal vulnerability in processing of absolute symbolic links (CVE-2014-9356)
- Fix decompression of xz image archives, preventing privilege escalation (CVE-2014-9357)
- Validate image IDs (CVE-2014-9358)
Runtime
- Fix an issue when image archives are being read slowly
Client
- Fix a regression related to stdin redirection
- Fix a regression with
docker cp
when destination is the current directory
1.3.2 (2014-11-20)
Security
- Fix tar breakout vulnerability
- Extractions are now sandboxed chroot
- Security options are no longer committed to images
Runtime
- Fix deadlock in
docker ps -f exited=1
- Fix a bug when
--volumes-from
references a container that failed to start
Registry
--insecure-registry
now accepts CIDR notation such as 10.1.0.0/16
- Private registries whose IPs fall in the 127.0.0.0/8 range do no need the
--insecure-registry
flag
- Skip the experimental registry v2 API when mirroring is enabled
1.3.1 (2014-10-28)
Security
- Prevent fallback to SSL protocols < TLS 1.0 for client, daemon and registry
- Secure HTTPS connection to registries with certificate verification and without HTTP fallback unless
--insecure-registry
is specified
Runtime
- Fix issue where volumes would not be shared
Client
- Fix issue with
--iptables=false
not automatically setting--ip-masq=false
- Fix docker run output to non-TTY stdout
Builder
- Fix escaping
$
for environment variables - Fix issue with lowercase
onbuild
Dockerfile instruction - Restrict environment variable expansion to
ENV
,ADD
,COPY
,WORKDIR
,EXPOSE
,VOLUME
andUSER
1.3.0 (2014-10-14)
Notable features since 1.2.0
- Docker
exec
allows you to run additional processes inside existing containers - Docker
create
gives you the ability to create a container via the CLI without executing a process --security-opts
options to allow user to customize container labels and apparmor profiles- Docker
ps
filters
- Wildcard support to COPY/ADD
- Move production URLs to get.docker.com from get.docker.io
- Allocate IP address on the bridge inside a valid CIDR
- Use drone.io for PR and CI testing
- Ability to setup an official registry mirror
- Ability to save multiple images with docker
save
1.2.0 (2014-08-20)
Runtime
- Make /etc/hosts /etc/resolv.conf and /etc/hostname editable at runtime
- Auto-restart containers using policies
- Use /var/lib/docker/tmp for large temporary files
--cap-add
and--cap-drop
to tweak what linux capability you want--device
to use devices in containers
Client
docker search
on private registries- Add
exited
filter todocker ps --filter
docker rm -f
now kills instead of stop
- Support for IPv6 addresses in
--dns
flag
Proxy
- Proxy instances in separate processes
- Small bug fix on UDP proxy
1.1.2 (2014-07-23)
Runtime
- Fix port allocation for existing containers
- Fix containers restart on daemon restart
Packaging
- Fix /etc/init.d/docker issue on Debian
1.1.1 (2014-07-09)
Builder
- Fix issue with ADD
1.1.0 (2014-07-03)
Notable features since 1.0.1
- Add
.dockerignore
support - Pause containers during
docker commit
- Add
--tail
todocker logs
Builder
- Allow a tar file as context for
docker build
- Fix issue with white-spaces and multi-lines in
Dockerfiles
Runtime
- Overall performance improvements
- Allow
/
as source ofdocker run -v
- Fix port allocation
- Fix bug in
docker save
- Add links information to
docker inspect
Client
- Improve command line parsing for
docker commit
Remote API
- Improve status code for the
start
andstop
endpoints
1.0.1 (2014-06-19)
Notable features since 1.0.0
- Enhance security for the LXC driver
Builder
- Fix
ONBUILD
instruction passed to grandchildren
Runtime
- Fix events subscription
- Fix /etc/hostname file with host networking
- Allow
-h
and--net=none
- Fix issue with hotplug devices in
--privileged
Client
- Fix artifacts with events
- Fix a panic with empty flags
- Fix
docker cp
on Mac OS X
Miscellaneous
- Fix compilation on Mac OS X
- Fix several races
1.0.0 (2014-06-09)
Notable features since 0.12.0
- Production support
0.12.0 (2014-06-05)
Notable features since 0.11.0
- 40+ various improvements to stability, performance and usability
- New
COPY
Dockerfile instruction to allow copying a local file from the context into the container without ever extracting if the file is a tar file - Inherit file permissions from the host on
ADD
- New
pause
andunpause
commands to allow pausing and unpausing of containers using cgroup freezer - The
images
command has a-f
/--filter
option to filter the list of images - Add
--force-rm
to clean up after a failed build - Standardize JSON keys in Remote API to CamelCase
- Pull from a docker run now assumes
latest
tag if not specified - Enhance security on Linux capabilities and device nodes
0.11.1 (2014-05-07)
Registry
- Fix push and pull to private registry
0.11.0 (2014-05-07)
Notable features since 0.10.0
- SELinux support for mount and process labels
- Linked containers can be accessed by hostname
- Use the net
--net
flag to allow advanced network configuration such as host networking so that containers can use the host's network interfaces - Add a ping endpoint to the Remote API to do healthchecks of your docker daemon
- Logs can now be returned with an optional timestamp
- Docker now works with registries that support SHA-512
- Multiple registry endpoints are supported to allow registry mirrors
0.10.0 (2014-04-08)
Builder
- Fix printing multiple messages on a single line. Fixes broken output during builds.
- Follow symlinks inside container's root for ADD build instructions.
- Fix EXPOSE caching.
Documentation
- Add the new options of
docker ps
to the documentation. - Add the options of
docker restart
to the documentation. - Update daemon docs and help messages for --iptables and --ip-forward.
- Updated apt-cacher-ng docs example.
- Remove duplicate description of --mtu from docs.
- Add missing -t and -v for
docker images
to the docs. - Add fixes to the cli docs.
- Update libcontainer docs.
- Update images in docs to remove references to AUFS and LXC.
- Update the nodejs_web_app in the docs to use the new epel RPM address.
- Fix external link on security of containers.
- Update remote API docs.
- Add image size to history docs.
- Be explicit about binding to all interfaces in redis example.
- Document DisableNetwork flag in the 1.10 remote api.
- Document that
--lxc-conf
is lxc only. - Add chef usage documentation.
- Add example for an image with multiple for
docker load
. - Explain what
docker run -a
does in the docs.
Contrib
- Add variable for DOCKER_LOGFILE to sysvinit and use append instead of overwrite in opening the logfile.
- Fix init script cgroup mounting workarounds to be more similar to cgroupfs-mount and thus work properly.
- Remove inotifywait hack from the upstart host-integration example because it's not necessary any more.
- Add check-config script to contrib.
- Fix fish shell completion.
Hack
- Clean up ""go test"" output from ""make test"" to be much more readable/scannable.
- Exclude more ""definitely not unit tested Go source code"" directories from hack/make/test.
- Generate md5 and sha256 hashes when building, and upload them via hack/release.sh.
- Include contributed completions in Ubuntu PPA.
- Add cli integration tests.
- Add tweaks to the hack scripts to make them simpler.
Remote API
- Add TLS auth support for API.
- Move git clone from daemon to client.
- Fix content-type detection in docker cp.
- Split API into 2 go packages.
Runtime
- Support hairpin NAT without going through Docker server.
- devicemapper: succeed immediately when removing non-existent devices.
- devicemapper: improve handling of devicemapper devices (add per device lock, increase sleep time and unlock while sleeping).
- devicemapper: increase timeout in waitClose to 10 seconds.
- devicemapper: ensure we shut down thin pool cleanly.
- devicemapper: pass info, rather than hash to activateDeviceIfNeeded, deactivateDevice, setInitialized, deleteDevice.
- devicemapper: avoid AB-BA deadlock.
- devicemapper: make shutdown better/faster.
- improve alpha sorting in mflag.
- Remove manual http cookie management because the cookiejar is being used.
- Use BSD raw mode on Darwin. Fixes nano, tmux and others.
- Add FreeBSD support for the client.
- Merge auth package into registry.
- Add deprecation warning for -t on
docker pull
. - Remove goroutine leak on error.
- Update parseLxcInfo to comply with new lxc1.0 format.
- Fix attach exit on darwin.
- Improve deprecation message.
- Retry to retrieve the layer metadata up to 5 times for
docker pull
. - Only unshare the mount namespace for execin.
- Merge existing config when committing.
- Disable daemon startup timeout.
- Fix issue #4681: add loopback interface when networking is disabled.
- Add failing test case for issue #4681.
- Send SIGTERM to child, instead of SIGKILL.
- Show the driver and the kernel version in
docker info
even when not in debug mode. - Always symlink /dev/ptmx for libcontainer. This fixes console related problems.
- Fix issue caused by the absence of /etc/apparmor.d.
- Don't leave empty cidFile behind when failing to create the container.
- Mount cgroups automatically if they're not mounted already.
- Use mock for search tests.
- Update to double-dash everywhere.
- Move .dockerenv parsing to lxc driver.
- Move all bind-mounts in the container inside the namespace.
- Don't use separate bind mount for container.
- Always symlink /dev/ptmx for libcontainer.
- Don't kill by pid for other drivers.
- Add initial logging to libcontainer.
- Sort by port in
docker ps
.
- Move networking drivers into runtime top level package.
- Add --no-prune to
docker rmi
. - Add time since exit in
docker ps
.
- graphdriver: add build tags.
- Prevent allocation of previously allocated ports & prevent improve port allocation.
- Add support for --since/--before in
docker ps
.
- Clean up container stop.
- Add support for configurable dns search domains.
- Add support for relative WORKDIR instructions.
- Add --output flag for docker save.
- Remove duplication of DNS entries in config merging.
- Add cpuset.cpus to cgroups and native driver options.
- Remove docker-ci.
- Promote btrfs. btrfs is no longer considered experimental.
- Add --input flag to
docker load
. - Return error when existing bridge doesn't match IP address.
- Strip comments before parsing line continuations to avoid interpreting instructions as comments.
- Fix TestOnlyLoopbackExistsWhenUsingDisableNetworkOption to ignore ""DOWN"" interfaces.
- Add systemd implementation of cgroups and make containers show up as systemd units.
- Fix commit and import when no repository is specified.
- Remount /var/lib/docker as --private to fix scaling issue.
- Use the environment's proxy when pinging the remote registry.
- Reduce error level from harmless errors.
- Allow --volumes-from to be individual files.
- Fix expanding buffer in StdCopy.
- Set error regardless of attach or stdin. This fixes #3364.
- Add support for --env-file to load environment variables from files.
- Symlink /etc/mtab and /proc/mounts.
- Allow pushing a single tag.
- Shut down containers cleanly at shutdown and wait forever for the containers to shut down. This makes container shutdown on daemon shutdown work properly via SIGTERM.
- Don't throw error when starting an already running container.
- Fix dynamic port allocation limit.
- remove setupDev from libcontainer.
- Add API version to
docker version
. - Return correct exit code when receiving signal and make SIGQUIT quit without cleanup.
- Fix --volumes-from mount failure.
- Allow non-privileged containers to create device nodes.
- Skip login tests because of external dependency on a hosted service.
- Deprecate
docker images --tree
anddocker images --viz
. - Deprecate
docker insert
. - Include base abstraction for apparmor. This fixes some apparmor related problems on Ubuntu 14.04.
- Add specific error message when hitting 401 over HTTP on push.
- Fix absolute volume check.
- Remove volumes-from from the config.
- Move DNS options to hostconfig.
- Update the apparmor profile for libcontainer.
- Add deprecation notice for
docker commit -run
.
0.9.1 (2014-03-24)
Builder
- Fix printing multiple messages on a single line. Fixes broken output during builds.
Documentation
- Fix external link on security of containers.
Contrib
- Fix init script cgroup mounting workarounds to be more similar to cgroupfs-mount and thus work properly.
- Add variable for DOCKER_LOGFILE to sysvinit and use append instead of overwrite in opening the logfile.
Hack
- Generate md5 and sha256 hashes when building, and upload them via hack/release.sh.
Remote API
- Fix content-type detection in
docker cp
.
Runtime
- Use BSD raw mode on Darwin. Fixes nano, tmux and others.
- Only unshare the mount namespace for execin.
- Retry to retrieve the layer metadata up to 5 times for
docker pull
. - Merge existing config when committing.
- Fix panic in monitor.
- Disable daemon startup timeout.
- Fix issue #4681: add loopback interface when networking is disabled.
- Add failing test case for issue #4681.
- Send SIGTERM to child, instead of SIGKILL.
- Show the driver and the kernel version in
docker info
even when not in debug mode. - Always symlink /dev/ptmx for libcontainer. This fixes console related problems.
- Fix issue caused by the absence of /etc/apparmor.d.
- Don't leave empty cidFile behind when failing to create the container.
- Improve deprecation message.
- Fix attach exit on darwin.
- devicemapper: improve handling of devicemapper devices (add per device lock, increase sleep time, unlock while sleeping).
- devicemapper: succeed immediately when removing non-existent devices.
- devicemapper: increase timeout in waitClose to 10 seconds.
- Remove goroutine leak on error.
- Update parseLxcInfo to comply with new lxc1.0 format.
0.9.0 (2014-03-10)
Builder
- Avoid extra mount/unmount during build. This fixes mount/unmount related errors during build.
- Add error to docker build --rm. This adds missing error handling.
- Forbid chained onbuild,
onbuild from
andonbuild maintainer
triggers. - Make
--rm
the default fordocker build
.
Documentation
- Download the docker client binary for Mac over https.
- Update the titles of the install instructions & descriptions.
- Add instructions for upgrading boot2docker.
- Add port forwarding example in OS X install docs.
- Attempt to disentangle repository and registry.
- Update docs to explain more about
docker ps
. - Update sshd example to use a Dockerfile.
- Rework some examples, including the Python examples.
- Update docs to include instructions for a container's lifecycle.
- Update docs documentation to discuss the docs branch.
- Don't skip cert check for an example & use HTTPS.
- Bring back the memory and swap accounting section which was lost when the kernel page was removed.
- Explain DNS warnings and how to fix them on systems running and using a local nameserver.
Contrib
- Add Tanglu support for mkimage-debootstrap.
- Add SteamOS support for mkimage-debootstrap.
Hack
- Get package coverage when running integration tests.
- Remove the Vagrantfile. This is being replaced with boot2docker.
- Fix tests on systems where aufs isn't available.
- Update packaging instructions and remove the dependency on lxc.
Remote API
- Move code specific to the API to the api package.
- Fix header content type for the API. Makes all endpoints use proper content type.
- Fix registry auth & remove ping calls from CmdPush and CmdPull.
- Add newlines to the JSON stream functions.
Runtime
- Do not ping the registry from the CLI. All requests to registries flow through the daemon.
- Check for nil information return in the lxc driver. This fixes panics with older lxc versions.
- Devicemapper: cleanups and fix for unmount. Fixes two problems which were causing unmount to fail intermittently.
- Devicemapper: remove directory when removing device. Directories don't get left behind when removing the device.
- Devicemapper: enable skip_block_zeroing. Improves performance by not zeroing blocks.
- Devicemapper: fix shutdown warnings. Fixes shutdown warnings concerning pool device removal.
- Ensure docker cp stream is closed properly. Fixes problems with files not being copied by
docker cp
. - Stop making
tcp://
default to127.0.0.1:4243
and remove the default port for tcp. - Fix
--run
indocker commit
. This makesdocker commit --run
work again. - Fix custom bridge related options. This makes custom bridges work again.
- Mount-bind the PTY as container console. This allows tmux/screen to run.
- Add the pure Go libcontainer library to make it possible to run containers using only features of the Linux kernel.
- Add native exec driver which uses libcontainer and make it the default exec driver.
- Add support for handling extended attributes in archives.
- Set the container MTU to be the same as the host MTU.
- Add simple sha256 checksums for layers to speed up
docker push
.
- Improve kernel version parsing.
- Allow flag grouping (
docker run -it
).
- Remove chroot exec driver.
- Fix divide by zero to fix panic.
- Rewrite
docker rmi
. - Fix docker info with lxc 1.0.0.
- Fix fedora tty with apparmor.
- Don't always append env vars, replace defaults with vars from config.
- Fix a goroutine leak.
- Switch to Go 1.2.1.
- Fix unique constraint error checks.
- Handle symlinks for Docker's data directory and for TMPDIR.
- Add deprecation warnings for flags (-flag is deprecated in favor of --flag)
- Add apparmor profile for the native execution driver.
- Move system specific code from archive to pkg/system.
- Fix duplicate signal for
docker run -i -t
(issue #3336). - Return correct process pid for lxc.
- Add a -G option to specify the group which unix sockets belong to.
- Add
-f
flag todocker rm
to force removal of running containers. - Kill ghost containers and restart all ghost containers when the docker daemon restarts.
- Add
DOCKER_RAMDISK
environment variable to make Docker work when the root is on a ramdisk.
0.8.1 (2014-02-18)
Builder
- Avoid extra mount/unmount during build. This removes an unneeded mount/unmount operation which was causing problems with devicemapper
- Fix regression with ADD of tar files. This stops Docker from decompressing tarballs added via ADD from the local file system
- Add error to
docker build --rm
. This adds a missing error check to ensure failures to remove containers are detected and reported
Documentation
- Update issue filing instructions
- Warn against the use of symlinks for Docker's storage folder
- Replace the Firefox example with an IceWeasel example
- Rewrite the PostgreSQL example using a Dockerfile and add more details to it
- Improve the OS X documentation
Remote API
- Fix broken images API for version less than 1.7
- Use the right encoding for all API endpoints which return JSON
- Move remote api client to api/
- Queue calls to the API using generic socket wait
Runtime
- Fix the use of custom settings for bridges and custom bridges
- Refactor the devicemapper code to avoid many mount/unmount race conditions and failures
- Remove two panics which could make Docker crash in some situations
- Don't ping registry from the CLI client
- Enable skip_block_zeroing for devicemapper. This stops devicemapper from always zeroing entire blocks
- Fix --run in
docker commit
. This makes docker commit store--run
in the image configuration - Remove directory when removing devicemapper device. This cleans up leftover mount directories
- Drop NET_ADMIN capability for non-privileged containers. Unprivileged containers can't change their network configuration
- Ensure
docker cp
stream is closed properly - Avoid extra mount/unmount during container registration. This removes an unneeded mount/unmount operation which was causing problems with devicemapper
- Stop allowing tcp:// as a default tcp bin address which binds to 127.0.0.1:4243 and remove the default port
- Mount-bind the PTY as container console. This allows tmux and screen to run in a container
- Clean up archive closing. This fixes and improves archive handling
- Fix engine tests on systems where temp directories are symlinked
- Add test methods for save and load
- Avoid temporarily unmounting the container when restarting it. This fixes a race for devicemapper during restart
- Support submodules when building from a GitHub repository
- Quote volume path to allow spaces
- Fix remote tar ADD behavior. This fixes a regression which was causing Docker to extract tarballs
0.8.0 (2014-02-04)
Notable features since 0.7.0
Images and containers can be removed much faster
Building an image from source with docker build is now much faster
The Docker daemon starts and stops much faster
The memory footprint of many common operations has been reduced, by streaming files instead of buffering them in memory, fixing memory leaks, and fixing various suboptimal memory allocations
Several race conditions were fixed, making Docker more stable under very high concurrency load. This makes Docker more stable and less likely to crash and reduces the memory footprint of many common operations
All packaging operations are now built on the Go language’s standard tar implementation, which is bundled with Docker itself. This makes packaging more portable across host distributions, and solves several issues caused by quirks and incompatibilities between different distributions of tar
Docker can now create, remove and modify larger numbers of containers and images graciously thanks to more aggressive releasing of system resources. For example the storage driver API now allows Docker to do reference counting on mounts created by the drivers With the ongoing changes to the networking and execution subsystems of docker testing these areas have been a focus of the refactoring. By moving these subsystems into separate packages we can test, analyze, and monitor coverage and quality of these packages
Many components have been separated into smaller sub-packages, each with a dedicated test suite. As a result the code is better-tested, more readable and easier to change
The ADD instruction now supports caching, which avoids unnecessarily re-uploading the same source content again and again when it hasn’t changed
The new ONBUILD instruction adds to your image a “trigger” instruction to be executed at a later time, when the image is used as the base for another build
Docker now ships with an experimental storage driver which uses the BTRFS filesystem for copy-on-write
Docker is officially supported on Mac OS X
The Docker daemon supports systemd socket activation
0.7.6 (2014-01-14)
Builder
- Do not follow symlink outside of build context
Runtime
- Remount bind mounts when ro is specified
- Use https for fetching docker version
Other
- Inline the test.docker.io fingerprint
- Add ca-certificates to packaging documentation
0.7.5 (2014-01-09)
Builder
- Disable compression for build. More space usage but a much faster upload
- Fix ADD caching for certain paths
- Do not compress archive from git build
Documentation
- Fix error in GROUP add example
- Make sure the GPG fingerprint is inline in the documentation
- Give more specific advice on setting up signing of commits for DCO
Runtime
- Fix misspelled container names
- Do not add hostname when networking is disabled
- Return most recent image from the cache by date
- Return all errors from docker wait
- Add Content-Type Header ""application/json"" to GET /version and /info responses
Other
- Update DCO to version 1.1
- Update Makefile to use ""docker:GIT_BRANCH"" as the generated image name
- Update Travis to check for new 1.1 DCO version
0.7.4 (2014-01-07)
Builder
- Fix ADD caching issue with . prefixed path
- Fix docker build on devicemapper by reverting sparse file tar option
- Fix issue with file caching and prevent wrong cache hit
- Use same error handling while unmarshalling CMD and ENTRYPOINT
Documentation
- Simplify and streamline Amazon Quickstart
- Install instructions use unprefixed Fedora image
- Update instructions for mtu flag for Docker on GCE
- Add Ubuntu Saucy to installation
- Fix for wrong version warning on master instead of latest
Runtime
- Only get the image's rootfs when we need to calculate the image size
- Correctly handle unmapping UDP ports
- Make CopyFileWithTar use a pipe instead of a buffer to save memory on docker build
- Fix login message to say pull instead of push
- Fix ""docker load"" help by removing ""SOURCE"" prompt and mentioning STDIN
- Make blank -H option default to the same as no -H was sent
- Extract cgroups utilities to own submodule
Other
- Add Travis CI configuration to validate DCO and gofmt requirements
- Add Developer Certificate of Origin Text
- Upgrade VBox Guest Additions
- Check standalone header when pinging a registry server
0.7.3 (2014-01-02)
Builder
- Update ADD to use the image cache, based on a hash of the added content
- Add error message for empty Dockerfile
Documentation
- Fix outdated link to the ""Introduction"" on www.docker.io
- Update the docs to get wider when the screen does
- Add information about needing to install LXC when using raw binaries
- Update Fedora documentation to disentangle the docker and docker.io conflict
- Add a note about using the new
-mtu
flag in several GCE zones
- Add FrugalWare installation instructions
- Add a more complete example of
docker run
- Fix API documentation for creating and starting Privileged containers
- Add missing ""name"" parameter documentation on ""/containers/create""
- Add a mention of
lxc-checkconfig
as a way to check for some of the necessary kernel configuration
- Update the 1.8 API documentation with some additions that were added to the docs for 1.7
Hack
- Add missing libdevmapper dependency to the packagers documentation
- Update minimum Go requirement to a hard line at Go 1.2+
- Many minor improvements to the Vagrantfile
- Add ability to customize dockerinit search locations when compiling (to be used very sparingly only by packagers of platforms who require a nonstandard location)
- Add coverprofile generation reporting
- Add
-a
to our Go build flags, removing the need for recompiling the stdlib manually
- Update Dockerfile to be more canonical and have less spurious warnings during build
- Fix some miscellaneous
docker pull
progress bar display issues
- Migrate more miscellaneous packages under the ""pkg"" folder
- Update TextMate highlighting to automatically be enabled for files named ""Dockerfile""
- Reorganize syntax highlighting files under a common ""contrib/syntax"" directory
- Update install.sh script ( https://get.docker.io/) to not fail if busybox fails to download or run at the end of the Ubuntu/Debian installation
- Add support for container names in bash completion
Packaging
- Add an official Docker client binary for Darwin (Mac OS X)
- Remove empty ""Vendor"" string and added ""License"" on deb package
- Add a stubbed version of ""/etc/default/docker"" in the deb package
Runtime
- Update layer application to extract tars in place, avoiding file churn while handling whiteouts
- Fix permissiveness of mtime comparisons in tar handling (since GNU tar and Go tar do not yet support sub-second mtime precision)
- Reimplement
docker top
in pure Go to work more consistently, and even inside Docker-in-Docker (thus removing the shell injection vulnerability present in some versions oflxc-ps
)
- Update
-H unix://
to work similarly to-H tcp://
by inserting the default values for missing portions
- Fix more edge cases regarding dockerinit and deleted or replaced docker or dockerinit files
- Update container name validation to include '.'
- Fix use of a symlink or non-absolute path as the argument to
-g
to work as expected
- Update to handle external mounts outside of LXC, fixing many small mounting quirks and making future execution backends and other features simpler
- Update to use proper box-drawing characters everywhere in
docker images -tree
- Move MTU setting from LXC configuration to directly use netlink
- Add
-S
option to external tar invocation for more efficient spare file handling
- Add arch/os info to User-Agent string, especially for registry requests
- Add
-mtu
option to Docker daemon for configuring MTU
- Fix
docker build
to exit with a non-zero exit code on error
- Add
DOCKER_HOST
environment variable to configure the client-H
flag without specifying it manually for every invocation
0.7.2 (2013-12-16)
Runtime
- Validate container names on creation with standard regex
- Increase maximum image depth to 127 from 42
- Continue to move api endpoints to the job api
- Add -bip flag to allow specification of dynamic bridge IP via CIDR
- Allow bridge creation when ipv6 is not enabled on certain systems
- Set hostname and IP address from within dockerinit
- Drop capabilities from within dockerinit
- Fix volumes on host when symlink is present the image
- Prevent deletion of image if ANY container is depending on it even if the container is not running
- Update docker push to use new progress display
- Use os.Lstat to allow mounting unix sockets when inspecting volumes
- Adjust handling of inactive user login
- Add missing defines in devicemapper for older kernels
- Allow untag operations with no container validation
- Add auth config to docker build
Documentation
- Add more information about Docker logging
- Add RHEL documentation
- Add a direct example for changing the CMD that is run in a container
- Update Arch installation documentation
- Add section on Trusted Builds
- Add Network documentation page
Other
- Add new cover bundle for providing code coverage reporting
- Separate integration tests in bundles
- Make Tianon the hack maintainer
- Update mkimage-debootstrap with more tweaks for keeping images small
- Use https to get the install script
- Remove vendored dotcloud/tar now that Go 1.2 has been released
0.7.1 (2013-12-05)
Documentation
- Add @SvenDowideit as documentation maintainer
- Add links example
- Add documentation regarding ambassador pattern
- Add Google Cloud Platform docs
- Add dockerfile best practices
- Update doc for RHEL
- Update doc for registry
- Update Postgres examples
- Update doc for Ubuntu install
- Improve remote api doc
Runtime
- Add hostconfig to docker inspect
- Implement
docker log -f
to stream logs - Add env variable to disable kernel version warning
- Add -format to
docker inspect
- Support bind-mount for files
- Fix bridge creation on RHEL
- Fix image size calculation
- Make sure iptables are called even if the bridge already exists
- Fix issue with stderr only attach
- Remove init layer when destroying a container
- Fix same port binding on different interfaces
docker build
now returns the correct exit code- Fix
docker port
to display correct port docker build
now check that the dockerfile exists client sidedocker attach
now returns the correct exit code- Remove the name entry when the container does not exist
Registry
- Improve progress bars, add ETA for downloads
- Simultaneous pulls now waits for the first to finish instead of failing
- Tag only the top-layer image when pushing to registry
- Fix issue with offline image transfer
- Fix issue preventing using ':' in password for registry
Other
- Add pprof handler for debug
- Create a Makefile
- Use stdlib tar that now includes fix
- Improve make.sh test script
- Handle SIGQUIT on the daemon
- Disable verbose during tests
- Upgrade to go1.2 for official build
- Improve unit tests
- The test suite now runs all tests even if one fails
- Refactor C in Go (Devmapper)
- Fix OS X compilation
0.7.0 (2013-11-25)
Notable features since 0.6.0
- Storage drivers: choose from aufs, device-mapper, or vfs.
- Standard Linux support: docker now runs on unmodified Linux kernels and all major distributions.
- Links: compose complex software stacks by connecting containers to each other.
- Container naming: organize your containers by giving them memorable names.
- Advanced port redirects: specify port redirects per interface, or keep sensitive ports private.
- Offline transfer: push and pull images to the filesystem without losing information.
- Quality: numerous bugfixes and small usability improvements. Significant increase in test coverage.
0.6.7 (2013-11-21)
Runtime
- Improve stability, fixes some race conditions
- Skip the volumes mounted when deleting the volumes of container.
- Fix layer size computation: handle hard links correctly
- Use the work Path for docker cp CONTAINER:PATH
- Fix tmp dir never cleanup
- Speedup docker ps
- More informative error message on name collisions
- Fix nameserver regex
- Always return long id's
- Fix container restart race condition
- Keep published ports on docker stop;docker start
- Fix container networking on Fedora
- Correctly express ""any address"" to iptables
- Fix network setup when reconnecting to ghost container
- Prevent deletion if image is used by a running container
- Lock around read operations in graph
RemoteAPI
- Return full ID on docker rmi
Client
- Add -tree option to images
- Offline image transfer
- Exit with status 2 on usage error and display usage on stderr
- Do not forward SIGCHLD to container
- Use string timestamp for docker events -since
Other
- Update to go 1.2rc5
- Add /etc/default/docker support to upstart
0.6.6 (2013-11-06)
Runtime
- Ensure container name on register
- Fix regression in /etc/hosts
- Add lock around write operations in graph
- Check if port is valid
- Fix restart runtime error with ghost container networking
- Add some more colors and animals to increase the pool of generated names
- Fix issues in docker inspect
- Escape apparmor confinement
- Set environment variables using a file.
- Prevent docker insert to erase something
- Prevent DNS server conflicts in CreateBridgeIface
- Validate bind mounts on the server side
- Use parent image config in docker build
- Fix regression in /etc/hosts
Client
- Add -P flag to publish all exposed ports
- Add -notrunc and -q flags to docker history
- Fix docker commit, tag and import usage
- Add stars, trusted builds and library flags in docker search
- Fix docker logs with tty
RemoteAPI
- Make /events API send headers immediately
- Do not split last column docker top
- Add size to history
Other
- Contrib: Desktop integration. Firefox usecase.
- Dockerfile: bump to go1.2rc3
0.6.5 (2013-10-29)
Runtime
- Containers can now be named
- Containers can now be linked together for service discovery
- 'run -a', 'start -a' and 'attach' can forward signals to the container for better integration with process supervisors
- Automatically start crashed containers after a reboot
- Expose IP, port, and proto as separate environment vars for container links
- Allow ports to be published to specific ips
- Prohibit inter-container communication by default
- Ignore ErrClosedPipe for stdin in Container.Attach
- Remove unused field kernelVersion
- Fix issue when mounting subdirectories of /mnt in container
- Fix untag during removal of images
- Check return value of syscall.Chdir when changing working directory inside dockerinit
Client
- Only pass stdin to hijack when needed to avoid closed pipe errors
- Use less reflection in command-line method invocation
- Monitor the tty size after starting the container, not prior
- Remove useless os.Exit() calls after log.Fatal
Hack
- Add initial init scripts library and a safer Ubuntu packaging script that works for Debian
- Add -p option to invoke debootstrap with http_proxy
- Update install.sh with $sh_c to get sudo/su for modprobe
- Update all the mkimage scripts to use --numeric-owner as a tar argument
- Update hack/release.sh process to automatically invoke hack/make.sh and bail on build and test issues
Other
- Documentation: Fix the flags for nc in example
- Testing: Remove warnings and prevent mount issues
- Testing: Change logic for tty resize to avoid warning in tests
- Builder: Fix race condition in docker build with verbose output
- Registry: Fix content-type for PushImageJSONIndex method
- Contrib: Improve helper tools to generate debian and Arch linux server images
0.6.4 (2013-10-16)
Runtime
- Add cleanup of container when Start() fails
- Add better comments to utils/stdcopy.go
- Add utils.Errorf for error logging
- Add -rm to docker run for removing a container on exit
- Remove error messages which are not actually errors
- Fix
docker rm
with volumes - Fix some error cases where an HTTP body might not be closed
- Fix panic with wrong dockercfg file
- Fix the attach behavior with -i
- Record termination time in state.
- Use empty string so TempDir uses the OS's temp dir automatically
- Make sure to close the network allocators
- Autorestart containers by default
- Bump vendor kr/pty to commit 3b1f6487b
(syscall.O_NOCTTY)
- lxc: Allow set_file_cap capability in container
- Move run -rm to the cli only
- Split stdout stderr
- Always create a new session for the container
Testing
- Add aggregated docker-ci email report
- Add cleanup to remove leftover containers
- Add nightly release to docker-ci
- Add more tests around auth.ResolveAuthConfig
- Remove a few errors in tests
- Catch errClosing error when TCP and UDP proxies are terminated
- Only run certain tests with TESTFLAGS='-run TestName' make.sh
- Prevent docker-ci to test closing PRs
- Replace panic by log.Fatal in tests
- Increase TestRunDetach timeout
Documentation
- Add initial draft of the Docker infrastructure doc
- Add devenvironment link to CONTRIBUTING.md
- Add
apt-get install curl
to Ubuntu docs - Add explanation for export restrictions
- Add .dockercfg doc
- Remove Gentoo install notes about #1422 workaround
- Fix help text for -v option
- Fix Ping endpoint documentation
- Fix parameter names in docs for ADD command
- Fix ironic typo in changelog
- Various command fixes in postgres example
- Document how to edit and release docs
- Minor updates to
postgresql_service.rst
- Clarify LGTM process to contributors
- Corrected error in the package name
- Document what
vagrant up
is actually doing
- improve doc search results
- Cleanup whitespace in API 1.5 docs
- use angle brackets in MAINTAINER example email
- Update archlinux.rst
- Changes to a new style for the docs. Includes version switcher.
- Formatting, add information about multiline json
- Improve registry and index REST API documentation
- Replace deprecated upgrading reference to docker-latest.tgz, which hasn't been updated since 0.5.3
- Update Gentoo installation documentation now that we're in the portage tree proper
- Cleanup and reorganize docs and tooling for contributors and maintainers
- Minor spelling correction of protocoll -> protocol
Contrib
- Add vim syntax highlighting for Dockerfiles from @honza
- Add mkimage-arch.sh
- Reorganize contributed completion scripts to add zsh completion
Hack
- Add vagrant user to the docker group
- Add proper bash completion for ""docker push""
- Add xz utils as a runtime dep
- Add cleanup/refactor portion of #2010 for hack and Dockerfile updates
- Add contrib/mkimage-centos.sh back (from #1621), and associated documentation link
- Add several of the small make.sh fixes from #1920, and make the output more consistent and contributor-friendly
- Add @tianon to hack/MAINTAINERS
- Improve network performance for VirtualBox
- Revamp install.sh to be usable by more people, and to use official install methods whenever possible (apt repo, portage tree, etc.)
- Fix contrib/mkimage-debian.sh apt caching prevention
- Add Dockerfile.tmLanguage to contrib
- Configured FPM to make /etc/init/docker.conf a config file
- Enable SSH Agent forwarding in Vagrant VM
- Several small tweaks/fixes for contrib/mkimage-debian.sh
Other
- Builder: Abort build if mergeConfig returns an error and fix duplicate error message
- Packaging: Remove deprecated packaging directory
- Registry: Use correct auth config when logging in.
- Registry: Fix the error message so it is the same as the regex
0.6.3 (2013-09-23)
Packaging
- Add 'docker' group on install for ubuntu package
- Update tar vendor dependency
- Download apt key over HTTPS
Runtime
- Only copy and change permissions on non-bindmount volumes
- Allow multiple volumes-from
- Fix HTTP imports from STDIN
Documentation
- Update section on extracting the docker binary after build
- Update development environment docs for new build process
- Remove 'base' image from documentation
Other
- Client: Fix detach issue
- Registry: Update regular expression to match index
0.6.2 (2013-09-17)
Runtime
- Add domainname support
- Implement image filtering with path.Match
- Remove unnecessary warnings
- Remove os/user dependency
- Only mount the hostname file when the config exists
- Handle signals within the
docker login
command
- UID and GID are now also applied to volumes
docker start
set error code upon errordocker run
set the same error code as the process started
Builder
- Add -rm option in order to remove intermediate containers
- Allow multiline for the RUN instruction
Registry
- Implement login with private registry
- Fix push issues
Other
- Hack: Vendor all dependencies
- Remote API: Bump to v1.5
- Packaging: Break down hack/make.sh into small scripts, one per 'bundle': test, binary, ubuntu etc.
- Documentation: General improvements
0.6.1 (2013-08-23)
Registry
- Pass ""meta"" headers in API calls to the registry
Packaging
- Use correct upstart script with new build tool
- Use libffi-dev, don't build it from sources
- Remove duplicate mercurial install command
0.6.0 (2013-08-22)
Runtime
- Add lxc-conf flag to allow custom lxc options
- Add an option to set the working directory
- Add Image name to LogEvent tests
- Add -privileged flag and relevant tests, docs, and examples
- Add websocket support to /container/
/attach/ws - Add warning when net.ipv4.ip_forwarding = 0
- Add hostname to environment
- Add last stable version in
docker version
- Fix race conditions in parallel pull
- Fix Graph ByParent() to generate list of child images per parent image.
- Fix typo: fmt.Sprint -> fmt.Sprintf
- Fix small \n error un docker build
- Fix to ""Inject dockerinit at /.dockerinit""
- Fix #910. print user name to docker info output
- Use Go 1.1.2 for dockerbuilder
- Use ranged for loop on channels
- Use utils.ParseRepositoryTag instead of strings.Split(name, "":"") in server.ImageDelete
- Improve CMD, ENTRYPOINT, and attach docs.
- Improve connect message with socket error
- Load authConfig only when needed and fix useless WARNING
- Show tag used when image is missing
- Apply volumes-from before creating volumes
- Make docker run handle SIGINT/SIGTERM
- Prevent crash when .dockercfg not readable
- Install script should be fetched over https, not http.
- API, issue 1471: Use groups for socket permissions
- Correctly detect IPv4 forwarding
- Mount /dev/shm as a tmpfs
- Switch from http to https for get.docker.io
- Let userland proxy handle container-bound traffic
- Update the Docker CLI to specify a value for the ""Host"" header.
- Change network range to avoid conflict with EC2 DNS
- Reduce connect and read timeout when pinging the registry
- Parallel pull
- Handle ip route showing mask-less IP addresses
- Allow ENTRYPOINT without CMD
- Always consider localhost as a domain name when parsing the FQN repos name
- Refactor checksum
Documentation
- Add MongoDB image example
- Add instructions for creating and using the docker group
- Add sudo to examples and installation to documentation
- Add ufw doc
- Add a reference to ps -a
- Add information about Docker's high level tools over LXC.
- Fix typo in docs for docker run -dns
- Fix a typo in the ubuntu installation guide
- Fix to docs regarding adding docker groups
- Update default -H docs
- Update readme with dependencies for building
- Update amazon.rst to explain that Vagrant is not necessary for running Docker on ec2
- PostgreSQL service example in documentation
- Suggest installing linux-headers by default.
- Change the twitter handle
- Clarify Amazon EC2 installation
- 'Base' image is deprecated and should no longer be referenced in the docs.
- Move note about officially supported kernel
- Solved the logo being squished in Safari
Builder
- Add USER instruction do Dockerfile
- Add workdir support for the Buildfile
- Add no cache for docker build
- Fix docker build and docker events output
- Only count known instructions as build steps
- Make sure ENV instruction within build perform a commit each time
- Forbid certain paths within docker build ADD
- Repository name (and optionally a tag) in build usage
- Make sure ADD will create everything in 0755
Remote API
- Sort Images by most recent creation date.
- Reworking opaque requests in registry module
- Add image name in /events
- Use mime pkg to parse Content-Type
- 650 http utils and user agent field
Hack
- Bash Completion: Limit commands to containers of a relevant state
- Add docker dependencies coverage testing into docker-ci
Packaging
- Docker-brew 0.5.2 support and memory footprint reduction
- Add new docker dependencies into docker-ci
- Revert ""docker.upstart: avoid spawning a
sh
process""
- Docker-brew and Docker standard library
- Release docker with docker
- Fix the upstart script generated by get.docker.io
- Enabled the docs to generate manpages.
- Revert Bind daemon to 0.0.0.0 in Vagrant.
Register
- Improve auth push
- Registry unit tests + mock registry
Tests
- Improve TestKillDifferentUser to prevent timeout on buildbot
- Fix typo in TestBindMounts (runContainer called without image)
- Improve TestGetContainersTop so it does not rely on sleep
- Relax the lo interface test to allow iface index != 1
- Add registry functional test to docker-ci
- Add some tests in server and utils
Other
- Contrib: bash completion script
- Client: Add docker cp command and copy api endpoint to copy container files/folders to the host
- Don't read from stdout when only attached to stdin
0.5.3 (2013-08-13)
Runtime
- Use docker group for socket permissions
- Spawn shell within upstart script
- Handle ip route showing mask-less IP addresses
- Add hostname to environment
Builder
- Make sure ENV instruction within build perform a commit each time
0.5.2 (2013-08-08)
- Builder: Forbid certain paths within docker build ADD
- Runtime: Change network range to avoid conflict with EC2 DNS
- API: Change daemon to listen on unix socket by default
0.5.1 (2013-07-30)
Runtime
- Add
ps
args todocker top
- Add support for container ID files (pidfile like)
- Add container=lxc in default env
- Support networkless containers with
docker run -n
anddocker -d -b=none
- Stdout/stderr logs are now stored in the same file as JSON
- Allocate a /16 IP range by default, with fallback to /24. Try 12 ranges instead of 3.
- Change .dockercfg format to json and support multiple auth remote
- Do not override volumes from config
- Fix issue with EXPOSE override
API
- Docker client now sets useragent (RFC 2616)
- Add /events endpoint
Builder
- ADD command now understands URLs
- CmdAdd and CmdEnv now respect Dockerfile-set ENV variables
- Create directories with 755 instead of 700 within ADD instruction
Hack
- Simplify unit tests with helpers
- Improve docker.upstart event
- Add coverage testing into docker-ci
0.5.0 (2013-07-17)
Runtime
- List all processes running inside a container with 'docker top'
- Host directories can be mounted as volumes with 'docker run -v'
- Containers can expose public UDP ports (eg, '-p 123/udp')
- Optionally specify an exact public port (eg. '-p 80:4500')
- 'docker login' supports additional options
- Don't save a container's hostname when committing an image.
Registry
- New image naming scheme inspired by Go packaging convention allows arbitrary combinations of registries
- Fix issues when uploading images to a private registry
Builder
- ENTRYPOINT instruction sets a default binary entry point to a container
- VOLUME instruction marks a part of the container as persistent data
- 'docker build' displays the full output of a build by default
0.4.8 (2013-07-01)
- Builder: New build operation ENTRYPOINT adds an executable entry point to the container. - Runtime: Fix a bug which caused 'docker run -d' to no longer print the container ID.
- Tests: Fix issues in the test suite
0.4.7 (2013-06-28)
Remote API
- The progress bar updates faster when downloading and uploading large files
- Fix a bug in the optional unix socket transport
Runtime
- Improve detection of kernel version
- Host directories can be mounted as volumes with 'docker run -b'
- fix an issue when only attaching to stdin
- Use 'tar --numeric-owner' to avoid uid mismatch across multiple hosts
Hack
- Improve test suite and dev environment
- Remove dependency on unit tests on 'os/user'
Other
- Registry: easier push/pull to a custom registry
- Documentation: add terminology section
0.4.6 (2013-06-22)
- Runtime: fix a bug which caused creation of empty images (and volumes) to crash.
0.4.5 (2013-06-21)
- Builder: 'docker build git://URL' fetches and builds a remote git repository
- Runtime: 'docker ps -s' optionally prints container size
- Tests: improved and simplified
- Runtime: fix a regression introduced in 0.4.3 which caused the logs command to fail.
- Builder: fix a regression when using ADD with single regular file.
0.4.4 (2013-06-19)
- Builder: fix a regression introduced in 0.4.3 which caused builds to fail on new clients.
0.4.3 (2013-06-19)
Builder
- ADD of a local file will detect tar archives and unpack them
- ADD improvements: use tar for copy + automatically unpack local archives
- ADD uses tar/untar for copies instead of calling 'cp -ar'
- Fix the behavior of ADD to be (mostly) reverse-compatible, predictable and well-documented.
- Fix a bug which caused builds to fail if ADD was the first command
- Nicer output for 'docker build'
Runtime
- Remove bsdtar dependency
- Add unix socket and multiple -H support
- Prevent rm of running containers
- Use go1.1 cookiejar
- Fix issue detaching from running TTY container
- Forbid parallel push/pull for a single image/repo. Fixes
#311
- Fix race condition within Run command when attaching.
Client
- HumanReadable ProgressBar sizes in pull
- Fix docker version's git commit output
API
- Send all tags on History API call
- Add tag lookup to history command. Fixes #882
Documentation
- Fix missing command in irc bouncer example
0.4.2 (2013-06-17)
- Packaging: Bumped version to work around an Ubuntu bug
0.4.1 (2013-06-17)
Remote Api
- Add flag to enable cross domain requests
- Add images and containers sizes in docker ps and docker images
Runtime
- Configure dns configuration host-wide with 'docker -d -dns'
- Detect faulty DNS configuration and replace it with a public default
- Allow docker run
: - You can now specify public port (ex: -p 80:4500)
- Improve image removal to garbage-collect unreferenced parents
Client
- Allow multiple params in inspect
- Print the container id before the hijack in
docker run
Registry
- Add regexp check on repo's name
- Move auth to the client
- Remove login check on pull
Other
- Vagrantfile: Add the rest api port to vagrantfile's port_forward
- Upgrade to Go 1.1
- Builder: don
t ignore last line in Dockerfile when it doesn
t end with \n
0.4.0 (2013-06-03)
Builder
- Introducing Builder
- 'docker build' builds a container, layer by layer, from a source repository containing a Dockerfile
Remote API
- Introducing Remote API
- control Docker programmatically using a simple HTTP/json API
Runtime
- Various reliability and usability improvements
0.3.4 (2013-05-30)
Builder
- 'docker build' builds a container, layer by layer, from a source repository containing a Dockerfile
- 'docker build -t FOO' applies the tag FOO to the newly built container.
Runtime
- Interactive TTYs correctly handle window resize
- Fix how configuration is merged between layers
Remote API
- Split stdout and stderr on 'docker run'
- Optionally listen on a different IP and port (use at your own risk)
Documentation
- Improve install instructions.
0.3.3 (2013-05-23)
- Registry: Fix push regression
- Various bugfixes
0.3.2 (2013-05-09)
Registry
- Improve the checksum process
- Use the size to have a good progress bar while pushing
- Use the actual archive if it exists in order to speed up the push
- Fix error 400 on push
Runtime
- Store the actual archive on commit
0.3.1 (2013-05-08)
Builder
- Implement the autorun capability within docker builder
- Add caching to docker builder
- Add support for docker builder with native API as top level command
- Implement ENV within docker builder
- Check the command existence prior create and add Unit tests for the case
- use any whitespaces instead of tabs
Runtime
- Add go version to debug infos
- Kernel version - don't show the dash if flavor is empty
Registry
- Add docker search top level command in order to search a repository
- Fix pull for official images with specific tag
- Fix issue when login in with a different user and trying to push
- Improve checksum - async calculation
Images
- Output graph of images to dot (graphviz)
- Fix ByParent function
Documentation
- New introduction and high-level overview
- Add the documentation for docker builder
- CSS fix for docker documentation to make REST API docs look better.
- Fix CouchDB example page header mistake
- Fix README formatting
- Update www.docker.io website.
Other
- Website: new high-level overview
- Makefile: Swap ""go get"" for ""go get -d"", especially to compile on go1.1rc
- Packaging: packaging ubuntu; issue #510: Use golang-stable PPA package to build docker
0.3.0 (2013-05-06)
Runtime
- Fix the command existence check
- strings.Split may return an empty string on no match
- Fix an index out of range crash if cgroup memory is not
Documentation
- Various improvements
- New example: sharing data between 2 couchdb databases
Other
- Vagrant: Use only one deb line in /etc/apt
- Registry: Implement the new registry
0.2.2 (2013-05-03)
- Support for data volumes ('docker run -v=PATH')
- Share data volumes between containers ('docker run -volumes-from')
- Improve documentation
- Upgrade to Go 1.0.3
- Various upgrades to the dev environment for contributors
0.2.1 (2013-05-01)
- 'docker commit -run' bundles a layer with default runtime options: command, ports etc.
- Improve install process on Vagrant
- New Dockerfile operation: ""maintainer""
- New Dockerfile operation: ""expose""
- New Dockerfile operation: ""cmd""
- Contrib script to build a Debian base layer
- 'docker -d -r': restart crashed containers at daemon startup
- Runtime: improve test coverage
0.2.0 (2013-04-23)
- Runtime: ghost containers can be killed and waited for
- Documentation: update install instructions
- Packaging: fix Vagrantfile
- Development: automate releasing binaries and ubuntu packages
- Add a changelog
- Various bugfixes
0.1.8 (2013-04-22)
- Dynamically detect cgroup capabilities
- Issue stability warning on kernels <3.8
- 'docker push' buffers on disk instead of memory
- Fix 'docker diff' for removed files
- Fix 'docker stop' for ghost containers
- Fix handling of pidfile
- Various bugfixes and stability improvements
0.1.7 (2013-04-18)
- Container ports are available on localhost
- 'docker ps' shows allocated TCP ports
- Contributors can run 'make hack' to start a continuous integration VM
- Streamline ubuntu packaging & uploading
- Various bugfixes and stability improvements
0.1.6 (2013-04-17)
- Record the author an image with 'docker commit -author'
0.1.5 (2013-04-17)
- Disable standalone mode
- Use a custom DNS resolver with 'docker -d -dns'
- Detect ghost containers
- Improve diagnosis of missing system capabilities
- Allow disabling memory limits at compile time
- Add debian packaging
- Documentation: installing on Arch Linux
- Documentation: running Redis on docker
- Fix lxc 0.9 compatibility
- Automatically load aufs module
- Various bugfixes and stability improvements
0.1.4 (2013-04-09)
- Full support for TTY emulation
- Detach from a TTY session with the escape sequence
C-p C-q
- Various bugfixes and stability improvements
- Minor UI improvements
- Automatically create our own bridge interface 'docker0'
0.1.3 (2013-04-04)
- Choose TCP frontend port with '-p :PORT'
- Layer format is versioned
- Major reliability improvements to the process manager
- Various bugfixes and stability improvements
0.1.2 (2013-04-03)
- Set container hostname with 'docker run -h'
- Selective attach at run with 'docker run -a [stdin[,stdout[,stderr]]]'
- Various bugfixes and stability improvements
- UI polish
- Progress bar on push/pull
- Use XZ compression by default
- Make IP allocator lazy
0.1.1 (2013-03-31)
- Display shorthand IDs for convenience
- Stabilize process management
- Layers can include a commit message
- Simplified 'docker attach'
- Fix support for re-attaching
- Various bugfixes and stability improvements
- Auto-download at run
- Auto-login on push
- Beefed up documentation
0.1.0 (2013-03-23)
Initial public release
- Implement registry in order to push/pull images
- TCP port allocation
- Fix termcaps on Linux
- Add documentation
- Add Vagrant support with Vagrantfile
- Add unit tests
- Add repository/tags to ease image management
- Improve the layer implementation",,,
17d55a17ea237ee3cc17dd979cdc1adb7cc15932f856a80317695387406ef60d,"Get Docker
Learn how to install Docker for Mac, Windows, or Linux and explore our developer tools.
Get DockerGen AI catalog New
Integrate AI solutions into your apps with minimal effort
Ask Gordon Beta
Your personal Docker expert, built right into Docker Desktop.
Boost your productivity with Ask Gordon, an AI-powered assistant designed to optimize your Docker workflows. From improving Dockerfiles to troubleshooting containers, Gordon is here to help.
Browse by section
Manage containers, applications, and images directly from your machine.
The definitive open source container client and runtime.
Package, test, and ship your applications.
Run your builds in the cloud.
Define and run multi-container applications with Docker.
Find and share container images and other artifacts.
Strengthen your software supply chain with Docker Scout.
Licensing for commercial use of Docker components.
Manage your billing and payment settings for your subscription.
Manage company and organization users, permissions, and more.
Security guardrails for both administrators and developers.
Testcontainers Cloud lets you run heavy test workloads remotely.
Community resources
Find fellow Docker enthusiasts, engage in insightful discussions, share knowledge, and collaborate on projects. Our communities offer a rich online experience for developers to create valuable connections that challenge and inspire!",,,
28630437e562831dce09fd8adecd80274e2c265b13490a61e6b16a24eb589327,"Getting started with Swarm mode
This tutorial introduces you to the features of Docker Engine Swarm mode. You may want to familiarize yourself with the key concepts before you begin.
The tutorial guides you through:
- Initializing a cluster of Docker Engines in swarm mode
- Adding nodes to the swarm
- Deploying application services to the swarm
- Managing the swarm once you have everything running
This tutorial uses Docker Engine CLI commands entered on the command line of a terminal window.
If you are brand new to Docker, see About Docker Engine.
Set up
To run this tutorial, you need:
- Three Linux hosts which can communicate over a network, with Docker installed
- The IP address of the manager machine
- Open ports between the hosts
Three networked host machines
This tutorial requires three Linux hosts which have Docker installed and can communicate over a network. These can be physical machines, virtual machines, Amazon EC2 instances, or hosted in some other way. Check out Deploy to Swarm for one possible set-up for the hosts.
One of these machines is a manager (called manager1
) and two of them are
workers (worker1
and worker2
).
Note
You can follow many of the tutorial steps to test single-node swarm as well, in which case you need only one host. Multi-node commands do not work, but you can initialize a swarm, create services, and scale them.
Install Docker Engine on Linux machines
If you are using Linux based physical computers or cloud-provided computers as hosts, simply follow the Linux install instructions for your platform. Spin up the three machines, and you are ready. You can test both single-node and multi-node swarm scenarios on Linux machines.
The IP address of the manager machine
The IP address must be assigned to a network interface available to the host operating system. All nodes in the swarm need to connect to the manager at the IP address.
Because other nodes contact the manager node on its IP address, you should use a fixed IP address.
You can run ifconfig
on Linux or macOS to see a list of the
available network interfaces.
The tutorial uses manager1
: 192.168.99.100
.
Open protocols and ports between the hosts
The following ports must be available. On some systems, these ports are open by default.
- Port
2377
TCP for communication with and between manager nodes - Port
7946
TCP/UDP for overlay network node discovery - Port
4789
UDP (configurable) for overlay network traffic
If you plan on creating an overlay network with encryption (--opt encrypted
),
you also need to ensure IP protocol 50 (IPSec ESP) traffic is allowed.
Port 4789
is the default value for the Swarm data path port, also known as the VXLAN port.
It is important to prevent any untrusted traffic from reaching this port, as VXLAN does not
provide authentication. This port should only be opened to a trusted network, and never at a
perimeter firewall.
If the network which Swarm traffic traverses is not fully trusted, it is strongly suggested that encrypted overlay networks be used. If encrypted overlay networks are in exclusive use, some additional hardening is suggested:
- Customize the default ingress network to use encryption
- Only accept encrypted packets on the Data Path Port:
# Example iptables rule (order and other tools may require customization)
iptables -I INPUT -m udp --dport 4789 -m policy --dir in --pol none -j DROP
Next steps
Next, you'll create a swarm.",,,
b5e643ae38b1fe2fa542c118796378faf97c7adeb19a27f3dd9f5cf9637eb4ae,"Create and manage a team
You can create teams for your organization in Docker Hub and the Docker Admin Console. You can configure repository access for a team in Docker Hub.
A team is a group of Docker users that belong to an organization. An organization can have multiple teams. An organization owner can then create new teams and add members to an existing team using their Docker ID or email address and by selecting a team the user should be part of. Members aren't required to be part of a team to be associated with an organization.
The organization owner can add additional organization owners to help them manage users, teams, and repositories in the organization by assigning them the owner role.
Organization owner
An organization owner is an administrator who has the following permissions:
- Manage repositories and add team members to the organization.
- Access private repositories, all teams, billing information, and organization settings.
- Specify permissions for each team in the organization.
- Enable SSO for the organization.
When SSO is enabled for your organization, the organization owner can also manage users. Docker can auto-provision Docker IDs for new end-users or users who'd like to have a separate Docker ID for company use through SSO enforcement.
The organization owner can also add additional organization owners to help them manage users, teams, and repositories in the organization.
Create a team
- Sign in to Docker Hub.
- Select Organizations and choose your organization.
- Select the Teams tab and then select Create Team.
- Fill out your team's information and select Create.
- Add members to your team.
- In Admin Console, select your organization.
- In the User management section, select Teams.
- Select Create team.
- Fill out your team's information and select Create.
- Add members to your team.
Configure repository permissions for a team
Organization owners can configure repository permissions on a per-team basis. For example, you can specify that all teams within an organization have ""Read and Write"" access to repositories A and B, whereas only specific teams have ""Admin"" access. Note that organization owners have full administrative access to all repositories within the organization.
To give a team access to a repository:
- Sign in to Docker Hub.
- Select Organizations and choose your organization.
- Select the Teams tab and select the team that you'd like to configure repository access to.
- Select the Permissions tab and select a repository from the Repository drop-down.
- Choose a permission from the Permissions drop-down list and select Add.
Organization owners can also assign members the editor role to grant partial administrative access. See Roles and permissions for more about the editor role.
Permissions reference
Read-only
access lets users view, search, and pull a private repository in the same way as they can a public repository.Read & Write
access lets users pull, push, and view a repository. In addition, it lets users view, cancel, retry or trigger buildsAdmin
access lets users pull, push, view, edit, and delete a repository. You can also edit build settings, and update the repositories description, collaborators rights, public/private visibility, and delete.
Permissions are cumulative. For example, if you have ""Read & Write"" permissions, you automatically have ""Read-only"" permissions:
| Action | Read-only | Read & Write | Admin |
|---|---|---|---|
| Pull a Repository | ✅ | ✅ | ✅ |
| View a Repository | ✅ | ✅ | ✅ |
| Push a Repository | ❌ | ✅ | ✅ |
| Edit a Repository | ❌ | ❌ | ✅ |
| Delete a Repository | ❌ | ❌ | ✅ |
| Update a Repository Description | ❌ | ❌ | ✅ |
| View Builds | ✅ | ✅ | ✅ |
| Cancel Builds | ❌ | ✅ | ✅ |
| Retry Builds | ❌ | ✅ | ✅ |
| Trigger Builds | ❌ | ✅ | ✅ |
| Edit Build Settings | ❌ | ❌ | ✅ |
Note
A user who hasn't verified their email address only has
Read-only
access to the repository, regardless of the rights their team membership has given them.
View a team's permissions for all repositories
To view a team's permissions across all repositories:
- Sign in to Docker Hub.
- Select Organizations and choose your organization.
- Select Teams and choose your team name.
- Select the Permissions tab, where you can view the repositories this team can access.
Delete a team
Organization owners can delete a team in Docker Hub or Admin Console. When you remove a team from your organization, this action revokes the members' access to the team's permitted resources. It won't remove users from other teams that they belong to, nor will it delete any resources.
- Sign in to Docker Hub.
- Select Organizations and choose your organization.
- Select the Teams tab.
- Select the name of the team that you want to delete.
- Select Settings.
- Select Delete Team.
- Review the confirmation message, then select Delete.
- In the Admin Console, select your organization.
- In the User management section, select Teams.
- Select the Actions icon next to the name of the team you want to delete.
- Select Delete team.
- Review the confirmation message, then select Delete.",,,
240c4e740e86642bb0afb2b94e5d2daa3834b67a6f59cecfc0b73e99b77a9d14,"Test before push with GitHub Actions
In some cases, you might want to validate that the image works as expected before pushing it. The following workflow implements several steps to achieve this:
- Build and export the image to Docker
- Test your image
- Multi-platform build and push the image
name: ci
on:
push:
env:
TEST_TAG: user/app:test
LATEST_TAG: user/app:latest
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up QEMU
uses: docker/setup-qemu-action@v3
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build and export to Docker
uses: docker/build-push-action@v6
with:
load: true
tags: ${{ env.TEST_TAG }}
- name: Test
run: |
docker run --rm ${{ env.TEST_TAG }}
- name: Build and push
uses: docker/build-push-action@v6
with:
platforms: linux/amd64,linux/arm64
push: true
tags: ${{ env.LATEST_TAG }}
Note
The
linux/amd64
image is only built once in this workflow. The image is built once, and the following steps use the internal cache from the firstBuild and push
step. The secondBuild and push
step only buildslinux/arm64
.",,,
e6ca253812e3becba666581bba78312a252ac3801ff6293c2bd1ae1a71e6f86c,"Docker guides
Explore our collection of guides to learn how Docker can optimize your development workflows and how to use it with specific languages, frameworks, or technologies.
Can't find the guide you're looking for? Open an issue on the docker/docs repository to let us know.
Featured guides
Set up your company for success with Docker
Get the most out of Docker by streamlining workflows, standardizing development environments, and ensuring smooth deployments across your company.
Docker Build Cloud: Reclaim your time with fast, multi-architecture builds
Build applications up to 39x faster using cloud-based resources, shared team cache, and native multi-architecture support.
Securing your software supply chain with Docker Scout
Enhance container security by automating vulnerability detection and remediation.
Mastering multi-platform builds, testing, and more with Docker Buildx Bake
Learn to automate Docker builds and testing with declarative configurations using Buildx Bake.
Mastering user and access management
Simplify user access while ensuring security and efficiency in Docker.
Mastering Testcontainers Cloud by Docker: streamlining integration testing with containers
Automate, scale, and optimize testing workflows with Testcontainers Cloud
All guides
Filtered results: showing out of guides.",,,
64bee6b8eb2509eeaa9b754a2d5989bfd51932da92982e40f573f998f8dd8cca,"Environment variables in Compose
By leveraging environment variables and interpolation in Docker Compose, you can create versatile and reusable configurations, making your Dockerized applications easier to manage and deploy across different environments.
Tip
Before using environment variables, read through all of the information first to get a full picture of environment variables in Docker Compose.
This section covers:
- How to set environment variables within your container's environment.
- How environment variable precedence works within your container's environment.
- Pre-defined environment variables.
It also covers:
- How interpolation can be used to set variables within your Compose file and how it relates to a container's environment.
- Some best practices.",,,
a9caac8c0a8ac1beec8bb8da6780d1662973fdb2eb9454aa25863c8be6232b34,"Virtual Machine Manager for Docker Desktop on Mac
The Virtual Machine Manager (VMM) in Docker Desktop for Mac is responsible for creating and managing the virtual machine used to run containers. Depending on your system architecture and performance needs, you can choose from multiple VMM options in Docker Desktop's settings. This page provides an overview of the available options.
Docker VMM
Docker VMM is a new, container-optimized hypervisor introduced in Docker Desktop 4.35 and available on Apple Silicon Macs only. Its enhanced speed and resource efficiency makes it an ideal choice for optimizing your workflow.
Docker VMM brings exciting advancements specifically tailored for Apple Silicon machines. By optimizing both the Linux kernel and hypervisor layers, Docker VMM delivers significant performance enhancements across common developer tasks.
Some key performance enhancements provided by Docker VMM include:
- Faster I/O operations: With a cold cache, iterating over a large shared filesystem with
find
is 2x faster than when the Apple Virtualization Framework is used. - Improved caching: With a warm cache, performance can improve by as much as 25x, even surpassing native Mac operations.
These improvements directly impact developers who rely on frequent file access and overall system responsiveness during containerized development. Docker VMM marks a significant leap in speed, enabling smoother workflows and faster iteration cycles.
Note
Docker VMM requires a minimum of 4GB of memory to be allocated to the Docker Linux VM. The memory needs to be increased before Docker VMM is enabled, and this can be done from the Resources tab in Settings.
Known issues
As Docker VMM is still in Beta, there are a few known limitations:
- Docker VMM does not currently support Rosetta, so emulation of amd64 architectures is slow. Docker is exploring potential solutions.
- Certain databases, like MongoDB and Cassandra, may fail when using virtiofs with Docker VMM. This issue is expected to be resolved in a future release.
Apple Virtualization Framework
The Apple Virtualization Framework is a stable and well-established option for managing virtual machines on Mac. It has been a reliable choice for many Mac users over the years. This framework is best suited for developers who prefer a proven solution with solid performance and broad compatibility.
QEMU (Legacy) for Apple Silicon
Note
QEMU will be deprecated in a future release.
QEMU is a legacy virtualization option for Apple Silicon Macs, primarily supported for older use cases.
Docker recommends transitioning to newer alternatives, such as Docker VMM or the Apple Virtualization Framework, as they offer superior performance and ongoing support. Docker VMM, in particular, offers substantial speed improvements and a more efficient development environment, making it a compelling choice for developers working with Apple Silicon.
Note that this is not related to using QEMU to emulate non-native architectures in multi-platform builds.
HyperKit (Legacy) for Intel-based Macs
Note
HyperKit will be deprecated in a future release.
HyperKit is another legacy virtualization option, specifically for Intel-based Macs. Like QEMU, it is still available but considered deprecated. Docker recommends switching to modern alternatives for better performance and to future-proof your setup.",,,
0b889777938231ed910483dd73506fc3baabbf9837d5e2c1dda6b41669305894,"How to back up and restore your Docker Desktop data
Use the following procedure to save and restore your images and container data. This is useful if you want to reset your VM disk or to move your Docker environment to a new computer, for example.
Important
If you use volumes or bind-mounts to store your container data, backing up your containers may not be needed, but make sure to remember the options that were used when creating the container or use a Docker Compose file if you want to re-create your containers with the same configuration after re-installation.
Save your data
Commit your containers to an image with
docker container commit
.Committing a container stores the container filesystem changes and some of the container's configuration, for example labels and environment-variables, as a local image. Be aware that environment variables may contain sensitive information such as passwords or proxy-authentication, so care should be taken when pushing the resulting image to a registry.
Also note that filesystem changes in volume that are attached to the container are not included in the image, and must be backed up separately.
If you used a named volume to store container data, such as databases, refer to the back up, restore, or migrate data volumes page in the storage section.
Use
docker push
to push any images you have built locally and want to keep to the Docker Hub registry.Make sure to configure the repository's visibility as ""private"" for images that should not be publicly accessible.
Alternatively, use
docker image save -o images.tar image1 [image2 ...]
to save any images you want to keep to a local tar file.
After backing up your data, you can uninstall the current version of Docker Desktop and install a different version or reset Docker Desktop to factory defaults.
Restore your data
Use
docker pull
to restore images you pushed to Docker Hub.If you backed up your images to a local tar file, use
docker image load -i images.tar
to restore previously saved images.Re-create your containers if needed, using
docker run
, or Docker Compose.
Refer to the backup, restore, or migrate data volumes page in the storage section to restore volume data.",,,
9d1a66f9fb180d7621cabdc221d146f5453eadec1c534240424f4a09adf01929,"OpenTelemetry for the Docker CLI
The Docker CLI supports
OpenTelemetry instrumentation
for emitting metrics about command invocations. This is disabled by default.
You can configure the CLI to start emitting metrics to the endpoint that you
specify. This allows you to capture information about your docker
command
invocations for more insight into your Docker usage.
Exporting metrics is opt-in, and you control where data is being sent by specifying the destination address of the metrics collector.
What is OpenTelemetry?
OpenTelemetry, or OTel for short, is an open observability framework for creating and managing telemetry data, such as traces, metrics, and logs. OpenTelemetry is vendor- and tool-agnostic, meaning that it can be used with a broad variety of Observability backends.
Support for OpenTelemetry instrumentation in the Docker CLI means that the CLI can emit information about events that take place, using the protocols and conventions defined in the Open Telemetry specification.
How it works
The Docker CLI doesn't emit telemetry data by default. Only if you've set an environment variable on your system will Docker CLI attempt to emit OpenTelemetry metrics, to the endpoint that you specify.
DOCKER_CLI_OTEL_EXPORTER_OTLP_ENDPOINT=<endpoint>
The variable specifies the endpoint of an OpenTelemetry collector, where telemetry data
about docker
CLI invocation should be sent. To capture the data, you'll need
an OpenTelemetry collector listening on that endpoint.
The purpose of a collector is to receive the telemetry data, process it, and exports it to a backend. The backend is where the telemetry data gets stored. You can choose from a number of different backends, such as Prometheus or InfluxDB.
Some backends provide tools for visualizing the metrics directly. Alternatively, you can also run a dedicated frontend with support for generating more useful graphs, such as Grafana.
Setup
To get started capturing telemetry data for the Docker CLI, you'll need to:
- Set the
DOCKER_CLI_OTEL_EXPORTER_OTLP_ENDPOINT
environment variable to point to an OpenTelemetry collector endpoint - Run an OpenTelemetry collector that receives the signals from CLI command invocations
- Run a backend for storing the data received from the collector
The following Docker Compose file bootstraps a set of services to get started with OpenTelemetry. It includes an OpenTelemetry collector that the CLI can send metrics to, and a Prometheus backend that scrapes the metrics off the collector.
name: cli-otel
services:
prometheus:
image: prom/prometheus
command:
- ""--config.file=/etc/prometheus/prom.yml""
ports:
# Publish the Prometheus frontend on localhost:9091
- 9091:9090
restart: always
volumes:
# Store Prometheus data in a volume:
- prom_data:/prometheus
# Mount the prom.yml config file
- ./prom.yml:/etc/prometheus/prom.yml
otelcol:
image: otel/opentelemetry-collector
restart: always
depends_on:
- prometheus
ports:
- 4317:4317
volumes:
# Mount the otelcol.yml config file
- ./otelcol.yml:/etc/otelcol/config.yaml
volumes:
prom_data:
This service assumes that the following two configuration files exist alongside
compose.yaml
:
- otelcol.yml
# Receive signals over gRPC and HTTP receivers: otlp: protocols: grpc: http: # Establish an endpoint for Prometheus to scrape from exporters: prometheus: endpoint: ""0.0.0.0:8889"" service: pipelines: metrics: receivers: [otlp] exporters: [prometheus]
- prom.yml
# Configure Prometheus to scrape the OpenTelemetry collector endpoint scrape_configs: - job_name: ""otel-collector"" scrape_interval: 1s static_configs: - targets: [""otelcol:8889""]
With these files in place:
Start the Docker Compose services:
$ docker compose up
Configure Docker CLI to export telemetry to the OpenTelemetry collector.
$ export DOCKER_CLI_OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
Run a
docker
command to trigger the CLI into sending a metric signal to the OpenTelemetry collector.$ docker version
To view telemetry metrics created by the CLI, open the Prometheus expression browser by going to http://localhost:9091/graph.
In the Query field, enter
command_time_milliseconds_total
, and execute the query to see the telemetry data.
Available metrics
Docker CLI currently exports a single metric, command.time
, which measures
the execution duration of a command in milliseconds. This metric has the
following attributes:
command.name
: the name of the commandcommand.status.code
: the exit code of the commandcommand.stderr.isatty
: true if stderr is attached to a TTYcommand.stdin.isatty
: true if stdin is attached to a TTYcommand.stdout.isatty
: true if stdout is attached to a TTY",,,
8b1cca1a0ac8c02a814e9ec989b2bf932d858a76df5331d4ea6c9e3fc2138e36,"Resource constraints
By default, a container has no resource constraints and can use as much of a
given resource as the host's kernel scheduler allows. Docker provides ways
to control how much memory, or CPU a container can use, setting runtime
configuration flags of the docker run
command. This section provides details
on when you should set such limits and the possible implications of setting them.
Many of these features require your kernel to support Linux capabilities. To
check for support, you can use the
docker info
command. If a capability
is disabled in your kernel, you may see a warning at the end of the output like
the following:
WARNING: No swap limit support
Consult your operating system's documentation for enabling them. See also the Docker Engine troubleshooting guide for more information.
Memory
Understand the risks of running out of memory
It's important not to allow a running container to consume too much of the
host machine's memory. On Linux hosts, if the kernel detects that there isn't
enough memory to perform important system functions, it throws an OOME
, or
Out Of Memory Exception
, and starts killing processes to free up
memory. Any process is subject to killing, including Docker and other important
applications. This can effectively bring the entire system down if the wrong
process is killed.
Docker attempts to mitigate these risks by adjusting the OOM priority on the
Docker daemon so that it's less likely to be killed than other processes
on the system. The OOM priority on containers isn't adjusted. This makes it more
likely for an individual container to be killed than for the Docker daemon
or other system processes to be killed. You shouldn't try to circumvent
these safeguards by manually setting --oom-score-adj
to an extreme negative
number on the daemon or a container, or by setting --oom-kill-disable
on a
container.
For more information about the Linux kernel's OOM management, see Out of Memory Management.
You can mitigate the risk of system instability due to OOME by:
- Perform tests to understand the memory requirements of your application before placing it into production.
- Ensure that your application runs only on hosts with adequate resources.
- Limit the amount of memory your container can use, as described below.
- Be mindful when configuring swap on your Docker hosts. Swap is slower than memory but can provide a buffer against running out of system memory.
- Consider converting your container to a service, and using service-level constraints and node labels to ensure that the application runs only on hosts with enough memory
Limit a container's access to memory
Docker can enforce hard or soft memory limits.
- Hard limits lets the container use no more than a fixed amount of memory.
- Soft limits lets the container use as much memory as it needs unless certain conditions are met, such as when the kernel detects low memory or contention on the host machine.
Some of these options have different effects when used alone or when more than one option is set.
Most of these options take a positive integer, followed by a suffix of b
, k
,
m
, g
, to indicate bytes, kilobytes, megabytes, or gigabytes.
| Option | Description |
|---|---|
-m or --memory= | The maximum amount of memory the container can use. If you set this option, the minimum allowed value is 6m (6 megabytes). That is, you must set the value to at least 6 megabytes. |
--memory-swap * | The amount of memory this container is allowed to swap to disk. See
--memory-swap details. |
--memory-swappiness | By default, the host kernel can swap out a percentage of anonymous pages used by a container. You can set --memory-swappiness to a value between 0 and 100, to tune this percentage. See
--memory-swappiness details. |
--memory-reservation | Allows you to specify a soft limit smaller than --memory which is activated when Docker detects contention or low memory on the host machine. If you use --memory-reservation , it must be set lower than --memory for it to take precedence. Because it is a soft limit, it doesn't guarantee that the container doesn't exceed the limit. |
--kernel-memory | The maximum amount of kernel memory the container can use. The minimum allowed value is 6m . Because kernel memory can't be swapped out, a container which is starved of kernel memory may block host machine resources, which can have side effects on the host machine and on other containers. See
--kernel-memory details. |
--oom-kill-disable | By default, if an out-of-memory (OOM) error occurs, the kernel kills processes in a container. To change this behavior, use the --oom-kill-disable option. Only disable the OOM killer on containers where you have also set the -m/--memory option. If the -m flag isn't set, the host can run out of memory and the kernel may need to kill the host system's processes to free memory. |
For more information about cgroups and memory in general, see the documentation for Memory Resource Controller.
--memory-swap
details
--memory-swap
is a modifier flag that only has meaning if --memory
is also
set. Using swap allows the container to write excess memory requirements to disk
when the container has exhausted all the RAM that's available to it. There is a
performance penalty for applications that swap memory to disk often.
Its setting can have complicated effects:
If
--memory-swap
is set to a positive integer, then both--memory
and--memory-swap
must be set.--memory-swap
represents the total amount of memory and swap that can be used, and--memory
controls the amount used by non-swap memory. So if--memory=""300m""
and--memory-swap=""1g""
, the container can use 300m of memory and 700m (1g - 300m
) swap.If
--memory-swap
is set to0
, the setting is ignored, and the value is treated as unset.If
--memory-swap
is set to the same value as--memory
, and--memory
is set to a positive integer, the container doesn't have access to swap. See Prevent a container from using swap.If
--memory-swap
is unset, and--memory
is set, the container can use as much swap as the--memory
setting, if the host container has swap memory configured. For instance, if--memory=""300m""
and--memory-swap
is not set, the container can use 600m in total of memory and swap.If
--memory-swap
is explicitly set to-1
, the container is allowed to use unlimited swap, up to the amount available on the host system.Inside the container, tools like
free
report the host's available swap, not what's available inside the container. Don't rely on the output offree
or similar tools to determine whether swap is present.
Prevent a container from using swap
If --memory
and --memory-swap
are set to the same value, this prevents
containers from using any swap. This is because --memory-swap
is the amount of
combined memory and swap that can be used, while --memory
is only the amount
of physical memory that can be used.
--memory-swappiness
details
- A value of 0 turns off anonymous page swapping.
- A value of 100 sets all anonymous pages as swappable.
- By default, if you don't set
--memory-swappiness
, the value is inherited from the host machine.
--kernel-memory
details
Kernel memory limits are expressed in terms of the overall memory allocated to a container. Consider the following scenarios:
- Unlimited memory, unlimited kernel memory: This is the default behavior.
- Unlimited memory, limited kernel memory: This is appropriate when the amount of memory needed by all cgroups is greater than the amount of memory that actually exists on the host machine. You can configure the kernel memory to never go over what's available on the host machine, and containers which need more memory need to wait for it.
- Limited memory, unlimited kernel memory: The overall memory is limited, but the kernel memory isn't.
- Limited memory, limited kernel memory: Limiting both user and kernel memory can be useful for debugging memory-related problems. If a container is using an unexpected amount of either type of memory, it runs out of memory without affecting other containers or the host machine. Within this setting, if the kernel memory limit is lower than the user memory limit, running out of kernel memory causes the container to experience an OOM error. If the kernel memory limit is higher than the user memory limit, the kernel limit doesn't cause the container to experience an OOM.
When you enable kernel memory limits, the host machine tracks ""high water mark""
statistics on a per-process basis, so you can track which processes (in this
case, containers) are using excess memory. This can be seen per process by
viewing /proc/<PID>/status
on the host machine.
CPU
By default, each container's access to the host machine's CPU cycles is unlimited. You can set various constraints to limit a given container's access to the host machine's CPU cycles. Most users use and configure the default CFS scheduler. You can also configure the real-time scheduler.
Configure the default CFS scheduler
The CFS is the Linux kernel CPU scheduler for normal Linux processes. Several runtime flags let you configure the amount of access to CPU resources your container has. When you use these settings, Docker modifies the settings for the container's cgroup on the host machine.
| Option | Description |
|---|---|
--cpus=<value> | Specify how much of the available CPU resources a container can use. For instance, if the host machine has two CPUs and you set --cpus=""1.5"" , the container is guaranteed at most one and a half of the CPUs. This is the equivalent of setting --cpu-period=""100000"" and --cpu-quota=""150000"" . |
--cpu-period=<value> | Specify the CPU CFS scheduler period, which is used alongside --cpu-quota . Defaults to 100000 microseconds (100 milliseconds). Most users don't change this from the default. For most use-cases, --cpus is a more convenient alternative. |
--cpu-quota=<value> | Impose a CPU CFS quota on the container. The number of microseconds per --cpu-period that the container is limited to before throttled. As such acting as the effective ceiling. For most use-cases, --cpus is a more convenient alternative. |
--cpuset-cpus | Limit the specific CPUs or cores a container can use. A comma-separated list or hyphen-separated range of CPUs a container can use, if you have more than one CPU. The first CPU is numbered 0. A valid value might be 0-3 (to use the first, second, third, and fourth CPU) or 1,3 (to use the second and fourth CPU). |
--cpu-shares | Set this flag to a value greater or less than the default of 1024 to increase or reduce the container's weight, and give it access to a greater or lesser proportion of the host machine's CPU cycles. This is only enforced when CPU cycles are constrained. When plenty of CPU cycles are available, all containers use as much CPU as they need. In that way, this is a soft limit. --cpu-shares doesn't prevent containers from being scheduled in Swarm mode. It prioritizes container CPU resources for the available CPU cycles. It doesn't guarantee or reserve any specific CPU access. |
If you have 1 CPU, each of the following commands guarantees the container at most 50% of the CPU every second.
$ docker run -it --cpus="".5"" ubuntu /bin/bash
Which is the equivalent to manually specifying --cpu-period
and --cpu-quota
;
$ docker run -it --cpu-period=100000 --cpu-quota=50000 ubuntu /bin/bash
Configure the real-time scheduler
You can configure your container to use the real-time scheduler, for tasks which can't use the CFS scheduler. You need to make sure the host machine's kernel is configured correctly before you can configure the Docker daemon or configure individual containers.
Warning
CPU scheduling and prioritization are advanced kernel-level features. Most users don't need to change these values from their defaults. Setting these values incorrectly can cause your host system to become unstable or unusable.
Configure the host machine's kernel
Verify that CONFIG_RT_GROUP_SCHED
is enabled in the Linux kernel by running
zcat /proc/config.gz | grep CONFIG_RT_GROUP_SCHED
or by checking for the
existence of the file /sys/fs/cgroup/cpu.rt_runtime_us
. For guidance on
configuring the kernel real-time scheduler, consult the documentation for your
operating system.
Configure the Docker daemon
To run containers using the real-time scheduler, run the Docker daemon with
the --cpu-rt-runtime
flag set to the maximum number of microseconds reserved
for real-time tasks per runtime period. For instance, with the default period of
1000000 microseconds (1 second), setting --cpu-rt-runtime=950000
ensures that
containers using the real-time scheduler can run for 950000 microseconds for every
1000000-microsecond period, leaving at least 50000 microseconds available for
non-real-time tasks. To make this configuration permanent on systems which use
systemd
, create a systemd unit file for the docker
service. For an example,
see the instruction on how to configure the daemon to use a proxy with a
systemd unit file.
Configure individual containers
You can pass several flags to control a container's CPU priority when you
start the container using docker run
. Consult your operating system's
documentation or the ulimit
command for information on appropriate values.
| Option | Description |
|---|---|
--cap-add=sys_nice | Grants the container the CAP_SYS_NICE capability, which allows the container to raise process nice values, set real-time scheduling policies, set CPU affinity, and other operations. |
--cpu-rt-runtime=<value> | The maximum number of microseconds the container can run at real-time priority within the Docker daemon's real-time scheduler period. You also need the --cap-add=sys_nice flag. |
--ulimit rtprio=<value> | The maximum real-time priority allowed for the container. You also need the --cap-add=sys_nice flag. |
The following example command sets each of these three flags on a debian:jessie
container.
$ docker run -it \
--cpu-rt-runtime=950000 \
--ulimit rtprio=99 \
--cap-add=sys_nice \
debian:jessie
If the kernel or Docker daemon isn't configured correctly, an error occurs.
GPU
Access an NVIDIA GPU
Prerequisites
Visit the official NVIDIA drivers page to download and install the proper drivers. Reboot your system once you have done so.
Verify that your GPU is running and accessible.
Install nvidia-container-toolkit
Follow the official NVIDIA Container Toolkit installation instructions.
Expose GPUs for use
Include the --gpus
flag when you start a container to access GPU resources.
Specify how many GPUs to use. For example:
$ docker run -it --rm --gpus all ubuntu nvidia-smi
Exposes all available GPUs and returns a result akin to the following:
+-------------------------------------------------------------------------------+
| NVIDIA-SMI 384.130 Driver Version: 384.130 |
|-------------------------------+----------------------+------------------------+
| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |
| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |
|===============================+======================+========================|
| 0 GRID K520 Off | 00000000:00:03.0 Off | N/A |
| N/A 36C P0 39W / 125W | 0MiB / 4036MiB | 0% Default |
+-------------------------------+----------------------+------------------------+
+-------------------------------------------------------------------------------+
| Processes: GPU Memory |
| GPU PID Type Process name Usage |
|===============================================================================|
| No running processes found |
+-------------------------------------------------------------------------------+
Use the device
option to specify GPUs. For example:
$ docker run -it --rm --gpus device=GPU-3a23c669-1f69-c64e-cf85-44e9b07e7a2a ubuntu nvidia-smi
Exposes that specific GPU.
$ docker run -it --rm --gpus '""device=0,2""' ubuntu nvidia-smi
Exposes the first and third GPUs.
Note
NVIDIA GPUs can only be accessed by systems running a single engine.
Set NVIDIA capabilities
You can set capabilities manually. For example, on Ubuntu you can run the following:
$ docker run --gpus 'all,capabilities=utility' --rm ubuntu nvidia-smi
This enables the utility
driver capability which adds the nvidia-smi
tool to
the container.
Capabilities as well as other configurations can be set in images via environment variables. More information on valid variables can be found in the nvidia-container-toolkit documentation. These variables can be set in a Dockerfile.
You can also use CUDA images which sets these variables automatically. See the official CUDA images NGC catalog page.",,,
e655e9ea7e4fbe7ed5edde0d46a26b00c416adede3e1a41653df1ee14cd14917,"Deploy Notary Server with Compose
The easiest way to deploy Notary Server is by using Docker Compose. To follow the procedure on this page, you must have already installed Docker Compose.
Clone the Notary repository.
$ git clone https://github.com/theupdateframework/notary.git
Build and start Notary Server with the sample certificates.
$ docker compose up -d
For more detailed documentation about how to deploy Notary Server, see the instructions to run a Notary service as well as the Notary repository for more information.
Make sure that your Docker or Notary client trusts Notary Server's certificate before you try to interact with the Notary server.
See the instructions for Docker or for Notary depending on which one you are using.
If you want to use Notary in production
Check back here for instructions after Notary Server has an official stable release. To get a head start on deploying Notary in production, see the Notary repository.",,,
f9815cd395e6372b49b0c0fe69c1d7f2370f769e6b3b52e502fb771b1b5ba939,"Install Docker Desktop on Linux
Docker Desktop terms
Commercial use of Docker Desktop in larger enterprises (more than 250 employees OR more than $10 million USD in annual revenue) requires a paid subscription.
This page contains information about general system requirements, supported platforms, and instructions on how to install Docker Desktop for Linux.
Important
Docker Desktop on Linux runs a Virtual Machine (VM) which creates and uses a custom docker context,
desktop-linux
, on startup.This means images and containers deployed on the Linux Docker Engine (before installation) are not available in Docker Desktop for Linux.
Important
For commercial use of Docker Engine obtained via Docker Desktop within larger enterprises (exceeding 250 employees OR with annual revenue surpassing $10 million USD), a paid subscription is required.
Docker Desktop for Linux provides a user-friendly graphical interface that simplifies the management of containers and services. It includes Docker Engine as this is the core technology that powers Docker containers. Docker Desktop for Linux also comes with additional features like Docker Scout and Docker Extensions.
Installing Docker Desktop and Docker Engine
Docker Desktop for Linux and Docker Engine can be installed side-by-side on the same machine. Docker Desktop for Linux stores containers and images in an isolated storage location within a VM and offers controls to restrict its resources. Using a dedicated storage location for Docker Desktop prevents it from interfering with a Docker Engine installation on the same machine.
While it's possible to run both Docker Desktop and Docker Engine simultaneously, there may be situations where running both at the same time can cause issues. For example, when mapping network ports (
-p
/--publish
) for containers, both Docker Desktop and Docker Engine may attempt to reserve the same port on your machine, which can lead to conflicts (""port already in use"").We generally recommend stopping the Docker Engine while you're using Docker Desktop to prevent the Docker Engine from consuming resources and to prevent conflicts as described above.
Use the following command to stop the Docker Engine service:
$ sudo systemctl stop docker docker.socket containerd
Depending on your installation, the Docker Engine may be configured to automatically start as a system service when your machine starts. Use the following command to disable the Docker Engine service, and to prevent it from starting automatically:
$ sudo systemctl disable docker docker.socket containerd
Switching between Docker Desktop and Docker Engine
The Docker CLI can be used to interact with multiple Docker Engines. For example, you can use the same Docker CLI to control a local Docker Engine and to control a remote Docker Engine instance running in the cloud. Docker Contexts allow you to switch between Docker Engines instances.
When installing Docker Desktop, a dedicated ""desktop-linux"" context is created to interact with Docker Desktop. On startup, Docker Desktop automatically sets its own context (
desktop-linux
) as the current context. This means that subsequent Docker CLI commands target Docker Desktop. On shutdown, Docker Desktop resets the current context to thedefault
context.Use the
docker context ls
command to view what contexts are available on your machine. The current context is indicated with an asterisk (*
);$ docker context ls NAME DESCRIPTION DOCKER ENDPOINT ... default * Current DOCKER_HOST based configuration unix:///var/run/docker.sock ... desktop-linux unix:///home/<user>/.docker/desktop/docker.sock ...
If you have both Docker Desktop and Docker Engine installed on the same machine, you can run the
docker context use
command to switch between the Docker Desktop and Docker Engine contexts. For example, use the ""default"" context to interact with the Docker Engine:$ docker context use default default Current context is now ""default""
And use the
desktop-linux
context to interact with Docker Desktop:$ docker context use desktop-linux desktop-linux Current context is now ""desktop-linux""
Refer to the Docker Context documentation for more details.
Supported platforms
Docker provides .deb
and .rpm
packages from the following Linux distributions
and architectures:
| Platform | x86_64 / amd64 |
|---|---|
| Ubuntu | ✅ |
| Debian | ✅ |
| Red Hat Enterprise Linux (RHEL) | ✅ |
| Fedora | ✅ |
An experimental package is available for Arch-based distributions. Docker has not tested or verified the installation.
Docker supports Docker Desktop on the current LTS release of the aforementioned distributions and the most recent version. As new versions are made available, Docker stops supporting the oldest version and supports the newest version.
General system requirements
To install Docker Desktop successfully, your Linux host must meet the following general requirements:
- 64-bit kernel and CPU support for virtualization.
- KVM virtualization support. Follow the KVM virtualization support instructions to check if the KVM kernel modules are enabled and how to provide access to the KVM device.
- QEMU must be version 5.2 or later. We recommend upgrading to the latest version.
- systemd init system.
- Gnome, KDE, or MATE Desktop environment.
- For many Linux distributions, the Gnome environment does not support tray icons. To add support for tray icons, you need to install a Gnome extension. For example, AppIndicator.
- At least 4 GB of RAM.
- Enable configuring ID mapping in user namespaces, see File sharing. Note that for Docker Desktop version 4.35 and later, this is not required anymore.
- Recommended:
Initialize
pass
for credentials management.
Docker Desktop for Linux runs a Virtual Machine (VM). For more information on why, see Why Docker Desktop for Linux runs a VM.
Note
Docker does not provide support for running Docker Desktop for Linux in nested virtualization scenarios. We recommend that you run Docker Desktop for Linux natively on supported distributions.
KVM virtualization support
Docker Desktop runs a VM that requires KVM support.
The kvm
module should load automatically if the host has virtualization support. To load the module manually, run:
$ modprobe kvm
Depending on the processor of the host machine, the corresponding module must be loaded:
$ modprobe kvm_intel # Intel processors
$ modprobe kvm_amd # AMD processors
If the above commands fail, you can view the diagnostics by running:
$ kvm-ok
To check if the KVM modules are enabled, run:
$ lsmod | grep kvm
kvm_amd 167936 0
ccp 126976 1 kvm_amd
kvm 1089536 1 kvm_amd
irqbypass 16384 1 kvm
Set up KVM device user permissions
To check ownership of /dev/kvm
, run :
$ ls -al /dev/kvm
Add your user to the kvm group in order to access the kvm device:
$ sudo usermod -aG kvm $USER
Sign out and sign back in so that your group membership is re-evaluated.
Where to go next
- Install Docker Desktop for Linux for your specific Linux distribution:",,,
4385e4f89e3b5c71bebd65a0803ceded2c11f6c249ae1b6f28a1a72258770d98,"Bake targets
A target in a Bake file represents a build invocation. It holds all the
information you would normally pass to a docker build
command using flags.
target ""webapp"" {
dockerfile = ""webapp.Dockerfile""
tags = [""docker.io/username/webapp:latest""]
context = ""https://github.com/username/webapp""
}
To build a target with Bake, pass name of the target to the bake
command.
$ docker buildx bake webapp
You can build multiple targets at once by passing multiple target names to the
bake
command.
$ docker buildx bake webapp api tests
Default target
If you don't specify a target when running docker buildx bake
, Bake will
build the target named default
.
target ""default"" {
dockerfile = ""webapp.Dockerfile""
tags = [""docker.io/username/webapp:latest""]
context = ""https://github.com/username/webapp""
}
To build this target, run docker buildx bake
without any arguments:
$ docker buildx bake
Target properties
The properties you can set for a target closely resemble the CLI flags for
docker build
, with a few additional properties that are specific to Bake.
For all the properties you can set for a target, see the Bake reference.
Grouping targets
You can group targets together using the group
block. This is useful when you
want to build multiple targets at once.
group ""all"" {
targets = [""webapp"", ""api"", ""tests""]
}
target ""webapp"" {
dockerfile = ""webapp.Dockerfile""
tags = [""docker.io/username/webapp:latest""]
context = ""https://github.com/username/webapp""
}
target ""api"" {
dockerfile = ""api.Dockerfile""
tags = [""docker.io/username/api:latest""]
context = ""https://github.com/username/api""
}
target ""tests"" {
dockerfile = ""tests.Dockerfile""
contexts = {
webapp = ""target:webapp"",
api = ""target:api"",
}
output = [""type=local,dest=build/tests""]
context = "".""
}
To build all the targets in a group, pass the name of the group to the bake
command.
$ docker buildx bake all
Additional resources
Refer to the following pages to learn more about Bake's features:
- Learn how to use variables in Bake to make your build configuration more flexible.
- Learn how you can use matrices to build multiple images with different configurations in Matrices.
- Head to the Bake file reference to learn about all the properties you can set in a Bake file, and its syntax.",,,
d815392c4707c1c7c97e0d493f23c3ee76adc420dd2bc2729c7351faa4ce5c32,"Use Compose Watch
The watch
attribute automatically updates and previews your running Compose services as you edit and save your code. For many projects, this enables a hands-off development workflow once Compose is running, as services automatically update themselves when you save your work.
watch
adheres to the following file path rules:
- All paths are relative to the project directory, apart from ignore file patterns
- Directories are watched recursively
- Glob patterns aren't supported
- Rules from
.dockerignore
apply- Use
ignore
option to define additional paths to be ignored (same syntax) - Temporary/backup files for common IDEs (Vim, Emacs, JetBrains, & more) are ignored automatically
.git
directories are ignored automatically
- Use
You don't need to switch on watch
for all services in a Compose project. In some instances, only part of the project, for example the Javascript frontend, might be suitable for automatic updates.
Compose Watch is designed to work with services built from local source code using the build
attribute. It doesn't track changes for services that rely on pre-built images specified by the image
attribute.
Compose Watch versus bind mounts
Compose supports sharing a host directory inside service containers. Watch mode does not replace this functionality but exists as a companion specifically suited to developing in containers.
More importantly, watch
allows for greater granularity than is practical with a bind mount. Watch rules let you ignore specific files or entire directories within the watched tree.
For example, in a JavaScript project, ignoring the node_modules/
directory has two benefits:
- Performance. File trees with many small files can cause high I/O load in some configurations
- Multi-platform. Compiled artifacts cannot be shared if the host OS or architecture is different to the container
For example, in a Node.js project, it's not recommended to sync the node_modules/
directory. Even though JavaScript is interpreted, npm
packages can contain native code that is not portable across platforms.
Configuration
The watch
attribute defines a list of rules that control automatic service updates based on local file changes.
Each rule requires, a path
pattern and action
to take when a modification is detected. There are two possible actions for watch
and depending on
the action
, additional fields might be accepted or required.
Watch mode can be used with many different languages and frameworks. The specific paths and rules will vary from project to project, but the concepts remain the same.
Prerequisites
In order to work properly, watch
relies on common executables. Make sure your service image contains the following binaries:
- stat
- mkdir
- rmdir
watch
also requires that the container's USER
can write to the target path so it can update files. A common pattern is for
initial content to be copied into the container using the COPY
instruction in a Dockerfile. To ensure such files are owned
by the configured user, use the COPY --chown
flag:
# Run as a non-privileged user
FROM node:18
RUN useradd -ms /bin/sh -u 1001 app
USER app
# Install dependencies
WORKDIR /app
COPY package.json package-lock.json ./
RUN npm install
# Copy source files into application directory
COPY --chown=app:app . /app
action
Sync
If action
is set to sync
, Compose makes sure any changes made to files on your host automatically match with the corresponding files within the service container.
sync
is ideal for frameworks that support ""Hot Reload"" or equivalent functionality.
More generally, sync
rules can be used in place of bind mounts for many development use cases.
Rebuild
If action
is set to rebuild
, Compose automatically builds a new image with BuildKit and replaces the running service container.
The behavior is the same as running docker compose up --build <svc>
.
Rebuild is ideal for compiled languages or as fallbacks for modifications to particular files that require a full
image rebuild (e.g. package.json
).
Sync + Restart
If action
is set to sync+restart
, Compose synchronizes your changes with the service containers and restarts it.
sync+restart
is ideal when config file changes, and you don't need to rebuild the image but just restart the main process of the service containers.
It will work well when you update a database configuration or your nginx.conf
file for example
Tip
Optimize your
Dockerfile
for speedy incremental rebuilds with image layer caching and multi-stage builds.
path
and target
The target
field controls how the path is mapped into the container.
For path: ./app/html
and a change to ./app/html/index.html
:
target: /app/html
->/app/html/index.html
target: /app/static
->/app/static/index.html
target: /assets
->/assets/index.html
ignore
The ignore
patterns are relative to the path
defined in the current watch
action, not to the project directory. In the following Example 1, the ignore path would be relative to the ./web
directory specified in the path
attribute.
Example 1
This minimal example targets a Node.js application with the following structure:
myproject/
├── web/
│ ├── App.jsx
│ ├── index.js
│ └── node_modules/
├── Dockerfile
├── compose.yaml
└── package.json
services:
web:
build: .
command: npm start
develop:
watch:
- action: sync
path: ./web
target: /src/web
ignore:
- node_modules/
- action: rebuild
path: package.json
In this example, when running docker compose up --watch
, a container for the web
service is launched using an image built from the Dockerfile
in the project's root.
The web
service runs npm start
for its command, which then launches a development version of the application with Hot Module Reload enabled in the bundler (Webpack, Vite, Turbopack, etc).
After the service is up, the watch mode starts monitoring the target directories and files.
Then, whenever a source file in the web/
directory is changed, Compose syncs the file to the corresponding location under /src/web
inside the container.
For example, ./web/App.jsx
is copied to /src/web/App.jsx
.
Once copied, the bundler updates the running application without a restart.
And in this case, the ignore
rule would apply to myproject/web/node_modules/
, not myproject/node_modules/
.
Unlike source code files, adding a new dependency can’t be done on-the-fly, so whenever package.json
is changed, Compose
rebuilds the image and recreates the web
service container.
This pattern can be followed for many languages and frameworks, such as Python with Flask: Python source files can be synced while a change to requirements.txt
should trigger a rebuild.
Example 2
Adapting the previous example to demonstrate sync+restart
:
services:
web:
build: .
command: npm start
develop:
watch:
- action: sync
path: ./web
target: /app/web
ignore:
- node_modules/
- action: sync+restart
path: ./proxy/nginx.conf
target: /etc/nginx/conf.d/default.conf
backend:
build:
context: backend
target: builder
This setup demonstrates how to use the sync+restart
action in Docker Compose to efficiently develop and test a Node.js application with a frontend web server and backend service. The configuration ensures that changes to the application code and configuration files are quickly synchronized and applied, with the web
service restarting as needed to reflect the changes.
Use watch
- Add
watch
sections to one or more services incompose.yaml
. - Run
docker compose up --watch
to build and launch a Compose project and start the file watch mode. - Edit service source files using your preferred IDE or editor.
Note
Watch can also be used with the dedicated
docker compose watch
command if you don't want to get the application logs mixed with the (re)build logs and filesystem sync events.
Tip
Check out
dockersamples/avatars
, or local setup for Docker docs for a demonstration of Composewatch
.
Feedback
We are actively looking for feedback on this feature. Give feedback or report any bugs you may find in the Compose Specification repository.",,,
adfd35523cebeaf44e695b96983f339c2bd796b0874ddb4f2d877da08cefb6fc,"FAQs for Docker Desktop for Mac
Why do I keep getting a notification telling me an application has changed my Desktop configurations?
You receive this notification because the Configuration integrity check feature has detected that a third-party application has altered your Docker Desktop configuration. This usually happens due to incorrect or missing symlinks. The notification ensures you are aware of these changes so you can review and repair any potential issues to maintain system reliability.
Opening the notification presents a pop-up window which provides detailed information about the detected integrity issues.
If you choose to ignore the notification, it will be shown again only at the next Docker Desktop startup. If you choose to repair your configuration, you won't be prompted again.
If you want to switch off Configuration integrity check notifications, navigate to Docker Desktop's settings and in the General tab, clear the Automatically check configuration setting.
If you have feedback on how to further improve the Configuration integrity check feature, fill out the feedback form.
What is HyperKit?
HyperKit is a hypervisor built on top of the Hypervisor.framework in macOS. It runs entirely in userspace and has no other dependencies.
We use HyperKit to eliminate the need for other VM products, such as Oracle VirtualBox or VMWare Fusion.
What is the benefit of HyperKit?
HyperKit is thinner than VirtualBox and VMWare fusion, and the version included is customized for Docker workloads on Mac.
Why is com.docker.vmnetd still running after I quit the app?
The privileged helper process com.docker.vmnetd
is started by launchd
and
runs in the background. The process does not consume any resources unless
Docker.app
connects to it, so it's safe to ignore.
Where does Docker Desktop store Linux containers and images?
Docker Desktop stores Linux containers and images in a single, large ""disk image"" file in the Mac filesystem. This is different from Docker on Linux, which usually stores containers and images in the /var/lib/docker
directory.
Where is the disk image file?
To locate the disk image file, select Settings from the Docker Desktop Dashboard then Advanced from the Resources tab.
The Advanced tab displays the location of the disk image. It also displays the maximum size of the disk image and the actual space the disk image is consuming. Note that other tools might display space usage of the file in terms of the maximum file size, and not the actual file size.
What if the file is too big?
If the disk image file is too big, you can:
- Move it to a bigger drive
- Delete unnecessary containers and images
- Reduce the maximum allowable size of the file
How do I move the file to a bigger drive?
To move the disk image file to a different location:
Select Settings then Advanced from the Resources tab.
In the Disk image location section, select Browse and choose a new location for the disk image.
Select Apply & Restart for the changes to take effect.
Important
Do not move the file directly in Finder as this can cause Docker Desktop to lose track of the file.
How do I delete unnecessary containers and images?
Check whether you have any unnecessary containers and images. If your client and daemon API are running version 1.25 or later (use the docker version
command on the client to check your client and daemon API versions), you can see the detailed space usage information by running:
$ docker system df -v
Alternatively, to list images, run:
$ docker image ls
To list containers, run:
$ docker container ls -a
If there are lots of redundant objects, run the command:
$ docker system prune
This command removes all stopped containers, unused networks, dangling images, and build cache.
It might take a few minutes to reclaim space on the host depending on the format of the disk image file. If the file is named:
Docker.raw
, space on the host is reclaimed within a few seconds.Docker.qcow2
, space is freed by a background process after a few minutes.
Space is only freed when images are deleted. Space is not freed automatically when files are deleted inside running containers. To trigger a space reclamation at any point, run the command:
$ docker run --privileged --pid=host docker/desktop-reclaim-space
Note that many tools report the maximum file size, not the actual file size. To query the actual size of the file on the host from a terminal, run:
$ cd ~/Library/Containers/com.docker.docker/Data/vms/0/data
$ ls -klsh Docker.raw
2333548 -rw-r--r--@ 1 username staff 64G Dec 13 17:42 Docker.raw
In this example, the actual size of the disk is 2333548
KB, whereas the maximum size of the disk is 64
GB.
How do I reduce the maximum size of the file?
To reduce the maximum size of the disk image file:
Select Settings then Advanced from the Resources tab.
The Disk image size section contains a slider that allows you to change the maximum size of the disk image. Adjust the slider to set a lower limit.
Select Apply & Restart.
When you reduce the maximum size, the current disk image file is deleted, and therefore, all containers and images are lost.
How do I add TLS certificates?
You can add trusted Certificate Authorities (CAs) (used to verify registry server certificates) and client certificates (used to authenticate to registries) to your Docker daemon.
Add custom CA certificates (server side)
All trusted CAs (root or intermediate) are supported. Docker Desktop creates a certificate bundle of all user-trusted CAs based on the Mac Keychain, and appends it to Moby trusted certificates. So if an enterprise SSL certificate is trusted by the user on the host, it is trusted by Docker Desktop.
To manually add a custom, self-signed certificate, start by adding the certificate to the macOS keychain, which is picked up by Docker Desktop. Here is an example:
$ sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain ca.crt
Or, if you prefer to add the certificate to your own local keychain only (rather than for all users), run this command instead:
$ security add-trusted-cert -d -r trustRoot -k ~/Library/Keychains/login.keychain ca.crt
See also, Directory structures for certificates.
Note
You need to restart Docker Desktop after making any changes to the keychain or to the
~/.docker/certs.d
directory in order for the changes to take effect.
For a complete explanation of how to do this, see the blog post Adding Self-signed Registry Certs to Docker & Docker Desktop for Mac.
Add client certificates
You can put your client certificates in
~/.docker/certs.d/<MyRegistry>:<Port>/client.cert
and
~/.docker/certs.d/<MyRegistry>:<Port>/client.key
.
When the Docker Desktop application starts, it copies the ~/.docker/certs.d
folder on your Mac to the /etc/docker/certs.d
directory on Moby (the Docker
Desktop xhyve
virtual machine).
Note
You need to restart Docker Desktop after making any changes to the keychain or to the
~/.docker/certs.d
directory in order for the changes to take effect.The registry cannot be listed as an insecure registry. Docker Desktop ignores certificates listed under insecure registries, and does not send client certificates. Commands like
docker run
that attempt to pull from the registry produce error messages on the command line, as well as on the registry.
Directory structures for certificates
If you have this directory structure, you do not need to manually add the CA certificate to your Mac OS system login:
/Users/<user>/.docker/certs.d/
└── <MyRegistry>:<Port>
├── ca.crt
├── client.cert
└── client.key
The following further illustrates and explains a configuration with custom certificates:
/etc/docker/certs.d/ <-- Certificate directory
└── localhost:5000 <-- Hostname:port
├── client.cert <-- Client certificate
├── client.key <-- Client key
└── ca.crt <-- Certificate authority that signed
the registry certificate
You can also have this directory structure, as long as the CA certificate is also in your keychain.
/Users/<user>/.docker/certs.d/
└── <MyRegistry>:<Port>
├── client.cert
└── client.key
To learn more about how to install a CA root certificate for the registry and how to set the client TLS certificate for verification, see Verify repository client with certificates in the Docker Engine topics.",,,
163269ceda37e89bf9eb9a4d0c209c81df99f54d7b41dd13bde4a408e98ad5ab,"Disable two-factor authentication on your Docker account
Warning
Disabling two-factor authentication results in decreased security for your Docker account.
- Sign in to your Docker account.
- Select your avatar and then from the drop-down menu, select Account settings.
- Select 2FA.
- Enter your password, then select Confirm.
- Select Disable 2FA.",,,
7d3527dbb3536e8d0483cca1bc25c2e8e69dd5e606cc64f67f91684a98391dcf,"Access authorization plugin
This document describes the Docker Engine plugins available in Docker Engine. To view information on plugins managed by Docker Engine, refer to Docker Engine plugin system.
Docker's out-of-the-box authorization model is all or nothing. Any user with permission to access the Docker daemon can run any Docker client command. The same is true for callers using Docker's Engine API to contact the daemon. If you require greater access control, you can create authorization plugins and add them to your Docker daemon configuration. Using an authorization plugin, a Docker administrator can configure granular access policies for managing access to the Docker daemon.
Anyone with the appropriate skills can develop an authorization plugin. These skills, at their most basic, are knowledge of Docker, understanding of REST, and sound programming knowledge. This document describes the architecture, state, and methods information available to an authorization plugin developer.
Basic principles
Docker's plugin infrastructure enables extending Docker by loading, removing and communicating with third-party components using a generic API. The access authorization subsystem was built using this mechanism.
Using this subsystem, you don't need to rebuild the Docker daemon to add an authorization plugin. You can add a plugin to an installed Docker daemon. You do need to restart the Docker daemon to add a new plugin.
An authorization plugin approves or denies requests to the Docker daemon based on both the current authentication context and the command context. The authentication context contains all user details and the authentication method. The command context contains all the relevant request data.
Authorization plugins must follow the rules described in Docker Plugin API. Each plugin must reside within directories described under the Plugin discovery section.
Note
The abbreviations
AuthZ
andAuthN
mean authorization and authentication respectively.
Default user authorization mechanism
If TLS is enabled in the
Docker daemon, the default user authorization flow extracts the user details from the certificate subject name.
That is, the User
field is set to the client certificate subject common name, and the AuthenticationMethod
field is set to TLS
.
Basic architecture
You are responsible for registering your plugin as part of the Docker daemon startup. You can install multiple plugins and chain them together. This chain can be ordered. Each request to the daemon passes in order through the chain. Only when all the plugins grant access to the resource, is the access granted.
When an HTTP request is made to the Docker daemon through the CLI or via the Engine API, the authentication subsystem passes the request to the installed authentication plugin(s). The request contains the user (caller) and command context. The plugin is responsible for deciding whether to allow or deny the request.
The sequence diagrams below depict an allow and deny authorization flow:
Each request sent to the plugin includes the authenticated user, the HTTP
headers, and the request/response body. Only the user name and the
authentication method used are passed to the plugin. Most importantly, no user
credentials or tokens are passed. Finally, not all request/response bodies
are sent to the authorization plugin. Only those request/response bodies where
the Content-Type
is either text/*
or application/json
are sent.
For commands that can potentially hijack the HTTP connection (HTTP Upgrade
), such as exec
, the authorization plugin is only called for the
initial HTTP requests. Once the plugin approves the command, authorization is
not applied to the rest of the flow. Specifically, the streaming data is not
passed to the authorization plugins. For commands that return chunked HTTP
response, such as logs
and events
, only the HTTP request is sent to the
authorization plugins.
During request/response processing, some authorization flows might need to do additional queries to the Docker daemon. To complete such flows, plugins can call the daemon API similar to a regular user. To enable these additional queries, the plugin must provide the means for an administrator to configure proper authentication and security policies.
Docker client flows
To enable and configure the authorization plugin, the plugin developer must support the Docker client interactions detailed in this section.
Setting up Docker daemon
Enable the authorization plugin with a dedicated command line flag in the
--authorization-plugin=PLUGIN_ID
format. The flag supplies a PLUGIN_ID
value. This value can be the plugin’s socket or a path to a specification file.
Authorization plugins can be loaded without restarting the daemon. Refer
to the
dockerd
documentation for more information.
$ dockerd --authorization-plugin=plugin1 --authorization-plugin=plugin2,...
Docker's authorization subsystem supports multiple --authorization-plugin
parameters.
Calling authorized command (allow)
$ docker pull centos
<...>
f1b10cd84249: Pull complete
<...>
Calling unauthorized command (deny)
$ docker pull centos
<...>
docker: Error response from daemon: authorization denied by plugin PLUGIN_NAME: volumes are not allowed.
Error from plugins
$ docker pull centos
<...>
docker: Error response from daemon: plugin PLUGIN_NAME failed with error: AuthZPlugin.AuthZReq: Cannot connect to the Docker daemon. Is the docker daemon running on this host?.
API schema and implementation
In addition to Docker's standard plugin registration method, each plugin should implement the following two methods:
/AuthZPlugin.AuthZReq
This authorize request method is called before the Docker daemon processes the client request./AuthZPlugin.AuthZRes
This authorize response method is called before the response is returned from Docker daemon to the client.
/AuthZPlugin.AuthZReq
Request
{
""User"": ""The user identification"",
""UserAuthNMethod"": ""The authentication method used"",
""RequestMethod"": ""The HTTP method"",
""RequestURI"": ""The HTTP request URI"",
""RequestBody"": ""Byte array containing the raw HTTP request body"",
""RequestHeader"": ""Byte array containing the raw HTTP request header as a map[string][]string ""
}
Response
{
""Allow"": ""Determined whether the user is allowed or not"",
""Msg"": ""The authorization message"",
""Err"": ""The error message if things go wrong""
}
/AuthZPlugin.AuthZRes
Request:
{
""User"": ""The user identification"",
""UserAuthNMethod"": ""The authentication method used"",
""RequestMethod"": ""The HTTP method"",
""RequestURI"": ""The HTTP request URI"",
""RequestBody"": ""Byte array containing the raw HTTP request body"",
""RequestHeader"": ""Byte array containing the raw HTTP request header as a map[string][]string"",
""ResponseBody"": ""Byte array containing the raw HTTP response body"",
""ResponseHeader"": ""Byte array containing the raw HTTP response header as a map[string][]string"",
""ResponseStatusCode"":""Response status code""
}
Response:
{
""Allow"": ""Determined whether the user is allowed or not"",
""Msg"": ""The authorization message"",
""Err"": ""The error message if things go wrong""
}
Request authorization
Each plugin must support two request authorization messages formats, one from the daemon to the plugin and then from the plugin to the daemon. The tables below detail the content expected in each message.
Daemon -> Plugin
| Name | Type | Description |
|---|---|---|
| User | string | The user identification |
| Authentication method | string | The authentication method used |
| Request method | enum | The HTTP method (GET/DELETE/POST) |
| Request URI | string | The HTTP request URI including API version (e.g., v.1.17/containers/json) |
| Request headers | map[string]string | Request headers as key value pairs (without the authorization header) |
| Request body | []byte | Raw request body |
Plugin -> Daemon
| Name | Type | Description |
|---|---|---|
| Allow | bool | Boolean value indicating whether the request is allowed or denied |
| Msg | string | Authorization message (will be returned to the client in case the access is denied) |
| Err | string | Error message (will be returned to the client in case the plugin encounter an error. The string value supplied may appear in logs, so should not include confidential information) |
Response authorization
The plugin must support two authorization messages formats, one from the daemon to the plugin and then from the plugin to the daemon. The tables below detail the content expected in each message.
Daemon -> Plugin
| Name | Type | Description |
|---|---|---|
| User | string | The user identification |
| Authentication method | string | The authentication method used |
| Request method | string | The HTTP method (GET/DELETE/POST) |
| Request URI | string | The HTTP request URI including API version (e.g., v.1.17/containers/json) |
| Request headers | map[string]string | Request headers as key value pairs (without the authorization header) |
| Request body | []byte | Raw request body |
| Response status code | int | Status code from the Docker daemon |
| Response headers | map[string]string | Response headers as key value pairs |
| Response body | []byte | Raw Docker daemon response body |
Plugin -> Daemon
| Name | Type | Description |
|---|---|---|
| Allow | bool | Boolean value indicating whether the response is allowed or denied |
| Msg | string | Authorization message (will be returned to the client in case the access is denied) |
| Err | string | Error message (will be returned to the client in case the plugin encounter an error. The string value supplied may appear in logs, so should not include confidential information) |",,,
b6698ad2abe06fa4bf867164c15b397aef9a753ef4db69f2feb7a077c239ed23,"Export to Docker with GitHub Actions
You may want your build result to be available in the Docker client through
docker images
to be able to use it in another step of your workflow:
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build
uses: docker/build-push-action@v6
with:
load: true
tags: myimage:latest
- name: Inspect
run: |
docker image inspect myimage:latest",,,
ed815d4ae2e38ca8b34aa74b90718e5e66f43b9ee82a4e26c8a8a8802093ef4a,"The build and publish process
This documentation is structured so that it matches the steps you need to take when creating your extension.
There are two main parts to creating a Docker extension:
- Build the foundations
- Publish the extension
Note
You do not need to pay to create a Docker extension. The Docker Extension SDK is licensed under the Apache 2.0 License and is free to use. Anyone can create new extensions and share them without constraints.
There is also no constraint on how each extension should be licensed, this is up to you to decide when creating a new extension.
Part one: Build the foundations
The build process consists of:
- Installing the latest version of Docker Desktop.
- Setting up the directory with files, including the extension’s source code and the required extension-specific files.
- Creating the
Dockerfile
to build, publish, and run your extension in Docker Desktop. - Configuring the metadata file which is required at the root of the image filesystem.
- Building and installing the extension.
For further inspiration, see the other examples in the samples folder.
Tip
Whilst creating your extension, make sure you follow the design and UI styling guidelines to ensure visual consistency and level AA accessibility standards.
Part two: Publish and distribute your extension
Docker Desktop displays published extensions in the Extensions Marketplace. The Extensions Marketplace is a curated space where developers can discover extensions to improve their developer experience and upload their own extension to share with the world.
If you want your extension published in the Marketplace, read the publish documentation.
Already built an extension?
Let us know about your experience using the feedback form.
What’s next?
If you want to get up and running with creating a Docker Extension, see the Quickstart guide.
Alternatively, get started with reading the ""Part one: Build"" section for more in-depth information about each step of the extension creation process.
For an in-depth tutorial of the entire build process, we recommend the following video walkthrough from DockerCon 2022.",,,
8db537fd1f1ef5e10e72816d5271462239737b4bea09f73ae46983a0326bce2e,"Set, use, and manage variables in a Compose file with interpolation
A Compose file can use variables to offer more flexibility. If you want to quickly switch between image tags to test multiple versions, or want to adjust a volume source to your local environment, you don't need to edit the Compose file each time, you can just set variables that insert values into your Compose file at run time.
Interpolation can also be used to insert values into your Compose file at run time, which is then used to pass variables into your container's environment
Below is a simple example:
$ cat .env
TAG=v1.5
$ cat compose.yaml
services:
web:
image: ""webapp:${TAG}""
When you run docker compose up
, the web
service defined in the Compose file
interpolates in the image webapp:v1.5
which was set in the .env
file. You can verify this with the
config command, which prints your resolved application config to the terminal:
$ docker compose config
services:
web:
image: 'webapp:v1.5'
Interpolation syntax
Interpolation is applied for unquoted and double-quoted values.
Both braced (${VAR}
) and unbraced ($VAR
) expressions are supported.
For braced expressions, the following formats are supported:
- Direct substitution
${VAR}
-> value ofVAR
- Default value
${VAR:-default}
-> value ofVAR
if set and non-empty, otherwisedefault
${VAR-default}
-> value ofVAR
if set, otherwisedefault
- Required value
${VAR:?error}
-> value ofVAR
if set and non-empty, otherwise exit with error${VAR?error}
-> value ofVAR
if set, otherwise exit with error
- Alternative value
${VAR:+replacement}
->replacement
ifVAR
is set and non-empty, otherwise empty${VAR+replacement}
->replacement
ifVAR
is set, otherwise empty
For more information, see Interpolation in the Compose Specification.
Ways to set variables with interpolation
Docker Compose can interpolate variables into your Compose file from multiple sources.
Note that when the same variable is declared by multiple sources, precedence applies:
- Variables from your shell environment
- If
--env-file
is not set, variables set by an.env
file in local working directory (PWD
) - Variables from a file set by
--env-file
or an.env
file in project directory
You can check variables and values used by Compose to interpolate the Compose model by running docker compose config --environment
.
.env
file
An .env
file in Docker Compose is a text file used to define variables that should be made available for interpolation when running docker compose up
. This file typically contains key-value pairs of variables, and it lets you centralize and manage configuration in one place. The .env
file is useful if you have multiple variables you need to store.
The .env
file is the default method for setting variables. The .env
file should be placed at the root of the project directory next to your compose.yaml
file. For more information on formatting an environment file, see
Syntax for environment files.
Basic example:
$ cat .env
## define COMPOSE_DEBUG based on DEV_MODE, defaults to false
COMPOSE_DEBUG=${DEV_MODE:-false}
$ cat compose.yaml
services:
webapp:
image: my-webapp-image
environment:
- DEBUG=${COMPOSE_DEBUG}
$ DEV_MODE=true docker compose config
services:
webapp:
environment:
DEBUG: ""true""
Additional information
If you define a variable in your
.env
file, you can reference it directly in yourcompose.yaml
with theenvironment
attribute. For example, if your.env
file contains the environment variableDEBUG=1
and yourcompose.yaml
file looks like this:services: webapp: image: my-webapp-image environment: - DEBUG=${DEBUG}
Docker Compose replaces
${DEBUG}
with the value from the.env
fileImportant
Be aware of Environment variables precedence when using variables in an
.env
file that as environment variables in your container's environment.You can place your
.env
file in a location other than the root of your project's directory, and then use the--env-file
option in the CLI so Compose can navigate to it.Your
.env
file can be overridden by another.env
if it is substituted with--env-file
.
Important
Substitution from
.env
files is a Docker Compose CLI feature.It is not supported by Swarm when running
docker stack deploy
.
.env
file syntax
The following syntax rules apply to environment files:
- Lines beginning with
#
are processed as comments and ignored. - Blank lines are ignored.
- Unquoted and double-quoted (
""
) values have interpolation applied. - Each line represents a key-value pair. Values can optionally be quoted.
VAR=VAL
->VAL
VAR=""VAL""
->VAL
VAR='VAL'
->VAL
- Inline comments for unquoted values must be preceded with a space.
VAR=VAL # comment
->VAL
VAR=VAL# not a comment
->VAL# not a comment
- Inline comments for quoted values must follow the closing quote.
VAR=""VAL # not a comment""
->VAL # not a comment
VAR=""VAL"" # comment
->VAL
- Single-quoted (
'
) values are used literally.VAR='$OTHER'
->$OTHER
VAR='${OTHER}'
->${OTHER}
- Quotes can be escaped with
\
.VAR='Let\'s go!'
->Let's go!
VAR=""{\""hello\"": \""json\""}""
->{""hello"": ""json""}
- Common shell escape sequences including
\n
,\r
,\t
, and\\
are supported in double-quoted values.VAR=""some\tvalue""
->some value
VAR='some\tvalue'
->some\tvalue
VAR=some\tvalue
->some\tvalue
Substitute with --env-file
You can set default values for multiple environment variables, in an .env
file and then pass the file as an argument in the CLI.
The advantage of this method is that you can store the file anywhere and name it appropriately, for example,
This file path is relative to the current working directory where the Docker Compose command is executed. Passing the file path is done using the --env-file
option:
$ docker compose --env-file ./config/.env.dev up
Additional information
- This method is useful if you want to temporarily override an
.env
file that is already referenced in yourcompose.yaml
file. For example you may have different.env
files for production (.env.prod
) and testing (.env.test
). In the following example, there are two environment files,.env
and.env.dev
. Both have different values set forTAG
.If the$ cat .env TAG=v1.5 $ cat ./config/.env.dev TAG=v1.6 $ cat compose.yaml services: web: image: ""webapp:${TAG}""
--env-file
is not used in the command line, the.env
file is loaded by default:Passing the$ docker compose config services: web: image: 'webapp:v1.5'
--env-file
argument overrides the default file path:When an invalid file path is being passed as an$ docker compose --env-file ./config/.env.dev config services: web: image: 'webapp:v1.6'
--env-file
argument, Compose returns an error:$ docker compose --env-file ./doesnotexist/.env.dev config ERROR: Couldn't find env file: /home/user/./doesnotexist/.env.dev
- You can use multiple
--env-file
options to specify multiple environment files, and Docker Compose reads them in order. Later files can override variables from earlier files.$ docker compose --env-file .env --env-file .env.override up
- You can override specific environment variables from the command line when starting containers.
$ docker compose --env-file .env.dev up -e DATABASE_URL=mysql://new_user:new_password@new_db:3306/new_database
local .env
file versus <project directory> .env
file
An .env
file can also be used to declare
pre-defined environment variables used to control Compose behavior and files to be loaded.
When executed without an explicit --env-file
flag, Compose searches for an .env
file in your working directory (
PWD) and loads values
both for self-configuration and interpolation. If the values in this file define the COMPOSE_FILE
pre-defined variable, which results in a project directory being set to another folder,
Compose will load a second .env
file, if present. This second .env
file has a lower precedence.
This mechanism makes it possible to invoke an existing Compose project with a custom set of variables as overrides, without the need to pass environment variables by the command line.
$ cat .env
COMPOSE_FILE=../compose.yaml
POSTGRES_VERSION=9.3
$ cat ../compose.yaml
services:
db:
image: ""postgres:${POSTGRES_VERSION}""
$ cat ../.env
POSTGRES_VERSION=9.2
$ docker compose config
services:
db:
image: ""postgres:9.3""
Substitute from the shell
You can use existing environment variables from your host machine or from the shell environment where you execute docker compose
commands. This lets you dynamically inject values into your Docker Compose configuration at runtime.
For example, suppose the shell contains POSTGRES_VERSION=9.3
and you supply the following configuration:
db:
image: ""postgres:${POSTGRES_VERSION}""
When you run docker compose up
with this configuration, Compose looks for the POSTGRES_VERSION
environment variable in the shell and substitutes its value in. For this example, Compose resolves the image to postgres:9.3
before running the configuration.
If an environment variable is not set, Compose substitutes with an empty string. In the previous example, if POSTGRES_VERSION
is not set, the value for the image option is postgres:
.
Note
postgres:
is not a valid image reference. Docker expects either a reference without a tag, likepostgres
which defaults to the latest image, or with a tag such aspostgres:15
.",,,
2ac13f5975442ce393e5201ee4c1c2042fd1d06985988dde582952d067286a5e,"Single sign-on overview
Single sign-on (SSO) lets users access Docker by authenticating using their identity providers (IdPs). SSO is available for a whole company, and all associated organizations within that company, or an individual organization that has a Docker Business subscription. To upgrade your existing account to a Docker Business subscription, see Upgrade your subscription.
How SSO works
When you enable SSO, Docker supports a non-IdP-initiated SSO flow for user login. Instead of users authenticating using their Docker username and password, they are redirected to your identity provider's authentication page to sign in. Users must sign in to Docker Hub or Docker Desktop to initiate the SSO authentication process.
The following diagram shows how SSO operates and is managed in Docker Hub and Docker Desktop. In addition, it provides information on how to authenticate between your IdP.
How to set it up
SSO is configured using the following steps:
- Configure SSO by creating and verifying a domain in Docker.
- Create your SSO connection in Docker and your IdP.
- Cross-connect Docker and your IdP.
- Test your connection.
- Provision users.
- Optional. Enforce sign-in.
- Manage your SSO configuration.
Once your SSO configuration is complete, a first-time user can sign in to Docker Hub or Docker Desktop using their company's domain email address. Once they sign in, they are added to your company, assigned to an organization, and if necessary, assigned to a team.
Prerequisites
Before configuring SSO, ensure you meet the following prerequisites:
- Notify your company about the new SSO sign in procedures.
- Verify that all users have Docker Desktop version 4.4.2 or later installed.
- If your organization is planning to enforce SSO, members using the Docker CLI are required to create a Personal Access Token (PAT). The PAT will be used instead of their username and password. Docker plans to deprecate signing in to the CLI with a password in the future, so using a PAT will be required to prevent issues with authentication. For more details see the security announcement.
- Ensure all your Docker users have a valid user on your IdP with the same email address as their Unique Primary Identifier (UPN).
- Confirm that all CI/CD pipelines have replaced their passwords with PATs.
- For your service accounts, add your additional domains or enable it in your IdP.
What's next?
- Start configuring SSO in Docker
- Explore the FAQs",,,
20494ba0812628fae6d25b7856f174387f65b8d8fe23a3f9fcbcdd6f89bd66bf,"Navigation
Table of contents
ddClient.desktopUI.navigate
enables navigation to specific screens of Docker Desktop such as the containers tab, the images tab, or a specific container's logs.
For example, navigate to a given container logs:
const id = '8c7881e6a107';
try {
await ddClient.desktopUI.navigate.viewContainerLogs(id);
} catch (e) {
console.error(e);
ddClient.desktopUI.toast.error(
`Failed to navigate to logs for container ""${id}"".`
);
}
Parameters
| Name | Type | Description |
|---|---|---|
id | string | The full container id, e.g. 46b57e400d801762e9e115734bf902a2450d89669d85881058a46136520aca28 . You can use the --no-trunc flag as part of the docker ps command to display the full container id. |
Returns
Promise
<void
>
A promise that fails if the container doesn't exist.
For more details about all navigation methods, see the Navigation API reference.
Deprecated navigation methods
These methods are deprecated and will be removed in a future version. Use the methods specified above.
window.ddClient.navigateToContainers();
// id - the full container id, e.g. `46b57e400d801762e9e115734bf902a2450d89669d85881058a46136520aca28`
window.ddClient.navigateToContainer(id);
window.ddClient.navigateToContainerLogs(id);
window.ddClient.navigateToContainerInspect(id);
window.ddClient.navigateToContainerStats(id);
window.ddClient.navigateToImages();
window.ddClient.navigateToImage(id, tag);
window.ddClient.navigateToVolumes();
window.ddClient.navigateToVolume(volume);
window.ddClient.navigateToDevEnvironments();",,,
7d7ebeccf461918182736fd0da79dbdf428fcf879ac02207d40036b59c143749,"Billing FAQs
What credit and debit cards are supported?
- Visa
- MasterCard
- American Express
- Discover
- JCB
- Diners
- UnionPay
What currency is supported?
United States dollar (USD).
What happens if my subscription payment fails?
If your subscription payment fails, there is a grace period of 15 days, including the due date. Docker retries to collect the payment 3 times using the following schedule:
- 3 days after the due date
- 5 days after the previous attempt
- 7 days after the previous attempt
Docker also sends an email notification Action Required - Credit Card Payment Failed
with an attached unpaid invoice after each failed payment attempt.
Once the grace period is over and the invoice is still not paid, the subscription downgrades to a free plan and all paid features are disabled.
Does Docker collect sales tax and/or VAT?
Docker began collecting sales tax on subscription fees for United States customers on July 1, 2024. For European customers, Docker will begin collecting VAT on March 1, 2025.
To ensure that tax assessments are correct, make sure that your billing information and VAT/Tax ID, if applicable, are updated. See Update the billing information.
How do I certify my tax exempt status?
If you're exempt from sales tax, you can register a valid tax exemption certificate with Docker's Support team. Contact Support to get started.
Does Docker offer academic pricing?
Contact the Docker Sales Team.
Do I need to do anything at the end of my subscription term?
No. All monthly and annual subscriptions are automatically renewed at the end of the term using the original form of payment.",,,
74751f53c266ecdb973b34ba1ae7d5fa6b0f7d1e44b172622d74322609884b05,"Limitations
ECI support for WSL
Note
Docker Desktop requires WSL 2 version 1.1.3.0 or later. To get the current version of WSL on your host, type
wsl --version
. If the command fails or if it returns a version number prior to 1.1.3.0, update WSL to the latest version by typingwsl --update
in a Windows command or PowerShell terminal.
ECI on WSL is not as secure as on Hyper-V because:
While ECI on WSL still hardens containers so that malicious workloads can't easily breach Docker Desktop's Linux VM, ECI on WSL can't prevent Docker Desktop users from breaching the Docker Desktop Linux VM. Such users can trivially access that VM (as root) with the
wsl -d docker-desktop
command, and use that access to modify Docker Engine settings inside the VM. This gives Docker Desktop users control of the Docker Desktop VM and lets them bypass Docker Desktop configs set by administrators via the settings-management feature. In contrast, ECI on Hyper-V does not let Docker Desktop users to breach the Docker Desktop Linux VM.With WSL 2, all WSL 2 distributions on the same Windows host share the same instance of the Linux kernel. As a result, Docker Desktop can't ensure the integrity of the kernel in the Docker Desktop Linux VM since another WSL 2 distribution could modify shared kernel settings. In contrast, when using Hyper-V, the Docker Desktop Linux VM has a dedicated kernel that is solely under the control of Docker Desktop.
The following table summarizes this.
| Security feature | ECI on WSL | ECI on Hyper-V | Comment |
|---|---|---|---|
| Strongly secure containers | Yes | Yes | Makes it harder for malicious container workloads to breach the Docker Desktop Linux VM and host. |
| Docker Desktop Linux VM protected from user access | No | Yes | On WSL, users can access Docker Engine directly or bypass Docker Desktop security settings. |
| Docker Desktop Linux VM has a dedicated kernel | No | Yes | On WSL, Docker Desktop can't guarantee the integrity of kernel level configs. |
In general, using ECI with Hyper-V is more secure than with WSL 2. But WSL 2 offers advantages for performance and resource utilization on the host machine, and it's an excellent way for users to run their favorite Linux distribution on Windows hosts and access Docker from within.
ECI protection for Docker builds with the ""docker"" driver
Prior to Docker Desktop 4.30, docker build
commands that use the buildx
docker
driver (the default) are not protected by ECI, in other words the build runs
rootful inside the Docker Desktop VM.
Starting with Docker Desktop 4.30, docker build
commands that use the buildx
docker
driver are protected by ECI, except when Docker Desktop is configured to use WSL 2
(on Windows hosts).
Note that docker build
commands that use the docker-container
driver are
always protected by ECI.
Docker Build and Buildx have some restrictions
With ECI enabled, Docker build --network=host
and Docker Buildx entitlements
(network.host
, security.insecure
) are not allowed. Builds that require
these won't work properly.
Kubernetes pods are not yet protected
When using the Docker Desktop integrated Kubernetes, pods are not yet protected by ECI. Therefore a malicious or privileged pod can compromise the Docker Desktop Linux VM and bypass security controls.
As an alternative, you can use the K8s.io KinD tool with ECI. In this case, each Kubernetes node runs inside an ECI-protected container, thereby more strongly isolating the Kubernetes cluster away from the underlying Docker Desktop Linux VM (and Docker Engine within). No special arrangements are needed, just enable ECI and run the KinD tool as usual.
Extension containers are not yet protected
Extension containers are also not yet protected by ECI. Ensure you extension containers come from trusted entities to avoid issues.
Docker Desktop Dev Environments are not yet protected
Containers launched by the Docker Desktop Dev Environments feature are not yet protected.
Docker Debug containers are not yet protected
Docker Debug containers are not yet protected by ECI.
Native Windows containers are not supported
ECI only works when Docker Desktop is in Linux containers mode (the default, most common mode). It's not supported when Docker Desktop is configured in native Windows containers mode (i.e., it's not supported on Windows hosts, when Docker Desktop is switched from its default Linux mode to native Windows mode).
Use in production
In general users should not experience differences between running a container
in Docker Desktop with ECI enabled, which uses the Sysbox runtime, and running
that same container in production, through the standard OCI runc
runtime.
However in some cases, typically when running advanced or privileged workloads in
containers, users may experience some differences. In particular, the container
may run with ECI but not with runc
, or vice-versa.",,,
82ffe373e67911776e836ac6f0ce511e21fa3775829e0a5e520ef20ec2b36b6e,"Authentication
Note
This page assumes that you already have an Identity Provider (IdP), such as Google, Entra ID (formerly Azure AD) or Okta, which handles the authentication process and returns an access token.
Learn how you can let users authenticate from your extension using OAuth 2.0 via a web browser, and return to your extension.
In OAuth 2.0, the term ""grant type"" refers to the way an application gets an access token. Although OAuth 2.0 defines several grant types, this page only describes how to authorize users from your extension using the Authorization Code grant type.
Authorization code grant flow
The Authorization Code grant type is used by confidential and public clients to exchange an authorization code for an access token.
After the user returns to the client via the redirect URL, the application gets the authorization code from the URL and uses it to request an access token.
The image above shows that:
- The Docker extension asks the user to authorize access to their data.
- If the user grants access, the extension then requests an access token from the service provider, passing the access grant from the user and authentication details to identify the client.
- The service provider then validates these details and returns an access token.
- The extension uses the access token to request the user data with the service provider.
OAuth 2.0 terminology
- Auth URL: The endpoint for the API provider authorization server, to retrieve the auth code.
- Redirect URI: The client application callback URL to redirect to after auth. This must be registered with the API provider.
Once the user enters the username and password, they're successfully authenticated.
Open a browser page to authenticate the user
From the extension UI, you can provide a button that, when selected, opens a new window in a browser to authenticate the user.
Use the ddClient.host.openExternal API to open a browser to the auth URL. For example:
window.ddClient.openExternal(""https://authorization-server.com/authorize?
response_type=code
&client_id=T70hJ3ls5VTYG8ylX3CZsfIu
&redirect_uri=${REDIRECT_URI});
Get the authorization code and access token
You can get the authorization code from the extension UI by listing docker-desktop://dashboard/extension-tab?extensionId=awesome/my-extension
as the redirect_uri
in the OAuth app you're using and concatenating the authorization code as a query parameter. The extension UI code will then be able to read the corresponding code query-param.
Important
Using this feature requires the extension SDK 0.3.3 in Docker Desktop. You need to ensure that the required SDK version for your extension set with
com.docker.desktop.extension.api.version
in image labels is higher than 0.3.3.
Authorization
This step is where the user enters their credentials in the browser. After the authorization is complete, the user is redirected back to your extension user interface, and the extension UI code can consume the authorization code that's part of the query parameters in the URL.
Exchange the Authorization Code
Next, you exchange the authorization code for an access token.
The extension must send a POST
request to the 0Auth authorization server with the following parameters:
POST https://authorization-server.com/token
&client_id=T70hJ3ls5VTYG8ylX3CZsfIu
&client_secret=YABbyHQShPeO1T3NDQZP8q5m3Jpb_UPNmIzqhLDCScSnRyVG
&redirect_uri=${REDIRECT_URI}
&code=N949tDLuf9ai_DaOKyuFBXStCNMQzuQbtC1QbvLv-AXqPJ_f
Note
The client's credentials are included in the
POST
query params in this example. OAuth authorization servers may require that the credentials are sent as a HTTP Basic Authentication header or might support different formats. See your OAuth provider docs for details.
Store the access token
The Docker Extensions SDK doesn't provide a specific mechanism to store secrets.
It's highly recommended that you use an external source of storage to store the access token.
Note
The user interface Local Storage is isolated between extensions (an extension can't access another extension's local storage), and each extension's local storage gets deleted when users uninstall an extension.
What's next
Learn how to publish and distribute your extension",,,
43946166eb9f3d9dd57a54554cc16bcfa08a1f7278714da8f287111ad877fa12,"Subscription FAQs
Table of contents
For more information on Docker subscriptions, see Docker subscription overview.
Can I transfer my subscription from one user or organization account to another?
Subscriptions are non-transferable.
Can I pause or delay my Docker subscription?
You can't pause or delay a subscription, but you can downgrade. If a subscription invoice hasn't been paid on the due date, there's a 15 day grace period, including the due date.
Does Docker offer academic pricing?
Contact the Docker Sales Team.
What ways can I contribute to Docker content?
Docker offers two programs:
You can also join the Developer Preview Program or sign up for early access programs for specific products to participate in research and try out new features.",,,
fbb60d634fc5e0399b310d2843cf3b343e37bf3bdc4264d7be6f0b10e361d33f,"Install Docker Desktop on Debian
Docker Desktop terms
Commercial use of Docker Desktop in larger enterprises (more than 250 employees OR more than $10 million USD in annual revenue) requires a paid subscription.
This page contains information on how to install, launch, and upgrade Docker Desktop on a Debian distribution.
Prerequisites
To install Docker Desktop successfully, you must:
Meet the general system requirements.
Have a 64-bit version of Debian 12.
For a Gnome Desktop environment, you must also install AppIndicator and KStatusNotifierItem Gnome extensions.
For non-Gnome Desktop environments,
gnome-terminal
must be installed:$ sudo apt install gnome-terminal
Install Docker Desktop
Recommended approach to install Docker Desktop on Debian:
Set up Docker's
apt
repository. See step one of Install using theapt
repository.Download the latest DEB package. For checksums, see the Release notes.
Install the package with apt as follows:
$ sudo apt-get update
$ sudo apt-get install ./docker-desktop-amd64.deb
Note
At the end of the installation process,
apt
displays an error due to installing a downloaded package. You can ignore this error message.N: Download is performed unsandboxed as root, as file '/home/user/Downloads/docker-desktop.deb' couldn't be accessed by user '_apt'. - pkgAcquire::Run (13: Permission denied)
By default, Docker Desktop is installed at /opt/docker-desktop
.
There are a few post-install configuration steps done through the post-install script contained in the deb package.
The post-install script:
- Sets the capability on the Docker Desktop binary to map privileged ports and set resource limits.
- Adds a DNS name for Kubernetes to
/etc/hosts
. - Creates a symlink from
/usr/local/bin/com.docker.cli
to/usr/bin/docker
. This is because the classic Docker CLI is installed at/usr/bin/docker
. The Docker Desktop installer also installs a Docker CLI binary that includes cloud-integration capabilities and is essentially a wrapper for the Compose CLI, at/usr/local/bin/com.docker.cli
. The symlink ensures that the wrapper can access the classic Docker CLI.
Launch Docker Desktop
To start Docker Desktop for Linux:
Navigate to the Docker Desktop application in your Gnome/KDE Desktop.
Select Docker Desktop to start Docker.
The Docker Subscription Service Agreement displays.
Select Accept to continue. Docker Desktop starts after you accept the terms.
Note that Docker Desktop won't run if you do not agree to the terms. You can choose to accept the terms at a later date by opening Docker Desktop.
For more information, see Docker Desktop Subscription Service Agreement. It is recommended that you also read the FAQs.
Alternatively, open a terminal and run:
$ systemctl --user start docker-desktop
When Docker Desktop starts, it creates a dedicated context that the Docker CLI can use as a target and sets it as the current context in use. This is to avoid a clash with a local Docker Engine that may be running on the Linux host and using the default context. On shutdown, Docker Desktop resets the current context to the previous one.
The Docker Desktop installer updates Docker Compose and the Docker CLI binaries
on the host. It installs Docker Compose V2 and gives users the choice to
link it as docker-compose from the Settings panel. Docker Desktop installs
the new Docker CLI binary that includes cloud-integration capabilities in /usr/local/bin/com.docker.cli
and creates a symlink to the classic Docker CLI at /usr/local/bin
.
After you’ve successfully installed Docker Desktop, you can check the versions of these binaries by running the following commands:
$ docker compose version
Docker Compose version v2.29.1
$ docker --version
Docker version 27.1.1, build 6312585
$ docker version
Client:
Version: 23.0.5
API version: 1.42
Go version: go1.21.12
<...>
To enable Docker Desktop to start on sign in, from the Docker menu, select Settings > General > Start Docker Desktop when you sign in to your computer.
Alternatively, open a terminal and run:
$ systemctl --user enable docker-desktop
To stop Docker Desktop, select the Docker menu icon to open the Docker menu and select Quit Docker Desktop.
Alternatively, open a terminal and run:
$ systemctl --user stop docker-desktop
Upgrade Docker Desktop
Once a new version for Docker Desktop is released, the Docker UI shows a notification. You need to download the new package each time you want to upgrade Docker Desktop and run:
$ sudo apt-get install ./docker-desktop-amd64.deb
Next steps
- Explore Docker's subscriptions to see what Docker can offer you.
- Take a look at the Docker workshop to learn how to build an image and run it as a containerized application.
- Explore Docker Desktop and all its features.
- Troubleshooting describes common problems, workarounds, how to run and submit diagnostics, and submit issues.
- FAQs provide answers to frequently asked questions.
- Release notes lists component updates, new features, and improvements associated with Docker Desktop releases.
- Back up and restore data provides instructions on backing up and restoring data related to Docker.",,,
9411b6dd9ac371c317916de348c683a457390ec23f6fd2369f7fd017283d030c,"Journald logging driver
The journald
logging driver sends container logs to the
systemd
journal.
Log entries can be retrieved using the journalctl
command, through use of the
journal
API, or using the docker logs
command.
In addition to the text of the log message itself, the journald
log driver
stores the following metadata in the journal with each message:
| Field | Description |
|---|---|
CONTAINER_ID | The container ID truncated to 12 characters. |
CONTAINER_ID_FULL | The full 64-character container ID. |
CONTAINER_NAME | The container name at the time it was started. If you use docker rename to rename a container, the new name isn't reflected in the journal entries. |
CONTAINER_TAG , SYSLOG_IDENTIFIER | The container tag ( log tag option documentation). |
CONTAINER_PARTIAL_MESSAGE | A field that flags log integrity. Improve logging of long log lines. |
IMAGE_NAME | The name of the container image. |
Usage
To use the journald
driver as the default logging driver, set the log-driver
and log-opts
keys to appropriate values in the daemon.json
file, which is
located in /etc/docker/
on Linux hosts or
C:\ProgramData\docker\config\daemon.json
on Windows Server. For more about
configuring Docker using daemon.json
, see
daemon.json.
The following example sets the log driver to journald
:
{
""log-driver"": ""journald""
}
Restart Docker for the changes to take effect.
To configure the logging driver for a specific container, use the --log-driver
flag on the docker run
command.
$ docker run --log-driver=journald ...
Options
Use the --log-opt NAME=VALUE
flag to specify additional journald
logging
driver options.
| Option | Required | Description |
|---|---|---|
tag | optional | Specify template to set CONTAINER_TAG and SYSLOG_IDENTIFIER value in journald logs. Refer to
log tag option documentation to customize the log tag format. |
labels | optional | Comma-separated list of keys of labels, which should be included in message, if these labels are specified for the container. |
labels-regex | optional | Similar to and compatible with labels. A regular expression to match logging-related labels. Used for advanced log tag options. |
env | optional | Comma-separated list of keys of environment variables, which should be included in message, if these variables are specified for the container. |
env-regex | optional | Similar to and compatible with env . A regular expression to match logging-related environment variables. Used for advanced
log tag options. |
If a collision occurs between label
and env
options, the value of the env
takes precedence. Each option adds additional fields to the attributes of a
logging message.
The following is an example of the logging options required to log to journald.
$ docker run \
--log-driver=journald \
--log-opt labels=location \
--log-opt env=TEST \
--env ""TEST=false"" \
--label location=west \
your/application
This configuration also directs the driver to include in the payload the label
location, and the environment variable TEST
. If the --env ""TEST=false""
or --label location=west
arguments were omitted, the corresponding key would
not be set in the journald log.
Note regarding container names
The value logged in the CONTAINER_NAME
field is the name of the container that
was set at startup. If you use docker rename
to rename a container, the new
name isn't reflected in the journal entries. Journal entries continue
to use the original name.
Retrieve log messages with journalctl
Use the journalctl
command to retrieve log messages. You can apply filter
expressions to limit the retrieved messages to those associated with a specific
container:
$ sudo journalctl CONTAINER_NAME=webserver
You can use additional filters to further limit the messages retrieved. The -b
flag only retrieves messages generated since the last system boot:
$ sudo journalctl -b CONTAINER_NAME=webserver
The -o
flag specifies the format for the retrieved log messages. Use -o json
to return the log messages in JSON format.
$ sudo journalctl -o json CONTAINER_NAME=webserver
View logs for a container with a TTY enabled
If TTY is enabled on a container you may see [10B blob data]
in the output
when retrieving log messages.
The reason for that is that \r
is appended to the end of the line and
journalctl
doesn't strip it automatically unless --all
is set:
$ sudo journalctl -b CONTAINER_NAME=webserver --all
Retrieve log messages with the journal
API
This example uses the systemd
Python module to retrieve container
logs:
import systemd.journal
reader = systemd.journal.Reader()
reader.add_match('CONTAINER_NAME=web')
for msg in reader:
print '{CONTAINER_ID_FULL}: {MESSAGE}'.format(**msg)",,,
bd427995026b30c8e9104f073393197643e10a70cfd310192097baa188019ab8,"Give feedback
Table of contents
There are many ways you can provide feedback on Docker Desktop or Docker Desktop features.
In-product feedback
On each Docker Desktop Dashboard view, there is a Give feedback link. This sends you to a Google feedback form where you can share your feedback and ideas.
You can also use the docker feedback
command to submit feedback directly from the command line.
Feedback via Docker Community forums
To get help from the community, review current user topics, join or start a discussion, sign in to the appropriate Docker forums:
Report bugs or problems on GitHub
To report bugs or problems, visit:
- Docker Desktop for Mac issues on GitHub
- Docker Desktop for Windows issues on GitHub
- Docker Desktop for Linux issues on GitHub
- Dev Environments issues on Github
- Docker Extensions issues on GitHub
Feedback via Community Slack channels
You can also provide feedback through the following Docker Community Slack channels:
- #docker-desktop-mac
- #docker-desktop-windows
- #docker-desktop-linux
- #docker-dev-environments
- #extensions",,,
bb6795309841e0aaebb292db0735792dafc9cedb58a140494d5da105f0161507,"Docker Desktop WSL 2 backend on Windows
Windows Subsystem for Linux (WSL) 2 is a full Linux kernel built by Microsoft, which lets Linux distributions run without managing virtual machines. With Docker Desktop running on WSL 2, users can leverage Linux workspaces and avoid maintaining both Linux and Windows build scripts. In addition, WSL 2 provides improvements to file system sharing and boot time.
Docker Desktop uses the dynamic memory allocation feature in WSL 2 to improve the resource consumption. This means Docker Desktop only uses the required amount of CPU and memory resources it needs, while allowing CPU and memory-intensive tasks such as building a container, to run much faster.
Additionally, with WSL 2, the time required to start a Docker daemon after a cold start is significantly faster.
Prerequisites
Before you turn on the Docker Desktop WSL 2 feature, ensure you have:
- At a minimum WSL version 1.1.3.0., but ideally the latest version of WSL to avoid Docker Desktop not working as expected.
- Met the Docker Desktop for Windows' system requirements.
- Installed the WSL 2 feature on Windows. For detailed instructions, refer to the Microsoft documentation.
Tip
For a better experience on WSL, consider enabling the WSL autoMemoryReclaim setting available since WSL 1.3.10 (experimental).
This feature enhances the Windows host's ability to reclaim unused memory within the WSL virtual machine, ensuring improved memory availability for other host applications. This capability is especially beneficial for Docker Desktop, as it prevents the WSL VM from retaining large amounts of memory (in GBs) within the Linux kernel's page cache during Docker container image builds, without releasing it back to the host when no longer needed within the VM.
Turn on Docker Desktop WSL 2
Important
To avoid any potential conflicts with using WSL 2 on Docker Desktop, you must uninstall any previous versions of Docker Engine and CLI installed directly through Linux distributions before installing Docker Desktop.
Download and install the latest version of Docker Desktop for Windows.
Follow the usual installation instructions to install Docker Desktop. Depending on which version of Windows you are using, Docker Desktop may prompt you to turn on WSL 2 during installation. Read the information displayed on the screen and turn on the WSL 2 feature to continue.
Start Docker Desktop from the Windows Start menu.
Navigate to Settings.
From the General tab, select Use WSL 2 based engine..
If you have installed Docker Desktop on a system that supports WSL 2, this option is turned on by default.
Select Apply & Restart.
Now docker
commands work from Windows using the new WSL 2 engine.
Tip
By default, Docker Desktop stores the data for the WSL 2 engine at
C:\Users\[USERNAME]\AppData\Local\Docker\wsl
. If you want to change the location, for example, to another drive you can do so via theSettings -> Resources -> Advanced
page from the Docker Dashboard. Read more about this and other Windows settings at Changing settings
Enabling Docker support in WSL 2 distributions
WSL 2 adds support for ""Linux distributions"" to Windows, where each distribution behaves like a VM except they all run on top of a single shared Linux kernel.
Docker Desktop does not require any particular Linux distributions to be installed. The docker
CLI and UI all work fine from Windows without any additional Linux distributions. However for the best developer experience, we recommend installing at least one additional distribution and enable Docker support:
Ensure the distribution runs in WSL 2 mode. WSL can run distributions in both v1 or v2 mode.
To check the WSL mode, run:
$ wsl.exe -l -v
To upgrade the Linux distribution to v2, run:
$ wsl.exe --set-version (distribution name) 2
To set v2 as the default version for future installations, run:
$ wsl.exe --set-default-version 2
When Docker Desktop starts, go to Settings > Resources > WSL Integration.
The Docker-WSL integration is enabled on the default WSL distribution, which is Ubuntu. To change your default WSL distribution, run:
$ wsl --set-default <distribution name>
If WSL integrations isn't available under Resources, Docker may be in Windows container mode. In your taskbar, select the Docker menu and then Switch to Linux containers.
Select Apply & Restart.
Note
With Docker Desktop version 4.30 and earlier, Docker Desktop installed two special-purpose internal Linux distributions
docker-desktop
anddocker-desktop-data
.docker-desktop
is used to run the Docker enginedockerd
, whiledocker-desktop-data
stores containers and images. Neither can be used for general development.With fresh installations of Docker Desktop 4.30 and later,
docker-desktop-data
is no longer created. Instead, Docker Desktop creates and manages its own virtual hard disk for storage. Thedocker-desktop
distribution is still created and used to run the Docker engine.Note that Docker Desktop version 4.30 and later keeps using the
docker-desktop-data
distribution if it was already created by an earlier version of Docker Desktop and has not been freshly installed or factory reset.
WSL 2 security in Docker Desktop
Docker Desktop’s WSL 2 integration operates within the existing security model of WSL and does not introduce additional security risks beyond standard WSL behavior.
Docker Desktop runs within its own dedicated WSL distribution, docker-desktop
, which follows the same isolation properties as any other WSL distribution. The only interaction between Docker Desktop and other installed WSL distributions occurs when the Docker Desktop WSL integration feature is enabled in settings. This feature allows easy access to the Docker CLI from integrated distributions.
WSL is designed to facilitate interoperability between Windows and Linux environments. Its file system is accessible from the Windows host \\wsl$
, meaning Windows processes can read and modify files within WSL. This behavior is not specific to Docker Desktop, but rather a core aspect of WSL itself.
For organizations concerned about security risks related to WSL and want stricter isolation and security controls, run Docker Desktop in Hyper-V mode instead of WSL 2. Alternatively, run your container workloads with Enhanced Container Isolation enabled.",,,
76890150dd36a6aa540594b4abbebb9e227817b12157a3f79a7af91e2ada7326,"Docker for GitHub Copilot
The Docker for GitHub Copilot extension integrates Docker's capabilities with GitHub Copilot, providing assistance with containerizing applications, generating Docker assets, and analyzing project vulnerabilities. This extension helps you streamline Docker-related tasks wherever GitHub Copilot Chat is available.
Key features
Key features of the Docker for GitHub Copilot extension include:
- Ask questions and receive responses about containerization in any context where GitHub Copilot Chat is available, such as on GitHub.com and in Visual Studio Code.
- Automatically generate Dockerfiles, Docker Compose files, and
.dockerignore
files for a project. - Open pull requests with generated Docker assets directly from the chat interface.
- Get summaries of project vulnerabilities from Docker Scout and receive next steps via the CLI.
Data Privacy
The Docker agent is trained exclusively on Docker's documentation and tools to assist with containerization and related tasks. It does not have access to your project's data outside the context of the questions you ask.
When using the Docker Extension for GitHub Copilot, GitHub Copilot may include a reference to the currently open file in its request if authorized by the user. The Docker agent can read the file to provide context-aware responses.
If the agent is requested to check for vulnerabilities or generate Docker-related assets, it will clone the referenced repository into in-memory storage to perform the necessary actions.
Source code or project metadata is never persistently stored. Questions and answers are retained for analytics and troubleshooting. Data processed by the Docker agent is never shared with third parties.
Supported languages
The Docker Extension for GitHub Copilot supports the following programming languages for tasks involving containerizing a project from scratch:
- Go
- Java
- JavaScript
- Python
- Rust
- TypeScript",,,
f4fa4c61d0c845bc211d2693eac132323ff2e7e6cd3fb06819f01af0be01284c,"AppArmor security profiles for Docker
AppArmor (Application Armor) is a Linux security module that protects an operating system and its applications from security threats. To use it, a system administrator associates an AppArmor security profile with each program. Docker expects to find an AppArmor policy loaded and enforced.
Docker automatically generates and loads a default profile for containers named
docker-default
. The Docker binary generates this profile in tmpfs
and then
loads it into the kernel.
Note
This profile is used on containers, not on the Docker daemon.
A profile for the Docker Engine daemon exists but it is not currently installed
with the deb
packages. If you are interested in the source for the daemon
profile, it is located in
contrib/apparmor
in the Docker Engine source repository.
Understand the policies
The docker-default
profile is the default for running containers. It is
moderately protective while providing wide application compatibility. The
profile is generated from the following
template.
When you run a container, it uses the docker-default
policy unless you
override it with the security-opt
option. For example, the following
explicitly specifies the default policy:
$ docker run --rm -it --security-opt apparmor=docker-default hello-world
Load and unload profiles
To load a new profile into AppArmor for use with containers:
$ apparmor_parser -r -W /path/to/your_profile
Then, run the custom profile with --security-opt
:
$ docker run --rm -it --security-opt apparmor=your_profile hello-world
To unload a profile from AppArmor:
# unload the profile
$ apparmor_parser -R /path/to/profile
Resources for writing profiles
The syntax for file globbing in AppArmor is a bit different than some other globbing implementations. It is highly suggested you take a look at some of the below resources with regard to AppArmor profile syntax.
Nginx example profile
In this example, you create a custom AppArmor profile for Nginx. Below is the custom profile.
#include <tunables/global>
profile docker-nginx flags=(attach_disconnected,mediate_deleted) {
#include <abstractions/base>
network inet tcp,
network inet udp,
network inet icmp,
deny network raw,
deny network packet,
file,
umount,
deny /bin/** wl,
deny /boot/** wl,
deny /dev/** wl,
deny /etc/** wl,
deny /home/** wl,
deny /lib/** wl,
deny /lib64/** wl,
deny /media/** wl,
deny /mnt/** wl,
deny /opt/** wl,
deny /proc/** wl,
deny /root/** wl,
deny /sbin/** wl,
deny /srv/** wl,
deny /tmp/** wl,
deny /sys/** wl,
deny /usr/** wl,
audit /** w,
/var/run/nginx.pid w,
/usr/sbin/nginx ix,
deny /bin/dash mrwklx,
deny /bin/sh mrwklx,
deny /usr/bin/top mrwklx,
capability chown,
capability dac_override,
capability setuid,
capability setgid,
capability net_bind_service,
deny @{PROC}/* w, # deny write for all files directly in /proc (not in a subdir)
# deny write to files not in /proc/<number>/** or /proc/sys/**
deny @{PROC}/{[^1-9],[^1-9][^0-9],[^1-9s][^0-9y][^0-9s],[^1-9][^0-9][^0-9][^0-9]*}/** w,
deny @{PROC}/sys/[^k]** w, # deny /proc/sys except /proc/sys/k* (effectively /proc/sys/kernel)
deny @{PROC}/sys/kernel/{?,??,[^s][^h][^m]**} w, # deny everything except shm* in /proc/sys/kernel/
deny @{PROC}/sysrq-trigger rwklx,
deny @{PROC}/mem rwklx,
deny @{PROC}/kmem rwklx,
deny @{PROC}/kcore rwklx,
deny mount,
deny /sys/[^f]*/** wklx,
deny /sys/f[^s]*/** wklx,
deny /sys/fs/[^c]*/** wklx,
deny /sys/fs/c[^g]*/** wklx,
deny /sys/fs/cg[^r]*/** wklx,
deny /sys/firmware/** rwklx,
deny /sys/kernel/security/** rwklx,
}
Save the custom profile to disk in the
/etc/apparmor.d/containers/docker-nginx
file.The file path in this example is not a requirement. In production, you could use another.
Load the profile.
$ sudo apparmor_parser -r -W /etc/apparmor.d/containers/docker-nginx
Run a container with the profile.
To run nginx in detached mode:
$ docker run --security-opt ""apparmor=docker-nginx"" \ -p 80:80 -d --name apparmor-nginx nginx
Exec into the running container.
$ docker container exec -it apparmor-nginx bash
Try some operations to test the profile.
root@6da5a2a930b9:~# ping 8.8.8.8 ping: Lacking privilege for raw socket. root@6da5a2a930b9:/# top bash: /usr/bin/top: Permission denied root@6da5a2a930b9:~# touch ~/thing touch: cannot touch 'thing': Permission denied root@6da5a2a930b9:/# sh bash: /bin/sh: Permission denied root@6da5a2a930b9:/# dash bash: /bin/dash: Permission denied
You just deployed a container secured with a custom apparmor profile.
Debug AppArmor
You can use dmesg
to debug problems and aa-status
check the loaded profiles.
Use dmesg
Here are some helpful tips for debugging any problems you might be facing with regard to AppArmor.
AppArmor sends quite verbose messaging to dmesg
. Usually an AppArmor line
looks like the following:
[ 5442.864673] audit: type=1400 audit(1453830992.845:37): apparmor=""ALLOWED"" operation=""open"" profile=""/usr/bin/docker"" name=""/home/jessie/docker/man/man1/docker-attach.1"" pid=10923 comm=""docker"" requested_mask=""r"" denied_mask=""r"" fsuid=1000 ouid=0
In the above example, you can see profile=/usr/bin/docker
. This means the
user has the docker-engine
(Docker Engine daemon) profile loaded.
Look at another log line:
[ 3256.689120] type=1400 audit(1405454041.341:73): apparmor=""DENIED"" operation=""ptrace"" profile=""docker-default"" pid=17651 comm=""docker"" requested_mask=""receive"" denied_mask=""receive""
This time the profile is docker-default
, which is run on containers by
default unless in privileged
mode. This line shows that apparmor has denied
ptrace
in the container. This is exactly as expected.
Use aa-status
If you need to check which profiles are loaded, you can use aa-status
. The
output looks like:
$ sudo aa-status
apparmor module is loaded.
14 profiles are loaded.
1 profiles are in enforce mode.
docker-default
13 profiles are in complain mode.
/usr/bin/docker
/usr/bin/docker///bin/cat
/usr/bin/docker///bin/ps
/usr/bin/docker///sbin/apparmor_parser
/usr/bin/docker///sbin/auplink
/usr/bin/docker///sbin/blkid
/usr/bin/docker///sbin/iptables
/usr/bin/docker///sbin/mke2fs
/usr/bin/docker///sbin/modprobe
/usr/bin/docker///sbin/tune2fs
/usr/bin/docker///sbin/xtables-multi
/usr/bin/docker///sbin/zfs
/usr/bin/docker///usr/bin/xz
38 processes have profiles defined.
37 processes are in enforce mode.
docker-default (6044)
...
docker-default (31899)
1 processes are in complain mode.
/usr/bin/docker (29756)
0 processes are unconfined but have a profile defined.
The above output shows that the docker-default
profile running on various
container PIDs is in enforce
mode. This means AppArmor is actively blocking
and auditing in dmesg
anything outside the bounds of the docker-default
profile.
The output above also shows the /usr/bin/docker
(Docker Engine daemon) profile
is running in complain
mode. This means AppArmor only logs to dmesg
activity outside the bounds of the profile. (Except in the case of Ubuntu
Trusty, where some interesting behaviors are enforced.)
Contribute to Docker's AppArmor code
Advanced users and package managers can find a profile for /usr/bin/docker
(Docker Engine daemon) underneath
contrib/apparmor
in the Docker Engine source repository.
The docker-default
profile for containers lives in
profiles/apparmor.",,,
7d3eee2cb3ec35b1c7bb6a2008cad0856d036f4a3c807f292cb330f7c71faf9d,"containerd image store
This page provides information about the ongoing integration of containerd
for
image and file system management in the Docker Engine.
Note
Images and containers are not shared between the classic image store and the new containerd image store. When you switch image stores, containers and images from the inactive store remain but are hidden until you switch back.
What is containerd?
containerd
is an abstraction of the low-level kernel features
used to run and manage containers on a system.
It's a platform used in container software like Docker and Kubernetes.
Docker Engine already uses containerd
for container lifecycle management,
which includes creating, starting, and stopping containers.
This page describes the next step of the containerd integration for Docker:
the containerd image store.
Image store
The image store is the component responsible for pushing, pulling, and storing images on the filesystem. The classic Docker image store is limited in the types of images that it supports. For example, it doesn't support image indices, containing manifest lists. When you create multi-platform images, for example, the image index resolves all the platform-specific variants of the image. An image index is also required when building images with attestations.
The containerd image store extends range of image types that the Docker Engine can natively interact with. While this is a low-level architectural change, it's a prerequisite for unlocking a range of new use cases, including:
- Build multi-platform images and images with attestations
- Support for using containerd snapshotters with unique characteristics, such as stargz for lazy-pulling images on container startup, or nydus and dragonfly for peer-to-peer image distribution.
- Ability to run Wasm containers
Enable the containerd image store
The containerd image store is enabled by default in Docker Desktop version 4.34 and later, but only for clean installs or if you perform a factory reset. If you upgrade from an earlier version of Docker Desktop, or if you use an older version of Docker Desktop you must manually switch to the containerd image store.
To manually enable this feature in Docker Desktop:
- Navigate to Settings in Docker Desktop.
- In the General tab, check Use containerd for pulling and storing images.
- Select Apply & Restart.
To disable the containerd image store, clear the Use containerd for pulling and storing images checkbox.
Build multi-platform images
The term multi-platform image refers to a bundle of images for multiple different architectures. Out of the box, the default builder for Docker Desktop doesn't support building multi-platform images.
$ docker build --platform=linux/amd64,linux/arm64 .
[+] Building 0.0s (0/0)
ERROR: Multi-platform build is not supported for the docker driver.
Switch to a different driver, or turn on the containerd image store, and try again.
Learn more at https://docs.docker.com/go/build-multi-platform/
Enabling the containerd image store lets you build multi-platform images and load them to your local image store:
Feedback
Thanks for trying the new features available with containerd
. Give feedback or
report any bugs you may find through the issues tracker on the
feedback form.",,,
421175d2e4cce599cabc1700a47ce1845481e74b9eb99056b5cd7afdd353c3c3,"Docker Scout release notes
This page contains information about the new features, improvements, known issues, and bug fixes in Docker Scout releases. These release notes cover the Docker Scout platform, including the Dashboard. For CLI release notes, refer to Docker Scout CLI release notes.
Q4 2024
New features and enhancements released in the fourth quarter of 2024.
2024-10-09
Policy Evaluation has graduated form Early Access to General Availability.
Docker Scout Dashboard UI changes:
- On the Docker Scout Dashboard, selecting a policy card now opens the policy details page instead of the policy results page.
- The policy results page and the policy details side panel are now read-only. Policy actions (edit, disable, delete) are now accessible from the policy details page.
Q3 2024
New features and enhancements released in the third quarter of 2024.
2024-09-30
In this release, we've changed how custom policies work. Before, custom policies were created by copying an out-of-the-box policy. Now, you can customize policies either by editing the default policy from a policy type which acts as a template. The default policies in Docker Scout are also implemented based on these types.
For more information, refer to policy types.
2024-09-09
This release changes how health scores are calculated in Docker Scout. The health score calculation now considers optional and custom policies that you have configured for your organization.
This means that if you have enabled, disabled, or customized any of the default policies, Docker Scout will now take those policies into account when calculating the health score for your organization's images.
If you haven't yet enabled Docker Scout for your organization, the health score calculation will be based on the out-of-the-box policies.
2024-08-13
This release changes the out-of-the-box policies to align with the policy configurations used to evaluate Docker Scout health scores.
The default out-of-the-box policies are now:
- No high-profile vulnerabilities
- No fixable critical or high vulnerabilities
- Approved Base Images
- Default non-root user
- Supply chain attestations
- Up-to-Date Base Images
- No AGPL v3 licenses
The configurations for these policies are now the same as the configurations used to calculate health scores. Previously, the out-of-the-box policies had different configurations than the health score policies.
Q2 2024
New features and enhancements released in the second quarter of 2024.
2024-06-27
This release introduces initial support for Exceptions in the Docker Scout Dashboard. Exceptions let you suppress vulnerabilities found in your images (false positives), using VEX documents. Attach VEX documents to images as attestations, or embed them on image filesystems, and Docker Scout will automatically detect and incorporate the VEX statements into the image analysis results.
The new Exceptions page lists all exceptions affecting images in your organization. You can also go to the image view in the Docker Scout Dashboard to see all exceptions that apply to a given image.
For more information, see Manage vulnerability exceptions.
2024-05-06
New HTTP endpoint that lets you scrape data from Docker Scout with Prometheus, to create your own vulnerability and policy dashboards with Grafana. For more information, see Docker Scout metrics exporter.
Q1 2024
New features and enhancements released in the first quarter of 2024.
2024-03-29
The No high-profile vulnerabilities policy now reports the xz
backdoor
vulnerability
CVE-2024-3094. Any
images in your Docker organization containing the version of xz/liblzma
with
the backdoor will be non-compliant with the No high-profile vulnerabilities
policy.
2024-03-20
The No fixable critical or high vulnerabilities policy now supports a Fixable vulnerabilities only configuration option, which lets you decide whether or not to only flag vulnerabilities with an available fix version.
2024-03-14
The All critical vulnerabilities policy has been removed. The No fixable critical or high vulnerabilities policy provides similar functionality, and will be updated in the future to allow for more extensive customization, making the now-removed All critical vulnerabilities policy redundant.
2024-01-26
Azure Container Registry integration graduated from Early Access to General Availability.
For more information and setup instructions, see Integrate Azure Container Registry.
2024-01-23
New Approved Base Images policy, which lets you restrict which base images you allow in your builds. You define the allowed base images using a pattern. Base images whose image reference don't match the specified patterns cause the policy to fail.
2024-01-12
New Default non-root user policy, which flags images that would run as the
root
superuser with full system administration privileges by default.
Specifying a non-root default user for your images can help strengthen your
runtime security.
2024-01-11
Beta launch of a new GitHub app for integrating Docker Scout with your source code management, and a remediation feature for helping you improve policy compliance.
Remediation is a new capability for Docker Scout to provide contextual, recommended actions based on policy evaluation results on how you can improve compliance.
The GitHub integration enhances the remediation feature. With the integration enabled, Docker Scout is able to connect analysis results to the source. This additional context about how your images are built is used to generate better, more precise recommendations.
For more information about the types of recommendations that Docker Scout can provide to help you improve policy compliance, see Remediation.
For more information about how to authorize the Docker Scout GitHub app on your source repositories, see Integrate Docker Scout with GitHub.
Q4 2023
New features and enhancements released in the fourth quarter of 2023.
2023-12-20
Azure Container Registry integration graduated from Beta to Early Access.
For more information and setup instructions, see Integrate Azure Container Registry.
2023-12-06
New SonarQube integration and related policy. SonarQube is an open-source platform for continuous inspection of code quality. This integration lets you add SonarQube's quality gates as a policy evaluation in Docker Scout. Enable the integration, push your images, and see the SonarQube quality gate conditions surfaced in the new SonarQube quality gates passed policy.
2023-12-01
Beta release of a new Azure Container Registry (ACR) integration, which lets Docker Scout pull and analyze images in ACR repositories automatically.
To learn more about the integration and how to get started, see Integrate Azure Container Registry.
2023-11-21
New configurable policies feature, which enables you to tweak the out-of-the-box policies according to your preferences, or disable them entirely if they don't quite match your needs. Some examples of how you can adapt policies for your organization include:
- Change the severity-thresholds that vulnerability-related policies use
- Customize the list of ""high-profile vulnerabilities""
- Add or remove software licenses to flag as ""copyleft""
For more information, see Configurable policies.
2023-11-10
New Supply chain attestations policy for helping you track whether your images are built with SBOM and provenance attestations. Adding attestations to images is a good first step in improving your supply chain conduct, and is often a prerequisite for doing more.
2023-11-01
New No high-profile vulnerabilities policy, which ensures your artifacts are free from a curated list of vulnerabilities widely recognized to be risky.
2023-10-04
This marks the General Availability (GA) release of Docker Scout.
The following new features are included in this release:
- Policy Evaluation (Early Access)
- Amazon ECR integration
- Sysdig integration
- JFrog Artifactory integration
Policy evaluation
Policy Evaluation is an early access feature that helps you ensure software integrity and track how your artifacts are doing over time. This release ships with four out-of-the-box policies, enabled by default for all organizations.
- Base images not up-to-date evaluates whether the base images are out of date, and require updating. Up-to-date base images help you ensure that your environments are reliable and secure.
- Critical and high vulnerabilities with fixes reports if there are vulnerabilities with critical or high severity in your images, and where there's a fix version available that you can upgrade to.
- All critical vulnerabilities looks out for any vulnerabilities of critical severity found in your images.
- Packages with AGPLv3, GPLv3 license helps you catch possibly unwanted copyleft licenses used in your images.
You can view and evaluate policy status for images using the Docker Scout
Dashboard and the docker scout policy
CLI command. For more information,
refer to the
Policy Evaluation documentation.
Amazon ECR integration
The new Amazon Elastic Container Registry (ECR) integration enables image analysis for images hosted in ECR repositories.
You set up the integration using a pre-configured CloudFormation stack template that bootstraps the necessary AWS resources in your account. Docker Scout automatically analyzes images that you push to your registry, storing only the metadata about the image contents, and not the container images themselves.
The integration offers a straightforward process for adding additional repositories, activating Docker Scout for specific repositories, and removing the integration if needed. To learn more, refer to the Amazon ECR integration documentation.
Sysdig integration
The new Sysdig integration gives you real-time security insights for your Kubernetes runtime environments.
Enabling this integration helps you address and prioritize risks for images used to run your production workloads. It also helps reduce monitoring noise, by automatically excluding vulnerabilities in programs that are never loaded into memory, using VEX documents.
For more information and getting started, see Sysdig integration documentation.
JFrog Artifactory integration
The new JFrog Artifactory integration enables automatic image analysis on Artifactory registries.
The integration involves deploying a Docker Scout Artifactory agent that polls for new images, performs analysis, and uploads results to Docker Scout, all while preserving the integrity of image data. Learn more in the Artifactory integration documentation
Known limitations
- Image analysis only works for Linux images
- Docker Scout can't process images larger than 12GB in compressed size
- Creating an image SBOM (part of image analysis) has a timeout limit of 4 minutes",,,
3afd7a826ac59929296fe6a0f6c44cd270cf48b7a966a8fe6fb4f89d27f2e575,"Generate a new recovery code
If you have lost your two-factor authentication recovery code and still have access to your Docker Hub account, you can generate a new recovery code.
- Sign in to your Docker account.
- Select your avatar and then from the drop-down menu, select Account settings.
- Select 2FA.
- Enter your password, then select Confirm.
- Select Generate new code.
This generates a new code. Select the visibility icon to view the code. Remember to save your recovery code and store it somewhere safe.",,,
59d314232c0c776a04419165cf47a17d84ad2efadcab1311dfbf34ec58902793,"Non-marketplace extensions
Install an extension not available in the Marketplace
Warning
Docker Extensions that are not in the Marketplace haven't gone through Docker's review process. Extensions can install binaries, invoke commands and access files on your machine. Installing them is at your own risk.
The Extensions Marketplace is the trusted and official place to install extensions from within Docker Desktop. These extensions have gone through a review process by Docker. However, other extensions can also be installed in Docker Desktop if you trust the extension author.
Given the nature of a Docker Extension (i.e. a Docker image) you can find other places where users have their extension's source code published. For example on GitHub, GitLab or even hosted in image registries like DockerHub or GHCR. You can install an extension that has been developed by the community or internally at your company from a teammate. You are not limited to installing extensions just from the Marketplace.
Note
Ensure the option Allow only extensions distributed through the Docker Marketplace is disabled. Otherwise, this prevents any extension not listed in the Marketplace, via the Extension SDK tools from, being installed. You can change this option in Settings.
To install an extension which is not present in the Marketplace, you can use the Extensions CLI that is bundled with Docker Desktop.
In a terminal, type docker extension install IMAGE[:TAG]
to install an extension by its image reference and optionally a tag. Use the -f
or --force
flag to avoid interactive confirmation.
Go to the Docker Desktop Dashboard to see the new extension installed.
List installed extensions
Regardless whether the extension was installed from the Marketplace or manually by using the Extensions CLI, you can use the docker extension ls
command to display the list of extensions installed.
As part of the output you'll see the extension ID, the provider, version, the title and whether it runs a backend container or has deployed binaries to the host, for example:
$ docker extension ls
ID PROVIDER VERSION UI VM HOST
john/my-extension John latest 1 tab(My-Extension) Running(1) -
Go to the Docker Desktop Dashboard, select Add Extensions and on the Managed tab to see the new extension installed.
Notice that an UNPUBLISHED
label displays which indicates that the extension has not been installed from the Marketplace.
Update an extension
To update an extension which isn't present in the Marketplace, in a terminal type docker extension update IMAGE[:TAG]
where the TAG
should be different from the extension that's already installed.
For instance, if you installed an extension with docker extension install john/my-extension:0.0.1
, you can update it by running docker extension update john/my-extension:0.0.2
.
Go to the Docker Desktop Dashboard to see the new extension updated.
Note
Extensions that aren't installed through the Marketplace don't receive update notifications from Docker Desktop.
Uninstall an extension
To uninstall an extension which is not present in the Marketplace, you can either navigate to the Managed tab in the Marketplace and select the Uninstall button, or from a terminal type docker extension uninstall IMAGE[:TAG]
.",,,
a2ea23bc9ef259c2e470485f1c1cb59e475935ed62162d214b84dd3306738670,"Docker Scout CLI release notes
This page contains information about the new features, improvements, known
issues, and bug fixes in the Docker Scout
CLI plugin
and the docker/scout-action
GitHub Action.
1.15.0
2024-10-31New
- New
--format=cyclonedx
flag for thedocker scout sbom
to output the SBOM in CycloneDX format.
Enhancements
- Use high-to-low sort order for CVE summary.
- Support for enabling and disabling repositories that enabled by
docker scout push
ordocker scout watch
.
Bug fixes
- Improve messaging when analyzing
oci
directories without attestations. Only single-platform images and multi-platform image with attestations are supported. Multi-platform images without attestations are not supported. - Improve classifiers and SBOM indexer:
- Add classifier for Liquibase
lpm
. - Add Rakudo Star/MoarVM binary classifier.
- Add binary classifiers for silverpeas utilities.
- Add classifier for Liquibase
- Improve reading and caching of attestations with the containerd image store.
1.14.0
2024-09-24New
- Add suppression information at the CVE level in the
docker scout cves
command.
Bug fixes
- Fix listing CVEs for dangling images, for example:
local://sha256:...
- Fix panic when analysing a file system input, for instance with
docker scout cves fs://.
1.13.0
2024-08-05New
- Add
--only-policy
filter option to thedocker scout quickview
,docker scout policy
anddocker scout compare
commands. - Add
--ignore-suppressed
filter option todocker scout cves
anddocker scout quickview
commands to filter out CVEs affected by exceptions.
Bug fixes and enhancements
Use conditional policy name in checks.
Add support for detecting the version of a Go project set using linker flags, for example:
$ go build -ldflags ""-X main.Version=1.2.3""
1.12.0
2024-07-31New
Only display vulnerabilities from the base image:
CLI$ docker scout cves --only-base IMAGE
GitHub Actionuses: docker/scout-action@v1 with: command: cves image: [IMAGE] only-base: true
Account for VEX in
quickview
command.CLI$ docker scout quickview IMAGE --only-vex-affected --vex-location ./path/to/my.vex.json
GitHub Actionuses: docker/scout-action@v1 with: command: quickview image: [IMAGE] only-vex-affected: true vex-location: ./path/to/my.vex.json
Account for VEX in
cves
command (GitHub Actions).GitHub Actionuses: docker/scout-action@v1 with: command: cves image: [IMAGE] only-vex-affected: true vex-location: ./path/to/my.vex.json
Bug fixes and enhancements
- Update
github.com/docker/docker
tov26.1.5+incompatible
to fix CVE-2024-41110. - Update Syft to 1.10.0.
1.11.0
2024-07-25New
Filter CVEs listed in the CISA Known Exploited Vulnerabilities catalog.
CLI$ docker scout cves [IMAGE] --only-cisa-kev ... (cropped output) ... ## Packages and Vulnerabilities 0C 1H 0M 0L io.netty/netty-codec-http2 4.1.97.Final pkg:maven/io.netty/netty-codec-http2@4.1.97.Final ✗ HIGH CVE-2023-44487 CISA KEV [OWASP Top Ten 2017 Category A9 - Using Components with Known Vulnerabilities] https://scout.docker.com/v/CVE-2023-44487 Affected range : <4.1.100 Fixed version : 4.1.100.Final CVSS Score : 7.5 CVSS Vector : CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H ... (cropped output) ...
GitHub Actionuses: docker/scout-action@v1 with: command: cves image: [IMAGE] only-cisa-kev: true
Add new classifiers:
spiped
swift
eclipse-mosquitto
znc
Bug fixes and enhancements
- Allow VEX matching when no subcomponents.
- Fix panic when attaching an invalid VEX document.
- Fix SPDX document root.
- Fix base image detection when image uses SCRATCH as the base image.
1.10.0
2024-06-26Bug fixes and enhancements
Add new classifiers:
irssi
Backdrop
CrateDB CLI (Crash)
monica
Openliberty
dumb-init
friendica
redmine
Fix whitespace-only originator on package breaking BuildKit exporters
Fix parsing image references in SPDX statement for images with a digest
Support
sbom://
prefix for image comparison:CLI$ docker scout compare sbom://image1.json --to sbom://image2.json
GitHub Actionuses: docker/scout-action@v1 with: command: compare image: sbom://image1.json to: sbom://image2.json
1.9.3
2024-05-28Bug fix
- Fix a panic while retrieving cached SBOMs.
1.9.1
2024-05-27New
Add support for the GitLab container scanning file format with
--format gitlab
ondocker scout cves
command.Here is an example pipeline:
docker-build: # Use the official docker image. image: docker:cli stage: build services: - docker:dind variables: DOCKER_IMAGE_NAME: $CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG before_script: - docker login -u ""$CI_REGISTRY_USER"" -p ""$CI_REGISTRY_PASSWORD"" $CI_REGISTRY # Install curl and the Docker Scout CLI - | apk add --update curl curl -sSfL https://raw.githubusercontent.com/docker/scout-cli/main/install.sh | sh -s -- apk del curl rm -rf /var/cache/apk/* # Login to Docker Hub required for Docker Scout CLI - echo ""$DOCKER_HUB_PAT"" | docker login --username ""$DOCKER_HUB_USER"" --password-stdin # All branches are tagged with $DOCKER_IMAGE_NAME (defaults to commit ref slug) # Default branch is also tagged with `latest` script: - docker buildx b --pull -t ""$DOCKER_IMAGE_NAME"" . - docker scout cves ""$DOCKER_IMAGE_NAME"" --format gitlab --output gl-container-scanning-report.json - docker push ""$DOCKER_IMAGE_NAME"" - | if [[ ""$CI_COMMIT_BRANCH"" == ""$CI_DEFAULT_BRANCH"" ]]; then docker tag ""$DOCKER_IMAGE_NAME"" ""$CI_REGISTRY_IMAGE:latest"" docker push ""$CI_REGISTRY_IMAGE:latest"" fi # Run this job in a branch where a Dockerfile exists rules: - if: $CI_COMMIT_BRANCH exists: - Dockerfile artifacts: reports: container_scanning: gl-container-scanning-report.json
Bug fixes and enhancements
- Support single-architecture images for
docker scout attest add
command - Indicate on the
docker scout quickview
anddocker scout recommendations
commands if image provenance was not created usingmode=max
. Withoutmode=max
, base images may be incorrectly detected, resulting in less accurate results.
1.9.0
2024-05-24Discarded in favor of 1.9.1.
1.8.0
2024-04-25Bug fixes and enhancements
Improve format of EPSS score and percentile.
Before:
EPSS Score : 0.000440 EPSS Percentile : 0.092510
After:
EPSS Score : 0.04% EPSS Percentile : 9th percentile
Fix markdown output of the
docker scout cves
command when analyzing local filesystem. docker/scout-cli#113
1.7.0
2024-04-15New
- The
docker scout push
command is now fully available: analyze images locally and push the SBOM to Docker Scout.
Bug fixes and enhancements
Fix adding attestations with
docker scout attestation add
to images in private repositoriesFix image processing for images based on the empty
scratch
base imageA new
sbom://
protocol for Docker Scout CLI commands let you read a Docker Scout SBOM from standard input.$ docker scout sbom IMAGE | docker scout qv sbom://
Add classifier for Joomla packages
1.6.4
2024-03-26Bug fixes and enhancements
- Fix epoch handling for RPM-based Linux distributions
1.6.3
2024-03-22Bug fixes and enhancements
- Improve package detection to ignore referenced but not installed packages.
1.6.2
2024-03-22Bug fixes and enhancements
- EPSS data is now fetched via the backend, as opposed to via the CLI client.
- Fix an issue when rendering markdown output using the
sbom://
prefix.
Removed
- The
docker scout cves --epss-date
anddocker scout cache prune --epss
flags have been removed.
1.6.1
2024-03-20Note
This release only affects the
docker/scout-action
GitHub Action.
New
Add support for passing in SBOM files in SDPX or in-toto SDPX format
uses: docker/scout-action@v1 with: command: cves image: sbom://alpine.spdx.json
Add support for SBOM files in
syft-json
formatuses: docker/scout-action@v1 with: command: cves image: sbom://alpine.syft.json
1.6.0
2024-03-19Note
This release only affects the CLI plugin, not the GitHub Action
New
Add support for passing in SBOM files in SDPX or in-toto SDPX format
$ docker scout cves sbom://path/to/sbom.spdx.json
Add support for SBOM files in
syft-json
format$ docker scout cves sbom://path/to/sbom.syft.json
Reads SBOM files from standard input
$ syft -o json alpine | docker scout cves sbom://
Prioritize CVEs by EPSS score
--epss
to display and prioritise the CVEs--epss-score
and--epss-percentile
to filter by score and percentile- Prune cached EPSS files with
docker scout cache prune --epss
Bug fixes and enhancements
Use Windows cache from WSL2
When inside WSL2 with Docker Desktop running, the Docker Scout CLI plugin now uses the cache from Windows. That way, if an image has been indexed for instance by Docker Desktop there's no need anymore to re-index it on WSL2 side.
Indexing is now blocked in the CLI if it has been disabled using Settings Management feature.
Fix a panic that would occur when analyzing a single-image
oci-dir
inputImprove local attestation support with the containerd image store
Earlier versions
Release notes for earlier versions of the Docker Scout CLI plugin are available on GitHub.",,,
1afd2c4255ff7a18e0342001d876b707f92e1756a125b0426207ca16f5db7c75,"Storage
By default all files created inside a container are stored on a writable container layer that sits on top of the read-only, immutable image layers.
Data written to the container layer doesn't persist when the container is destroyed. This means that it can be difficult to get the data out of the container if another process needs it.
The writable layer is unique per container. You can't easily extract the data from the writeable layer to the host, or to another container.
Storage mount options
Docker supports the following types of storage mounts for storing data outside of the writable layer of the container:
No matter which type of mount you choose to use, the data looks the same from within the container. It is exposed as either a directory or an individual file in the container's filesystem.
Volume mounts
Volumes are persistent storage mechanisms managed by the Docker daemon. They retain data even after the containers using them are removed. Volume data is stored on the filesystem on the host, but in order to interact with the data in the volume, you must mount the volume to a container. Directly accessing or interacting with the volume data is unsupported, undefined behavior, and may result in the volume or its data breaking in unexpected ways.
Volumes are ideal for performance-critical data processing and long-term storage needs. Since the storage location is managed on the daemon host, volumes provide the same raw file performance as accessing the host filesystem directly.
Bind mounts
Bind mounts create a direct link between a host system path and a container, allowing access to files or directories stored anywhere on the host. Since they aren't isolated by Docker, both non-Docker processes on the host and container processes can modify the mounted files simultaneously.
Use bind mounts when you need to be able to access files from both the container and the host.
tmpfs mounts
A tmpfs mount stores files directly in the host machine's memory, ensuring the data is not written to disk. This storage is ephemeral: the data is lost when the container is stopped or restarted, or when the host is rebooted. tmpfs mounts do not persist data either on the Docker host or within the container's filesystem.
These mounts are suitable for scenarios requiring temporary, in-memory storage, such as caching intermediate data, handling sensitive information like credentials, or reducing disk I/O. Use tmpfs mounts only when the data does not need to persist beyond the current container session.
Named pipes
Named pipes can be used for communication between the Docker host and a container. Common use case is to run a third-party tool inside of a container and connect to the Docker Engine API using a named pipe.
Next steps
- Learn more about volumes.
- Learn more about bind mounts.
- Learn more about tmpfs mounts.
- Learn more about storage drivers, which are not related to bind mounts or volumes, but allow you to store data in a container's writable layer.",,,
1101d54af78b11b06f3fa68fe0273fa7fa150667456be1250cf929432e3b7d17,"Storage drivers
To use storage drivers effectively, it's important to know how Docker builds and stores images, and how these images are used by containers. You can use this information to make informed choices about the best way to persist data from your applications and avoid performance problems along the way.
Storage drivers versus Docker volumes
Docker uses storage drivers to store image layers, and to store data in the writable layer of a container. The container's writable layer doesn't persist after the container is deleted, but is suitable for storing ephemeral data that is generated at runtime. Storage drivers are optimized for space efficiency, but (depending on the storage driver) write speeds are lower than native file system performance, especially for storage drivers that use a copy-on-write filesystem. Write-intensive applications, such as database storage, are impacted by a performance overhead, particularly if pre-existing data exists in the read-only layer.
Use Docker volumes for write-intensive data, data that must persist beyond the container's lifespan, and data that must be shared between containers. Refer to the volumes section to learn how to use volumes to persist data and improve performance.
Images and layers
A Docker image is built up from a series of layers. Each layer represents an instruction in the image's Dockerfile. Each layer except the very last one is read-only. Consider the following Dockerfile:
# syntax=docker/dockerfile:1
FROM ubuntu:22.04
LABEL org.opencontainers.image.authors=""org@example.com""
COPY . /app
RUN make /app
RUN rm -r $HOME/.cache
CMD python /app/app.py
This Dockerfile contains four commands. Commands that modify the filesystem create
a new layer. The FROM
statement starts out by creating a layer from the ubuntu:22.04
image. The LABEL
command only modifies the image's metadata, and doesn't produce
a new layer. The COPY
command adds some files from your Docker client's current
directory. The first RUN
command builds your application using the make
command,
and writes the result to a new layer. The second RUN
command removes a cache
directory, and writes the result to a new layer. Finally, the CMD
instruction
specifies what command to run within the container, which only modifies the
image's metadata, which doesn't produce an image layer.
Each layer is only a set of differences from the layer before it. Note that both
adding, and removing files will result in a new layer. In the example above,
the $HOME/.cache
directory is removed, but will still be available in the
previous layer and add up to the image's total size. Refer to the
Best practices for writing Dockerfiles
and
use multi-stage builds
sections to learn how to optimize your Dockerfiles for efficient images.
The layers are stacked on top of each other. When you create a new container,
you add a new writable layer on top of the underlying layers. This layer is often
called the ""container layer"". All changes made to the running container, such as
writing new files, modifying existing files, and deleting files, are written to
this thin writable container layer. The diagram below shows a container based
on an ubuntu:15.04
image.
A storage driver handles the details about the way these layers interact with each other. Different storage drivers are available, which have advantages and disadvantages in different situations.
Container and layers
The major difference between a container and an image is the top writable layer. All writes to the container that add new or modify existing data are stored in this writable layer. When the container is deleted, the writable layer is also deleted. The underlying image remains unchanged.
Because each container has its own writable container layer, and all changes are stored in this container layer, multiple containers can share access to the same underlying image and yet have their own data state. The diagram below shows multiple containers sharing the same Ubuntu 15.04 image.
Docker uses storage drivers to manage the contents of the image layers and the writable container layer. Each storage driver handles the implementation differently, but all drivers use stackable image layers and the copy-on-write (CoW) strategy.
Note
Use Docker volumes if you need multiple containers to have shared access to the exact same data. Refer to the volumes section to learn about volumes.
Container size on disk
To view the approximate size of a running container, you can use the docker ps -s
command. Two different columns relate to size.
size
: the amount of data (on disk) that's used for the writable layer of each container.virtual size
: the amount of data used for the read-only image data used by the container plus the container's writable layersize
. Multiple containers may share some or all read-only image data. Two containers started from the same image share 100% of the read-only data, while two containers with different images which have layers in common share those common layers. Therefore, you can't just total the virtual sizes. This over-estimates the total disk usage by a potentially non-trivial amount.
The total disk space used by all of the running containers on disk is some
combination of each container's size
and the virtual size
values. If
multiple containers started from the same exact image, the total size on disk for
these containers would be SUM (size
of containers) plus one image size
(virtual size
- size
).
This also doesn't count the following additional ways a container can take up disk space:
- Disk space used for log files stored by the logging-driver. This can be non-trivial if your container generates a large amount of logging data and log rotation isn't configured.
- Volumes and bind mounts used by the container.
- Disk space used for the container's configuration files, which are typically small.
- Memory written to disk (if swapping is enabled).
- Checkpoints, if you're using the experimental checkpoint/restore feature.
The copy-on-write (CoW) strategy
Copy-on-write is a strategy of sharing and copying files for maximum efficiency. If a file or directory exists in a lower layer within the image, and another layer (including the writable layer) needs read access to it, it just uses the existing file. The first time another layer needs to modify the file (when building the image or running the container), the file is copied into that layer and modified. This minimizes I/O and the size of each of the subsequent layers. These advantages are explained in more depth below.
Sharing promotes smaller images
When you use docker pull
to pull down an image from a repository, or when you
create a container from an image that doesn't yet exist locally, each layer is
pulled down separately, and stored in Docker's local storage area, which is
usually /var/lib/docker/
on Linux hosts. You can see these layers being pulled
in this example:
$ docker pull ubuntu:22.04
22.04: Pulling from library/ubuntu
f476d66f5408: Pull complete
8882c27f669e: Pull complete
d9af21273955: Pull complete
f5029279ec12: Pull complete
Digest: sha256:6120be6a2b7ce665d0cbddc3ce6eae60fe94637c6a66985312d1f02f63cc0bcd
Status: Downloaded newer image for ubuntu:22.04
docker.io/library/ubuntu:22.04
Each of these layers is stored in its own directory inside the Docker host's
local storage area. To examine the layers on the filesystem, list the contents
of /var/lib/docker/<storage-driver>
. This example uses the overlay2
storage driver:
$ ls /var/lib/docker/overlay2
16802227a96c24dcbeab5b37821e2b67a9f921749cd9a2e386d5a6d5bc6fc6d3
377d73dbb466e0bc7c9ee23166771b35ebdbe02ef17753d79fd3571d4ce659d7
3f02d96212b03e3383160d31d7c6aeca750d2d8a1879965b89fe8146594c453d
ec1ec45792908e90484f7e629330666e7eee599f08729c93890a7205a6ba35f5
l
The directory names don't correspond to the layer IDs.
Now imagine that you have two different Dockerfiles. You use the first one to
create an image called acme/my-base-image:1.0
.
# syntax=docker/dockerfile:1
FROM alpine
RUN apk add --no-cache bash
The second one is based on acme/my-base-image:1.0
, but has some additional
layers:
# syntax=docker/dockerfile:1
FROM acme/my-base-image:1.0
COPY . /app
RUN chmod +x /app/hello.sh
CMD /app/hello.sh
The second image contains all the layers from the first image, plus new layers
created by the COPY
and RUN
instructions, and a read-write container layer.
Docker already has all the layers from the first image, so it doesn't need to
pull them again. The two images share any layers they have in common.
If you build images from the two Dockerfiles, you can use docker image ls
and
docker image history
commands to verify that the cryptographic IDs of the shared
layers are the same.
Make a new directory
cow-test/
and change into it.Within
cow-test/
, create a new file calledhello.sh
with the following contents.#!/usr/bin/env bash echo ""Hello world""
Copy the contents of the first Dockerfile above into a new file called
Dockerfile.base
.Copy the contents of the second Dockerfile above into a new file called
Dockerfile
.Within the
cow-test/
directory, build the first image. Don't forget to include the final.
in the command. That sets thePATH
, which tells Docker where to look for any files that need to be added to the image.$ docker build -t acme/my-base-image:1.0 -f Dockerfile.base . [+] Building 6.0s (11/11) FINISHED => [internal] load build definition from Dockerfile.base 0.4s => => transferring dockerfile: 116B 0.0s => [internal] load .dockerignore 0.3s => => transferring context: 2B 0.0s => resolve image config for docker.io/docker/dockerfile:1 1.5s => [auth] docker/dockerfile:pull token for registry-1.docker.io 0.0s => CACHED docker-image://docker.io/docker/dockerfile:1@sha256:9e2c9eca7367393aecc68795c671... 0.0s => [internal] load .dockerignore 0.0s => [internal] load build definition from Dockerfile.base 0.0s => [internal] load metadata for docker.io/library/alpine:latest 0.0s => CACHED [1/2] FROM docker.io/library/alpine 0.0s => [2/2] RUN apk add --no-cache bash 3.1s => exporting to image 0.2s => => exporting layers 0.2s => => writing image sha256:da3cf8df55ee9777ddcd5afc40fffc3ead816bda99430bad2257de4459625eaa 0.0s => => naming to docker.io/acme/my-base-image:1.0 0.0s
Build the second image.
$ docker build -t acme/my-final-image:1.0 -f Dockerfile . [+] Building 3.6s (12/12) FINISHED => [internal] load build definition from Dockerfile 0.1s => => transferring dockerfile: 156B 0.0s => [internal] load .dockerignore 0.1s => => transferring context: 2B 0.0s => resolve image config for docker.io/docker/dockerfile:1 0.5s => CACHED docker-image://docker.io/docker/dockerfile:1@sha256:9e2c9eca7367393aecc68795c671... 0.0s => [internal] load .dockerignore 0.0s => [internal] load build definition from Dockerfile 0.0s => [internal] load metadata for docker.io/acme/my-base-image:1.0 0.0s => [internal] load build context 0.2s => => transferring context: 340B 0.0s => [1/3] FROM docker.io/acme/my-base-image:1.0 0.2s => [2/3] COPY . /app 0.1s => [3/3] RUN chmod +x /app/hello.sh 0.4s => exporting to image 0.1s => => exporting layers 0.1s => => writing image sha256:8bd85c42fa7ff6b33902ada7dcefaaae112bf5673873a089d73583b0074313dd 0.0s => => naming to docker.io/acme/my-final-image:1.0 0.0s
Check out the sizes of the images.
$ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE acme/my-final-image 1.0 8bd85c42fa7f About a minute ago 7.75MB acme/my-base-image 1.0 da3cf8df55ee 2 minutes ago 7.75MB
Check out the history of each image.
$ docker image history acme/my-base-image:1.0 IMAGE CREATED CREATED BY SIZE COMMENT da3cf8df55ee 5 minutes ago RUN /bin/sh -c apk add --no-cache bash # bui… 2.15MB buildkit.dockerfile.v0 <missing> 7 weeks ago /bin/sh -c #(nop) CMD [""/bin/sh""] 0B <missing> 7 weeks ago /bin/sh -c #(nop) ADD file:f278386b0cef68136… 5.6MB
Some steps don't have a size (
0B
), and are metadata-only changes, which do not produce an image layer and don't take up any size, other than the metadata itself. The output above shows that this image consists of 2 image layers.$ docker image history acme/my-final-image:1.0 IMAGE CREATED CREATED BY SIZE COMMENT 8bd85c42fa7f 3 minutes ago CMD [""/bin/sh"" ""-c"" ""/app/hello.sh""] 0B buildkit.dockerfile.v0 <missing> 3 minutes ago RUN /bin/sh -c chmod +x /app/hello.sh # buil… 39B buildkit.dockerfile.v0 <missing> 3 minutes ago COPY . /app # buildkit 222B buildkit.dockerfile.v0 <missing> 4 minutes ago RUN /bin/sh -c apk add --no-cache bash # bui… 2.15MB buildkit.dockerfile.v0 <missing> 7 weeks ago /bin/sh -c #(nop) CMD [""/bin/sh""] 0B <missing> 7 weeks ago /bin/sh -c #(nop) ADD file:f278386b0cef68136… 5.6MB
Notice that all steps of the first image are also included in the final image. The final image includes the two layers from the first image, and two layers that were added in the second image.
The
<missing>
lines in thedocker history
output indicate that those steps were either built on another system and part of thealpine
image that was pulled from Docker Hub, or were built with BuildKit as builder. Before BuildKit, the ""classic"" builder would produce a new ""intermediate"" image for each step for caching purposes, and theIMAGE
column would show the ID of that image.BuildKit uses its own caching mechanism, and no longer requires intermediate images for caching. Refer to BuildKit to learn more about other enhancements made in BuildKit.
Check out the layers for each image
Use the
docker image inspect
command to view the cryptographic IDs of the layers in each image:$ docker image inspect --format ""{{json .RootFS.Layers}}"" acme/my-base-image:1.0 [ ""sha256:72e830a4dff5f0d5225cdc0a320e85ab1ce06ea5673acfe8d83a7645cbd0e9cf"", ""sha256:07b4a9068b6af337e8b8f1f1dae3dd14185b2c0003a9a1f0a6fd2587495b204a"" ]
$ docker image inspect --format ""{{json .RootFS.Layers}}"" acme/my-final-image:1.0 [ ""sha256:72e830a4dff5f0d5225cdc0a320e85ab1ce06ea5673acfe8d83a7645cbd0e9cf"", ""sha256:07b4a9068b6af337e8b8f1f1dae3dd14185b2c0003a9a1f0a6fd2587495b204a"", ""sha256:cc644054967e516db4689b5282ee98e4bc4b11ea2255c9630309f559ab96562e"", ""sha256:e84fb818852626e89a09f5143dbc31fe7f0e0a6a24cd8d2eb68062b904337af4"" ]
Notice that the first two layers are identical in both images. The second image adds two additional layers. Shared image layers are only stored once in
/var/lib/docker/
and are also shared when pushing and pulling an image to an image registry. Shared image layers can therefore reduce network bandwidth and storage.Tip
Format output of Docker commands with the
--format
option.The examples above use the
docker image inspect
command with the--format
option to view the layer IDs, formatted as a JSON array. The--format
option on Docker commands can be a powerful feature that allows you to extract and format specific information from the output, without requiring additional tools such asawk
orsed
. To learn more about formatting the output of docker commands using the--format
flag, refer to the format command and log output section. We also pretty-printed the JSON output using thejq
utility for readability.
Copying makes containers efficient
When you start a container, a thin writable container layer is added on top of the other layers. Any changes the container makes to the filesystem are stored here. Any files the container doesn't change don't get copied to this writable layer. This means that the writable layer is as small as possible.
When an existing file in a container is modified, the storage driver performs a
copy-on-write operation. The specific steps involved depend on the specific
storage driver. For the overlay2
driver, the copy-on-write operation follows
this rough sequence:
- Search through the image layers for the file to update. The process starts at the newest layer and works down to the base layer one layer at a time. When results are found, they're added to a cache to speed future operations.
- Perform a
copy_up
operation on the first copy of the file that's found, to copy the file to the container's writable layer. - Any modifications are made to this copy of the file, and the container can't see the read-only copy of the file that exists in the lower layer.
Btrfs, ZFS, and other drivers handle the copy-on-write differently. You can read more about the methods of these drivers later in their detailed descriptions.
Containers that write a lot of data consume more space than containers
that don't. This is because most write operations consume new space in the
container's thin writable top layer. Note that changing the metadata of files,
for example, changing file permissions or ownership of a file, can also result
in a copy_up
operation, therefore duplicating the file to the writable layer.
Tip
Use volumes for write-heavy applications.
Don't store the data in the container for write-heavy applications. Such applications, for example write-intensive databases, are known to be problematic particularly when pre-existing data exists in the read-only layer.
Instead, use Docker volumes, which are independent of the running container, and designed to be efficient for I/O. In addition, volumes can be shared among containers and don't increase the size of your container's writable layer. Refer to the use volumes section to learn about volumes.
A copy_up
operation can incur a noticeable performance overhead. This overhead
is different depending on which storage driver is in use. Large files,
lots of layers, and deep directory trees can make the impact more noticeable.
This is mitigated by the fact that each copy_up
operation only occurs the first
time a given file is modified.
To verify the way that copy-on-write works, the following procedure spins up 5
containers based on the acme/my-final-image:1.0
image we built earlier and
examines how much room they take up.
From a terminal on your Docker host, run the following
docker run
commands. The strings at the end are the IDs of each container.$ docker run -dit --name my_container_1 acme/my-final-image:1.0 bash \ && docker run -dit --name my_container_2 acme/my-final-image:1.0 bash \ && docker run -dit --name my_container_3 acme/my-final-image:1.0 bash \ && docker run -dit --name my_container_4 acme/my-final-image:1.0 bash \ && docker run -dit --name my_container_5 acme/my-final-image:1.0 bash 40ebdd7634162eb42bdb1ba76a395095527e9c0aa40348e6c325bd0aa289423c a5ff32e2b551168b9498870faf16c9cd0af820edf8a5c157f7b80da59d01a107 3ed3c1a10430e09f253704116965b01ca920202d52f3bf381fbb833b8ae356bc 939b3bf9e7ece24bcffec57d974c939da2bdcc6a5077b5459c897c1e2fa37a39 cddae31c314fbab3f7eabeb9b26733838187abc9a2ed53f97bd5b04cd7984a5a
Run the
docker ps
command with the--size
option to verify the 5 containers are running, and to see each container's size.$ docker ps --size --format ""table {{.ID}}\t{{.Image}}\t{{.Names}}\t{{.Size}}"" CONTAINER ID IMAGE NAMES SIZE cddae31c314f acme/my-final-image:1.0 my_container_5 0B (virtual 7.75MB) 939b3bf9e7ec acme/my-final-image:1.0 my_container_4 0B (virtual 7.75MB) 3ed3c1a10430 acme/my-final-image:1.0 my_container_3 0B (virtual 7.75MB) a5ff32e2b551 acme/my-final-image:1.0 my_container_2 0B (virtual 7.75MB) 40ebdd763416 acme/my-final-image:1.0 my_container_1 0B (virtual 7.75MB)
The output above shows that all containers share the image's read-only layers (7.75MB), but no data was written to the container's filesystem, so no additional storage is used for the containers.
Note
This step requires a Linux machine, and doesn't work on Docker Desktop, as it requires access to the Docker Daemon's file storage.
While the output of
docker ps
provides you information about disk space consumed by a container's writable layer, it doesn't include information about metadata and log-files stored for each container.More details can be obtained by exploring the Docker Daemon's storage location (
/var/lib/docker
by default).$ sudo du -sh /var/lib/docker/containers/* 36K /var/lib/docker/containers/3ed3c1a10430e09f253704116965b01ca920202d52f3bf381fbb833b8ae356bc 36K /var/lib/docker/containers/40ebdd7634162eb42bdb1ba76a395095527e9c0aa40348e6c325bd0aa289423c 36K /var/lib/docker/containers/939b3bf9e7ece24bcffec57d974c939da2bdcc6a5077b5459c897c1e2fa37a39 36K /var/lib/docker/containers/a5ff32e2b551168b9498870faf16c9cd0af820edf8a5c157f7b80da59d01a107 36K /var/lib/docker/containers/cddae31c314fbab3f7eabeb9b26733838187abc9a2ed53f97bd5b04cd7984a5a
Each of these containers only takes up 36k of space on the filesystem.
Per-container storage
To demonstrate this, run the following command to write the word 'hello' to a file on the container's writable layer in containers
my_container_1
,my_container_2
, andmy_container_3
:$ for i in {1..3}; do docker exec my_container_$i sh -c 'printf hello > /out.txt'; done
Running the
docker ps
command again afterward shows that those containers now consume 5 bytes each. This data is unique to each container, and not shared. The read-only layers of the containers aren't affected, and are still shared by all containers.$ docker ps --size --format ""table {{.ID}}\t{{.Image}}\t{{.Names}}\t{{.Size}}"" CONTAINER ID IMAGE NAMES SIZE cddae31c314f acme/my-final-image:1.0 my_container_5 0B (virtual 7.75MB) 939b3bf9e7ec acme/my-final-image:1.0 my_container_4 0B (virtual 7.75MB) 3ed3c1a10430 acme/my-final-image:1.0 my_container_3 5B (virtual 7.75MB) a5ff32e2b551 acme/my-final-image:1.0 my_container_2 5B (virtual 7.75MB) 40ebdd763416 acme/my-final-image:1.0 my_container_1 5B (virtual 7.75MB)
The previous examples illustrate how copy-on-write filesystems help make containers efficient. Not only does copy-on-write save space, but it also reduces container start-up time. When you create a container (or multiple containers from the same image), Docker only needs to create the thin writable container layer.
If Docker had to make an entire copy of the underlying image stack each time it
created a new container, container creation times and disk space used would be
significantly increased. This would be similar to the way that virtual machines
work, with one or more virtual disks per virtual machine. The
vfs
storage
doesn't provide a CoW filesystem or other optimizations. When using this storage
driver, a full copy of the image's data is created for each container.",,,
504a7bc5575ea591ec0de37e87dae46f03634283e584be6877eee7a8067ed25d,"Archive or unarchive a repository
You can archive a repository on Docker Hub to mark it as read-only and indicate that it's no longer actively maintained. This helps prevent the use of outdated or unsupported images in workflows. Archived repositories can also be unarchived if needed.
Docker Hub highlights repositories that haven't been updated in over a year by displaying an icon ( ) next to them on the Repositories page. Consider reviewing these highlighted repositories and archiving them if necessary.
When a repository is archived, the following occurs:
- The repository information can't be modified.
- New images can't be pushed to the repository.
- An Archived label is displayed on the public repository page.
- Users can still pull the images.
You can unarchive an archived repository to remove the archived state. When unarchived, the following occurs:
- The repository information can be modified.
- New images can be pushed to the repository.
- The Archived label is removed on the public repository page.
Archive a repository
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears.
Select the Settings tab.
Select Archive repository.
Enter the name of your repository to confirm.
Select Archive.
Unarchive a repository
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears.
Select the Settings tab.
Select Unarchive repository.",,,
ac51c9de46cbecabdc228a7ff074ed7567ec124b677a5a9b7fd7fa581ebb8d87,"Advanced integration
Compose Bridge can also function as a kubectl
plugin, allowing you to integrate its capabilities directly into your Kubernetes command-line operations. This integration simplifies the process of converting and deploying applications from Docker Compose to Kubernetes.
Use compose-bridge
as a kubectl
plugin
To use the compose-bridge
binary as a kubectl
plugin, you need to make sure that the binary is available in your PATH and the name of the binary is prefixed with kubectl-
.
Rename or copy the
compose-bridge
binary tokubectl-compose_bridge
:$ mv /path/to/compose-bridge /usr/local/bin/kubectl-compose_bridge
Ensure that the binary is executable:
$ chmod +x /usr/local/bin/kubectl-compose_bridge
Verify that the plugin is recognized by
kubectl
:$ kubectl plugin list
In the output, you should see
kubectl-compose_bridge
.Now you can use
compose-bridge
as akubectl
plugin:$ kubectl compose-bridge [command]
Replace [command]
with any compose-bridge
command you want to use.",,,
edde484d730f2e46b5177382b30b8dc9940de6525b4a84b9d9cac09f7628f3e5,"OCI and Docker exporters
The oci
exporter outputs the build result into an
OCI image layout
tarball. The docker
exporter behaves the same way, except it exports a Docker
image layout instead.
The
docker
driver doesn't support these exporters. You
must use docker-container
or some other driver if you want to generate these
outputs.
Synopsis
Build a container image using the oci
and docker
exporters:
$ docker buildx build --output type=oci[,parameters] .
$ docker buildx build --output type=docker[,parameters] .
The following table describes the available parameters:
| Parameter | Type | Default | Description |
|---|---|---|---|
name | String | Specify image name(s) | |
dest | String | Path | |
tar | true ,false | true | Bundle the output into a tarball layout |
compression | uncompressed ,gzip ,estargz ,zstd | gzip | Compression type, see compression |
compression-level | 0..22 | Compression level, see compression | |
force-compression | true ,false | false | Forcefully apply compression, see compression |
oci-mediatypes | true ,false | Use OCI media types in exporter manifests. Defaults to true for type=oci , and false for type=docker . See
OCI Media types | |
annotation.<key> | String | Attach an annotation with the respective key and value to the built image,see
annotations |
Annotations
These exporters support adding OCI annotation using annotation
parameter,
followed by the annotation name using dot notation. The following example sets
the org.opencontainers.image.title
annotation:
$ docker buildx build \
--output ""type=<type>,name=<registry>/<image>,annotation.org.opencontainers.image.title=<title>"" .
For more information about annotations, see BuildKit documentation.
Further reading
For more information on the oci
or docker
exporters, see the
BuildKit README.",,,
9b06e5a52f7bc940c751f9ed0bdb081f37948bfe8b2f71b6b768c42f1f2df860,"Manage nodes in a swarm
As part of the swarm management lifecycle, you may need to:
List nodes
To view a list of nodes in the swarm run docker node ls
from a manager node:
$ docker node ls
ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS
46aqrk4e473hjbt745z53cr3t node-5 Ready Active Reachable
61pi3d91s0w3b90ijw3deeb2q node-4 Ready Active Reachable
a5b2m3oghd48m8eu391pefq5u node-3 Ready Active
e7p8btxeu3ioshyuj6lxiv6g0 node-2 Ready Active
ehkv3bcimagdese79dn78otj5 * node-1 Ready Active Leader
The AVAILABILITY
column shows whether or not the scheduler can assign tasks to
the node:
Active
means that the scheduler can assign tasks to the node.Pause
means the scheduler doesn't assign new tasks to the node, but existing tasks remain running.Drain
means the scheduler doesn't assign new tasks to the node. The scheduler shuts down any existing tasks and schedules them on an available node.
The MANAGER STATUS
column shows node participation in the Raft consensus:
- No value indicates a worker node that does not participate in swarm management.
Leader
means the node is the primary manager node that makes all swarm management and orchestration decisions for the swarm.Reachable
means the node is a manager node participating in the Raft consensus quorum. If the leader node becomes unavailable, the node is eligible for election as the new leader.Unavailable
means the node is a manager that can't communicate with other managers. If a manager node becomes unavailable, you should either join a new manager node to the swarm or promote a worker node to be a manager.
For more information on swarm administration refer to the Swarm administration guide.
Inspect an individual node
You can run docker node inspect <NODE-ID>
on a manager node to view the
details for an individual node. The output defaults to JSON format, but you can
pass the --pretty
flag to print the results in human-readable format. For example:
$ docker node inspect self --pretty
ID: ehkv3bcimagdese79dn78otj5
Hostname: node-1
Joined at: 2016-06-16 22:52:44.9910662 +0000 utc
Status:
State: Ready
Availability: Active
Manager Status:
Address: 172.17.0.2:2377
Raft Status: Reachable
Leader: Yes
Platform:
Operating System: linux
Architecture: x86_64
Resources:
CPUs: 2
Memory: 1.954 GiB
Plugins:
Network: overlay, host, bridge, overlay, null
Volume: local
Engine Version: 1.12.0-dev
Update a node
You can modify node attributes to:
Change node availability
Changing node availability lets you:
- Drain a manager node so that it only performs swarm management tasks and is unavailable for task assignment.
- Drain a node so you can take it down for maintenance.
- Pause a node so it can't receive new tasks.
- Restore unavailable or paused nodes availability status.
For example, to change a manager node to Drain
availability:
$ docker node update --availability drain node-1
node-1
See list nodes for descriptions of the different availability options.
Add or remove label metadata
Node labels provide a flexible method of node organization. You can also use node labels in service constraints. Apply constraints when you create a service to limit the nodes where the scheduler assigns tasks for the service.
Run docker node update --label-add
on a manager node to add label metadata to
a node. The --label-add
flag supports either a <key>
or a <key>=<value>
pair.
Pass the --label-add
flag once for each node label you want to add:
$ docker node update --label-add foo --label-add bar=baz node-1
node-1
The labels you set for nodes using docker node update
apply only to the node
entity within the swarm. Do not confuse them with the Docker daemon labels for
dockerd.
Therefore, node labels can be used to limit critical tasks to nodes that meet certain requirements. For example, schedule only on machines where special workloads should be run, such as machines that meet PCI-SS compliance.
A compromised worker could not compromise these special workloads because it cannot change node labels.
Engine labels, however, are still useful because some features that do not affect secure orchestration of containers might be better off set in a decentralized manner. For instance, an engine could have a label to indicate that it has a certain type of disk device, which may not be relevant to security directly. These labels are more easily ""trusted"" by the swarm orchestrator.
Refer to the docker service create
CLI reference
for more information about service constraints.
Promote or demote a node
You can promote a worker node to the manager role. This is useful when a manager node becomes unavailable or if you want to take a manager offline for maintenance. Similarly, you can demote a manager node to the worker role.
Note
Regardless of your reason to promote or demote a node, you must always maintain a quorum of manager nodes in the swarm. For more information refer to the Swarm administration guide.
To promote a node or set of nodes, run docker node promote
from a manager
node:
$ docker node promote node-3 node-2
Node node-3 promoted to a manager in the swarm.
Node node-2 promoted to a manager in the swarm.
To demote a node or set of nodes, run docker node demote
from a manager node:
$ docker node demote node-3 node-2
Manager node-3 demoted in the swarm.
Manager node-2 demoted in the swarm.
docker node promote
and docker node demote
are convenience commands for
docker node update --role manager
and docker node update --role worker
respectively.
Install plugins on swarm nodes
If your swarm service relies on one or more
plugins, these plugins need to be available on
every node where the service could potentially be deployed. You can manually
install the plugin on each node or script the installation. You can also deploy
the plugin in a similar way as a global service using the Docker API, by specifying
a PluginSpec
instead of a ContainerSpec
.
Note
There is currently no way to deploy a plugin to a swarm using the Docker CLI or Docker Compose. In addition, it is not possible to install plugins from a private repository.
The
PluginSpec
is defined by the plugin developer. To add the plugin to all Docker nodes, use
the
service/create
API, passing
the PluginSpec
JSON defined in the TaskTemplate
.
Leave the swarm
Run the docker swarm leave
command on a node to remove it from the swarm.
For example to leave the swarm on a worker node:
$ docker swarm leave
Node left the swarm.
When a node leaves the swarm, Docker Engine stops running in Swarm mode. The orchestrator no longer schedules tasks to the node.
If the node is a manager node, you receive a warning about maintaining the
quorum. To override the warning, pass the --force
flag. If the last manager
node leaves the swarm, the swarm becomes unavailable requiring you to take
disaster recovery measures.
For information about maintaining a quorum and disaster recovery, refer to the Swarm administration guide.
After a node leaves the swarm, you can run docker node rm
on a
manager node to remove the node from the node list.
For instance:
$ docker node rm node-2",,,
a89fab9b861b228d95a046f6153d2bdfe635f94e1ac1b046ad3c3d1b8d9337cc,"Docker Engine 26.1 release notes
This page describes the latest changes, additions, known issues, and fixes for Docker Engine version 26.1.
For more information about:
- Deprecated and removed features, see Deprecated Engine Features.
- Changes to the Engine API, see Engine API version history.
26.1.4
2024-06-05For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 26.1.4 milestone
- moby/moby, 26.1.4 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
Security
This release updates the Go runtime to 1.21.11 which contains security fixes for:
- CVE-2024-24789
- CVE-2024-24790
- A symlink time of check to time of use race condition during directory removal reported by Addison Crump.
Bug fixes and enhancements
- Fixed an issue where promoting a node immediately after another node was demoted could cause the promotion to fail. moby/moby#47870
- Prevent the daemon log from being spammed with
superfluous response.WriteHeader call ...
messages. moby/moby#47843 - Don't show empty hints when plugins return an empty hook message. docker/cli#5083
- Fix a compatibility issue with Visual Studio Container Tools. docker/cli#5095
Packaging updates
- Update containerd (static binaries only) to v1.7.17. moby/moby#47841
- CVE-2024-24789, CVE-2024-24790: Update Go runtime to 1.21.11. moby/moby#47904
- Update Compose to v2.27.1. docker/docker-ce-packages#1022
- Update Buildx to v0.14.1. docker/docker-ce-packages#1021
26.1.3
2024-05-16For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 26.1.3 milestone
- moby/moby, 26.1.3 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
Bug fixes and enhancements
- Fix a regression that prevented the use of DNS servers within a
--internal
network. moby/moby#47832 - When the internal DNS server's own address is supplied as an external server address, ignore it to avoid unproductive recursion. moby/moby#47833
Packaging updates
- Allow runc to kill containers when confined to the runc profile in AppArmor version 4.0.0 and later. moby/moby#47829
26.1.2
2024-05-08For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 26.1.2 milestone
- moby/moby, 26.1.2 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
Bug fixes and enhancements
- Fix an issue where the CLI process would sometimes hang when a container failed to start. docker/cli#5062
Packaging updates
- Update Go runtime to 1.21.10. moby/moby#47806
26.1.1
2024-04-30For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 26.1.1 milestone
- moby/moby, 26.1.1 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
Bug fixes and enhancements
- Fix
docker run -d
printing ancontext canceled
spurious error when OpenTelemetry is configured. docker/cli#5044 - Experimental environment variable
DOCKER_BRIDGE_PRESERVE_KERNEL_LL=1
will prevent the daemon from removing the kernel-assigned link local address on a Linux bridge. moby/moby#47775 - Resolve an issue preventing container creation on hosts with a read-only
/proc/sys/net
filesystem. If IPv6 cannot be disabled on an interface due to this, either disable IPv6 by default on the host or ensure/proc/sys/net
is read-write. To bypass the error, set the environment variableDOCKER_ALLOW_IPV6_ON_IPV4_INTERFACE=1
before starting the Docker daemon. moby/moby#47769
Note
The
DOCKER_ALLOW_IPV6_ON_IPV4_INTERFACE
is added as a temporary fix and will be phased out in a future major release, when the IPv6 enablement process has been improved.
Packaging updates
- Update BuildKit to v0.13.2. moby/moby#47762
- Update Compose to v2.27.0. docker/docker-ce-packages#1017
26.1.0
2024-04-22For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 26.1.0 milestone
- moby/moby, 26.1.0 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
New
- Added configurable OpenTelemetry utilities and basic instrumentation to commands. For more information, see OpenTelemetry for the Docker CLI. docker/cli#4889
Bug fixes and enhancements
Native Windows containers are configured with an internal DNS server for container name resolution, and external DNS servers for other lookups. Not all resolvers, including
nslookup
, fall back to the external resolvers when they get aSERVFAIL
answer from the internal server. So, the internal DNS server can now be configured to forward requests to the external resolvers, by setting afeature
option in thedaemon.json
file:{ ""features"": { ""windows-dns-proxy"": true } }
Note
- This will be the new default behavior in Docker Engine 27.0.
- The
windows-dns-proxy
feature flag will be removed in a future release.
Swarm: Fix
Subpath
not being passed to the container config. moby/moby#47711Classic builder: Fix cache miss on
WORKDIR <directory>/
build step (directory with a trailing slash). moby/moby#47723containerd image store: Fix
docker images
failing when any image in the store has unexpected target. moby/moby#47738",,,
95acb6c0164520390a8c1241024943714747989afa9132bce84acc1e7a73d3c5,"Deprecated Docker Engine features
This page provides an overview of features that are deprecated in Engine. Changes in packaging, and supported (Linux) distributions are not included. To learn about end of support for Linux distributions, refer to the release notes.
Feature deprecation policy
As changes are made to Docker there may be times when existing features need to be removed or replaced with newer features. Before an existing feature is removed it is labeled as ""deprecated"" within the documentation and remains in Docker for at least one stable release unless specified explicitly otherwise. After that time it may be removed.
Users are expected to take note of the list of deprecated features each release and plan their migration away from those features, and (if applicable) towards the replacement features as soon as possible.
Deprecated engine features
The following table provides an overview of the current status of deprecated features:
Deprecated: the feature is marked ""deprecated"" and should no longer be used.
The feature may be removed, disabled, or change behavior in a future release. The ""Deprecated"" column contains the release in which the feature was marked deprecated, whereas the ""Remove"" column contains a tentative release in which the feature is to be removed. If no release is included in the ""Remove"" column, the release is yet to be decided on.
Removed: the feature was removed, disabled, or hidden.
Refer to the linked section for details. Some features are ""soft"" deprecated, which means that they remain functional for backward compatibility, and to allow users to migrate to alternatives. In such cases, a warning may be printed, and users should not rely on this feature.
Configuration for pushing non-distributable artifacts
Deprecated in Release: v28.0 Target For Removal In Release: v29.0
Non-distributable artifacts (also called foreign layers) were introduced in docker v1.12 to accommodate Windows images for which the EULA did not allow layers to be distributed through registries other than those hosted by Microsoft. The concept of foreign / non-distributable layers was adopted by the OCI distribution spec in oci#233. These restrictions were relaxed later to allow distributing these images through non-public registries, for which a configuration was added in Docker v17.0.6.0.
In 2022, Microsoft updated the EULA and removed these restrictions, followed by the OCI distribution specification deprecating foreign layers in oci#965. In 2023, Microsoft removed the use of foreign data layers for their images, making this functionality obsolete.
Docker v28.0 deprecates the --allow-nondistributable-artifacts
daemon flag and
corresponding allow-nondistributable-artifacts
field in daemon.json
. Setting
either option no longer takes an effect, but a deprecation warning log is added
to raise awareness about the deprecation. This warning is planned to become an
error in the Docker v29.0.
Users currently using these options are therefore recommended to remove this option from their configuration to prevent the daemon from starting when upgrading to Docker v29.0.
The AllowNondistributableArtifactsCIDRs
and AllowNondistributableArtifactsHostnames
fields in the RegistryConfig
of the GET /info
API response are also deprecated.
For API version v1.48 and lower, the fields are still included in the response
but always null
. In API version v1.49 and higher, the field will be omitted
entirely.
--time
option on docker stop
and docker restart
Deprecated in Release: v28.0
The --time
option for the docker stop
, docker container stop
, docker restart
,
and docker container restart
commands has been renamed to --timeout
for
consistency with other uses of timeout options. The --time
option is now
deprecated and hidden, but remains functional for backward compatibility.
Users are encouraged to migrate to using the --timeout
option instead.
Non-standard fields in image inspect
Deprecated in Release: v27.0 Target For Removal In Release: v28.0
The Config
field returned shown in docker image inspect
(and as returned by
the GET /images/{name}/json
API endpoint) returns additional fields that are
not part of the image's configuration and not part of the
Docker image specification
and
OCI image specification.
These fields are never set (and always return the default value for the type), but are not omitted in the response when left empty. As these fields were not intended to be part of the image configuration response, they are deprecated, and will be removed from the API in thee next release.
The following fields are currently included in the API response, but are not
part of the underlying image's Config
field, and deprecated:
Hostname
Domainname
AttachStdin
AttachStdout
AttachStderr
Tty
OpenStdin
StdinOnce
Image
NetworkDisabled
(already omitted unless set)MacAddress
(already omitted unless set)StopTimeout
(already omitted unless set)
Graphdriver plugins (experimental)
Deprecated in Release: v27.0 Disabled by default in Release: v27.0 Target For Removal In Release: v28.0
Graphdriver plugins are an experimental feature that allow extending the Docker Engine with custom storage drivers for storing images and containers. This feature was not maintained since its inception, and will no longer be supported in upcoming releases.
Support for graphdriver plugins is disabled by default in v27.0, and will be
removed v28.0. An DOCKERD_DEPRECATED_GRAPHDRIVER_PLUGINS
environment variable
is provided in v27.0 to re-enable the feature. This environment variable must
be set to a non-empty value in the daemon's environment.
The DOCKERD_DEPRECATED_GRAPHDRIVER_PLUGINS
environment variable, along with
support for graphdriver plugins, will be removed in v28.0. Users of this feature
are recommended to instead configure the Docker Engine to use the
containerd image store
and a custom
snapshotter
API CORS headers
Deprecated in Release: v27.0 Disabled by default in Release: v27.0 Removed in release: v28.0
The api-cors-header
configuration option for the Docker daemon is insecure,
and is therefore deprecated and scheduled for removal.
Incorrectly setting this option could leave a window of opportunity
for unauthenticated cross-origin requests to be accepted by the daemon.
In Docker Engine v27.0, this flag can still be set,
but it has no effect unless the environment variable
DOCKERD_DEPRECATED_CORS_HEADER
is also set to a non-empty value.
This flag has been removed altogether in v28.0.
This is a breaking change for authorization plugins and other programs that depend on this option for accessing the Docker API from a browser. If you need to access the API through a browser, use a reverse proxy.
Unauthenticated TCP connections
Deprecated in Release: v26.0 Target For Removal In Release: v28.0
Configuring the Docker daemon to listen on a TCP address will require mandatory
TLS verification. This change aims to ensure secure communication by preventing
unauthorized access to the Docker daemon over potentially insecure networks.
This mandatory TLS requirement applies to all TCP addresses except tcp://localhost
.
In version 27.0 and later, specifying --tls=false
or --tlsverify=false
CLI flags
causes the daemon to fail to start if it's also configured to accept remote connections over TCP.
This also applies to the equivalent configuration options in daemon.json
.
To facilitate remote access to the Docker daemon over TCP, you'll need to implement TLS verification. This secures the connection by encrypting data in transit and providing a mechanism for mutual authentication.
For environments remote daemon access isn't required, we recommend binding the Docker daemon to a Unix socket. For daemons where remote access is required and where TLS encryption is not feasible, you may want to consider using SSH as an alternative solution.
For further information, assistance, and step-by-step instructions on configuring TLS (or SSH) for the Docker daemon, refer to Protect the Docker daemon socket.
Container
and ContainerConfig
fields in Image inspect
Deprecated in Release: v25.0 Target For Removal In Release: v26.0
The Container
and ContainerConfig
fields returned by docker inspect
are
mostly an implementation detail of the classic (non-BuildKit) image builder.
These fields are not portable and are empty when using the
BuildKit-based builder (enabled by default since v23.0).
These fields are deprecated in v25.0 and will be omitted starting from v26.0.
If image configuration of an image is needed, you can obtain it from the
Config
field.
Deprecate legacy API versions
Deprecated in Release: v25.0 Target For Removal In Release: v26.0
The Docker daemon provides a versioned API for backward compatibility with old clients. Docker clients can perform API-version negotiation to select the most recent API version supported by the daemon (downgrading to and older version of the API when necessary). API version negotiation was introduced in Docker v1.12.0 (API 1.24), and clients before that used a fixed API version.
Docker Engine versions through v25.0 provide support for all API versions included in stable releases for a given platform. For Docker daemons on Linux, the earliest supported API version is 1.12 (corresponding with Docker Engine v1.0.0), whereas for Docker daemons on Windows, the earliest supported API version is 1.24 (corresponding with Docker Engine v1.12.0).
Support for legacy API versions (providing old API versions on current versions of the Docker Engine) is primarily intended to provide compatibility with recent, but still supported versions of the client, which is a common scenario (the Docker daemon may be updated to the latest release, but not all clients may be up-to-date or vice versa). Support for API versions before that (API versions provided by EOL versions of the Docker Daemon) is provided on a ""best effort"" basis.
Use of old API versions is rare, and support for legacy API versions involves significant complexity (Docker 1.0.0 having been released 10 years ago). Because of this, we'll start deprecating support for legacy API versions.
Docker Engine v25.0 by default disables API version older than 1.24 (aligning the minimum supported API version between Linux and Windows daemons). When connecting with a client that uses an API version older than 1.24, the daemon returns an error. The following example configures the Docker CLI to use API version 1.23, which produces an error:
DOCKER_API_VERSION=1.23 docker version
Error response from daemon: client version 1.23 is too old. Minimum supported API version is 1.24,
upgrade your client to a newer version
An environment variable (DOCKER_MIN_API_VERSION
) is introduced that allows
re-enabling older API versions in the daemon. This environment variable must
be set in the daemon's environment (for example, through a
systemd override
file), and the specified
API version must be supported by the daemon (1.12
or higher on Linux, or
1.24
or higher on Windows).
Support for API versions lower than 1.24
will be permanently removed in Docker
Engine v26, and the minimum supported API version will be incrementally raised
in releases following that.
We do not recommend depending on the DOCKER_MIN_API_VERSION
environment
variable other than for exceptional cases where it's not possible to update
old clients, and those clients must be supported.
Container short ID in network Aliases field
Deprecated in Release: v25.0 Removed In Release: v26.0
The Aliases
field returned by docker inspect
contains the container short
ID once the container is started. This behavior is deprecated in v25.0 but
kept until the next release, v26.0. Starting with that version, the Aliases
field will only contain the aliases set through the docker container create
and docker run
flag --network-alias
.
A new field DNSNames
containing the container name (if one was specified),
the hostname, the network aliases, as well as the container short ID, has been
introduced in v25.0 and should be used instead of the Aliases
field.
IsAutomated field, and is-automated
filter on docker search
Deprecated in Release: v25.0 Target For Removal In Release: v26.0
The is_automated
field has been deprecated by Docker Hub's search API.
Consequently, the IsAutomated
field in image search will always be set
to false
in future, and searching for ""is-automated=true"" will yield no
results.
The AUTOMATED
column has been removed from the default docker search
and docker image search
output in v25.0, and the corresponding IsAutomated
templating option will be removed in v26.0.
Logentries logging driver
Deprecated in Release: v24.0 Removed in Release: v25.0
The logentries service SaaS was shut down on November 15, 2022, rendering this logging driver non-functional. Users should no longer use this logging driver, and the driver has been removed in Docker 25.0. Existing containers using this logging-driver are migrated to use the ""local"" logging driver after upgrading.
OOM-score adjust for the daemon
Deprecated in Release: v24.0 Removed in Release: v25.0
The oom-score-adjust
option was added to prevent the daemon from being
OOM-killed before other processes. This option was mostly added as a
convenience, as running the daemon as a systemd unit was not yet common.
Having the daemon set its own limits is not best-practice, and something better handled by the process-manager starting the daemon.
Docker v20.10 and newer no longer adjust the daemon's OOM score by default, instead setting the OOM-score to the systemd unit (OOMScoreAdjust) that's shipped with the packages.
Users currently depending on this feature are recommended to adjust the daemon's OOM score using systemd or through other means, when starting the daemon.
BuildKit build information
Deprecated in Release: v23.0 Removed in Release: v24.0
Build information structures have been introduced in BuildKit v0.10.0 and are generated with build metadata that allows you to see all the sources (images, Git repositories) that were used by the build with their exact versions and also the configuration that was passed to the build. This information is also embedded into the image configuration if one is generated.
Legacy builder for Linux images
Deprecated in Release: v23.0
Docker v23.0 now uses BuildKit by default to build Linux images, and uses the
Buildx CLI component for
docker build
. With this change, docker build
now exposes all advanced features
that BuildKit provides and which were previously only available through the
docker buildx
subcommands.
The Buildx component is installed automatically when installing the docker
CLI
using our .deb
or .rpm
packages, and statically linked binaries are provided
both on download.docker.com
, and through the
docker/buildx-bin
image
on Docker Hub. Refer the
Buildx section for
detailed instructions on installing the Buildx component.
This release marks the beginning of the deprecation cycle of the classic (""legacy"") builder for Linux images. No active development will happen on the classic builder (except for bugfixes). BuildKit development started five Years ago, left the ""experimental"" phase since Docker 18.09, and is already the default builder for Docker Desktop. While we're comfortable that BuildKit is stable for general use, there may be some changes in behavior. If you encounter issues with BuildKit, we encourage you to report issues in the BuildKit issue tracker on GitHub{:target=""blank"" rel=""noopener"" class=""""}
Classic builder for building Windows images
BuildKit does not (yet) provide support for building Windows images, and
docker build
continues to use the classic builder to build native Windows images on Windows daemons.
Legacy builder fallback
Deprecated in Release: v23.0
Docker v23.0 now uses BuildKit by default to build Linux images, which requires the Buildx component to build images with BuildKit. There may be situations where the Buildx component is not available, and BuildKit cannot be used.
To provide a smooth transition to BuildKit as the default builder, Docker v23.0 has an automatic fallback for some situations, or produces an error to assist users to resolve the problem.
In situations where the user did not explicitly opt-in to use BuildKit (i.e.,
DOCKER_BUILDKIT=1
is not set), the CLI automatically falls back to the classic
builder, but prints a deprecation warning:
DEPRECATED: The legacy builder is deprecated and will be removed in a future release.
Install the buildx component to build images with BuildKit:
https://docs.docker.com/go/buildx/
This situation may occur if the docker
CLI is installed using the static binaries,
and the Buildx component is not installed or not installed correctly. This fallback
will be removed in a future release, therefore we recommend to
install the Buildx component
and use BuildKit for your builds, or opt-out of using BuildKit with DOCKER_BUILDKIT=0
.
If you opted-in to use BuildKit (DOCKER_BUILDKIT=1
), but the Buildx component
is missing, an error is printed instead, and the docker build
command fails:
ERROR: BuildKit is enabled but the buildx component is missing or broken.
Install the buildx component to build images with BuildKit:
https://docs.docker.com/go/buildx/
We recommend to
install the Buildx component
to continue using BuildKit for your builds, but alternatively, users can either
unset the DOCKER_BUILDKIT
environment variable to fall back to the legacy builder,
or opt-out of using BuildKit with DOCKER_BUILDKIT=0
.
Be aware that the classic builder is deprecated so both the automatic fallback and opting-out of using BuildKit will no longer be possible in a future release.
Btrfs storage driver on CentOS 7 and RHEL 7
Removed in Release: v23.0
The btrfs
storage driver on CentOS and RHEL was provided as a technology preview
by CentOS and RHEL, but has been deprecated since the
Red Hat Enterprise Linux 7.4 release,
and removed in CentOS 8 and RHEL 8. Users of the btrfs
storage driver on CentOS
are recommended to migrate to a different storage driver, such as overlay2
, which
is now the default storage driver. Docker 23.0 continues to provide the btrfs
storage driver to allow users to migrate to an alternative driver. The next release
of Docker will no longer provide this driver.
Support for encrypted TLS private keys
Deprecated in Release: v20.10
Removed in Release: v23.0
Use of encrypted TLS private keys has been deprecated, and has been removed. Golang has deprecated support for legacy PEM encryption (as specified in RFC 1423), as it is insecure by design (see https://go-review.googlesource.com/c/go/+/264159).
This feature allowed using an encrypted private key with a supplied password, but did not provide additional security as the encryption is known to be broken, and the key is sitting next to the password in the filesystem. Users are recommended to decrypt the private key, and store it un-encrypted to continue using it.
Kubernetes stack and context support
Deprecated in Release: v20.10 Removed in Release: v23.0
Following the deprecation of
Compose on Kubernetes,
support for Kubernetes in the stack
and context
commands has been removed from
the CLI, and options related to this functionality are now either ignored, or may
produce an error.
The following command-line flags are removed from the docker context
subcommands:
--default-stack-orchestrator
- swarm is now the only (and default) orchestrator for stacks.--kubernetes
- the Kubernetes endpoint can no longer be stored indocker context
.--kubeconfig
- exporting a context as a kubeconfig file is no longer supported.
The output produced by the docker context inspect
subcommand no longer contains
information about StackOrchestrator
and Kubernetes
endpoints for new contexts.
The following command-line flags are removed from the docker stack
subcommands:
--kubeconfig
- using a kubeconfig file as context is no longer supported.--namespace
- configuring the Kubernetes namespace for stacks is no longer supported.--orchestrator
- swarm is now the only (and default) orchestrator for stacks.
The DOCKER_STACK_ORCHESTRATOR
, DOCKER_ORCHESTRATOR
, and KUBECONFIG
environment
variables, as well as the stackOrchestrator
option in the ~/.docker/config.json
CLI configuration file are no longer used, and ignored.
Pulling images from non-compliant image registries
Deprecated in Release: v20.10
Docker Engine v20.10 and up includes optimizations to verify if images in the local image cache need updating before pulling, preventing the Docker Engine from making unnecessary API requests. These optimizations require the container image registry to conform to the Open Container Initiative Distribution Specification.
While most registries conform to the specification, we encountered some registries
to be non-compliant, resulting in docker pull
to fail.
As a temporary solution, Docker Engine v20.10 includes a fallback mechanism to
allow docker pull
to be functional when using a non-compliant registry. A
warning message is printed in this situation:
WARNING Failed to pull manifest by the resolved digest. This registry does not
appear to conform to the distribution registry specification; falling back to
pull by tag. This fallback is DEPRECATED, and will be removed in a future
release.
The fallback is added to allow users to either migrate their images to a compliant registry, or for these registries to become compliant.
Note that this fallback only addresses failures on docker pull
. Other commands,
such as docker stack deploy
, or pulling images with containerd
will continue
to fail.
Given that other functionality is still broken with these registries, we consider this fallback a temporary solution, and will remove the fallback in an upcoming major release.
Linux containers on Windows (LCOW) (experimental)
Deprecated in Release: v20.10 Removed in Release: v23.0
The experimental feature to run Linux containers on Windows (LCOW) was introduced as a technical preview in Docker 17.09. While many enhancements were made after its introduction, the feature never reached completeness, and development has now stopped in favor of running Docker natively on Linux in WSL2.
Developers who want to run Linux workloads on a Windows host are encouraged to use Docker Desktop with WSL2 instead.
BLKIO weight options with cgroups v1
Deprecated in Release: v20.10
Specifying blkio weight (docker run --blkio-weight
and docker run --blkio-weight-device
)
is now marked as deprecated when using cgroups v1 because the corresponding features
were
removed in Linux kernel v5.0 and up.
When using cgroups v2, the --blkio-weight
options are implemented using
`io.weight.
Kernel memory limit
Deprecated in Release: v20.10 Removed in Release: v23.0
Specifying kernel memory limit (docker run --kernel-memory
) is no longer supported
because the
Linux kernel deprecated kmem.limit_in_bytes
in v5.4.
The OCI runtime specification now marks this option (as well as --kernel-memory-tcp
)
as
""NOT RECOMMENDED"",
and OCI runtimes such as runc
no longer support this option.
Docker API v1.42 and up now ignores this option when set. Older versions of the API continue to accept the option, but depending on the OCI runtime used, may take no effect.
Note
While not deprecated (yet) in Docker, the OCI runtime specification also deprecated the
memory.kmem.tcp.limit_in_bytes
option. When usingrunc
as runtime, this option takes no effect. The Linux kernel did not explicitly deprecate this feature, and there is a tracking ticket in therunc
issue tracker to determine if this option should be reinstated or if this was an oversight of the Linux kernel maintainers (see opencontainers/runc#3174).The
memory.kmem.tcp.limit_in_bytes
option is only supported with cgroups v1, and not available on installations running with cgroups v2. This option is only supported by the API, and not exposed on thedocker
command-line.
Classic Swarm and overlay networks using cluster store
Deprecated in Release: v20.10 Removed in Release: v23.0
Standalone (""classic"") Swarm has been deprecated, and with that the use of overlay
networks using an external key/value store. The corresponding--cluster-advertise
,
--cluster-store
, and --cluster-store-opt
daemon options have been removed.
Support for legacy ~/.dockercfg
configuration files
Deprecated in Release: v20.10 Removed in Release: v23.0
The Docker CLI up until v1.7.0 used the ~/.dockercfg
file to store credentials
after authenticating to a registry (docker login
). Docker v1.7.0 replaced this
file with a new CLI configuration file, located in ~/.docker/config.json
. When
implementing the new configuration file, the old file (and file-format) was kept
as a fall-back, to assist existing users with migrating to the new file.
Given that the old file format encourages insecure storage of credentials (credentials are stored unencrypted), and that no version of the CLI since Docker v1.7.0 has created this file, support for this file, and its format has been removed.
Configuration options for experimental CLI features
Deprecated in Release: v19.03
Removed in Release: v23.0
The DOCKER_CLI_EXPERIMENTAL
environment variable and the corresponding experimental
field in the CLI configuration file are deprecated. Experimental features are
enabled by default, and these configuration options are no longer functional.
Starting with v23.0, the Docker CLI no longer prints Experimental
for the client
in the output of docker version
, and the field has been removed from the JSON
format.
CLI plugins support
Deprecated in Release: v20.10
CLI Plugin API is now marked as deprecated.
Dockerfile legacy ENV name value
syntax
Deprecated in Release: v20.10
The Dockerfile ENV
instruction allows values to be set using either ENV name=value
or ENV name value
. The latter (ENV name value
) form can be ambiguous, for example,
the following defines a single env-variable (ONE
) with value ""TWO= THREE=world""
,
but may have intended to be setting three env-vars:
ENV ONE TWO= THREE=world
This format also does not allow setting multiple environment-variables in a single
ENV
line in the Dockerfile.
Use of the ENV name value
syntax is discouraged, and may be removed in a future
release. Users are encouraged to update their Dockerfiles to use the ENV name=value
syntax, for example:
ENV ONE="""" TWO="""" THREE=""world""
docker build --stream
flag (experimental)
Deprecated in Release: v20.10 Removed in Release: v20.10
Docker v17.07 introduced an experimental --stream
flag on docker build
which
allowed the build-context to be incrementally sent to the daemon, instead of
unconditionally sending the whole build-context.
This functionality has been reimplemented as part of BuildKit, which uses streaming
by default and the --stream
option will be ignored when using the classic builder,
printing a deprecation warning instead.
Users that want to use this feature are encouraged to enable BuildKit by setting
the DOCKER_BUILDKIT=1
environment variable or through the daemon or CLI configuration
files.
fluentd-async-connect
log opt
Deprecated in Release: v20.10 Removed in Release: v28.0
The --log-opt fluentd-async-connect
option for the fluentd logging driver is
deprecated in favor of --log-opt fluentd-async
.
A deprecation message is logged in the daemon logs if the old option is used:
fluent#New: AsyncConnect is now deprecated, use Async instead
Users are encouraged to use the fluentd-async
option going forward, as support
for the old option will be removed in a future release.
Pushing and pulling with image manifest v2 schema 1
Deprecated in Release: v19.03
Disabled by default in Release: v26.0
Target For Removal In Release: v27.0
The image manifest v2 schema 1 and ""Docker Image v1"" formats were deprecated in favor of the v2 schema 2 and OCI image spec formats.
These legacy formats should no longer be used, and users are recommended to update images to use current formats, or to upgrade to more current images. Starting with Docker v26.0, pulling these images is disabled by default, and produces an error when attempting to pull the image:
$ docker pull ubuntu:10.04
Error response from daemon:
[DEPRECATION NOTICE] Docker Image Format v1 and Docker Image manifest version 2, schema 1 support is disabled by default and will be removed in an upcoming release.
Suggest the author of docker.io/library/ubuntu:10.04 to upgrade the image to the OCI Format or Docker Image manifest v2, schema 2.
More information at https://docs.docker.com/go/deprecated-image-specs/
An environment variable (DOCKER_ENABLE_DEPRECATED_PULL_SCHEMA_1_IMAGE
) is
added in Docker v26.0 that allows re-enabling support for these image formats
in the daemon. This environment variable must be set to a non-empty value in
the daemon's environment (for example, through a
systemd override file).
Support for the DOCKER_ENABLE_DEPRECATED_PULL_SCHEMA_1_IMAGE
environment variable
will be removed in Docker v27.0 after which this functionality is removed permanently.
docker engine
subcommands
Deprecated in Release: v19.03
Removed in Release: v20.10
The docker engine activate
, docker engine check
, and docker engine update
provided an alternative installation method to upgrade Docker Community engines
to Docker Enterprise, using an image-based distribution of the Docker Engine.
This feature was only available on Linux, and only when executed on a local node.
Given the limitations of this feature, and the feature not getting widely adopted,
the docker engine
subcommands will be removed, in favor of installation through
standard package managers.
Top-level docker deploy
subcommand (experimental)
Deprecated in Release: v19.03
Removed in Release: v20.10
The top-level docker deploy
command (using the ""Docker Application Bundle""
(.dab) file format was introduced as an experimental feature in Docker 1.13 /
17.03, but superseded by support for Docker Compose files using the docker stack deploy
subcommand.
docker stack deploy
using ""dab"" files (experimental)
Deprecated in Release: v19.03
Removed in Release: v20.10
With no development being done on this feature, and no active use of the file
format, support for the DAB file format and the top-level docker deploy
command
(hidden by default in 19.03), will be removed, in favour of docker stack deploy
using compose files.
Support for the overlay2.override_kernel_check
storage option
Deprecated in Release: v19.03 Removed in Release: v24.0
This daemon configuration option disabled the Linux kernel version check used to detect if the kernel supported OverlayFS with multiple lower dirs, which is required for the overlay2 storage driver. Starting with Docker v19.03.7, the detection was improved to no longer depend on the kernel version, so this option was no longer used.
AuFS storage driver
Deprecated in Release: v19.03 Removed in Release: v24.0
The aufs
storage driver is deprecated in favor of overlay2
, and has been
removed in a Docker Engine v24.0. Users of the aufs
storage driver must
migrate to a different storage driver, such as overlay2
, before upgrading
to Docker Engine v24.0.
The aufs
storage driver facilitated running Docker on distros that have no
support for OverlayFS, such as Ubuntu 14.04 LTS, which originally shipped with
a 3.14 kernel.
Now that Ubuntu 14.04 is no longer a supported distro for Docker, and overlay2
is available to all supported distros (as they are either on kernel 4.x, or have
support for multiple lowerdirs backported), there is no reason to continue
maintenance of the aufs
storage driver.
Legacy overlay storage driver
Deprecated in Release: v18.09 Removed in Release: v24.0
The overlay
storage driver is deprecated in favor of the overlay2
storage
driver, which has all the benefits of overlay
, without its limitations (excessive
inode consumption). The legacy overlay
storage driver has been removed in
Docker Engine v24.0. Users of the overlay
storage driver should migrate to the
overlay2
storage driver before upgrading to Docker Engine v24.0.
The legacy overlay
storage driver allowed using overlayFS-backed filesystems
on kernels older than v4.x. Now that all supported distributions are able to run overlay2
(as they are either on kernel 4.x, or have support for multiple lowerdirs
backported), there is no reason to keep maintaining the overlay
storage driver.
Device mapper storage driver
Deprecated in Release: v18.09 Disabled by default in Release: v23.0 Removed in Release: v25.0
The devicemapper
storage driver is deprecated in favor of overlay2
, and has
been removed in Docker Engine v25.0. Users of the devicemapper
storage driver
must migrate to a different storage driver, such as overlay2
, before upgrading
to Docker Engine v25.0.
The devicemapper
storage driver facilitates running Docker on older (3.x) kernels
that have no support for other storage drivers (such as overlay2, or btrfs).
Now that support for overlay2
is added to all supported distros (as they are
either on kernel 4.x, or have support for multiple lowerdirs backported), there
is no reason to continue maintenance of the devicemapper
storage driver.
Use of reserved namespaces in engine labels
Deprecated in Release: v18.06
Removed In Release: v20.10
The namespaces com.docker.*
, io.docker.*
, and org.dockerproject.*
in engine labels
were always documented to be reserved, but there was never any enforcement.
Usage of these namespaces will now cause a warning in the engine logs to discourage their use, and will error instead in v20.10 and above.
--disable-legacy-registry
override daemon option
Disabled In Release: v17.12
Removed In Release: v19.03
The --disable-legacy-registry
flag was disabled in Docker 17.12 and will print
an error when used. For this error to be printed, the flag itself is still present,
but hidden. The flag has been removed in Docker 19.03.
Interacting with V1 registries
Disabled By Default In Release: v17.06
Removed In Release: v17.12
Version 1.8.3 added a flag (--disable-legacy-registry=false
) which prevents the
Docker daemon from pull
, push
, and login
operations against v1
registries. Though enabled by default, this signals the intent to deprecate
the v1 protocol.
Support for the v1 protocol to the public registry was removed in 1.13. Any mirror configurations using v1 should be updated to use a v2 registry mirror.
Starting with Docker 17.12, support for V1 registries has been removed, and the
--disable-legacy-registry
flag can no longer be used, and dockerd
will fail to
start when set.
Asynchronous service create
and service update
as default
Deprecated In Release: v17.05
Disabled by default in release: v17.10
Docker 17.05 added an optional --detach=false
option to make the
docker service create
and docker service update
work synchronously. This
option will be enabled by default in Docker 17.10, at which point the --detach
flag can be used to use the previous (asynchronous) behavior.
The default for this option will also be changed accordingly for docker service rollback
and docker service scale
in Docker 17.10.
-g
and --graph
flags on dockerd
Deprecated In Release: v17.05
Removed In Release: v23.0
The -g
or --graph
flag for the dockerd
or docker daemon
command was
used to indicate the directory in which to store persistent data and resource
configuration and has been replaced with the more descriptive --data-root
flag. These flags were deprecated and hidden in v17.05, and removed in v23.0.
Top-level network properties in NetworkSettings
Deprecated In Release: v1.13.0
Target For Removal In Release: v17.12
When inspecting a container, NetworkSettings
contains top-level information
about the default (""bridge"") network;
EndpointID
, Gateway
, GlobalIPv6Address
, GlobalIPv6PrefixLen
, IPAddress
,
IPPrefixLen
, IPv6Gateway
, and MacAddress
.
These properties are deprecated in favor of per-network properties in
NetworkSettings.Networks
. These properties were already ""deprecated"" in
Docker 1.9, but kept around for backward compatibility.
Refer to #17538 for further information.
filter
option for /images/json
endpoint
Deprecated In Release: v1.13.0
Removed In Release: v20.10
The filter
option to filter the list of image by reference (name or name:tag)
is now implemented as a regular filter, named reference
.
repository:shortid
image references
Deprecated In Release: v1.13.0
Removed In Release: v17.12
The repository:shortid
syntax for referencing images is very little used,
collides with tag references, and can be confused with digest references.
Support for the repository:shortid
notation to reference images was removed
in Docker 17.12.
docker daemon
subcommand
Deprecated In Release: v1.13.0
Removed In Release: v17.12
The daemon is moved to a separate binary (dockerd
), and should be used instead.
Duplicate keys with conflicting values in engine labels
Deprecated In Release: v1.13.0
Removed In Release: v17.12
When setting duplicate keys with conflicting values, an error will be produced, and the daemon will fail to start.
MAINTAINER
in Dockerfile
Deprecated In Release: v1.13.0
MAINTAINER
was an early very limited form of LABEL
which should be used instead.
API calls without a version
Deprecated In Release: v1.13.0
Target For Removal In Release: v17.12
API versions should be supplied to all API calls to ensure compatibility with
future Engine versions. Instead of just requesting, for example, the URL
/containers/json
, you must now request /v1.25/containers/json
.
Backing filesystem without d_type
support for overlay/overlay2
Deprecated In Release: v1.13.0
Removed In Release: v17.12
The overlay and overlay2 storage driver does not work as expected if the backing
filesystem does not support d_type
. For example, XFS does not support d_type
if it is formatted with the ftype=0
option.
Support for these setups has been removed, and Docker v23.0 and up now fails to
start when attempting to use the overlay2
or overlay
storage driver on a
backing filesystem without d_type
support.
Refer to #27358 for details.
--automated
and --stars
flags on docker search
Deprecated in Release: v1.12.0
Removed In Release: v20.10
The docker search --automated
and docker search --stars
options are deprecated.
Use docker search --filter=is-automated=<true|false>
and docker search --filter=stars=...
instead.
-h
shorthand for --help
Deprecated In Release: v1.12.0
Target For Removal In Release: v17.09
The shorthand (-h
) is less common than --help
on Linux and cannot be used
on all subcommands (due to it conflicting with, e.g. -h
/ --hostname
on
docker create
). For this reason, the -h
shorthand was not printed in the
""usage"" output of subcommands, nor documented, and is now marked ""deprecated"".
-e
and --email
flags on docker login
Deprecated In Release: v1.11.0
Removed In Release: v17.06
The docker login
no longer automatically registers an account with the target registry if the given username doesn't exist. Due to this change, the email flag is no longer required, and will be deprecated.
Separator (:
) of --security-opt
flag on docker run
Deprecated In Release: v1.11.0
Target For Removal In Release: v17.06
The flag --security-opt
doesn't use the colon separator (:
) anymore to divide keys and values, it uses the equal symbol (=
) for consistency with other similar flags, like --storage-opt
.
Ambiguous event fields in API
Deprecated In Release: v1.10.0
The fields ID
, Status
and From
in the events API have been deprecated in favor of a more rich structure.
See the events API documentation for the new format.
-f
flag on docker tag
Deprecated In Release: v1.10.0
Removed In Release: v1.12.0
To make tagging consistent across the various docker
commands, the -f
flag on the docker tag
command is deprecated. It is no longer necessary to specify -f
to move a tag from one image to another. Nor will docker
generate an error if the -f
flag is missing and the specified tag is already in use.
HostConfig at API container start
Deprecated In Release: v1.10.0
Removed In Release: v1.12.0
Passing an HostConfig
to POST /containers/{name}/start
is deprecated in favor of
defining it at container creation (POST /containers/create
).
--before
and --since
flags on docker ps
Deprecated In Release: v1.10.0
Removed In Release: v1.12.0
The docker ps --before
and docker ps --since
options are deprecated.
Use docker ps --filter=before=...
and docker ps --filter=since=...
instead.
Driver-specific log tags
Deprecated In Release: v1.9.0
Removed In Release: v1.12.0
Log tags are now generated in a standard way across different logging drivers.
Because of which, the driver specific log tag options syslog-tag
, gelf-tag
and
fluentd-tag
have been deprecated in favor of the generic tag
option.
$ docker --log-driver=syslog --log-opt tag=""{{.ImageName}}/{{.Name}}/{{.ID}}""
Docker Content Trust ENV passphrase variables name change
Deprecated In Release: v1.9.0
Removed In Release: v1.12.0
Since 1.9, Docker Content Trust Offline key has been renamed to Root key and the Tagging key has been renamed to Repository key. Due to this renaming, we're also changing the corresponding environment variables
- DOCKER_CONTENT_TRUST_OFFLINE_PASSPHRASE is now named DOCKER_CONTENT_TRUST_ROOT_PASSPHRASE
- DOCKER_CONTENT_TRUST_TAGGING_PASSPHRASE is now named DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE
/containers/(id or name)/copy
endpoint
Deprecated In Release: v1.8.0
Removed In Release: v1.12.0
The endpoint /containers/(id or name)/copy
is deprecated in favor of /containers/(id or name)/archive
.
LXC built-in exec driver
Deprecated In Release: v1.8.0
Removed In Release: v1.10.0
The built-in LXC execution driver, the lxc-conf flag, and API fields have been removed.
Old Command Line Options
Deprecated In Release: v1.8.0
Removed In Release: v1.10.0
The flags -d
and --daemon
are deprecated. Use the separate dockerd
binary instead.
The following single-dash (-opt
) variant of certain command line options
are deprecated and replaced with double-dash options (--opt
):
docker attach -nostdin
docker attach -sig-proxy
docker build -no-cache
docker build -rm
docker commit -author
docker commit -run
docker events -since
docker history -notrunc
docker images -notrunc
docker inspect -format
docker ps -beforeId
docker ps -notrunc
docker ps -sinceId
docker rm -link
docker run -cidfile
docker run -dns
docker run -entrypoint
docker run -expose
docker run -link
docker run -lxc-conf
docker run -n
docker run -privileged
docker run -volumes-from
docker search -notrunc
docker search -stars
docker search -t
docker search -trusted
docker tag -force
The following double-dash options are deprecated and have no replacement:
docker run --cpuset
docker run --networking
docker ps --since-id
docker ps --before-id
docker search --trusted
Deprecated In Release: v1.5.0
Removed In Release: v1.12.0
The single-dash (-help
) was removed, in favor of the double-dash --help
--api-enable-cors
flag on dockerd
Deprecated In Release: v1.6.0
Removed In Release: v17.09
The flag --api-enable-cors
is deprecated since v1.6.0. Use the flag
--api-cors-header
instead.
--run
flag on docker commit
Deprecated In Release: v0.10.0
Removed In Release: v1.13.0
The flag --run
of the docker commit
command (and its short version -run
) were deprecated in favor
of the --changes
flag that allows to pass Dockerfile
commands.
Three arguments form in docker import
Deprecated In Release: v0.6.7
Removed In Release: v1.12.0
The docker import
command format file|URL|- [REPOSITORY [TAG]]
is deprecated since November 2013. It's no longer supported.",,,
3ab45a76eeb0d6d50b31648c0f96de8a44c572bc989064ae9d62d1ae227c2cb8,"Just-in-Time provisioning
Just-in-Time (JIT) provisioning automatically creates and updates user accounts after every successful single sign-on (SSO) authentication. JIT verifies that the user signing in belongs to the organization and the teams assigned to them in your identity provider (IdP). When you create your SSO connection, JIT provisioning is turned on by default.
SSO authentication with JIT provisioning enabled
When a user signs in with SSO and your SSO configuration has JIT provisioning enabled, the following steps occur automatically:
The system checks if a Docker account exists for the user's email address.
- If an account exists: The system uses the existing account and updates the user's full name if necessary.
- If no account exists: A new Docker account is created using basic user attributes (email, name, and surname). A unique username is generated based on the user's email, name, and random numbers to ensure all usernames are unique across the platform.
The system checks for any pending invitations to the SSO organization.
- Invitation found: The invitation is automatically accepted.
- Invitation includes a specific group: The user is added to that group within the SSO organization.
The system verifies if the IdP has shared group mappings during authentication.
- Group mappings provided: The user is assigned to the relevant organizations and teams.
- No group mappings provided: The system checks if the user is already part of the organization. If not, the user is added to the default organization and team configured in the SSO connection.
The following graphic provides an overview of SSO authentication with JIT enabled:
SSO authentication with JIT provisioning disabled
When JIT provisioning is disabled in your SSO connection, the following actions occur during authentication:
The system checks if a Docker account exists for the user's email address.
- If an account exists: The system uses the existing account and updates the user's full name if necessary.
- If no account exists: A new Docker account is created using basic user attributes (email, name, and surname). A unique username is generated based on the user's email, name, and random numbers to ensure all usernames are unique across the platform.
The system checks for any pending invitations to the SSO organization.
- Invitation found: If the user is a member of the organization or has a pending invitation, sign-in is successful, and the invitation is automatically accepted.
- No invitation found: If the user is not a member of the organization and has no pending invitation, the sign-in fails, and an
Access denied
error appears. The user must contact an administrator to be invited to the organization.
With JIT disabled, group mapping is only available if you have SCIM enabled. If SCIM is not enabled, users won't be auto-provisioned to groups.
The following graphic provides an overview of SSO authentication with JIT disabled:
Disable JIT provisioning
Warning
Disabling JIT provisioning may disrupt your users' access and workflows. With JIT disabled, users will not be automatically added to your organization. Users must already be a member of the organization or have a pending invitation to successfully sign in through SSO. To auto-provision users with JIT disabled, use SCIM.
You may want to disable JIT provisioning for reasons such as the following:
- You have multiple organizations, have SCIM enabled, and want SCIM to be the source of truth for provisioning
- You want to control and restrict usage based on your organization's security configuration, and want to use SCIM to provision access
Users are provisioned with JIT by default. If you enable SCIM, you can disable JIT:
- In the Admin Console, select your organization.
- Select SSO and SCIM.
- In the SSO connections table, select the Action icon and then Disable JIT provisioning.
- Select Disable to confirm.",,,
423985fa2cd9b96c02a489765b65b56c40c38d566d1d916020bd930e5621bc61,"Docker Hub quickstart
Docker Hub provides a vast library of pre-built images and resources, accelerating development workflows and reducing setup time. You can build upon pre-built images from Docker Hub and then use repositories to share and distribute your own images with your team or millions of other developers.
This guide shows you how to find and run a pre-built image. It then walks you through creating a custom image and sharing it through Docker Hub.
Prerequisites
Step 1: Find an image in Docker Hub's library
You can search for content in Docker Hub itself, in the Docker Desktop Dashboard, or by using the CLI.
To search or browse for content on Docker Hub:
Navigate to the Docker Hub Explore page.
On the Explore page, you can browse by catalog or category, or use the search to quickly find content.
Under Categories, select Web servers.
After the results are displayed, you can further filter the results using the filters on the left side of the page.
In the filters, select Docker Official Image.
Filtering by Trusted Content ensures that you see only high-quality, secure images curated by Docker and verified publishing partners.
In the results, select the nginx image.
Selecting the image opens the image's page where you can learn more about how to use the image. On the page, you'll also find the
docker pull
command to pull the image.
Open the Docker Desktop Dashboard.
Select the Docker Hub view.
In the Docker Hub view, you can browse by catalog or category, or use the search to quickly find content.
Leave the search box empty and then select Search.
The search results are shown with additional filters now next to the search box.
Select the search filter icon, and then select Docker Official Image and Web Servers.
In the results, select the nginx image.
Open a terminal window.
Tip
The Docker Desktop Dashboard contains a built-in terminal. At the bottom of the Dashboard, select >_ Terminal to open it.
In the terminal, run the following command.
$ docker search --filter is-official=true nginx
Unlike the Docker Hub and Docker Desktop interfaces, you can't browse by category using the
docker search
command. For more details about the command, see docker search.
Now that you've found an image, it's time to pull and run it on your device.
Step 2: Pull and run an image from Docker Hub
You can run images from Docker Hub using the CLI or Docker Desktop Dashboard.
In the Docker Desktop Dashboard, select the nginx image in the Docker Hub view. For more details, see Step 1: Find an image in Docker Hub's library.
On the nginx screen, select Run.
If the image doesn't exist on your device, it is automatically pulled from Docker Hub. Pulling the image may take a few seconds or minutes depending on your connection. After the image has been pulled, a window appears in Docker Desktop and you can specify run options.
In the Host port option, specify
8080
.Select Run.
The container logs appear after the container starts.
Select the 8080:80 link to open the server, or visit https://localhost:8080 in your web browser.
In the Docker Desktop Dashboard, select the Stop button to stop the container.
Open a terminal window.
Tip
The Docker Desktop Dashboard contains a built-in terminal. At the bottom of the Dashboard, select >_ Terminal to open it.
In your terminal, run the following command to pull and run the Nginx image.
$ docker run -p 8080:80 --rm nginx
The
docker run
command automatically pulls and runs the image without the need to rundocker pull
first. To learn more about the command and its options, see thedocker run
CLI reference. After running the command, you should see output similar to the following.Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx a480a496ba95: Pull complete f3ace1b8ce45: Pull complete 11d6fdd0e8a7: Pull complete f1091da6fd5c: Pull complete 40eea07b53d8: Pull complete 6476794e50f4: Pull complete 70850b3ec6b2: Pull complete Digest: sha256:28402db69fec7c17e179ea87882667f1e054391138f77ffaf0c3eb388efc3ffb Status: Downloaded newer image for nginx:latest /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf 10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf /docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh /docker-entrypoint.sh: Configuration complete; ready for start up 2024/11/07 21:43:41 [notice] 1#1: using the ""epoll"" event method 2024/11/07 21:43:41 [notice] 1#1: nginx/1.27.2 2024/11/07 21:43:41 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14) 2024/11/07 21:43:41 [notice] 1#1: OS: Linux 6.10.11-linuxkit 2024/11/07 21:43:41 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576 2024/11/07 21:43:41 [notice] 1#1: start worker processes 2024/11/07 21:43:41 [notice] 1#1: start worker process 29 ...
Visit https://localhost:8080 to view the default Nginx page and verify that the container is running.
In the terminal, press
Ctrl+C to stop the container.
You've now run a web server without any set up or configuration. Docker Hub provides instant access to pre-built, ready-to-use container images, letting you quickly pull and run applications without needing to install or configure software manually. With Docker Hub's vast library of images, you can experiment with and deploy applications effortlessly, boosting productivity and making it easy to try out new tools, set up development environments, or build on top of existing software.
You can also extend images from Docker Hub, letting you quickly build and customize your own images to suit specific needs.
Step 3: Build and push an image to Docker Hub
Create a Dockerfile to specify your application:
FROM nginx RUN echo ""<h1>Hello world from Docker!</h1>"" > /usr/share/nginx/html/index.html
This Dockerfile extends the Nginx image from Docker Hub to create a simple website. With just a few lines, you can easily set up, customize, and share a static website using Docker.
Run the following command to build your image. Replace
<YOUR-USERNAME>
with your Docker ID.$ docker build -t <YOUR-USERNAME>/nginx-custom .
This command builds your image and tags it so that Docker understands which repository to push it to in Docker Hub. To learn more about the command and its options, see the
docker build
CLI reference. After running the command, you should see output similar to the following.[+] Building 0.6s (6/6) FINISHED docker:desktop-linux => [internal] load build definition from Dockerfile 0.0s => => transferring dockerfile: 128B 0.0s => [internal] load metadata for docker.io/library/nginx:latest 0.0s => [internal] load .dockerignore 0.0s => => transferring context: 2B 0.0s => [1/2] FROM docker.io/library/nginx:latest 0.1s => [2/2] RUN echo ""<h1>Hello world from Docker!</h1>"" > /usr/share/ 0.2s => exporting to image 0.1s => => exporting layers 0.0s => => writing image sha256:f85ab68f4987847713e87a95c39009a5c9f4ad78 0.0s => => naming to docker.io/mobyismyname/nginx-custom 0.0s
Run the following command to test your image. Replace
<YOUR-USERNAME>
with your Docker ID.$ docker run -p 8080:80 --rm <YOUR-USERNAME>/nginx-custom
Visit https://localhost:8080 to view the page. You should see
Hello world from Docker!
.In the terminal, press CTRL+C to stop the container.
Sign in to Docker Desktop. You must be signed in before pushing an image to Docker Hub.
Run the following command to push your image to Docker Hub. Replace
<YOUR-USERNAME>
with your Docker ID.$ docker push <YOUR-USERNAME>/nginx-custom
Note
You must be signed in to Docker Hub through Docker Desktop or the command line, and you must also name your images correctly, as per the above steps.
The command pushes the image to Docker Hub and automatically creates the repository if it doesn't exist. To learn more about the command, see the
docker push
CLI reference. After running the command, you should see output similar to the following.Using default tag: latest The push refers to repository [docker.io/mobyismyname/nginx-custom] d0e011850342: Pushed e4e9e9ad93c2: Mounted from library/nginx 6ac729401225: Mounted from library/nginx 8ce189049cb5: Mounted from library/nginx 296af1bd2844: Mounted from library/nginx 63d7ce983cd5: Mounted from library/nginx b33db0c3c3a8: Mounted from library/nginx 98b5f35ea9d3: Mounted from library/nginx latest: digest: sha256:7f5223ae866e725a7f86b856c30edd3b86f60d76694df81d90b08918d8de1e3f size: 1985
Now that you've created a repository and pushed your image, it's time to view your repository and explore its options.
Step 4: View your repository on Docker Hub and explore options
You can view your Docker Hub repositories in the Docker Hub or Docker Desktop interface.
Go to Docker Hub and sign in.
After signing in, you should be on the Repositories page. If not, then go to the Repositories page.
Find the nginx-custom repository and select that row.
After selecting the repository, you should see more details and options for your repository.
Sign in to Docker Desktop.
Select the Images view.
Select the Hub repositories tab.
A list of your Docker Hub repositories appears.
Find the nginx-custom repository, hover over the row, and then select View in Hub.
Docker Hub opens and you are able to view more details about the image.
You've now verified that your repository exists on Docker Hub, and you've discovered more options for it. View the next steps to learn more about some of these options.
Next steps
Add repository information to help users find and use your image.",,,
5d3627add19da009b245756dcb1f7d15b577e2c4491436fd46a65b4202206de0,"Use Swarm mode routing mesh
Docker Engine Swarm mode makes it easy to publish ports for services to make them available to resources outside the swarm. All nodes participate in an ingress routing mesh. The routing mesh enables each node in the swarm to accept connections on published ports for any service running in the swarm, even if there's no task running on the node. The routing mesh routes all incoming requests to published ports on available nodes to an active container.
To use the ingress network in the swarm, you need to have the following ports open between the swarm nodes before you enable Swarm mode:
- Port
7946
TCP/UDP for container network discovery. - Port
4789
UDP (configurable) for the container ingress network.
When setting up networking in a Swarm, special care should be taken. Consult the tutorial for an overview.
You must also open the published port between the swarm nodes and any external resources, such as an external load balancer, that require access to the port.
You can also bypass the routing mesh for a given service.
Publish a port for a service
Use the --publish
flag to publish a port when you create a service. target
is used to specify the port inside the container, and published
is used to
specify the port to bind on the routing mesh. If you leave off the published
port, a random high-numbered port is bound for each service task. You
need to inspect the task to determine the port.
$ docker service create \
--name <SERVICE-NAME> \
--publish published=<PUBLISHED-PORT>,target=<CONTAINER-PORT> \
<IMAGE>
Note
The older form of this syntax is a colon-separated string, where the published port is first and the target port is second, such as
-p 8080:80
. The new syntax is preferred because it is easier to read and allows more flexibility.
The <PUBLISHED-PORT>
is the port where the swarm makes the service available.
If you omit it, a random high-numbered port is bound.
The <CONTAINER-PORT>
is the port where the container listens. This parameter
is required.
For example, the following command publishes port 80 in the nginx container to port 8080 for any node in the swarm:
$ docker service create \
--name my-web \
--publish published=8080,target=80 \
--replicas 2 \
nginx
When you access port 8080 on any node, Docker routes your request to an active container. On the swarm nodes themselves, port 8080 may not actually be bound, but the routing mesh knows how to route the traffic and prevents any port conflicts from happening.
The routing mesh listens on the published port for any IP address assigned to the node. For externally routable IP addresses, the port is available from outside the host. For all other IP addresses the access is only available from within the host.
You can publish a port for an existing service using the following command:
$ docker service update \
--publish-add published=<PUBLISHED-PORT>,target=<CONTAINER-PORT> \
<SERVICE>
You can use docker service inspect
to view the service's published port. For
instance:
$ docker service inspect --format=""{{json .Endpoint.Spec.Ports}}"" my-web
[{""Protocol"":""tcp"",""TargetPort"":80,""PublishedPort"":8080}]
The output shows the <CONTAINER-PORT>
(labeled TargetPort
) from the containers and the
<PUBLISHED-PORT>
(labeled PublishedPort
) where nodes listen for requests for the service.
Publish a port for TCP only or UDP only
By default, when you publish a port, it is a TCP port. You can
specifically publish a UDP port instead of or in addition to a TCP port. When
you publish both TCP and UDP ports, if you omit the protocol specifier,
the port is published as a TCP port. If you use the longer syntax (recommended),
set the protocol
key to either tcp
or udp
.
TCP only
Long syntax:
$ docker service create --name dns-cache \
--publish published=53,target=53 \
dns-cache
Short syntax:
$ docker service create --name dns-cache \
-p 53:53 \
dns-cache
TCP and UDP
Long syntax:
$ docker service create --name dns-cache \
--publish published=53,target=53 \
--publish published=53,target=53,protocol=udp \
dns-cache
Short syntax:
$ docker service create --name dns-cache \
-p 53:53 \
-p 53:53/udp \
dns-cache
UDP only
Long syntax:
$ docker service create --name dns-cache \
--publish published=53,target=53,protocol=udp \
dns-cache
Short syntax:
$ docker service create --name dns-cache \
-p 53:53/udp \
dns-cache
Bypass the routing mesh
By default, swarm services which publish ports do so using the routing mesh. When you connect to a published port on any swarm node (whether it is running a given service or not), you are redirected to a worker which is running that service, transparently. Effectively, Docker acts as a load balancer for your swarm services.
You can bypass the routing mesh, so that when you access the bound port on a
given node, you are always accessing the instance of the service running on
that node. This is referred to as host
mode. There are a few things to keep
in mind.
If you access a node which is not running a service task, the service does not listen on that port. It is possible that nothing is listening, or that a completely different application is listening.
If you expect to run multiple service tasks on each node (such as when you have 5 nodes but run 10 replicas), you cannot specify a static target port. Either allow Docker to assign a random high-numbered port (by leaving off the
published
), or ensure that only a single instance of the service runs on a given node, by using a global service rather than a replicated one, or by using placement constraints.
To bypass the routing mesh, you must use the long --publish
service and
set mode
to host
. If you omit the mode
key or set it to ingress
, the
routing mesh is used. The following command creates a global service using
host
mode and bypassing the routing mesh.
$ docker service create --name dns-cache \
--publish published=53,target=53,protocol=udp,mode=host \
--mode global \
dns-cache
Configure an external load balancer
You can configure an external load balancer for swarm services, either in combination with the routing mesh or without using the routing mesh at all.
Using the routing mesh
You can configure an external load balancer to route requests to a swarm service. For example, you could configure HAProxy to balance requests to an nginx service published to port 8080.
In this case, port 8080 must be open between the load balancer and the nodes in the swarm. The swarm nodes can reside on a private network that is accessible to the proxy server, but that is not publicly accessible.
You can configure the load balancer to balance requests between every node in
the swarm even if there are no tasks scheduled on the node. For example, you
could have the following HAProxy configuration in /etc/haproxy/haproxy.cfg
:
global
log /dev/log local0
log /dev/log local1 notice
...snip...
# Configure HAProxy to listen on port 80
frontend http_front
bind *:80
stats uri /haproxy?stats
default_backend http_back
# Configure HAProxy to route requests to swarm nodes on port 8080
backend http_back
balance roundrobin
server node1 192.168.99.100:8080 check
server node2 192.168.99.101:8080 check
server node3 192.168.99.102:8080 check
When you access the HAProxy load balancer on port 80, it forwards requests to nodes in the swarm. The swarm routing mesh routes the request to an active task. If, for any reason the swarm scheduler dispatches tasks to different nodes, you don't need to reconfigure the load balancer.
You can configure any type of load balancer to route requests to swarm nodes. To learn more about HAProxy, see the HAProxy documentation.
Without the routing mesh
To use an external load balancer without the routing mesh, set --endpoint-mode
to dnsrr
instead of the default value of vip
. In this case, there is not a
single virtual IP. Instead, Docker sets up DNS entries for the service such that
a DNS query for the service name returns a list of IP addresses, and the client
connects directly to one of these.
You can't use --endpoint-mode dnsrr
together with --publish mode=ingress
.
You must run your own load balancer in front of the service. A DNS query for
the service name on the Docker host returns a list of IP addresses for the
nodes running the service. Configure your load balancer to consume this list
and balance the traffic across the nodes.
See
Configure service discovery.",,,
2acb6a07400f813e4e5743f3527e5bf3fdd90534f4e6b4b9d24d052f1bc9d4ee,"Read the daemon logs
The daemon logs may help you diagnose problems. The logs may be saved in one of a few locations, depending on the operating system configuration and the logging subsystem used:
| Operating system | Location |
|---|---|
| Linux | Use the command journalctl -xu docker.service (or read /var/log/syslog or /var/log/messages , depending on your Linux Distribution) |
macOS (dockerd logs) | ~/Library/Containers/com.docker.docker/Data/log/vm/dockerd.log |
macOS (containerd logs) | ~/Library/Containers/com.docker.docker/Data/log/vm/containerd.log |
Windows (WSL2) (dockerd logs) | %LOCALAPPDATA%\Docker\log\vm\dockerd.log |
Windows (WSL2) (containerd logs) | %LOCALAPPDATA%\Docker\log\vm\containerd.log |
| Windows (Windows containers) | Logs are in the Windows Event Log |
To view the dockerd
logs on macOS, open a terminal Window, and use the tail
command with the -f
flag to ""follow"" the logs. Logs will be printed until you
terminate the command using CTRL+c
:
$ tail -f ~/Library/Containers/com.docker.docker/Data/log/vm/dockerd.log
2021-07-28T10:21:21Z dockerd time=""2021-07-28T10:21:21.497642089Z"" level=debug msg=""attach: stdout: begin""
2021-07-28T10:21:21Z dockerd time=""2021-07-28T10:21:21.497714291Z"" level=debug msg=""attach: stderr: begin""
2021-07-28T10:21:21Z dockerd time=""2021-07-28T10:21:21.499798390Z"" level=debug msg=""Calling POST /v1.41/containers/35fc5ec0ffe1ad492d0a4fbf51fd6286a087b89d4dd66367fa3b7aec70b46a40/wait?condition=removed""
2021-07-28T10:21:21Z dockerd time=""2021-07-28T10:21:21.518403686Z"" level=debug msg=""Calling GET /v1.41/containers/35fc5ec0ffe1ad492d0a4fbf51fd6286a087b89d4dd66367fa3b7aec70b46a40/json""
2021-07-28T10:21:21Z dockerd time=""2021-07-28T10:21:21.527074928Z"" level=debug msg=""Calling POST /v1.41/containers/35fc5ec0ffe1ad492d0a4fbf51fd6286a087b89d4dd66367fa3b7aec70b46a40/start""
2021-07-28T10:21:21Z dockerd time=""2021-07-28T10:21:21.528203579Z"" level=debug msg=""container mounted via layerStore: &{/var/lib/docker/overlay2/6e76ffecede030507fcaa576404e141e5f87fc4d7e1760e9ce5b52acb24
...
^C
Enable debugging
There are two ways to enable debugging. The recommended approach is to set the
debug
key to true
in the daemon.json
file. This method works for every
Docker platform.
Edit the
daemon.json
file, which is usually located in/etc/docker/
. You may need to create this file, if it doesn't yet exist. On macOS or Windows, don't edit the file directly. Instead, edit the file through the Docker Desktop settings.If the file is empty, add the following:
{ ""debug"": true }
If the file already contains JSON, just add the key
""debug"": true
, being careful to add a comma to the end of the line if it's not the last line before the closing bracket. Also verify that if thelog-level
key is set, it's set to eitherinfo
ordebug
.info
is the default, and possible values aredebug
,info
,warn
,error
,fatal
.Send a
HUP
signal to the daemon to cause it to reload its configuration. On Linux hosts, use the following command.$ sudo kill -SIGHUP $(pidof dockerd)
On Windows hosts, restart Docker.
Instead of following this procedure, you can also stop the Docker daemon and
restart it manually with the debug flag -D
. However, this may result in Docker
restarting with a different environment than the one the hosts' startup scripts
create, and this may make debugging more difficult.
Force a stack trace to be logged
If the daemon is unresponsive, you can force a full stack trace to be logged by
sending a SIGUSR1
signal to the daemon.
Linux:
$ sudo kill -SIGUSR1 $(pidof dockerd)
Windows Server:
Download docker-signal.
Get the process ID of dockerd
Get-Process dockerd
.Run the executable with the flag
--pid=<PID of daemon>
.
This forces a stack trace to be logged but doesn't stop the daemon. Daemon logs show the stack trace or the path to a file containing the stack trace if it was logged to a file.
The daemon continues operating after handling the SIGUSR1
signal and dumping
the stack traces to the log. The stack traces can be used to determine the state
of all goroutines and threads within the daemon.
View stack traces
The Docker daemon log can be viewed by using one of the following methods:
- By running
journalctl -u docker.service
on Linux systems usingsystemctl
/var/log/messages
,/var/log/daemon.log
, or/var/log/docker.log
on older Linux systems
Note
It isn't possible to manually generate a stack trace on Docker Desktop for Mac or Docker Desktop for Windows. However, you can click the Docker taskbar icon and choose Troubleshoot to send information to Docker if you run into issues.
Look in the Docker logs for a message like the following:
...goroutine stacks written to /var/run/docker/goroutine-stacks-2017-06-02T193336z.log
The locations where Docker saves these stack traces and dumps depends on your operating system and configuration. You can sometimes get useful diagnostic information straight from the stack traces and dumps. Otherwise, you can provide this information to Docker for help diagnosing the problem.",,,
f2843c30dff6dfce8f90fec28bc90a001e94c74d0d3661ac75c8eb1283708e97,"Create a swarm
After you complete the tutorial setup steps, you're ready to create a swarm. Make sure the Docker Engine daemon is started on the host machines.
Open a terminal and ssh into the machine where you want to run your manager node. This tutorial uses a machine named
manager1
.Run the following command to create a new swarm:
$ docker swarm init --advertise-addr <MANAGER-IP>
In the tutorial, the following command creates a swarm on the
manager1
machine:$ docker swarm init --advertise-addr 192.168.99.100 Swarm initialized: current node (dxn1zf6l61qsb1josjja83ngz) is now a manager. To add a worker to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \ 192.168.99.100:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
The
--advertise-addr
flag configures the manager node to publish its address as192.168.99.100
. The other nodes in the swarm must be able to access the manager at the IP address.The output includes the commands to join new nodes to the swarm. Nodes will join as managers or workers depending on the value for the
--token
flag.Run
docker info
to view the current state of the swarm:$ docker info Containers: 2 Running: 0 Paused: 0 Stopped: 2 ...snip... Swarm: active NodeID: dxn1zf6l61qsb1josjja83ngz Is Manager: true Managers: 1 Nodes: 1 ...snip...
Run the
docker node ls
command to view information about nodes:$ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS dxn1zf6l61qsb1josjja83ngz * manager1 Ready Active Leader
The
*
next to the node ID indicates that you're currently connected on this node.Docker Engine Swarm mode automatically names the node with the machine host name. The tutorial covers other columns in later steps.
Next steps
Next, you'll add two more nodes to the cluster.",,,
60cd1c397692218cda059099190be82641c62307893ca9d1d33b7e33b27bf17c,"Build drivers
Build drivers are configurations for how and where the BuildKit backend runs. Driver settings are customizable and allows fine-grained control of the builder. Buildx supports the following drivers:
docker
: uses the BuildKit library bundled into the Docker daemon.docker-container
: creates a dedicated BuildKit container using Docker.kubernetes
: creates BuildKit pods in a Kubernetes cluster.remote
: connects directly to a manually managed BuildKit daemon.
Different drivers support different use cases. The default docker
driver
prioritizes simplicity and ease of use. It has limited support for advanced
features like caching and output formats, and isn't configurable. Other drivers
provide more flexibility and are better at handling advanced scenarios.
The following table outlines some differences between drivers.
| Feature | docker | docker-container | kubernetes | remote |
|---|---|---|---|---|
| Automatically load image | ✅ | |||
| Cache export | ✓* | ✅ | ✅ | ✅ |
| Tarball output | ✅ | ✅ | ✅ | |
| Multi-arch images | ✅ | ✅ | ✅ | |
| BuildKit configuration | ✅ | ✅ | Managed externally |
* The docker
driver doesn't support all cache export options.
See
Cache storage backends for more information.
Loading to local image store
Unlike when using the default docker
driver, images built using other drivers
aren't automatically loaded into the local image store. If you don't specify an
output, the build result is exported to the build cache only.
To build an image using a non-default driver and load it to the image store,
use the --load
flag with the build command:
$ docker buildx build --load -t <image> --builder=container .
...
=> exporting to oci image format 7.7s
=> => exporting layers 4.9s
=> => exporting manifest sha256:4e4ca161fa338be2c303445411900ebbc5fc086153a0b846ac12996960b479d3 0.0s
=> => exporting config sha256:adf3eec768a14b6e183a1010cb96d91155a82fd722a1091440c88f3747f1f53f 0.0s
=> => sending tarball 2.8s
=> importing to docker
With this option, the image is available in the image store after the build finishes:
$ docker image ls
REPOSITORY TAG IMAGE ID CREATED SIZE
<image> latest adf3eec768a1 2 minutes ago 197MB
Load by default
You can configure the custom build drivers to behave in a similar way to the
default docker
driver, and load images to the local image store by default.
To do so, set the default-load
driver option when creating the builder:
$ docker buildx create --driver-opt default-load=true
Note that, just like with the docker
driver, if you specify a different
output format with --output
, the result will not be loaded to the image store
unless you also explicitly specify --output type=docker
or use the --load
flag.
What's next
Read about each driver:",,,
bd32a4b49e23f3dd819ed0fceed506e1a101c98ae621bf56ca74c47255a01989,"Move images between repositories
Consolidating and organizing your Docker images across repositories can streamline your workflows, whether you're managing personal projects or contributing to an organization. This topic explains how to move images between Docker Hub repositories, ensuring that your content remains accessible and organized under the correct accounts or namespaces.
Personal to personal
When consolidating personal repositories, you can pull private images from the initial repository and push them into another repository owned by you. To avoid losing your private images, perform the following steps:
Sign up for a new Docker account with a personal subscription.
Sign in to Docker using your original Docker account
Pull your images:
$ docker pull namespace1/docker101tutorial
Tag your private images with your newly created Docker username, for example:
$ docker tag namespace1/docker101tutorial new_namespace/docker101tutorial
Using
docker login
from the CLI, sign in with your newly created Docker account, and push your newly tagged private images to your new Docker account namespace:$ docker push new_namespace/docker101tutorial
The private images that existed in your previous account are now available in your new account.
Personal to an organization
To avoid losing your private images, you can pull your private images from your personal account and push them to an organization that's owned by you.
Navigate to Docker Hub and select Organizations.
Select the applicable organization and verify that your user account is a member of the organization.
Sign in to Docker Hub using your original Docker account, and pull your images:
$ docker pull namespace1/docker101tutorial
Tag your images with your new organization namespace:
$ docker tag namespace1/docker101tutorial <new_org>/docker101tutorial
Push your newly tagged images to your new org namespace:
$ docker push new_org/docker101tutorial
The private images that existed in your user account are now available for your organization.",,,
61833389c06450b745ecce56720e44533d27555945d4e7a9d21d174f0b8923c0,"Docker Verified Publisher Program
The Docker Verified Publisher Program provides high-quality images from commercial publishers verified by Docker.
These images help development teams build secure software supply chains, minimizing exposure to malicious content early in the process to save time and money later.
Images that are part of this program have a special badge on Docker Hub making it easier for users to identify projects that Docker has verified as high-quality commercial publishers.
The Docker Verified Publisher Program (DVP) provides several features and benefits to Docker Hub publishers. The program grants the following perks based on participation tier:
- Repository logo
- Verified publisher badge
- Priority search ranking in Docker Hub
- Insights and analytics
- Vulnerability analysis
- Additional Docker Business seats
- Removal of rate limiting for developers
- Co-marketing opportunities
Repository logo
DVP organizations can upload custom images for individual repositories on Docker Hub. This lets you override the default organization-level logo on a per-repository basis.
Only a user with administrative access (owner or team member with administrator permission) over the repository can change the repository logo.
Image requirements
- The supported filetypes for the logo image are JPEG and PNG.
- The minimum allowed image size in pixels is 120×120.
- The maximum allowed image size in pixels is 1000×1000.
- The maximum allowed image file size is 5MB.
Set the repository logo
- Sign in to Docker Hub.
- Go to the page of the repository that you want to change the logo for.
- Select the upload logo button, represented by a camera icon ( ) overlaying the current repository logo.
- In the dialog that opens, select the PNG image that you want to upload to set it as the logo for the repository.
Remove the logo
Select the Clear button ( ) to remove a logo.
Removing the logo makes the repository default to using the organization logo, if set, or the following default logo if not.
Verified publisher badge
Images that are part of this program have a badge on Docker Hub making it easier for developers to identify projects that Docker has verified as high quality publishers and with content they can trust.
Insights and analytics
The insights and analytics service provides usage metrics for how the community uses Docker images, granting insight into user behavior.
The usage metrics show the number of image pulls by tag or by digest, and breakdowns by geolocation, cloud provider, client, and more.
You can select the time span for which you want to view analytics data. You can also export the data in either a summary or raw format.
Vulnerability analysis
Docker Scout provides automatic vulnerability analysis for DVP images published to Docker Hub. Scanning images ensures that the published content is secure, and proves to developers that they can trust the image.
You can enable analysis on a per-repository basis. For more about using this feature, see Basic vulnerability scanning.
Who's eligible to become a verified publisher?
Any independent software vendor who distributes software on Docker Hub can join the Verified Publisher Program. Find out more by heading to the Docker Verified Publisher Program page.",,,
3e9f1e1678f231fffbdbc1627458c94f8d54bad355f4dd790e466f2aefce8ff3,"Configure Settings Management with a JSON file
This page contains information on how to configure Settings Management with an admin-settings.json
file. You can specify and lock configuration parameters to create a standardized Docker Desktop environment across your company or organization.
Settings Management is designed specifically for organizations who don’t give developers root access to their machines.
Prerequisites
You first need to enforce sign-in to ensure that all Docker Desktop developers authenticate with your organization. Since Settings Management requires a Docker Business subscription, enforced sign-in guarantees that only authenticated users have access and that the feature consistently takes effect across all users, even though it may still work without enforced sign-in.
Step one: Create the admin-settings.json
file and save it in the correct location
You can either use the --admin-settings
installer flag on
macOS or
Windows to automatically create the admin-settings.json
and save it in the correct location, or set it up manually.
To set it up manually:
Create a new, empty JSON file and name it
admin-settings.json
.Save the
admin-settings.json
file on your developers' machines in the following locations:- Mac:
/Library/Application\ Support/com.docker.docker/admin-settings.json
- Windows:
C:\ProgramData\DockerDesktop\admin-settings.json
- Linux:
/usr/share/docker-desktop/admin-settings.json
By placing this file in a protected directory, developers are unable to modify it.
Important
It is assumed that you have the ability to push the
admin-settings.json
settings file to the locations specified through a device management software such as Jamf.- Mac:
Step two: Configure the settings you want to lock in
Note
Some of the configuration parameters only apply to certain platforms or to specific Docker Desktop versions. This is highlighted in the following table.
The admin-settings.json
file requires a nested list of configuration parameters, each of which must contain the locked
parameter. You can add or remove configuration parameters as per your requirements.
If locked: true
, users aren't able to edit this setting from Docker Desktop or the CLI.
If locked: false
, it's similar to setting a factory default in that:
For new installs,
locked: false
pre-populates the relevant settings in the Docker Desktop Dashboard, but users are able to modify it.If Docker Desktop is already installed and being used,
locked: false
is ignored. This is because existing users of Docker Desktop may have already updated a setting, which in turn will have been written to the relevant config file, for example thesettings-store.json
(orsettings.json
for Docker Desktop versions 4.34 and earlier) ordaemon.json
. In these instances, the user's preferences are respected and the values aren't altered. These can be controlled by settinglocked: true
.
The following admin-settings.json
code and table provides an example of the required syntax and descriptions for parameters and values:
{
""configurationFileVersion"": 2,
""exposeDockerAPIOnTCP2375"": {
""locked"": true,
""value"": false
},
""proxy"": {
""locked"": true,
""mode"": ""system"",
""http"": """",
""https"": """",
""exclude"": [],
""windowsDockerdPort"": 65000,
""enableKerberosNtlm"": false
},
""containersProxy"": {
""locked"": true,
""mode"": ""manual"",
""http"": """",
""https"": """",
""exclude"": [],
""pac"":"""",
""transparentPorts"": """"
},
""enhancedContainerIsolation"": {
""locked"": true,
""value"": true,
""dockerSocketMount"": {
""imageList"": {
""images"": [
""docker.io/localstack/localstack:*"",
""docker.io/testcontainers/ryuk:*""
]
},
""commandList"": {
""type"": ""deny"",
""commands"": [""push""]
}
}
},
""linuxVM"": {
""wslEngineEnabled"": {
""locked"": false,
""value"": false
},
""dockerDaemonOptions"": {
""locked"": false,
""value"":""{\""debug\"": false}""
},
""vpnkitCIDR"": {
""locked"": false,
""value"":""192.168.65.0/24""
}
},
""kubernetes"": {
""locked"": false,
""enabled"": false,
""showSystemContainers"": false,
""imagesRepository"": """"
},
""windowsContainers"": {
""dockerDaemonOptions"": {
""locked"": false,
""value"":""{\""debug\"": false}""
}
},
""disableUpdate"": {
""locked"": false,
""value"": false
},
""analyticsEnabled"": {
""locked"": false,
""value"": true
},
""extensionsEnabled"": {
""locked"": true,
""value"": false
},
""scout"": {
""locked"": false,
""sbomIndexing"": true,
""useBackgroundIndexing"": true
},
""allowExperimentalFeatures"": {
""locked"": false,
""value"": false
},
""allowBetaFeatures"": {
""locked"": false,
""value"": false
},
""blockDockerLoad"": {
""locked"": false,
""value"": true
},
""filesharingAllowedDirectories"": [
{
""path"": ""$HOME"",
""sharedByDefault"": true
},
{
""path"":""$TMP"",
""sharedByDefault"": false
}
],
""useVirtualizationFrameworkVirtioFS"": {
""locked"": true,
""value"": true
},
""useVirtualizationFrameworkRosetta"": {
""locked"": true,
""value"": true
},
""useGrpcfuse"": {
""locked"": true,
""value"": true
},
""displayedOnboarding"": {
""locked"": true,
""value"": true
},
""desktopTerminalEnabled"": {
""locked"": false,
""value"": false
}
}
General
| Parameter | OS | Description | Version |
|---|---|---|---|
configurationFileVersion | Specifies the version of the configuration file format. | ||
analyticsEnabled | If value is set to false, Docker Desktop doesn't send usage statistics to Docker. | ||
disableUpdate | If value is set to true, checking for and notifications about Docker Desktop updates is disabled. | ||
extensionsEnabled | If value is set to false, Docker extensions are disabled. | ||
blockDockerLoad | If value is set to true , users are no longer able to run
docker load and receive an error if they try to. | ||
displayedOnboarding | If value is set to true , the onboarding survey will not be displayed to new users. Setting value to false has no effect. | Docker Desktop version 4.30 and later | |
desktopTerminalEnabled | If value is set to false , developers cannot use the Docker terminal to interact with the host machine and execute commands directly from Docker Desktop. | ||
exposeDockerAPIOnTCP2375 | Windows only | Exposes the Docker API on a specified port. If value is set to true, the Docker API is exposed on port 2375. Note: This is unauthenticated and should only be enabled if protected by suitable firewall rules. |
File sharing and emulation
| Parameter | OS | Description | Version |
|---|---|---|---|
filesharingAllowedDirectories | Specify which paths your developers can add file shares to. Also accepts $HOME , $TMP , or $TEMP as path variables. When a path is added, its subdirectories are allowed. If sharedByDefault is set to true , that path will be added upon factory reset or when Docker Desktop first starts. | ||
useVirtualizationFrameworkVirtioFS | macOS only | If value is set to true , VirtioFS is set as the file sharing mechanism. Note: If both useVirtualizationFrameworkVirtioFS and useGrpcfuse have value set to true , VirtioFS takes precedence. Likewise, if both useVirtualizationFrameworkVirtioFS and useGrpcfuse have value set to false , osxfs is set as the file sharing mechanism. | |
useGrpcfuse | macOS only | If value is set to true , gRPC Fuse is set as the file sharing mechanism. | |
useVirtualizationFrameworkRosetta | macOS only | If value is set to true , Docker Desktop turns on Rosetta to accelerate x86_64/amd64 binary emulation on Apple Silicon. Note: This also automatically enables Use Virtualization framework . | Docker Desktop version 4.29 and later. |
Docker Scout
| Parameter | OS | Description | Version |
|---|---|---|---|
scout | Setting useBackgroundIndexing to false disables automatic indexing of images loaded to the image store. Setting sbomIndexing to false prevents users from being able to index image by inspecting them in Docker Desktop or using docker scout CLI commands. |
Proxy
| Parameter | OS | Description | Version |
|---|---|---|---|
proxy | If mode is set to system instead of manual , Docker Desktop gets the proxy values from the system and ignores and values set for http , https and exclude . Change mode to manual to manually configure proxy servers. If the proxy port is custom, specify it in the http or https property, for example ""https"": ""http://myotherproxy.com:4321"" . The exclude property specifies a comma-separated list of hosts and domains to bypass the proxy. | ||
windowsDockerdPort | Windows only | Exposes Docker Desktop's internal proxy locally on this port for the Windows Docker daemon to connect to. If it is set to 0, a random free port is chosen. If the value is greater than 0, use that exact value for the port. The default value is -1 which disables the option. | |
enableKerberosNtlm | When set to true , Kerberos and NTLM authentication is enabled. Default is false . For more information, see the settings documentation. | Docker Desktop version 4.32 and later. |
Container proxy
| Parameter | OS | Description | Version |
|---|---|---|---|
containersProxy | Creates air-gapped containers. For more information see Air-Gapped Containers. | Docker Desktop version 4.29 and later. |
Linux VM
| Parameter | OS | Description | Version |
|---|---|---|---|
linuxVM | Parameters and settings related to Linux VM options - grouped together here for convenience. | ||
wslEngineEnabled | Windows only | If value is set to true, Docker Desktop uses the WSL 2 based engine. This overrides anything that may have been set at installation using the --backend=<backend name> flag. | |
dockerDaemonOptions | If value is set to true, it overrides the options in the Docker Engine config file. See the
Docker Engine reference. Note that for added security, a few of the config attributes may be overridden when Enhanced Container Isolation is enabled. | ||
vpnkitCIDR | Overrides the network range used for vpnkit DHCP/DNS for *.docker.internal |
Windows containers
| Parameter | OS | Description | Version |
|---|---|---|---|
windowsContainers | Parameters and settings related to windowsContainers options - grouped together here for convenience. | ||
dockerDaemonOptions | Overrides the options in the Linux daemon config file. See the Docker Engine reference. |
Note
This setting is not available to configure via the Docker Admin Console.
Kubernetes
| Parameter | OS | Description | Version |
|---|---|---|---|
kubernetes | If enabled is set to true, a Kubernetes single-node cluster is started when Docker Desktop starts. If showSystemContainers is set to true, Kubernetes containers are displayed in the Docker Desktop Dashboard and when you run docker ps . imagesRepository lets you specify which repository Docker Desktop pulls the Kubernetes images from. For example, ""imagesRepository"": ""registry-1.docker.io/docker"" . |
Features in development
| Parameter | OS | Description | Version |
|---|---|---|---|
allowExperimentalFeatures | If value is set to false , experimental features are disabled. | ||
allowBetaFeatures | If value is set to false , beta features are disabled. | ||
enableDockerAI | If value is set to false , Docker AI (Ask Gordon) features are disabled. |
Enhanced Container Isolation
| Parameter | OS | Description | Version |
|---|---|---|---|
enhancedContainerIsolation | If value is set to true, Docker Desktop runs all containers as unprivileged, via the Linux user-namespace, prevents them from modifying sensitive configurations inside the Docker Desktop VM, and uses other advanced techniques to isolate them. For more information, see
Enhanced Container Isolation. | ||
dockerSocketMount | By default, enhanced container isolation blocks bind-mounting the Docker Engine socket into containers (e.g., docker run -v /var/run/docker.sock:/var/run/docker.sock ... ). This lets you relax this in a controlled way. See
ECI Configuration for more info. | ||
imageList | Indicates which container images are allowed to bind-mount the Docker Engine socket. | ||
commandList | Restricts the commands that containers can issue via the bind-mounted Docker Engine socket. |
Step three: Re-launch Docker Desktop
Note
Test the changes made through the
admin-settings.json
file locally to see if the settings work as expected.
For settings to take effect:
- On a new install, developers need to launch Docker Desktop and authenticate to their organization.
- On an existing install, developers need to quit Docker Desktop through the Docker menu, and then re-launch Docker Desktop. If they are already signed in, they don't need to sign in again for the changes to take effect.
Important
Selecting Restart from the Docker menu isn't enough as it only restarts some components of Docker Desktop.
So as not to disrupt your developers' workflow, Docker doesn't automatically mandate that developers re-launch and re-authenticate once a change has been made.
In Docker Desktop, developers see the relevant settings grayed out and the message Locked by your administrator.",,,
766fcf0ddd8dee5c4b2392d9b2222f44d94c9564b7083bd7f789b7d0457536f5,"buildkitd.toml
The TOML file used to configure the buildkitd daemon settings has a short list of global settings followed by a series of sections for specific areas of daemon configuration.
The file path is /etc/buildkit/buildkitd.toml
for rootful mode,
~/.config/buildkit/buildkitd.toml
for rootless mode.
The following is a complete buildkitd.toml
configuration example.
Note that some configuration options are only useful in edge cases.
# debug enables additional debug logging
debug = true
# trace enables additional trace logging (very verbose, with potential performance impacts)
trace = true
# root is where all buildkit state is stored.
root = ""/var/lib/buildkit""
# insecure-entitlements allows insecure entitlements, disabled by default.
insecure-entitlements = [ ""network.host"", ""security.insecure"" ]
[log]
# log formatter: json or text
format = ""text""
[dns]
nameservers=[""1.1.1.1"",""8.8.8.8""]
options=[""edns0""]
searchDomains=[""example.com""]
[grpc]
address = [ ""tcp://0.0.0.0:1234"" ]
# debugAddress is address for attaching go profiles and debuggers.
debugAddress = ""0.0.0.0:6060""
uid = 0
gid = 0
[grpc.tls]
cert = ""/etc/buildkit/tls.crt""
key = ""/etc/buildkit/tls.key""
ca = ""/etc/buildkit/tlsca.crt""
[otel]
# OTEL collector trace socket path
socketPath = ""/run/buildkit/otel-grpc.sock""
[cdi]
# Disables support of the Container Device Interface (CDI).
disabled = true
# List of directories to scan for CDI spec files. For more details about CDI
# specification, please refer to https://github.com/cncf-tags/container-device-interface/blob/main/SPEC.md#cdi-json-specification
specDirs = [""/etc/cdi"", ""/var/run/cdi"", ""/etc/buildkit/cdi""]
# config for build history API that stores information about completed build commands
[history]
# maxAge is the maximum age of history entries to keep, in seconds.
maxAge = 172800
# maxEntries is the maximum number of history entries to keep.
maxEntries = 50
[worker.oci]
enabled = true
# platforms is manually configure platforms, detected automatically if unset.
platforms = [ ""linux/amd64"", ""linux/arm64"" ]
snapshotter = ""auto"" # overlayfs or native, default value is ""auto"".
rootless = false # see docs/rootless.md for the details on rootless mode.
# Whether run subprocesses in main pid namespace or not, this is useful for
# running rootless buildkit inside a container.
noProcessSandbox = false
# gc enables/disables garbage collection
gc = true
# reservedSpace is the minimum amount of disk space guaranteed to be
# retained by this buildkit worker - any usage below this threshold will not
# be reclaimed during garbage collection.
# all disk space parameters can be an integer number of bytes (e.g.
# 512000000), a string with a unit (e.g. ""512MB""), or a string percentage
# of the total disk space (e.g. ""10%"")
reservedSpace = ""30%""
# maxUsedSpace is the maximum amount of disk space that may be used by
# this buildkit worker - any usage above this threshold will be reclaimed
# during garbage collection.
maxUsedSpace = ""60%""
# minFreeSpace is the target amount of free disk space that the garbage
# collector will attempt to leave - however, it will never be bought below
# reservedSpace.
minFreeSpace = ""20GB""
# alternate OCI worker binary name(example 'crun'), by default either
# buildkit-runc or runc binary is used
binary = """"
# name of the apparmor profile that should be used to constrain build containers.
# the profile should already be loaded (by a higher level system) before creating a worker.
apparmor-profile = """"
# limit the number of parallel build steps that can run at the same time
max-parallelism = 4
# maintain a pool of reusable CNI network namespaces to amortize the overhead
# of allocating and releasing the namespaces
cniPoolSize = 16
[worker.oci.labels]
""foo"" = ""bar""
[[worker.oci.gcpolicy]]
# reservedSpace is the minimum amount of disk space guaranteed to be
# retained by this policy - any usage below this threshold will not be
# reclaimed during # garbage collection.
reservedSpace = ""512MB""
# maxUsedSpace is the maximum amount of disk space that may be used by this
# policy - any usage above this threshold will be reclaimed during garbage
# collection.
maxUsedSpace = ""1GB""
# minFreeSpace is the target amount of free disk space that the garbage
# collector will attempt to leave - however, it will never be bought below
# reservedSpace.
minFreeSpace = ""10GB""
# keepDuration can be an integer number of seconds (e.g. 172800), or a
# string duration (e.g. ""48h"")
keepDuration = ""48h""
filters = [ ""type==source.local"", ""type==exec.cachemount"", ""type==source.git.checkout""]
[[worker.oci.gcpolicy]]
all = true
reservedSpace = 1024000000
[worker.containerd]
address = ""/run/containerd/containerd.sock""
enabled = true
platforms = [ ""linux/amd64"", ""linux/arm64"" ]
namespace = ""buildkit""
# gc enables/disables garbage collection
gc = true
# reservedSpace is the minimum amount of disk space guaranteed to be
# retained by this buildkit worker - any usage below this threshold will not
# be reclaimed during garbage collection.
# all disk space parameters can be an integer number of bytes (e.g.
# 512000000), a string with a unit (e.g. ""512MB""), or a string percentage
# of the total disk space (e.g. ""10%"")
reservedSpace = ""30%""
# maxUsedSpace is the maximum amount of disk space that may be used by
# this buildkit worker - any usage above this threshold will be reclaimed
# during garbage collection.
maxUsedSpace = ""60%""
# minFreeSpace is the target amount of free disk space that the garbage
# collector will attempt to leave - however, it will never be bought below
# reservedSpace.
minFreeSpace = ""20GB""
# maintain a pool of reusable CNI network namespaces to amortize the overhead
# of allocating and releasing the namespaces
cniPoolSize = 16
# defaultCgroupParent sets the parent cgroup of all containers.
defaultCgroupParent = ""buildkit""
[worker.containerd.labels]
""foo"" = ""bar""
# configure the containerd runtime
[worker.containerd.runtime]
name = ""io.containerd.runc.v2""
path = ""/path/to/containerd/runc/shim""
options = { BinaryName = ""runc"" }
[[worker.containerd.gcpolicy]]
reservedSpace = 512000000
keepDuration = 172800
filters = [ ""type==source.local"", ""type==exec.cachemount"", ""type==source.git.checkout""]
[[worker.containerd.gcpolicy]]
all = true
reservedSpace = 1024000000
# registry configures a new Docker register used for cache import or output.
[registry.""docker.io""]
# mirror configuration to handle path in case a mirror registry requires a /project path rather than just a host:port
mirrors = [""yourmirror.local:5000"", ""core.harbor.domain/proxy.docker.io""]
http = true
insecure = true
ca=[""/etc/config/myca.pem""]
[[registry.""docker.io"".keypair]]
key=""/etc/config/key.pem""
cert=""/etc/config/cert.pem""
# optionally mirror configuration can be done by defining it as a registry.
[registry.""yourmirror.local:5000""]
http = true
# Frontend control
[frontend.""dockerfile.v0""]
enabled = true
[frontend.""gateway.v0""]
enabled = true
# If allowedRepositories is empty, all gateway sources are allowed.
# Otherwise, only the listed repositories are allowed as a gateway source.
#
# NOTE: Only the repository name (without tag) is compared.
#
# Example:
# allowedRepositories = [ ""docker-registry.wikimedia.org/repos/releng/blubber/buildkit"" ]
allowedRepositories = []
[system]
# how often buildkit scans for changes in the supported emulated platforms
platformsCacheMaxAge = ""1h""",,,
37492ec0f6400791761e74b0f41343a7f801b77aa07cbab4de8a915dad815f3a,"Expression evaluation in Bake
Bake files in the HCL format support expression evaluation, which lets you perform arithmetic operations, conditionally set values, and more.
Arithmetic operations
You can perform arithmetic operations in expressions. The following example shows how to multiply two numbers.
sum = 7*6
target ""default"" {
args = {
answer = sum
}
}
Printing the Bake file with the --print
flag shows the evaluated value for
the answer
build argument.
$ docker buildx bake --print
{
""target"": {
""default"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""args"": {
""answer"": ""42""
}
}
}
}
Ternary operators
You can use ternary operators to conditionally register a value.
The following example adds a tag only when a variable is not empty, using the
built-in notequal
function.
variable ""TAG"" {}
target ""default"" {
context="".""
dockerfile=""Dockerfile""
tags = [
""my-image:latest"",
notequal("""",TAG) ? ""my-image:${TAG}"": """",
]
}
In this case, TAG
is an empty string, so the resulting build configuration
only contains the hard-coded my-image:latest
tag.
$ docker buildx bake --print
{
""target"": {
""default"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""tags"": [""my-image:latest""]
}
}
}
Expressions with variables
You can use expressions with variables to conditionally set values, or to perform arithmetic operations.
The following example uses expressions to set values based on the value of
variables. The v1
build argument is set to ""higher"" if the variable FOO
is
greater than 5, otherwise it is set to ""lower"". The v2
build argument is set
to ""yes"" if the IS_FOO
variable is true, otherwise it is set to ""no"".
variable ""FOO"" {
default = 3
}
variable ""IS_FOO"" {
default = true
}
target ""app"" {
args = {
v1 = FOO > 5 ? ""higher"" : ""lower""
v2 = IS_FOO ? ""yes"" : ""no""
}
}
Printing the Bake file with the --print
flag shows the evaluated values for
the v1
and v2
build arguments.
$ docker buildx bake --print app
{
""group"": {
""default"": {
""targets"": [""app""]
}
},
""target"": {
""app"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""args"": {
""v1"": ""lower"",
""v2"": ""yes""
}
}
}
}",,,
62ed345067fe90275d810f35b43468fa994a6d3d5072c99145cce330005b228c,"Administration
Administrators can manage companies and organizations using the Docker Admin Console, or manage organizations in Docker Hub.
The Docker Admin Console is available for customers with a Docker Business subscription. The Docker Admin Console provides administrators with centralized observability, access management, and controls for their company and organizations. To provide these features, Docker uses the following hierarchy and roles.
- Company: A company simplifies the management of Docker organizations and settings. Creating a company is optional and only available to Docker Business subscribers.
- Company owner: A company can have multiple owners. Company owners have company-wide observability and can manage company-wide settings that apply to all associated organizations. In addition, company owners have the same access as organization owners for all associated organizations.
- Organization: An organization is a collection of teams and repositories. Docker Team and Business subscribers must have at least one organization.
- Organization owner: An organization can have multiple owners. Organization owners have observability into their organization and can manage its users and settings.
- Team: A team is a group of Docker members that belong to an organization. Organization and company owners can group members into additional teams to configure repository permissions on a per-team basis. Using teams to group members is optional.
- Member: A member is a Docker user that's a member of an organization. Organization and company owners can assign roles to members to define their permissions.",,,
26d6226d0c820344fe76b91c6e64d1f6e3ca69d977cfcd026e66d433efab1873,"Named contexts with GitHub Actions
You can define
additional build contexts,
and access them in your Dockerfile with FROM name
or --from=name
. When
Dockerfile defines a stage with the same name it's overwritten.
This can be useful with GitHub Actions to reuse results from other builds or pin an image to a specific tag in your workflow.
Pin image to a tag
Replace alpine:latest
with a pinned one:
# syntax=docker/dockerfile:1
FROM alpine
RUN echo ""Hello World""
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build
uses: docker/build-push-action@v6
with:
build-contexts: |
alpine=docker-image://alpine:3.21
tags: myimage:latest
Use image in subsequent steps
By default, the
Docker Setup Buildx
action uses docker-container
as a build driver, so built Docker images aren't
loaded automatically.
With named contexts you can reuse the built image:
# syntax=docker/dockerfile:1
FROM alpine
RUN echo ""Hello World""
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
with:
driver: docker
- name: Build base image
uses: docker/build-push-action@v6
with:
context: ""{{defaultContext}}:base""
load: true
tags: my-base-image:latest
- name: Build
uses: docker/build-push-action@v6
with:
build-contexts: |
alpine=docker-image://my-base-image:latest
tags: myimage:latest
Using with a container builder
As shown in the previous section we are not using the default
docker-container
driver for building with
named contexts. That's because this driver can't load an image from the Docker
store as it's isolated. To solve this problem you can use a
local registry
to push your base image in your workflow:
# syntax=docker/dockerfile:1
FROM alpine
RUN echo ""Hello World""
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
services:
registry:
image: registry:2
ports:
- 5000:5000
steps:
- name: Set up QEMU
uses: docker/setup-qemu-action@v3
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
with:
# network=host driver-opt needed to push to local registry
driver-opts: network=host
- name: Build base image
uses: docker/build-push-action@v6
with:
context: ""{{defaultContext}}:base""
tags: localhost:5000/my-base-image:latest
push: true
- name: Build
uses: docker/build-push-action@v6
with:
build-contexts: |
alpine=docker-image://localhost:5000/my-base-image:latest
tags: myimage:latest",,,
3fc81ed33b5597e4d0985c8a132b8ce9ee904f86c3cdc527801a934eecfa3f5e,"Customize Compose Bridge
This page explains how Compose Bridge utilizes templating to efficiently translate Docker Compose files into Kubernetes manifests. It also explain how you can customize these templates for your specific requirements and needs, or how you can build your own transformation.
How it works
Compose bridge uses transformations to let you convert a Compose model into another form.
A transformation is packaged as a Docker image that receives the fully-resolved Compose model as /in/compose.yaml
and can produce any target format file under /out
.
Compose Bridge provides its transformation for Kubernetes using Go templates, so that it is easy to extend for customization by just replacing or appending your own templates.
Syntax
Compose Bridge make use of templates to transform a Compose configuration file into Kubernetes manifests. Templates are plain text files that use the Go templating syntax. This enables the insertion of logic and data, making the templates dynamic and adaptable according to the Compose model.
When a template is executed, it must produce a YAML file which is the standard format for Kubernetes manifests. Multiple files can be generated as long as they are separated by ---
Each YAML output file begins with custom header notation, for example:
#! manifest.yaml
In the following example, a template iterates over services defined in a compose.yaml
file. For each service, a dedicated Kubernetes manifest file is generated, named according to the service and containing specified configurations.
{{ range $name, $service := .services }}
---
#! {{ $name }}-manifest.yaml
# Generated code, do not edit
key: value
## ...
{{ end }}
Input
The input Compose model is the canonical YAML model you can get by running docker compose config
. Within the templates, data from the compose.yaml
is accessed using dot notation, allowing you to navigate through nested data structures. For example, to access the deployment mode of a service, you would use service.deploy.mode
:
# iterate over a yaml sequence
{{ range $name, $service := .services }}
# access a nested attribute using dot notation
{{ if eq $service.deploy.mode ""global"" }}
kind: DaemonSet
{{ end }}
{{ end }}
You can check the Compose Specification JSON schema to have a full overview of the Compose model. This schema outlines all possible configurations and their data types in the Compose model.
Helpers
As part of the Go templating syntax, Compose Bridge offers a set of YAML helper functions designed to manipulate data within the templates efficiently:
seconds
: Converts a duration into an integeruppercase
: Converts a string into upper case characterstitle
: Converts a string by capitalizing the first letter of each wordsafe
: Converts a string into a safe identifier, replacing all characters (except lowercase a-z) with-
truncate
: Removes the N first elements from a listjoin
: Groups elements from a list into a single string, using a separatorbase64
: Encodes a string as base64 used in Kubernetes for encoding secretsmap
: Transforms a value according to mappings expressed as""value -> newValue""
stringsindent
: Writes string content indented by N spaceshelmValue
: Writes the string content as a template value in the final file
In the following example, the template checks if a healthcheck interval is specified for a service, applies the seconds
function to convert this interval into seconds and assigns the value to the periodSeconds
attribute.
{{ if $service.healthcheck.interval }}
periodSeconds: {{ $service.healthcheck.interval | seconds }}{{ end }}
{{ end }}
Customization
As Kubernetes is a versatile platform, there are many ways to map Compose concepts into Kubernetes resource definitions. Compose Bridge lets you customize the transformation to match your own infrastructure decisions and preferences, with various level of flexibility and effort.
Modify the default templates
You can extract templates used by the default transformation docker/compose-bridge-kubernetes
,
by running compose-bridge transformations create --from docker/compose-bridge-kubernetes my-template
and adjusting the templates to match your needs.
The templates are extracted into a directory named after your template name, in this case my-template
.
It includes a Dockerfile that lets you create your own image to distribute your template, as well as a directory containing the templating files.
You are free to edit the existing files, delete them, or
add new ones to subsequently generate Kubernetes manifests that meet your needs.
You can then use the generated Dockerfile to package your changes into a new transformation image, which you can then use with Compose Bridge:
$ docker build --tag mycompany/transform --push .
You can then use your transformation as a replacement:
$ compose-bridge convert --transformations mycompany/transform
Add your own templates
For resources that are not managed by Compose Bridge's default transformation,
you can build your own templates. The compose.yaml
model may not offer all
the configuration attributes required to populate the target manifest. If this is the case, you can
then rely on Compose custom extensions to better describe the
application, and offer an agnostic transformation.
For example, if you add x-virtual-host
metadata
to service definitions in the compose.yaml
file, you can use the following custom attribute
to produce Ingress rules:
{{ $project := .name }}
#! {{ $name }}-ingress.yaml
# Generated code, do not edit
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
name: virtual-host-ingress
namespace: {{ $project }}
spec:
rules:
{{ range $name, $service := .services }}
{{ if $service.x-virtual-host }}
- host: ${{ $service.x-virtual-host }}
http:
paths:
- path: ""/""
backend:
service:
name: ${{ name }}
port:
number: 80
{{ end }}
{{ end }}
Once packaged into a Docker image, you can use this custom template when transforming Compose models into Kubernetes in addition to other transformations:
$ compose-bridge convert \
--transformation docker/compose-bridge-kubernetes \
--transformation mycompany/transform
Build your own transformation
While Compose Bridge templates make it easy to customize with minimal changes, you may want to make significant changes, or rely on an existing conversion tool.
A Compose Bridge transformation is a Docker image that is designed to get a Compose model
from /in/compose.yaml
and produce platform manifests under /out
. This simple
contract makes it easy to bundle an alternate transformation using
Kompose:
FROM alpine
# Get kompose from github release page
RUN apk add --no-cache curl
ARG VERSION=1.32.0
RUN ARCH=$(uname -m | sed 's/armv7l/arm/g' | sed 's/aarch64/arm64/g' | sed 's/x86_64/amd64/g') && \
curl -fsL \
""https://github.com/kubernetes/kompose/releases/download/v${VERSION}/kompose-linux-${ARCH}"" \
-o /usr/bin/kompose
RUN chmod +x /usr/bin/kompose
CMD [""/usr/bin/kompose"", ""convert"", ""-f"", ""/in/compose.yaml"", ""--out"", ""/out""]
This Dockerfile bundles Kompose and defines the command to run this tool according to the Compose Bridge transformation contract.",,,
15e09577fa37560f96fa3125112f405c356675ede8e166c4fed5f7e1186e26a2,"Pause Docker Desktop
When Docker Desktop is paused, the Linux VM running Docker Engine is paused, the current state of all your containers are saved in memory, and all processes are frozen. This reduces the CPU and memory usage and helps you retain a longer battery life on your laptop.
You can manually pause Docker Desktop by selecting the Docker menu and then Pause. To manually resume Docker Desktop, select the Resume option in the Docker menu, or run any Docker CLI command.
When you manually pause Docker Desktop, a paused status displays on the Docker menu and on the Docker Desktop Dashboard. You can still access the Settings and the Troubleshoot menu.
Tip
The Resource Saver feature, available in Docker Desktop version 4.24 and later, is enabled by default and provides better CPU and memory savings than the manual Pause feature. See here for more info.",,,
4a0f84b12d15f77009c32443b186303949a37af86d04e8f0e950f764c9bf32e5,"Registry cache
The registry
cache storage can be thought of as an extension to the inline
cache. Unlike the inline
cache, the registry
cache is entirely separate from
the image, which allows for more flexible usage - registry
-backed cache can do
everything that the inline cache can do, and more:
- Allows for separating the cache and resulting image artifacts so that you can distribute your final image without the cache inside.
- It can efficiently cache multi-stage builds in
max
mode, instead of only the final stage. - It works with other exporters for more flexibility, instead of only the
image
exporter.
This cache storage backend is not supported with the default docker
driver.
To use this feature, create a new builder using a different driver. See
Build drivers for more information.
Synopsis
Unlike the simpler inline
cache, the registry
cache supports several
configuration parameters:
$ docker buildx build --push -t <registry>/<image> \
--cache-to type=registry,ref=<registry>/<cache-image>[,parameters...] \
--cache-from type=registry,ref=<registry>/<cache-image> .
The following table describes the available CSV parameters that you can pass to
--cache-to
and --cache-from
.
| Name | Option | Type | Default | Description |
|---|---|---|---|---|
ref | cache-to ,cache-from | String | Full name of the cache image to import. | |
mode | cache-to | min ,max | min | Cache layers to export, see cache mode. |
oci-mediatypes | cache-to | true ,false | true | Use OCI media types in exported manifests, see OCI media types. |
image-manifest | cache-to | true ,false | false | When using OCI media types, generate an image manifest instead of an image index for the cache image, see OCI media types. |
compression | cache-to | gzip ,estargz ,zstd | gzip | Compression type, see cache compression. |
compression-level | cache-to | 0..22 | Compression level, see cache compression. | |
force-compression | cache-to | true ,false | false | Forcibly apply compression, see cache compression. |
ignore-error | cache-to | Boolean | false | Ignore errors caused by failed cache exports. |
You can choose any valid value for ref
, as long as it's not the same as the
target location that you push your image to. You might choose different tags
(e.g. foo/bar:latest
and foo/bar:build-cache
), separate image names (e.g.
foo/bar
and foo/bar-cache
), or even different repositories (e.g.
docker.io/foo/bar
and ghcr.io/foo/bar
). It's up to you to decide the
strategy that you want to use for separating your image from your cache images.
If the --cache-from
target doesn't exist, then the cache import step will
fail, but the build continues.
Further reading
For an introduction to caching see Docker build cache.
For more information on the registry
cache backend, see the
BuildKit README.",,,
10c3df87f79f4e89e563c1afcc247656caeb89683186d981f7df86b1ad4ef0ee,"Install Docker Engine on Raspberry Pi OS (32-bit)
To get started with Docker Engine on Raspberry Pi OS, make sure you meet the prerequisites, and then follow the installation steps.
Important
This installation instruction refers to the 32-bit (armhf) version of Raspberry Pi OS. If you're using the 64-bit (arm64) version, follow the instructions for Debian.
Prerequisites
Firewall limitations
Warning
Before you install Docker, make sure you consider the following security implications and firewall incompatibilities.
- If you use ufw or firewalld to manage firewall settings, be aware that when you expose container ports using Docker, these ports bypass your firewall rules. For more information, refer to Docker and ufw.
- Docker is only compatible with
iptables-nft
andiptables-legacy
. Firewall rules created withnft
are not supported on a system with Docker installed. Make sure that any firewall rulesets you use are created withiptables
orip6tables
, and that you add them to theDOCKER-USER
chain, see Packet filtering and firewalls.
OS requirements
To install Docker Engine, you need one of the following OS versions:
- 32-bit Raspberry Pi OS Bookworm 12 (stable)
- 32-bit Raspberry Pi OS Bullseye 11 (oldstable)
Uninstall old versions
Before you can install Docker Engine, you need to uninstall any conflicting packages.
Your Linux distribution may provide unofficial Docker packages, which may conflict with the official packages provided by Docker. You must uninstall these packages before you install the official version of Docker Engine.
The unofficial packages to uninstall are:
docker.io
docker-compose
docker-doc
podman-docker
Moreover, Docker Engine depends on containerd
and runc
. Docker Engine
bundles these dependencies as one bundle: containerd.io
. If you have
installed the containerd
or runc
previously, uninstall them to avoid
conflicts with the versions bundled with Docker Engine.
Run the following command to uninstall all conflicting packages:
$ for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do sudo apt-get remove $pkg; done
apt-get
might report that you have none of these packages installed.
Images, containers, volumes, and networks stored in /var/lib/docker/
aren't
automatically removed when you uninstall Docker. If you want to start with a
clean installation, and prefer to clean up any existing data, read the
uninstall Docker Engine section.
Installation methods
You can install Docker Engine in different ways, depending on your needs:
Docker Engine comes bundled with Docker Desktop for Linux. This is the easiest and quickest way to get started.
Set up and install Docker Engine from Docker's
apt
repository.Install it manually and manage upgrades manually.
Use a convenience script. Only recommended for testing and development environments.
Install using the apt
repository
Before you install Docker Engine for the first time on a new host machine, you
need to set up the Docker apt
repository. Afterward, you can install and update
Docker from the repository.
Set up Docker's
apt
repository.# Add Docker's official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/raspbian/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \ ""deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/raspbian \ $(. /etc/os-release && echo ""$VERSION_CODENAME"") stable"" | \ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update
Install the Docker packages.
To install the latest version, run:
$ sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
To install a specific version of Docker Engine, start by listing the available versions in the repository:
# List the available versions: $ apt-cache madison docker-ce | awk '{ print $3 }' 5:28.0.1-1~raspbian.12~bookworm 5:28.0.0-1~raspbian.12~bookworm ...
Select the desired version and install:
$ VERSION_STRING=5:28.0.1-1~raspbian.12~bookworm $ sudo apt-get install docker-ce=$VERSION_STRING docker-ce-cli=$VERSION_STRING containerd.io docker-buildx-plugin docker-compose-plugin
Verify that the installation is successful by running the
hello-world
image:$ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Upgrade Docker Engine
To upgrade Docker Engine, follow step 2 of the installation instructions, choosing the new version you want to install.
Install from a package
If you can't use Docker's apt
repository to install Docker Engine, you can
download the deb
file for your release and install it manually. You need to
download a new file each time you want to upgrade Docker Engine.
Select your Raspberry Pi OS version in the list.
Go to
pool/stable/
and select the applicable architecture (amd64
,armhf
,arm64
, ors390x
).Download the following
deb
files for the Docker Engine, CLI, containerd, and Docker Compose packages:containerd.io_<version>_<arch>.deb
docker-ce_<version>_<arch>.deb
docker-ce-cli_<version>_<arch>.deb
docker-buildx-plugin_<version>_<arch>.deb
docker-compose-plugin_<version>_<arch>.deb
Install the
.deb
packages. Update the paths in the following example to where you downloaded the Docker packages.$ sudo dpkg -i ./containerd.io_<version>_<arch>.deb \ ./docker-ce_<version>_<arch>.deb \ ./docker-ce-cli_<version>_<arch>.deb \ ./docker-buildx-plugin_<version>_<arch>.deb \ ./docker-compose-plugin_<version>_<arch>.deb
The Docker daemon starts automatically.
Verify that the installation is successful by running the
hello-world
image:$ sudo service docker start $ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Upgrade Docker Engine
To upgrade Docker Engine, download the newer package files and repeat the installation procedure, pointing to the new files.
Install using the convenience script
Docker provides a convenience script at
https://get.docker.com/ to install Docker into
development environments non-interactively. The convenience script isn't
recommended for production environments, but it's useful for creating a
provisioning script tailored to your needs. Also refer to the
install using the repository steps to learn
about installation steps to install using the package repository. The source code
for the script is open source, and you can find it in the
docker-install
repository on GitHub.
Always examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script:
- The script requires
root
orsudo
privileges to run. - The script attempts to detect your Linux distribution and version and configure your package management system for you.
- The script doesn't allow you to customize most installation parameters.
- The script installs dependencies and recommendations without asking for confirmation. This may install a large number of packages, depending on the current configuration of your host machine.
- By default, the script installs the latest stable release of Docker, containerd, and runc. When using this script to provision a machine, this may result in unexpected major version upgrades of Docker. Always test upgrades in a test environment before deploying to your production systems.
- The script isn't designed to upgrade an existing Docker installation. When using the script to update an existing installation, dependencies may not be updated to the expected version, resulting in outdated versions.
Tip
Preview script steps before running. You can run the script with the
--dry-run
option to learn what steps the script will run when invoked:$ curl -fsSL https://get.docker.com -o get-docker.sh $ sudo sh ./get-docker.sh --dry-run
This example downloads the script from https://get.docker.com/ and runs it to install the latest stable release of Docker on Linux:
$ curl -fsSL https://get.docker.com -o get-docker.sh
$ sudo sh get-docker.sh
Executing docker install script, commit: 7cae5f8b0decc17d6571f9f52eb840fbc13b2737
<...>
You have now successfully installed and started Docker Engine. The docker
service starts automatically on Debian based distributions. On RPM
based
distributions, such as CentOS, Fedora, RHEL or SLES, you need to start it
manually using the appropriate systemctl
or service
command. As the message
indicates, non-root users can't run Docker commands by default.
Use Docker as a non-privileged user, or install in rootless mode?
The installation script requires
root
orsudo
privileges to install and use Docker. If you want to grant non-root users access to Docker, refer to the post-installation steps for Linux. You can also install Docker withoutroot
privileges, or configured to run in rootless mode. For instructions on running Docker in rootless mode, refer to run the Docker daemon as a non-root user (rootless mode).
Install pre-releases
Docker also provides a convenience script at
https://test.docker.com/ to install pre-releases of
Docker on Linux. This script is equal to the script at get.docker.com
, but
configures your package manager to use the test channel of the Docker package
repository. The test channel includes both stable and pre-releases (beta
versions, release-candidates) of Docker. Use this script to get early access to
new releases, and to evaluate them in a testing environment before they're
released as stable.
To install the latest version of Docker on Linux from the test channel, run:
$ curl -fsSL https://test.docker.com -o test-docker.sh
$ sudo sh test-docker.sh
Upgrade Docker after using the convenience script
If you installed Docker using the convenience script, you should upgrade Docker using your package manager directly. There's no advantage to re-running the convenience script. Re-running it can cause issues if it attempts to re-install repositories which already exist on the host machine.
Uninstall Docker Engine
Uninstall the Docker Engine, CLI, containerd, and Docker Compose packages:
$ sudo apt-get purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras
Images, containers, volumes, or custom configuration files on your host aren't automatically removed. To delete all images, containers, and volumes:
$ sudo rm -rf /var/lib/docker $ sudo rm -rf /var/lib/containerd
Remove source list and keyrings
$ sudo rm /etc/apt/sources.list.d/docker.list $ sudo rm /etc/apt/keyrings/docker.asc
You have to delete any edited configuration files manually.
Next steps
- Continue to Post-installation steps for Linux.",,,
99a9482c392a48d3a4ce64f5fb7cb37b918dea1543106b36d0d6f194d0adbfbd,"Use the Docker socket from the extension backend
Extensions can invoke Docker commands directly from the frontend with the SDK.
In some cases, it is useful to also interact with Docker Engine from the backend.
Extension backend containers can mount the Docker socket and use it to interact with Docker Engine from the extension backend logic. Learn more about the Docker Engine socket
However, when mounting the Docker socket from an extension container that lives in the Desktop virtual machine, you want
to mount the Docker socket from inside the VM, and not mount /var/run/docker.sock
from the host filesystem (using
the Docker socket from the host can lead to permission issues in containers).
In order to do so, you can use /var/run/docker.sock.raw
. Docker Desktop mounts the socket that lives in the Desktop VM, and not from the host.
services:
myExtension:
image: ${DESKTOP_PLUGIN_IMAGE}
volumes:
- /var/run/docker.sock.raw:/var/run/docker.sock",,,
21e5f989d75dd354819cab48c25f32e80d7e42d95e795a8543ac5e384329490e,"Manuals
This section contains user guides on how to install, set up, configure, and use Docker products.
Open source
Open source development and containerization technologies.
Products
End-to-end developer solutions for innovative teams.
Platform
Documentation related to the Docker platform, such as administration and subscription management for organizations.",,,
a82849c5271d7be4411b6cb757640c24efe535c5d5b57adcc1033d72523e0d33,"Customize log driver output
The tag
log option specifies how to format a tag that identifies the
container's log messages. By default, the system uses the first 12 characters of
the container ID. To override this behavior, specify a tag
option:
$ docker run --log-driver=fluentd --log-opt fluentd-address=myhost.local:24224 --log-opt tag=""mailer""
Docker supports some special template markup you can use when specifying a tag's value:
| Markup | Description |
|---|---|
{{.ID}} | The first 12 characters of the container ID. |
{{.FullID}} | The full container ID. |
{{.Name}} | The container name. |
{{.ImageID}} | The first 12 characters of the container's image ID. |
{{.ImageFullID}} | The container's full image ID. |
{{.ImageName}} | The name of the image used by the container. |
{{.DaemonName}} | The name of the Docker program (docker ). |
For example, specifying a --log-opt tag=""{{.ImageName}}/{{.Name}}/{{.ID}}""
value yields syslog
log lines like:
Aug 7 18:33:19 HOSTNAME hello-world/foobar/5790672ab6a0[9103]: Hello from Docker.
At startup time, the system sets the container_name
field and {{.Name}}
in
the tags. If you use docker rename
to rename a container, the new name isn't
reflected in the log messages. Instead, these messages continue to use the
original container name.",,,
6c74b3e66c1a6d99e2437978453509c20b7889eea855ad382238a5da599e3845,"Manage tags and labels with GitHub Actions
If you want an ""automatic"" tag management and OCI Image Format Specification for labels, you can do it in a dedicated setup step. The following workflow will use the Docker Metadata Action to handle tags and labels based on GitHub Actions events and Git metadata:
name: ci
on:
schedule:
- cron: ""0 10 * * *""
push:
branches:
- ""**""
tags:
- ""v*.*.*""
pull_request:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Docker meta
id: meta
uses: docker/metadata-action@v5
with:
# list of Docker images to use as base name for tags
images: |
name/app
ghcr.io/username/app
# generate Docker tags based on the following events/attributes
tags: |
type=schedule
type=ref,event=branch
type=ref,event=pr
type=semver,pattern={{version}}
type=semver,pattern={{major}}.{{minor}}
type=semver,pattern={{major}}
type=sha
- name: Login to Docker Hub
if: github.event_name != 'pull_request'
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Login to GHCR
if: github.event_name != 'pull_request'
uses: docker/login-action@v3
with:
registry: ghcr.io
username: ${{ github.repository_owner }}
password: ${{ secrets.GITHUB_TOKEN }}
- name: Set up QEMU
uses: docker/setup-qemu-action@v3
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build and push
uses: docker/build-push-action@v6
with:
push: ${{ github.event_name != 'pull_request' }}
tags: ${{ steps.meta.outputs.tags }}
labels: ${{ steps.meta.outputs.labels }}",,,
50987749b6fedc11d9b8990857b845cc8d8c90239fb6ff2aba79b3112d5379f6,"Docker Compose release notes
For more detailed information, see the release notes in the Compose repo.
2.33.1
2025-02-21Bug fixes and enhancements
- Added support for
gw_priority
,enable_ipv4
(requires Docker v28.0) - Fixed an issue with the navigation menu
- Improved error message when using non-file secret/config with read-only service
Update
- Dependencies upgrade: bump docker engine and cli to v28.0.0
2.33.0
2025-02-13Bug fixes and enhancements
- Introduced a hint to promote the use of Bake
- Introduced support for the
additional_context
attribute referencing another service - Added support for
BUILDKIT_PROGRESS
- Compose now warns you when a published Compose application includes environment variables
- Added a
--with-env
flag to publish a Compose application with environment variables - Updated
ls --quiet
help description - Fixed multiple issues delegating build to Bake
- Updated help in
stats
command - Fixed support for ""builtin"" seccomp profile
- Fixed support for
watch
with multiple services - Removed exit code per error type used by legacy metrics system
- Fixed test coverage for
compatibility
- Removed raw os.Args sent to OpenTelemetry
- Enabled copyloopvar linter
- Fixed provenance for binaries and generate SBOM
- Main branch for docs upstream validation is now used
- Added codeowners file
- Added Docker Engine v28.x to the test-matrix
Update
- Dependencies upgrade: Bump compose-go v2.4.8
- Dependencies upgrade: Bump buildx v0.20.1
- Dependencies upgrade: Bump docker to v27.5.1
- Dependencies upgrade: Bump golangci-lint to v1.63.4
- Dependencies upgrade: Bump golang.org/x/sys from 0.28.0 to 0.30.0
- Dependencies upgrade: Bump github.com/moby/term v0.5.2
- Dependencies upgrade: Bump github.com/otiai10/copy from 1.14.0 to 1.14.1
- Dependencies upgrade: Bump github.com/jonboulle/clockwork from 0.4.0 to 0.5.0
- Dependencies upgrade: Bump github.com/spf13/pflag from 1.0.5 to 1.0.6
- Dependencies upgrade: Bump golang.org/x/sync from 0.10.0 to 0.11.0
- Dependencies upgrade: Bump gotest.tools/v3 from 3.5.1 to 3.5.2
2.32.4
2025-01-16Bug fixes and enhancements
- Fixed an issue where the Compose version did not display properly when using
docker compose version
2.32.3
2025-01-13Note
Binaries from the Compose GitHub repository may not display the version number properly. If you rely on
docker compose version
in your development or CI processes, upgrade to Compose version 2.32.4.
Bug fixes and enhancements
- Fixed an issue where Compose would override a service-level MAC address with the main network MAC address
- Fixed a log rendering issue during concurrent builds
2.32.2
2025-01-07Update
- Dependencies upgrade: bump compose-go to v2.4.7
- Dependencies upgrade: bump golang to v1.22.10
Bug fixes and enhancements
- Added
--pull
flag to thedocker compose run
command - Fixed a bug which meant the
restart
action ofwatch
mode didn't monitor bind mounts - Fixed an issue recreating containers when using anonymous volumes
2.32.1
2024-12-16Bug fixes and enhancements
- Fixed a bug recreating containers when not needed
2.32.0
2024-12-13Update
- Dependencies upgrade: bump docker + buildx to latest release
- Dependencies upgrade: bump otel dependencies to v1.28.0 and v0.53.0
- Dependencies upgrade: bump golang.org/x/sys 0.28.0
- Dependencies upgrade: bump golang.org/x/crypto to 0.31.0
- Dependencies upgrade: bump google.golang.org/grpc to 1.68.1
- Dependencies upgrade: bump golang.org/x/sync 0.10.0
- Dependencies upgrade: bump xx to v1.6.1
Bug fixes and enhancements
- Improved support when building with Bake
- Added
restart
andsync+exec
watch actions - Compose now recreates containers when the volume or network configuration changes
- Fixed support for
mac_address
- Fixed
pull --quiet
to only hide progress, not global status - Fixed an issue where only the
rebuild
watch action now requires a build declaration - Compose now logs
watch
configuration error when enabled through the Compose menu
2.31.0
2024-11-28Update
- Dependencies upgrade: bump compose-go to v2.4.5
- Dependencies upgrade: bump docker engine and cli to v27.4.0-rc.2
- Dependencies upgrade: bump buildx to v0.18.0
- Dependencies upgrade: bump buildkit to v0.17.1
Bug fixes and enhancements
- Added the ability to use Docker Buildx Bake to build Docker Compose services
- Added
commit
command to create new images from running containers - Fixed an issue where network changes were not detected
- Fixed an issue where containers stopped sequentially which slowed down the restart process
2.30.3
2024-11-07Update
- Dependencies upgrade: bump compose-go to v2.4.4
Bug fixes and enhancements
- Fixed an issue re-starting services that should not when using
--watch
- Improve the fix of using same YAML anchor multiple times in a Compose file
2.30.2
2024-11-05Update
- Dependencies upgrade: bump compose-go to v2.4.3
Bug fixes and enhancements
- Fixed an issue re-creating services when updating its profiles
- Fixed a regression when using the same YAML anchor multiple times in a Compose file
2.30.1
2024-10-30Update
- Dependencies upgrade: bump compose-go to v2.4.2
Bug fixes and enhancements
- Fixed a regression when using stdin as input for
-f
flag - Fixed a regression when using the same YAML anchor multiple times in a Compose file
2.30.0
2024-10-29Update
- Dependencies upgrade: bump compose-go to v2.4.1
- Dependencies upgrade: bump docker engine and cli to v27.3.1
Bug fixes and enhancements
- Introduction of service hooks support.
- Addition of alpha
generate
command. - Addition of
export
command. - Added support for CDI device requests using
devices
in the Compose file. - A lot a bug fixes.
2.29.7
2024-09-20Bug fixes and enhancements
- Fixed a regression when using mount API for bind mounts.
2.29.6
2024-09-19Update
- Dependencies upgrade: bump docker engine and cli to v27.3.0-rc.2
Bug fixes and enhancements
- Fixed an issue with Windows Containers bind mounts.
2.29.5
2024-09-17Bug fixes and enhancements
- Fixed an issue with bind mounts on WSL2.
2.29.4
2024-09-16Update
- Dependencies upgrade: bump buildx to v0.17.1
- Dependencies upgrade: bump docker engine and cli to v27.3.0-rc.1
Bug fixes and enhancements
- Fixed an issue with services not stopping when restarting diverged dependencies.
- Fixed potential
nil
pointer error on the OTEL client.
2.29.3
2024-09-12Update
- Dependencies upgrade: bump compose-go to v2.2.0
- Dependencies upgrade: bump docker engine and cli to v27.2.1
Bug fixes and enhancements
- Combination of bind mount and
rebuild
are now allowed withwatch
. - Fixed a bug recreating containers when
--no-deps
is used withup
. - Fixed a bug not closing streams when reattaching containers.
- Restored recreation of anonymous volumes when using
-V
or--renew-anon-volumes
.
2.29.2
2024-08-16Update
- Dependencies upgrade: bump compose-go to v2.1.6
- Dependencies upgrade: bump docker engine and cli to v27.1.2
- Dependencies upgrade: bump buildx to v0.16.2
- Dependencies upgrade: bump buildkit to v0.15.2
- Dependencies upgrade: bump golang to v1.21.12
- Dependencies upgrade: bump sys to v0.22.0
- Dependencies upgrade: bump flock to v0.12.1
Bug fixes and enhancements
- Fixed the docs on
docker compose kill
usage. - Fixed redundant condition from
toAPIBuildOptions
in build.go. - Fixed initial Watch
sync
after Compose restarts with introduction ofx-initialSync
. - Fixed an issue which stopped the Compose process for a single container on
sync-restart
Watch action.
2.29.1
2024-07-23Update
- Dependencies upgrade: bump compose-go to v2.1.5.
- Dependencies upgrade: bump docker engine and cli to v27.1.0.
Bug fixes and enhancements
- Enhance JSON progress events with more fields.
2.29.0
2024-07-17Update
- Dependencies upgrade: bump compose-go to v2.1.4
- Dependencies upgrade: bump docker engine and cli to v27.0.3
- Dependencies upgrade: bump buildx to 0.16.0
- Dependencies upgrade: bump buildkit to 0.15.0
- Dependencies upgrade: bump containerd to 1.7.19
Bug fixes and enhancements
- Added a JSON stream progress writer.
- Added a
--prune
flag to thedocker compose watch
command. - Unnecessary resources are now excluded after services have been selected.
- Empty variables with no value are unset in containers.
2.28.1
2024-06-24Bug fixes and enhancements
- Fixed progress display, broken in
v2.28.0
, when TTY mode available.
2.28.0
2024-06-21Update
- Dependencies upgrade: bump compose-go to v2.1.3
- Dependencies upgrade: bump docker engine and cli to v27.0.1-rc.1
2.27.3
2024-06-21Update
- Dependencies upgrade: bump buildx to 0.15.1
- Dependencies upgrade: bump buildkit to 0.14.1
2.27.2
2024-06-20Update
- Dependencies upgrade: bump golang to 1.21.11
- Dependencies upgrade: bump docker engine and cli to v26.1.4
- Dependencies upgrade: bump buildx to 0.15.0
- Dependencies upgrade: bump buildkit to 0.14.0
- Dependencies upgrade: bump containerd to 1.7.18
Bug fixes and enhancements
- Added an
--environment
flag to theconfig
command - Fixed a bug which caused the
watch
process to hang when used as flag with theup
command - Fixed usage of
COMPOSE_PROFILES
in.env
file
2.27.1
2024-05-24Update
- Dependencies upgrade: bump compose-go to v2.1.1
- Dependencies upgrade: bump docker engine and cli to v26.1.3
- Dependencies upgrade: bump buildx to 0.14.1
- Dependencies upgrade: bump containerd to 1.7.17
Bug fixes and enhancements
- Added a navigation menu in the CLI where you can open your Compose file in Docker Desktop
- Added documentation for
--menu
flag indocker compose up
- Fixed a bug with
--resolve-image-digests
used with--no-interpolate
- You can now use a local
.env
file to overrideCOMPOSE_*
environment variables
2.27.0
2024-04-24Update
- Dependencies upgrade: bump golang to 1.21.9
- Dependencies upgrade: bump compose-go to v2.1.0
- Dependencies upgrade: bump docker engine and cli to v26.1.0
Bug fixes and enhancements
- Introduced
--abort-on-container-failure
flag - Introduced
--all-resources
to not exclude resources not used by services - Introduced support for
build.entitlements
- Fixed a bug so Docker Compose now ignores missing containers when
docker compose down/stop -p
is run - Fixed support for
--flag=value
syntax in compatibility mode
2.26.1
2024-03-29Update
- Dependencies upgrade: opencontainers/image-spec v1.1.0
Bug fixes and enhancements
- Added image pull failure reason in output
- Fixed crash when running up with
--no-build
and--watch
- Fixed crash when no TTY available and menu enabled
- Improved legibility of menu actions
2.26.0
2024-03-22Update
- Dependencies upgrade: bump compose-go v2.0.2
- Dependencies upgrade: bump docker v26.0.0
Bug fixes and enhancements
- Reduced timeout of the Otel tracing command
- Fixed
config --format json
- Fixed documentation on default build image name
- Introduced Synchronized file shares for bind mounts in Compose
- Added support for
annotations
- Introduced
config --variables
to list Compose model variables - Added a navigation menu within
docker compose up
2.25.0
2024-03-15Update
- Dependencies upgrade: bump compose-go v2.0.0
Bug fixes and enhancements
- Restored
config
behaviour until--no-interpolate
is set - Fixed service name shell completion
- Added
--watch
flag toup
command
2.24.7
2024-03-06Update
- Dependencies upgrade: bump golang to 1.21.8
- Dependencies upgrade: bump compose-go to 2.0.0-rc8
- Dependencies upgrade: bump docker to v24.0.4
Bug fixes and enhancements
- Compose now ensures stable priority sort order for networks
- Fixed interpolation with curly braces (e.g. JSON) in default values
- Fixed validation for non-unique
container_name
values - Fixed validation for
develop.watch
- Fixed environment loading for
include
- Fixed panic when merging labels/networks
- Added support for
--no-path-resolution
when usinginclude
- Fixed missing project name errors
- Fixed
--no-interpolate
flag onconfig
- Added a workaround for file lock issues with Watch mode on Windows
- Fixed duplicate exit code status messages
- Compose now respects
COMPOSE_REMOVE_ORPHANS
onup
2.24.6
2024-02-15Update
- Dependencies upgrade: bump cli to 25.0.3
- Dependencies upgrade: bump compose-go to 2.0.0-rc.7
Bug fixes and enhancements
- Fixed issue of
.env
file loading when project file is set viaCOMPOSE_FILE
variable - Aligned
ps --status=exited
behaviour with the Docker CLI behaviour - Fixed a deadlock when collecting large logs
2.24.5
2024-01-30Bug fixes and enhancements
- Fixed ""failed to solve: changes out of order"" errors when building images on Windows.
2.24.4
2024-01-29Update
- Dependencies upgrade: bump cli to 25.0.1
- Dependencies upgrade: bump docker to 25.0.1
- Dependencies upgrade: bump compose-go to 2.0.0-rc.3
Bug fixes and enhancements
- Fixed issue when checking external network existence when swarm is enabled.
- Added support for
storage_opt
attribute.
2.24.3
2024-01-24This release fixes a build issue with Docker Desktop for Windows introduced in Compose v2.24.0.
Update
- Compose now uses a custom version of
fsutils
library.
2.24.2
2024-01-22Update
- Dependencies upgrade: bump cli to 25.0.0 GA
- Dependencies upgrade: bump compose-go to 2.0.0-rc.2
2.24.1
2024-01-18Update
- Dependencies upgrade: bump cli to 25.0.0-rc3
- Dependencies upgrade: bump docker to 25.0.0-rc3
- Dependencies upgrade: bump compose-go to 2.0.0-rc.1
- Dependencies upgrade: bump containerd to 1.7.12
Bug fixes and enhancements
- Reworked the display of container status during
up
- Fixed the engine version required to use
healthcheck.start_interval
- Removed
watch
subcommand from thealpha
command - Fixed a bug when handling received signals
2.24.0
2024-01-11Update
- Dependencies upgrade: bump cli to 25.0.0-beta.3
- Dependencies upgrade: bump compose-go to 2.0.0-beta.3
- Dependencies upgrade: bump golang to 1.21.6
Bug fixes and enhancements
- Introduced
docker compose attach
to attach local standard input, output, and error streams to a service's running container. - Introduced
docker compose stats
to display a live stream of container(s) resource usage statistics. - Introduced
docker compose ps --orphans
to include/exclude services not declared. - Introduced
docker compose logs --index
to select a replica container. - Introduced
docker compose build --with-dependencies
to also build dependencies. - Added source policies for build.
- Included disabled services for shell completion.
- Restored
Project
in ps JSON output. - Added OCI 1.0 fallback support for AWS ECR.
- Build now does not require environment to be resolved.
- Compose now sends out a cancel event on SIGINT/SIGTERM signal for
compose up
. - Fixed log by exposing services ports when
--verbose
. - Fixed inlined and environment-defined configs to be mounted under /<id> until an explicit target is set.
- Fixed combination of
--pull always --no-build
. - Fixed race condition in log printer.
- Fixed
docker compose up
teardown when command context is cancelled.
2.23.3
2023-11-22Update
- Dependencies upgrade: bump buildx to v0.12.0
2.23.2
2023-11-21Update
- Dependencies upgrade: bump buildkit 0.12.3
- Dependencies upgrade: bump docker 24.0.7
- Dependencies upgrade: bump cli 24.0.7
- Dependencies upgrade: bump 1.20.2
Bug fixes and enhancements
- Compose now supports
builds.tags
withpush
command. - Compose Watch now re-builds service images at startup.
- Now
--remove-orphans
doesn't manage disabled services as orphaned. - Compose displays
Building
output log only if there is at least one service to build.
2.23.1
2023-11-16Update
- Dependencies upgrade: bump compose-go to v1.20.1
Bug fixes and enhancements
- Aligned Compose with OCI artifact best practices.
- Introduced
--resolve-image-digests
so users can seal service images by digest when publishing a Compose application. - Improved Compose Watch configuration logging.
- Compose now rejects a Compose file using
secrets|configs.driver
ortemplate_driver
. - Compose now fails to start if a dependency is missing.
- Fixed SIGTERM support to stop/kill stack.
- Fixed a
--hash
regression. - Fixed ""Application failed to start after update"" when an external network is on a watched service.
- Fixed
--pull
documentation. - Fixed display by adding newline in cmd/compose/build.go.
- Compose is rendered quiet after filtering applied.
- Stripped project prefix from docker-compose up output.
2.23.0
2023-10-18Update
- Dependencies upgrade: bump compose-go to v1.20.0
- Dependencies upgrade: bump containerd to 1.7.7
Bug fixes and enhancements
- Added dry-run support for publish command
- Added
COMPOSE_ENV_FILES
env variable to pass a list of env files - Added
sync+restart
action tocompose watch
- Aligned
compose ps
output with Docker CLI by default and introduced--no-trunc
to keep the previous behaviour - Fixed hashes inconsistency between
up
andconfigure
- Enabled profiles when
down
ran with explicit service names - Fixed an issue when the pull policy provided was invalid
2.22.0
2023-09-21Note
The
watch
command is now generally available (GA). You can directly use it from the root commanddocker compose watch
. For more information, see File watch.
Update
- Dependencies upgrade: bump golang to 1.21.1
- Dependencies upgrade: bump compose-go to v1.19.0
- Dependencies upgrade: bump buildkit to v0.12.2
Bug fixes and enhancements
- Added experimental support for the
publish
command. - The command
watch
now builds and launches the project during startup. - Added
policy
option to the--pull
flag. - Fixed various race and deadlock conditions for
up
command on exit. - Fixed multi-platform issues on build.
- Enabled services that are explicitly requested even when their
profiles
aren't activated. - Fixed a
config
issue when the declaredenv_file
is missing. - Passed BuildOptions to
up
andrun
commands.
2.21.0
2023-08-30Note
The format of
docker compose ps
anddocker compose ps --format=json
changed to better align withdocker ps
output. See compose#10918.
Update
- Dependencies upgrade: bump compose-go to v1.18.3
Bug fixes and enhancements
- Changed
docker compose ps
anddocker compose ps --format=json
output to align with Docker CLI. - Added support for multi-document YAML files.
- Added support for loading remote Compose files from Git repos with
include
(experimental). - Fixed incorrect proxy variables during build.
- Fixed truncated container logs on container exit.
- Fixed ""no such service"" errors when using
include
with--profile
. - Fixed
.env
overrides when usinginclude
.
2.20.3
2023-08-11Update
- Dependencies upgrade: bump golang to 1.21.0
- Dependencies upgrade: bump compose-go to v1.18.1
- Dependencies upgrade: bump buildkit to v0.12.1
Bug fixes and enhancements
- Improved speed and reliability of
watch
sync. - Added builder's name on the first build line.
- Improved shell completion for
--project-directory
and--profile
. - Fixed build issue with proxy configuration not passing to legacy builder.
- Removed unnecessary warning when an option dependency exists successfully.
2.20.2
2023-07-19Bug fixes and enhancements
- Added support for the
depends_on.required
attribute. - Fixed an issue where build tries to push unnamed service images.
- Fixed a bug which meant the target secret path on Windows was not checked.
- Fixed a bug resolving build context path for services using
extends.file
.
2.20.1
2023-07-18Update
- Dependencies upgrade: bump golang to 1.20.6
- Dependencies upgrade: bump buildx to v0.11.2
- Dependencies upgrade: bump buildkit to v0.12
- Dependencies upgrade: bump docker-cli to v24.0.5-dev
2.20.0
2023-07-11Update
- Dependencies upgrade: bump docker/cli-docs-tools to v0.6.0
- Dependencies upgrade: bump docker to v24.0.4
- Dependencies upgrade: bump buildx to v0.11.1
Bug fixes and enhancements
- Introduced the
wait
command. - Added support of
--builder
andBUILDX_BUILDER
to thebuild
command. - Added support for the
include
andattach
attributes from the Compose Specification. - Fixed a DryRun mode issue when initializing CLI client.
- Fixed a bug with random missing network when a service has more than one.
- Fixed the Secrets file permission value to comply with the Compose Specification.
- Fixed an issue about
no-deps
flag not being applied. - Fixed some source code comments.
- Fixed a bug when
--index
is not set select. - Fixed a process leak in the wait e2e test.
- Improved some test speeds.
2.19.1
2023-06-29Update
- Dependencies upgrade: bump compose-go to v1.15.1
Bug fixes and enhancements
- Fixed sporadic ""container not connected to network"" errors on
compose up
. - Fixed ""please specify build context"" errors on
compose build
. - Compose now warns if using a bind mount in a service
watch
configuration.
2.19.0
2023-06-21Update
- Dependencies upgrade: bump compose-go to v1.15.0
- Dependencies upgrade: bump buildx to v0.11.0
- Dependencies upgrade: bump docker to v24.0.2
- Dependencies upgrade: bump golang to 1.20.5
Bug fixes and enhancements
- Introduced the ability to select a single service to be stopped by
compose down
. - Added
--progress
as top-level flag to configure progress UI style. - Introduced
run --cap-add
to run maintenance commands using service image. - Fixed a bug during detection of swarm mode.
- Fixed a bug when setting the project name via
COMPOSE_PROJECT_NAME
environment variable. - Adjusted the display of the volumes flag with the help of
down
command. - Fixed a bug in the
up
command which should not silently ignore missingdepends_on
services. - Aligned forward signal to container behaviour with the
docker run
one. - Compose now detects network name conflict.
- Fixed a typo in the warning message about an existing volume.
- Compose now detects new services started after
compose -p x logs -f
command. - Fixed a bug when
compose
was used as project name. - Fixed a bug in the
watch
command when a directory does not exist. - Removed default timeout of 10 seconds when restarting or stopping services.
- Fixed a bug in
watch
which applied the ""rebuild"" strategy by default. - Fixed a race condition, waiting for containers when one exit.
- Added a warning telling users that uid,gid,mode are not implemented for
build.secrets
. - Fixed a bug in
watch
which was watching the whole build context instead of only configured paths. - Compose now sorts containers by creation date to scale down the older ones first.
- Fixed a bug in the docs generation task for Windows environments.
- Updated the docs to reflect Dry Run mode is feature complete.
- Improved the diagnostic message on network label mismatch.
- Fixed a bug which was rendering
Building
section when there was no build involved. - Fixed a bug in code coverage metrics.
- Added OTEL initialization.
- Added a GitHub action to trigger Docker Desktop e2e tests with Compose edge versions.
- Added more ignore rules to dependabot.
2.18.1
2023-05-17Bug fixes and enhancements
- Fixed ""Image not found"" errors when building images
2.18.0
2023-05-16Update
- Dependencies upgrade: bump compose-go to v1.13.5
- Dependencies upgrade: bump buildkit to v0.11.6
- Dependencies upgrade: bump docker to v23.0.5
Bug fixes and enhancements
- Added dry run support using
--dry-run
- Added the first (alpha) implementation of the
viz
sub-command - Introduced
--no-path-resolution
to skip relative path to be resolved - Introduced
COMPOSE_ANSI
to define the--ansi
default value - Introduced
COMPOSE_STATUS_STDOUT
to get status messages sent to stdout - Fixed the BuildKit progressui integration
- Fixed a bug to stop blocking the events loop collecting logs
- Restored support for
--memory
- Fixed a bug which meant containers didn't stop after termination
- Compose now lets users declare the build secret target
- Fixed a bug which caused a container to be recreated when the config has not changed
- Fixed a race condition when
--parallel
is used with a large number of dependent services - Compose now checks the local image matches the required platform
- Fixed local image removal when
compose down
is ran with--project-name
- Compose now detects the active endpoint trying to remove the network and skips with a warning
- Removed unnecessary [] output
- Compose detects that a Windows terminal is not a
console.File
to avoid a panic --parallel
now has precedence overCOMPOSE_PARALLEL_LIMIT
- Compose now reports that the external network is not found when Swarm is disabled
2.17.2
2023-03-26Update
- Dependencies upgrade: bump compose-go to v1.13.2
Bug fixes and enhancements
- Fixed invalid project name error for directories with uppercase characters or
.
in the name. Fixed compose#10405
2.17.1
2023-03-24Update
- Dependencies upgrade: bump buildkit to v0.11.5
- Dependencies upgrade: bump compose-go to v1.13.1
- Dependencies upgrade: bump golang to 1.20.2
Bug fixes and enhancements
- Fixed panic on
alpha watch
command. Pull Request compose#10393 - Prevented conflicts for services named
extensions
. Fixed compose-go#247 - Compose now validates project names more consistently. Fixed compose-go#363
2.17.0
2023-03-23Upgrade notes
- Project name validation is more strictly enforced. Project names can only include letters, numbers,
_
,-
and must be lowercase and start with a letter or number. - Boolean fields in YAML must be either
true
orfalse
. Deprecated YAML 1.1 values such as ""on"" or ""no"" are not supported. - Duplicate YAML merge keys (
<<
) are rejected.
Update
- Dependencies upgrade: bump buildkit to v0.11.4
- Dependencies upgrade: bump buildx to v0.10.4
- Dependencies upgrade: bump containerd to 1.6.18
- Dependencies upgrade: bump compose-go to v1.13.0
Bug fixes and enhancements
- Introduced
--wait-timeout
onup
command. Fixed compose#10269 - Made
compose service --hash
output sort by service name. Pull Request compose#10278 - Compose now renders a compact TUI progress report to monitor layers download. Pull Request compose#10281
- Introduced
restart
fordepends_on
. Fixed compose#10284 - Added support of
NO_COLOR
env var. Fixed compose#10340 - Progress writer now uses
dockercli.Err
stream. Fixed compose#10366 - Added support for
additional_contexts
in thebuild
service configuration. Fixed compose#9461 compose#9961 - Added file delete/rename handling in
watch
mode. Pull Request compose#10386 - Introduced an
ignore
attribute inwatch
mode. Pull Request compose#10385 - Compose now uses progress writer to show copies status. Pull Request compose#10387
- Updated reference documentation for
-p
/--project-name
flag. Fixed docs#16915, compose-spec#311 - Introduced a
replace
label to track the relationship between old and new containers of a service. Fixed compose#9600 - Fixed a bug that meant dependent services were not restarted after a service was restarted. Fixed compose#10263
- Compose now ignores services without a build section in
watch
mode. Fixed compose#10270 - Compose now applies config options for pseudo-subcommands. Fixed compose#10286
- Compose manages only containers with config_hash labels (i.e, created by compose). Fixed compose#10317
- Compose triggers an error if the project name is empty after normalization. Fixed compose#10313
- Compose restarts only needed services by checking
depends_on
relations. Fixed compose#10337 - Fixed a display issue on small terminals. Fixed compose#10322
- Fixed an issue with building the built images IDs collection. Pull Request compose#10372
- Use configured name separator to define oneoff container name. Fixed compose#10354
- Fixed concurrent map read/write issue when recreating containers. Fixed compose#10319
- Compose now supports Dry Run mode for
stop
andrm
commands. Pull Request compose#10257 - Compose now supports Dry Run mode for
pull
command. Pull Request compose#10341 - Compose now supports Dry Run mode for
push
command. Pull Request compose#10355 - Compose now supports Dry Run mode for
exec
command. Pull Request compose#10252 - Compose now supports Dry Run mode for
restart
command. Pull Request compose#10339
2.16.0
2023-02-08Update
- Dependencies upgrade: bump docker to v23.0.0
- Dependencies upgrade: bump docker-cli to v23.0.0
- Dependencies upgrade: bump buildkit to v0.11.2
- Dependencies upgrade: bump buildx to v0.10.2
- Dependencies upgrade: bump containerd to 1.6.16
- Dependencies upgrade: bump golang to 1.20
Bug fixes and enhancements
- Introduced
--remove-orphans
for thecompose create
command. Fixed compose#9718 - Shortened the TTY output when the terminal is too small. Fixed compose#9962
- Added
remove-orphans
functionality to run. Fixed compose#9718 - Introduced the experimental
watch
command. Pull Request compose#10163 - Compose now allows TTY to be allocated with
-t
. Fixed compose#10161 - Introduced the experimental
dry-run
command. Pull Request compose#10173 - Updated the documentation to explain ways to configure parallelism. Pull Request compose#10198
- Aligned the
logs
command with docker CLI by aliasing-n
for--tail
. Fixed compose#10199 - Added support for
docker compose build --push
. Pull Request compose#10148 - Added
--scale
to thecompose create
command. Fixed compose#10208 - Renamed
convert
toconfig
to align with the Compose V1 UX. Pull Request compose#10214 - Compose now passes the proxy config as build args. Fixed compose#8797
- Fixed parsing issue in
compose up
by ignoring containers not created by Compose. Fixed compose#10162 - Fixed the goroutine leak in log formatter initialization. Fixed compose#10157
- Fixed an issue where compose logs don't exit when all running containers have been stopped. Pull Request compose#10181
- Fixed the documentation to reflect
docker compose ps
being aligned withdocker ps
. Pull Request compose#10195 - Fixed an issue where the remote Buildx driver was not found. Fixed compose#9893
- Improved logging when recreating a service container. Pull request compose#10236
- Fixed an issue so Compose now only waits for containers concerned by the wait condition. Fixed compose#10200
- Compose now prevents assignment to entry in nil map. Fixed compose#10244
- Added a dedicated GitHub Action workflow for Cucumber tests. Pull Request compose#10165
- Cleaned the TUI lines when switching in compact log mode. Fixed compose#10201
- Added Tilt watcher to detect code changes in watch mode. Pull Request compose#10218
- Compose now supports Dry Run mode for
kill
command. Fixed compose#10210 - Compose now supports Dry Run mode for
pause
command.Fixed compose#10217 - Compose now supports Dry Run mode for
cp
command.Fixed compose#10235
2.15.1
2023-01-09Update
- Dependencies upgrade to fix Golan CVE-2022-27664 and CVE-2022-32149
Bug fixes and enhancements
- Added support for UTS namespace. Fixed compose#8408
- Fixed filtering issue when no filter set. Fixed roadmap#418
- Fixed concurrent map writes issue during build step. Pull Request compose#10151
- Fixed issue when stdin is not a terminal. Fixed compose#9739
2.15.0
2023-01-05Update
- Dependencies upgrade: bump compose-go to v1.8.1
- Dependencies upgrade: bump cli-docs-tool to 0.5.1
Bug fixes and enhancements
- Added support of the
privileged
attribute in theservice.build
section. Pull Request compose#10112 - Introduced
--ignore-buildable
to ignore buildable images on pull. Fixed compose#8805 - Introduced
--no-attach
to ignore some service outputs. Fixed compose#8546 - Fixed issue with
logs
whendriver:none
is set. Fixed compose#9030 - Compose now relies on dockerCLI.streams. Pull Request compose#10082
- Fixed issue with service hash that MUST exclude replicas. Fixed compose#10077
- Compose now checks service names based on project, not running containers. Fixed compose#9951
- Fixed security opts support (seccomp and unconfined). Fixed compose#9505
- Fixed empty file when using compose config in case of smaller source files. Fixed compose#10121
- Fixed issue with
--pull
not applied oncompose up
. Fixed compose#10125 - Compose should ignore not only auto-removed containers but also ""removal in progress"" for orphan containers. Pull Request compose#10136
- Compose limits build concurrency according to
--parallel
. Fixed compose#9091
2.14.2
2022-12-20Update
- Dependencies upgrade: bump containerd to 1.6.14
Bug fixes and enhancements
- Compose now uses DOCKER_DEFAULT_PLATFORM to determine the platform when creating a container. Fixed compose#10041
- Compose now detects when dependency failed to start. Fixed compose#9732
- Fixed WCOW volume mounts. Fixed compose#9577
- List only running containers when using
--all=false
. Fixed compose#10085 - Fixed a regression when running pull
--ignore-pull-failures
. Fixed compose#10089 - Fixed CPU quota issue. Fixed compose#10073
- Fixed race condition on compose logs. Fixed compose#8880
- Updated projectOptions to be public by renaming it to ProjectOptions. Fixed compose#100102
2.14.1
2022-12-15Updates
- Dependencies upgrade: bump Go to 1.19.4
- Dependencies upgrade: bump containerd to 1.6.12
Bug fixes and enhancements
- Added
--parallel
to limit concurrent engine calls. Pull Request compose#10030 - Distinguished stdout and stderr in
up
logs. Fixed compose#8098 - Aligned
compose ps
output withdocker ps
. Fixed compose#6867 - Added
--include-deps
to push command. Pull Request compose#10044 - Introduced
--timestamp
option oncompose up
. Fixed compose#5730 - Compose now applies uid/gid when creating a secret from the environment. Pull Request compose#10084
- Fixed deadlock when waiting for attached-dependencies. Fixed compose#10021
- Fixed race condition when collecting pulled images IDs. Fixed compose#9897
- Compose doesn't stop the
pull
command for images that can be built. Fixed compose#8724 - Fixed corner case when there's no container to attach to. Fixed compose#8752
- Compose containers' startup must run sequentially for engine to assign distinct ports within a configured range. Fixed compose#8530
- Fixed parsing of
repository:tag
. Fixed compose#9208 - Load project from files when explicitly set by user. Fixed compose#9554
2.14.0
2022-12-02Updates
- Dependencies upgrade: bump compose-go to v1.8.0
- Dependencies upgrade: bump Go to 1.19.3
Bug fixes and enhancements
- Added
oom_score_adj
field to service definition. Pull Request compose#10019 - Added mode field for tmpfs mount permissions. Pull Request compose#10031
- Compose now only stops services started by
up
when interrupted. Fixed compose#10028 - Compose now loads implicit profiles for targeted services. Fixed compose#10025
- Compose does not require
service.build.platforms
to be set ifservice.platform
is set. Fixed compose#10017 - Plain output is used during buildx image builds if
--ansi=never
is set. Fixed compose#10020 COMPOSE_IGNORE_ORPHANS
environment variable now behaves more consistently. Fixed compose#10035- Compose now uses the correct image name separator in
convert
. Fixed compose#9904 - Fixed
run
for services usingnetwork_mode: service:NAME
. Fixed compose#10036
2.13.0
2022-11-23Updates
- Dependencies upgrade: bump containerd to 1.6.10
- Dependencies upgrade: bump docker-credential-helpers to v0.7.0
- Update CI dependencies. Pull Request compose#9982
Bug fixes and enhancements
- Added a
no-consistency
option toconvert
command. Fixed compose#9963 - Added a
build
option torun
command. Fixed compose#10003 - Fixed mapping
restart_policy.condition
to engine supported values. Fixed compose#8756, docs#15936 - Fixed missing support of
deploy.reservation.memory
. Fixed compose#9902 - Fixed a bug to prevent usage of
COMPOSE_PROFILES
when--profile
arg is used. Fixed compose#9895 - Fixed a bug to prevent pulling a service's image when depending on a service which will build this image. Fixed compose#9983
- Fixed parsing issue when a container number label is not found. Fixed compose#10004
- Compose now uses the platform value defined by
DOCKER_DEFAULT_PLATFORM
when noservice.platform
defined. Fixed compose#9889 - Removed usage of the deprecated dependency
gotest.tools
v2. Pull Request compose#9935 - Excluded issues labeled with
kind/feature
from stale bot process. Fixed compose#9988
2.12.2
2022-10-21Updates
- Updated Docker Engine API to restore compatibility with Golang 1.18 needed for Linux packaging. Pull Request compose#9940
For the full change log or additional information, check the Compose repository 2.12.2 release page.
2.12.1
2022-10-21Security
- Updated Docker Engine API to apply fix of CVE-2022-39253. Pull Request compose#9934
For the full change log or additional information, check the Compose repository 2.12.1 release page.
2.12.0
2022-10-18Updates
CI update to the documentation repository path
Upgraded to compose-go from 1.5.1 to 1.6.0
Updated to go 1.19.2 to address CVE-2022-2879, CVE-2022-2880, CVE-2022-41715
Bug fixes and enhancements
- Added a
quiet
option when pushing an image. Fixed compose#9089 - Fixed a misleading error message for
port
command. Pull Request compose#9909 - Fixed a bug to prevent failure when Compose tries to remove a non-existing container. Fixed by compose#9896
- Switched GitHub issue template form
For the full change log or additional information, check the Compose repository 2.12.0 release page.
2.11.2
2022-09-27Note
- Updates on environment file syntax & interpolation: see compose#9879
- Setting
DOCKER_HOST
via.env
files is not supported in Compose v2
Updates
- Upgraded to compose-go from 1.5.1 to 1.6.0
Bug fixes and enhancements
- Fixed a bug to prevent ""invalid template"" errors on valid environment variable values. Fixes compose##9806, compose##9746, compose##9704, compose##9294
- Fixed a bug to ensure new images from
docker compose build
are used. Fixes compose#9856 - Fixed cross-architecture builds when
DOCKER_DEFAULT_PLATFORM
not set. Fixes compose#9864 - Fixed intermittent conflict errors when using
depends_on
. Fixes compose#9014 - Cleared service
CMD
when entry point is overridden. Fixes compose#9622 - Configured default builder export when no
build.platforms
defined. Fixes compose#9856 - Fixed a bug to keep the platform defined, in priority, via DOCKER_DEFAULT_PLATFORM or the
service.platform
attribute. Fixes compose#9864 - Removed support for
DOCKER_HOST
in.env
files. Fixes compose#9210 - Fixed a bug to ensure clean service command if entry point is overridden in run command. Fixes compose#9622
- Deps: fixed race condition during graph traversal. Fixes compose#9014
- CI now runs on Windows & macOS including E2E tests via Docker Desktop
- Added more information when
service.platform
isn't part ofservice.build.platforms
- GitHub Workflows security hardening
For the full change log or additional information, check the Compose repository 2.11.2 release page.
2.11.1
2022-09-20Bug fixes and enhancements
- Fixed a bug to keep
depends_on
condition when service hasvolumes_from
. Fixes compose#9843 - Fixed a bug to keep the platform defined at service level during build if no build platforms. Fixes compose#9729
- Fixed a bug to keep the platform defined via DOCKER_DEFAULT_PLATFORM during build if no build platforms provided. Fixes compose#9853
For the full change log or additional information, check the Compose repository 2.11.1 release page.
2.11.0
2022-09-14Updates
- Dependencies upgrade: bump Golang to 1.19.1
- Dependencies upgrade: bump github.com/docker/go-units from 0.4.0 to 0.5.0
- Dependencies upgrade: bump github.com/cnabio/cnab-to-oci from 0.3.6 to 0.3.7
- Dependencies upgrade: bump go.opentelemetry.io/otel from 1.9.0 to 1.10.0
- Dependencies upgrade: bump github.com/AlecAivazis/survey/v2 from 2.3.5
- Dependencies upgrade: bump go.opentelemetry.io/otel from 1.4.1 to 1.9.0
- Dependencies upgrade: bump compose-go from 1.5.0 to 1.5.1
Bug fixes and enhancements
- Added platforms build. Fixes compose-spec#267
- Logs now filter to services from current Compose file. Fixes compose#9801
- Added an improved output warning when pulling images. Fixes compose#9820
- Fixed a bug to ensure correct capture of exit code when service has dependencies. Fixes compose#9778
- Fixed
down
with--rmi
. Fixes compose#9655 - Fixed docker-compose convert that turns $ into $$ when using the --no-interpolate option. Fixes compose#9160
- Fixed
build.go
access custom labels directly cause panic. See compose#9810 - Applied newly loaded envvars to ""DockerCli"" and ""APIClient"". Fixes compose#9210
- Only attempt to start specified services on
compose start [services]
. Fixes compose#9796 compose#9807 - Label built images for reliable cleanup on
down
. Fixes compose#9655
For the full change log or additional information, check the Compose repository 2.11.0 release page.
2.10.2
2022-08-26Bug fixes and enhancements
- Properly respect
DOCKER_TLS_VERIFY
andDOCKER_CERT_PATH
environment variables. Fixes compose#9789. - Improved
Makefile
used in docker/docker-ce-packaging#742.
For the full change log or additional information, check the Compose repository 2.10.2 release page.
2.10.1
2022-08-24Updates
- Dependencies update: Bumped github.com/moby/buildkit from 0.10.3 to 0.10.4.
Bug fixes and enhancements
- Fixed image pulls being skipped when
pull_policy
was not set. Fixes compose#9773. - Restored
.sha256
checksum files in release artifacts. Fixes compose#9772. - Removed error message showing exit code when using --exit-code-from. Fixes compose#9782.
- Fixed
compose pull
to pull images even when they existed locally iftag=latest
. - CI: Fixed checksums checking and brought back individual checksum files.
For the full change log or additional information, check the Compose repository 2.10.1 release page.
2.10.0
2022-08-19New
- Applied newly loaded environment variables to
DockerCli
andAPIClient
. Fixes compose#9210. - Added support for windows/arm64 and linux/riscv64.
Updates
- Updated Dockerfile syntax to latest stable and renamed docs Dockerfile.
- Dependencies update: Upgraded BuildKit & docker/distribution.
- Dependencies update: Updated Docker CLI version used in CI to v20.10.17.
- Dependencies update: Bumped github.com/containerd/containerd from 1.6.6 to 1.6.7.
- Dependencies update: Bump github.com/containerd/containerd from 1.6.7 to 1.6.8.
- Dependencies update: Bumped to Go 1.18.5.
- Dependencies update: Bumped github.com/cnabio/cnab-to-oci from 0.3.5 to 0.3.6.
Bug fixes and enhancements
- Reverted environment variables precedence to OS over
.env
file. Fixes compose#9737. - Updated usage strings for consistency.
- Resolved environment variables case-insensitively on Windows. Fixes compose#9431.
- Fixed
compose up
so dependency containers aren't stopped when a stop signal is issued. This keeps parity with v1 behavior-wise. - Fixes compose#9696.
- Fixed commands that start/restart/pause/unpause so that, if ran from the Compose file, the Compose model is also applied. Fixes compose#9705 and compose#9705.
- Removed extra whitespaces in help text of some subcommands.
- Fixed
compose create
to not override service pull policy when the value from the command line is configured as the default. Fixes compose#9717. - Filtered out ""commandConn.Close- warning"" message. Fixes compose#8544.
- Fixed up/start/run to not wait for disabled dependency. Fixes compose#9591.
- Applied Compose model on
compose kill
, added--remove-orphans
option. Fixes compose#9742. - Fixed
compose pull
to avoid pulling the same images multiple times. Fixes compose#8768. - Fixed version of golangci-lint to v1.47.3, issue with v1.48.0 for now.
For the full change log, check the Compose repository 2.10.0 release page.
2.9.0
2022-08-7Important
Compose v2.9.0 contains changes to the environment variable's precedence that have since been reverted. We recommend using v2.10+ to avoid compatibility issues.
Note
This release reverts the breaking changes introduced in Compose v2.8.0 by
compose-go v1.3.0
.
Updates
- Updated
compose-go
to v1.4.0 as previous version introduced breaking changes. Fixes compose#9700.
Bug fixes and enhancements
- Overwritten parent commands PreRun code for
compose version
. Fixes compose#9698. - Fixed
LinkLocalIPs
in V2. Fixes compose#9692. - Linked to
BUILDING.md
for testing instructions. Fixes compose#9439.
For the full change log or additional information, check the Compose repository 2.9.0 release page.
2.8.0
2022-07-29Important
This release introduced a breaking change via
compose-go v1.3.0
and this PR. In this release, Docker Compose recreates new resources (networks, volumes, secrets, configs, etc.) with new names, using a-
(dash) instead an_
(underscore) and tries to connect to or use these newly created resources instead of your existing ones!Please use Compose the v2.9.0 release instead.
New
- Introduced
--pull
flag to allow the force pull of updated service images. Fixes compose#9451. - Increased code quality by adding
gocritic
to the linters.
Bug fixes and enhancements
- Fixed interpolation error message output. Fixes compose-spec/compose-go#292.
- Defined precedence of the environment variables evaluation. Fixes compose#9521, compose#9638, compose#9608, compose#9578. compose#9468, and compose#9683.
- Docs CI: Fixed to use push-to-fork when creating a PR.
- Used environmental variable for golang's version and updates GitHub Actions from v2 to v3.
- Used google/addlicense instead of kunalkushwaha/ltag.
For the full change log or additional information, check the Compose repository 2.8.0 release page.
2.7.0
2022-07-20New
- Added support for environment secrets during build step. Fixes compose#9606.
Updates
- Dependencies upgrade: bumped go to 1.18.4.
- Dependencies upgrade: bumped compose-go to v1.2.9.
Bug fixes and enhancements
- Networks: prevented issues due to duplicate names. Fixes moby/moby#18864.
- Fixed issue with close networks name on
compose up
andcompose down
commands. Fixes compose#9630. - Used appropriate dependency condition for one-shot containers when running
compose up --wait
. Fixes compose#9606. - Fixed environment variable expansion.
- Validated depended-on services exist in consistency check. Fixes compose#8910.
- Fixed hash usage in environment values. Fixes compose#9509.
- Docker Build: added fix to respect dependency order for classic builder. Fixes compose#8538.
- Fixed panic caused by empty string argument. Fixes compose-switch#35.
- Fixed start/restart as to not impact one-off containers. Fixes compose#9509.
- Fixed to keep the container reference when
volumes_from
targets a container and not a service. Fixes compose#8874. - build.go: added fix to initialize
CustomLabels
map ifnil
. - Added new targets to build Compose binary before running e2e tests.
- CI: released workflow to open a PR on docs repo with latest changes.
- e2e: added test for
ps
. - e2e: split out pause tests and add more cases.
- e2e: add more start/stop test cases.
For the full change log or additional information, check the Compose repository 2.7.0 release page.
2.6.1
2022-06-23New
- Added support for setting secrets from environment variable. Fixes compose-spec/compose-spec#251.
Updates
- Upgrade: compose-go v1.2.8.
- Upgrade: buildx v0.8.2.
- Dependencies upgrade: bumped runc to 1.1.2.
- Dependencies upgrade: bumped golang to 1.18.3.
- Dependencies upgrade: bumped compose-go to v1.2.8.
- Dependencies upgrade: bumped github.com/theupdateframework/notary from 0.6.1 to 0.7.0.
- Dependencies upgrade: bumped github.com/cnabio/cnab-to-oci from 0.3.1-beta1 to 0.3.3.
- Dependencies upgrade: bumped github.com/hashicorp/go-version from 1.3.0 to 1.5.0.
- Dependencies upgrade: bumped github.com/stretchr/testify from 1.7.0 to 1.7.2.
- Dependencies upgrade: bumped github.com/docker/buildx from 0.8.1 to 0.8.2.
- Dependencies upgrade: bumped github.com/AlecAivazis/survey/v2 from 2.3.2 to 2.3.5.
- Dependencies upgrade: bumped github.com/containerd/containerd from 1.6.2 to 1.6.6.
Bug fixes and enhancements
- Added links to container create request. Fixes #9513.
- Fixed
compose run
to start only direct dependencies. Fixes #9459. - Fixed
compose up
'service not found' errors when using--no-deps
option. Fixes #9427. - Fixed
compose down
to respectCOMPOSE_REMOVE_ORPHANS
environment variable. Fixes #9562. - Fixed project-level bind mount volumes. Fixes docker/for-mac#6317.
- Fixed parsing of properties
deploy.limits.cpus
anddeploy.limits.pids
to respect floating-point values. Fixes #9542 and #9501. - Fixed
compose ps
output to list all exposed ports. Fixes #9257. - Fixed spelling mistakes in
compose ps
code. - Fixed
docker compose
to honor--no-ansi
even when deprecated option is requested. - Fixed network name and network ID possible ambiguity.
- e2e: added test for
ps
. - e2e: unmarshalled json into container summaries.
- e2e: fixed subtests and block parallel unsafe tests.
- e2e: isolated test command env from system env.
- e2e: fixed spurious
ps
failures. - e2e: ensured all compose commands standalone compatible.
- e2e: improved test output on failures.
For the full change log or additional information, check the Compose repository 2.6.1 release page.
2.6.0
2022-05-30New
- Added the tags property to the build section. In this property tags can be defined to be applied to the final image, in addition to the one defined in the image property.
- Added end-to-end tests to ensure there is no regression on environment variables precedence.
- Added ddev's end-to-end test.
Updates
- Dependencies update: bumping compose-go to 1.2.6.
- Dependencies update: bumping compose-go to 1.2.7.
- Dependencies update: bumping golang to 1.18.
Bug fixes and enhancements
- Fixed
compose up
to attach only to services declared in project with enabled profiles. Fixes #9286. - Fixed flickering prompt when pulling same image from multiple services. Fixes #9469.
- Fixed compose go to import .env file to OS environment to allow setting variables (such as DOCKER_BUILDKIT) through this file. Fixes #9345.
- Fixed
TestLocalComposeUp
that failed locally. - Fixed local run of make
e2e-compose-standalone
.
For the full change log or additional information, check the Compose repository 2.6.0 release page.
2.5.1
2022-05-17Updates
- Dependencies updates: bumping compose-go to 1.2.5.
Bug fixes and enhancements
- Fixed resolution of project's working directive absolute path when a relative path is declared using '--env-file'. Fixes docker/for-mac#6229.
- Fixed
compose down
: now rejects all arguments in order to clarify usage. Fixes #9151. - Fixed
compose down
: now exits with status=0 if there is nothing to remove. Fixes #9426. - Fixed extra space printed in logs output lines with --no-log-prefix option. Fixes #9464.
- Clarified what the default work dir is when multiple compose files are passed.
- cp command: copy to all containers of a service as default behavior.
For the full change log or additional information, check the Compose repository 2.5.1 release page.
2.5.0
2022-04-29Bug fixes and enhancements
- Fixed panic with
compose down
command when-p
flag specified. Fixes #9353. - Passed newly created project as input to start services (
docker compose up
). Fixes #9356. - Included services declared under links in docker-compose file as implicit dependencies. Fixes #9301.
- Added changes
docker compose pull
command to respect defined policy: 1) skip services configured aspull_policy: never
and 2) ignore those with an existing image andpull_policy: missing
. Fixes #3660. - Error building project from resources is no longer ignored in order to prevent
down
panic. Fixes #9383. - Enforced project name to be lowercase. Fixes #9378.
- Added support to build-time secrets. Fixes #6358.
- Changed
compose-go
to allow (re)building volume string to be used by enginebind
API when mount can't be used. Fixes #9380. - Provided checksums.txt file and added
--binary
to allow verification in different OS. Fixes #9388. - Added changes so locally pulled image's ID is inspected and persisted to
com.docker.compose.image
. Fixes #9357. - Fixed issue regarding IPAM gateway setup. Fixes #9330.
- Added support for ppc64le architecture for docker compose binary.
- Fixed search/replace typo in
--no-TTY
documentation.
For the full change log or additional information, check the Compose repository 2.5.0 release page.
2.4.1
2022-04-04Bug fixes and enhancements
- Passed the
--rm flag
value as is to the Docker CLI when running a container with this flag. Fixes #9314. - Added ssh config to the build options when building an image from a
docker compose up
command. Fixes #9338. - Added inspection to container checking if a TTY is required. Running services with
tty:true
specified now show console output. Fixes #9288.
For the full change log or additional information, check the Compose repository 2.4.1 release page.
2.4.0
2022-04-1Updates
- Dependencies update: Bumped buildx to v0.8.1. to fix possible panic on handling build context scanning errors.
Bug fixes and enhancements
- Passed the interactive flag '-i' from the Compose CLI to the Docker one to run exec command. Fixes #9315.
- Compose commands now take the value of
COMPOSE_PROJECT_NAME
environmental variable into consideration. Fixes #9316. - Fixed issue of
compose down
command that when executed in contexts without any services started or resources to be deleted was returning an error. Error was due to command trying to delete an inexistent default network. Fixes #9333. - Introduced support for
cache_from
,cache_to
,no_cache
andpull
attributes in the build section. These attributes allow forcing a complete rebuild from sources and checking with registry for images used. These changes provide the basis for offering--no-cache
and--pull
options for compose build (or equivalent) command down the line. - Introduced support of an
--ssh
flag for thebuild
command from CLI and Compose file. Fixes #7025. - Fixed typo in
--ssh
flag description. Related to #7025. - Pinned Kubernetes dependencies to the same version as in buildx.
- Passed the interactive flag from the Compose CLI to the Docker one to run exec command.
- Fixed race condition on start-stop end-to-end tests running in parallel.
- Removed code regarding an obsolete warning.
- Vendor: github.com/containerd/containerd v1.6.2. Includes a fix for CVE-2022-24769 (doesn't affect our codebase).
For the full change log or additional information, check the Compose repository 2.4.0 release page.
2.3.4
2022-03-25New
- Introduced changes to use RunExec and RunStart from docker/cli to handle all the interactive/tty/* terminal logic.
Removed
- Removed a container with no candidate now produces a warning instead of an error. Fixes #9255.
- Removed the ""Deprecated"" mentions from -i and -t options to run and exec commands. These options are on by default and in use. Fixes #9229.
- Removed the ""Deprecated"" mention from the --filter flag, to keep consistency with other commands.
- Removed the need to get the original compose.yaml file to run 'docker compose kill'.
Updates
- Dependencies update: Bumped github.com/spf13/cobra from 1.3.0 to 1.4.0. Cobra library no longer requires Viper and all of its indirect dependencies See cobra's release page.
- Dependencies update: Bumped buildx from v0.7.1 to v0.8.0.
Bug fixes and enhancements
- Recovered behavior for 'compose up -d' of recreating containers of compose file images with refreshed content. Fixes #9259.
- Docker compose --status, --filter and --format flags documentation updates.
docker compose down -v
now does not remove external volumes and networks as per the option's expected and documented behavior. Whenever project is specified it is also now used to enforce down to only remove resources listed in compose.yaml file. Fixes #9172, #9145.- Changed Compose API reference docs automation to pick up diffs code vs. docs.
For the full change log or additional information, check the Compose repository 2.3.4 release page.
Other Releases
(2022-03-8 to 2022-04-14)
For the releases later than 1.29.2 and earlier than 2.3.4, please check the Compose repository release pages.
1.29.2
(2021-05-10)
Miscellaneous
Removed the prompt to use
docker-compose
in theup
command.Bumped
py
to1.10.0
inrequirements-indirect.txt
.
1.29.1
(2021-04-13)
Bugs
Fixed invalid handler warning on Windows builds.
Fixed config hash to trigger container re-creation on IPC mode updates.
Fixed conversion map for
placement.max_replicas_per_node
.Removed extra scan suggestion on build.
1.29.0
(2021-04-06)
Features
Added profile filter to
docker-compose config
.Added a
depends_on
condition to wait for successful service completion.
Miscellaneous
Added an image scan message on build.
Updated warning message for
--no-ansi
to mention--ansi never
as alternative.Bumped docker-py to 5.0.0.
Bumped PyYAML to 5.4.1.
Bumped python-dotenv to 0.17.0.
1.28.6
(2021-03-23)
Bug fixes
Made
--env-file
relative to the current working directory. Environment file paths set with--env-file
are now relative to the current working directory and override the default.env
file located in the project directory.Fixed missing service property
storage_opt
by updating the Compose schema.Fixed build
extra_hosts
list format.Removed additional error message on
exec
.
Miscellaneous
- Added
compose.yml
andcompose.yaml
to the default filename list.
1.28.5
(2021-02-26)
Bugs
Fixed the OpenSSL version mismatch error when shelling out to the SSH client (via bump to docker-py 4.4.4 which contains the fix).
Added missing build flags to the native builder:
platform
,isolation
andextra_hosts
.Removed info message on native build.
Fixed the log fetching bug when service logging driver is set to 'none'.
1.28.4
(2021-02-18)
Bug fixes
- Fixed SSH port parsing by bumping docker-py to 4.4.3.
Miscellaneous
- Bumped Python to 3.7.10.
1.28.3
(2021-02-17)
Bug fixes
Fixed SSH hostname parsing when it contains a leading 's'/'h', and removed the quiet option that was hiding the error (via docker-py bump to 4.4.2).
Fixed key error for
--no-log-prefix
option.Fixed incorrect CLI environment variable name for service profiles:
COMPOSE_PROFILES
instead ofCOMPOSE_PROFILE
.Fixed the fish completion.
Miscellaneous
Bumped cryptography to 3.3.2.
Removed the log driver filter.
For a list of PRs and issues fixed in this release, see Compose 1.28.3.
1.28.2
(2021-01-26)
Bug fixes
Revert to Python 3.7 bump for Linux static builds
Add bash completion for
docker-compose logs|up --no-log-prefix
Miscellaneous
- CI setup update
1.28.0
(2021-01-20)
Features
Added support for NVIDIA GPUs through device requests.
Added support for service profiles.
Changed the SSH connection approach to the Docker CLI by shelling out to the local SSH client. Set the
COMPOSE_PARAMIKO_SSH=1
environment variable to enable the old behavior.Added a flag to disable log prefix.
Added a flag for ANSI output control.
Docker Compose now uses the native Docker CLI's
build
command when building images. Set theCOMPOSE_DOCKER_CLI_BUILD=0
environment variable to disable this feature.
Bug fixes
Made
parallel_pull=True
by default.Restored the warning for configs in non-swarm mode.
Took
--file
into account when definingproject_dir
.Fixed a service attach bug on
compose up
.
Miscellaneous
Added usage metrics.
Synced schema with COMPOSE specification.
Improved failure report for missing mandatory environment variables.
Bumped
attrs
to 20.3.0.Bumped
more_itertools
to 8.6.0.Bumped
cryptograhy
to 3.2.1.Bumped
cffi
to 1.14.4.Bumped
virtualenv
to 20.2.2.Bumped
bcrypt
to 3.2.0.Bumped GitPython to 3.1.11.
Bumped
docker-py
to 4.4.1.Bumped Python to 3.9.
Linux: bumped Debian base image from stretch to buster (required for Python 3.9).
macOS: Bumped OpenSSL 1.1.1g to 1.1.1h, and Python 3.7.7 to 3.9.0.
Bumped PyInstaller to 4.1.
Relaxed the restriction on base images to latest minor.
Updated READMEs.
1.27.4
(2020-09-24)
Bug fixes
Removed path checks for bind mounts.
Fixed port rendering to output long form syntax for non-v1.
Added protocol to the Docker socket address.
1.27.3
(2020-09-16)
Bug fixes
Merged
max_replicas_per_node
ondocker-compose config
.Fixed
depends_on
serialization ondocker-compose config
.Fixed scaling when some containers are not running on
docker-compose up
.Enabled relative paths for
driver_opts.device
forlocal
driver.Allowed strings for
cpus
fields.
1.27.2
(2020-09-10)
Bug fixes
- Fixed bug on
docker-compose run
container attach.
1.27.1
(2020-09-10)
Bug fixes
Fixed
docker-compose run
whenservice.scale
is specified.Allowed the
driver
property for external networks as a temporary workaround for the Swarm network propagation issue.Pinned the new internal schema version to
3.9
as the default.Preserved the version number configured in the Compose file.
1.27.0
(2020-09-07)
Features
Merged 2.x and 3.x Compose formats and aligned with
COMPOSE_SPEC
schema.Implemented service mode for
ipc
.Passed
COMPOSE_PROJECT_NAME
environment variable in container mode.Made
run
behave in the same way asup
.Used
docker build
ondocker-compose run
whenCOMPOSE_DOCKER_CLI_BUILD
environment variable is set.Used the docker-py default API version for engine queries (
auto
).Parsed
network_mode
on build.
Bug fixes
Ignored build context path validation when building is not required.
Fixed float to bytes conversion via docker-py bump to 4.3.1.
Fixed the scale bug when the deploy section is set.
Fixed
docker-py
bump insetup.py
.Fixed experimental build failure detection.
Fixed context propagation to the Docker CLI.
Miscellaneous
Bumped
docker-py
to 4.3.1.Bumped
tox
to 3.19.0.Bumped
virtualenv
to 20.0.30.Added script for Docs synchronization.
1.26.2
(2020-07-02)
Bug fixes
- Enforced
docker-py
4.2.2 as minimum version when installing with pip.
1.26.1
(2020-06-30)
Features
- Bumped
docker-py
from 4.2.1 to 4.2.2.
Bug fixes
Enforced
docker-py
4.2.1 as minimum version when installing with pip.Fixed context load for non-docker endpoints.
1.26.0
(2020-06-03)
Features
Added
docker context
support.Added missing test dependency
ddt
tosetup.py
.Added
--attach-dependencies
to commandup
for attaching to dependencies.Allowed compatibility option with
COMPOSE_COMPATIBILITY
environment variable.Bumped
Pytest
to 5.3.4 and add refactor compatibility with the new version.Bumped
OpenSSL
from 1.1.1f to 1.1.1g.Bumped
certifi
from 2019.11.28 to 2020.4.5.1.Bumped
docker-py
from 4.2.0 to 4.2.1.
Bug fixes
Properly escaped values coming from
env_files
.Synchronized compose-schemas with upstream (docker/cli).
Removed
None
entries on exec command.Added
distribution
package to get distribution information.Added
python-dotenv
to delegate.env
file processing.Stopped adjusting output on terminal width when piped into another command.
Showed an error message when
version
attribute is malformed.Fixed HTTPS connection when
DOCKER_HOST
is remote.
1.25.5
(2020-04-10)
Features
Bumped OpenSSL from 1.1.1d to 1.1.1f.
Added Compose version 3.8.
- Limited service scale to the size specified by the field
deploy.placement.max_replicas_per_node
.
- Limited service scale to the size specified by the field
1.25.4
(2020-02-03)
Bug fixes
Fixed the CI script to enforce the minimal MacOS version to 10.11.
Fixed docker-compose exec for keys with no value on environment files.
1.25.3
(2020-01-23)
Bug fixes
Fixed the CI script to enforce the compilation with Python3.
Updated the binary's sha256 on the release page.
1.25.2
(2020-01-20)
New features
- Docker Compose now allows the compatibility option with
COMPOSE_COMPATIBILITY
environment variable.
Bug fixes
Fixed an issue that caused Docker Compose to crash when the
version
field was set to an invalid value. Docker Compose now displays an error message when invalid values are used in the version field.Fixed an issue that caused Docker Compose to render messages incorrectly when running commands outside a terminal.
1.25.1
(2020-01-06)
Bugfixes
Decoded the
APIError
explanation to Unicode before using it to create and start a container.Docker Compose discards
com.docker.compose.filepaths
labels that haveNone
as value. This usually occurs when labels originate from stdin.Added OS X binary as a directory to solve slow start up time issues caused by macOS Catalina binary scan.
Passed the
HOME
environment variable in container mode when running withscript/run/run.sh
.Docker Compose now reports images that cannot be pulled, however, are required to be built.
1.25.0
(2019-11-18)
New features
Set no-colors to true by changing
CLICOLOR
env variable to0
.Added working directory, config files, and env file to service labels.
Added ARM build dependencies.
Added BuildKit support (use
DOCKER_BUILDKIT=1
andCOMPOSE_DOCKER_CLI_BUILD=1
).Raised Paramiko to version 2.6.0.
Added the following tags:
docker-compose:latest
,docker-compose:<version>-alpine
, anddocker-compose:<version>-debian
.Raised
docker-py
to version 4.1.0.Enhanced support for
requests
, up to version 2.22.0.Removed empty tag on
build:cache_from
.Dockerfile
enhancement that provides for the generation oflibmusl
binaries for Alpine Linux.Pulling only of images that cannot be built.
The
scale
attribute now accepts0
as a value.Added a
--quiet
option and a--no-rm
option to thedocker-compose build
command.Added a
--no-interpolate
option to thedocker-compose config
command.Raised OpenSSL for MacOS build from
1.1.0
to1.1.1c
.Added support for the
docker-compose.yml
file'scredential_spec
configuration option.Resolution of digests without having to pull the image.
Upgraded
pyyaml
to version4.2b1
.Lowered the severity to
warning
for instances in whichdown
attempts to remove a non-existent image.Mandated the use of improved API fields for project events, when possible.
Updated
setup.py
for modernpypi/setuptools
, and removedpandoc
dependencies.Removed
Dockerfile.armhf
, which is no longer required.
Bug fixes
Made container service color deterministic, including the removal of the color red.
Fixed non-ASCII character errors (Python 2 only).
Changed image sizing to decimal format, to align with Docker CLI.
tty
size acquired through Python POSIX support.Fixed same file
extends
optimization.Fixed
stdin_open
.Fixed the issue of
--remove-orphans
being ignored encountered during use withup --no-start
option.Fixed
docker-compose ps --all
command.Fixed the
depends_on
dependency recreation behavior.Fixed bash completion for the
docker-compose build --memory
command.Fixed the misleading environmental variables warning that occurs when the
docker-compose exec
command is performed.Fixed the failure check in the
parallel_execute_watch function
.Fixed the race condition that occurs following the pulling of an image.
Fixed error on duplicate mount points (a configuration error message now displays).
Fixed the merge on
networks
section.Compose container is always connected to
stdin
by default.Fixed the presentation of failed services on the
docker-compose start
command when containers are not available.
1.24.1
(2019-06-24)
This release contains minor improvements and bug fixes.
1.24.0
(2019-03-28)
Features
Added support for connecting to the Docker Engine using the
ssh
protocol.Added an
--all
flag todocker-compose ps
to include stopped one-off containers in the command's output.Added bash completion for
ps --all|-a
.Added support for credential_spec.
Added
--parallel
todocker build
's options inbash
andzsh
completion.
Bug fixes
Fixed a bug where some valid credential helpers weren't properly handled by Compose when attempting to pull images from private registries.
Fixed an issue where the output of
docker-compose start
before containers were created was misleading.Compose will no longer accept whitespace in variable names sourced from environment files. This matches the Docker CLI behavior.
Compose will now report a configuration error if a service attempts to declare duplicate mount points in the volumes section.
Fixed an issue with the containerized version of Compose that prevented users from writing to stdin during interactive sessions started by
run
orexec
.One-off containers started by
run
no longer adopt the restart policy of the service, and are instead set to never restart.Fixed an issue that caused some container events to not appear in the output of the
docker-compose events
command.Missing images will no longer stop the execution of
docker-compose down
commands. A warning is now displayed instead.Force
virtualenv
version for macOS CI.Fixed merging of Compose files when network has
None
config.Fixed
CTRL+C
issues by enablingbootloader_ignore_signals
inpyinstaller
.Bumped
docker-py
version to3.7.2
to fix SSH and proxy configuration issues.Fixed release script and some typos on release documentation.
1.23.2
(2018-11-28)
Bug fixes
Reverted a 1.23.0 change that appended random strings to container names created by
docker-compose up
, causing addressability issues.Note
Containers created by
docker-compose run
will continue to use randomly generated names to avoid collisions during parallel runs.Fixed an issue where some
dockerfile
paths would fail unexpectedly when attempting to build on Windows.Fixed a bug where build context URLs would fail to build on Windows.
Fixed a bug that caused
run
andexec
commands to fail for some otherwise accepted values of the--host
parameter.Fixed an issue where overrides for the
storage_opt
andisolation
keys in service definitions weren't properly applied.Fixed a bug where some invalid Compose files would raise an uncaught exception during validation.
1.23.1
(2018-11-01)
Bug fixes
Fixed a bug where working with containers created with a version of Compose earlier than
1.23.0
would cause unexpected crashes.Fixed an issue where the behavior of the
--project-directory
flag would vary depending on which subcommand was used.
1.23.0
(2018-10-30)
Important note
The default naming scheme for containers created by Compose in this version
has changed from <project>_<service>_<index>
to
<project>_<service>_<index>_<slug>
, where <slug>
is a randomly-generated
hexadecimal string. Please make sure to update scripts relying on the old
naming scheme accordingly before upgrading.
Features
Logs for containers restarting after a crash will now appear in the output of the
up
andlogs
commands.Added
--hash
option to thedocker-compose config
command, allowing users to print a hash string for each service's configuration to facilitate rolling updates.Added
--parallel
flag to thedocker-compose build
command, allowing Compose to build up to 5 images simultaneously.Output for the
pull
command now reports status / progress even when pulling multiple images in parallel.For images with multiple names, Compose will now attempt to match the one present in the service configuration in the output of the
images
command.
Bug fixes
Fixed an issue where parallel
run
commands for the same service would fail due to name collisions.Fixed an issue where paths longer than 260 characters on Windows clients would cause
docker-compose build
to fail.Fixed a bug where attempting to mount
/var/run/docker.sock
with Docker Desktop for Windows would result in failure.The
--project-directory
option is now used by Compose to determine where to look for the.env
file.docker-compose build
no longer fails when attempting to pull an image with credentials provided by the gcloud credential helper.Fixed the
--exit-code-from
option indocker-compose up
to always report the actual exit code even when the watched container is not the cause of the exit.Fixed an issue that would prevent recreating a service in some cases where a volume would be mapped to the same mountpoint as a volume declared within the Dockerfile for that image.
Fixed a bug that caused hash configuration with multiple networks to be inconsistent, causing some services to be unnecessarily restarted.
Fixed a bug that would cause failures with variable substitution for services with a name containing one or more dot characters.
Fixed a pipe handling issue when using the containerized version of Compose.
Fixed a bug causing
external: false
entries in the Compose file to be printed asexternal: true
in the output ofdocker-compose config
.Fixed a bug where issuing a
docker-compose pull
command on services without a defined image key would cause Compose to crash.Volumes and binds are now mounted in the order they are declared in the service definition.
Miscellaneous
- The
zsh
completion script has been updated with new options, and no longer suggests container names where service names are expected.
1.22.0
(2018-07-17)
New features
Compose format version 3.7
Introduced version 3.7 of the
docker-compose.yml
specification. This version requires Docker Engine 18.06.0 or above.Added support for
rollback_config
in the deploy configurationAdded support for the
init
parameter in service configurationsAdded support for extension fields in service, network, volume, secret, and config configurations
Compose format version 2.4
- Added support for extension fields in service, network, and volume configurations
Bug fixes
Fixed a bug that prevented deployment with some Compose files when
DOCKER_DEFAULT_PLATFORM
was setCompose will no longer try to create containers or volumes with invalid starting characters
Fixed several bugs that prevented Compose commands from working properly with containers created with an older version of Compose
Fixed an issue with the output of
docker-compose config
with the--compatibility-mode
flag enabled when the source file contains attachable networksFixed a bug that prevented the
gcloud
credential store from working properly when used with the Compose binary on UNIXFixed a bug that caused connection errors when trying to operate over a non-HTTPS TCP connection on Windows
Fixed a bug that caused builds to fail on Windows if the Dockerfile was located in a subdirectory of the build context
Fixed an issue that prevented proper parsing of UTF-8 BOM encoded Compose files on Windows
Fixed an issue with handling of the double-wildcard (
**
) pattern in.dockerignore
files when usingdocker-compose build
Fixed a bug that caused auth values in legacy
.dockercfg
files to be ignoreddocker-compose build
will no longer attempt to create image names starting with an invalid character
1.21.2
(2018-05-03)
Bug fixes
- Fixed a bug where the ip_range attribute in IPAM configs was prevented from passing validation
1.21.1
(2018-04-27)
Bug fixes
In 1.21.0, we introduced a change to how project names are sanitized for internal use in resource names. This caused issues when manipulating an existing, deployed application whose name had changed as a result. This release properly detects resources using ""legacy"" naming conventions.
Fixed an issue where specifying an in-context Dockerfile using an absolute path would fail despite being valid.
Fixed a bug where IPAM option changes were incorrectly detected, preventing redeployments.
Validation of v2 files now properly checks the structure of IPAM configs.
Improved support for credentials stores on Windows to include binaries using extensions other than
.exe
. The list of valid extensions is determined by the contents of thePATHEXT
environment variable.Fixed a bug where Compose would generate invalid binds containing duplicate elements with some v3.2 files, triggering errors at the Engine level during deployment.
1.21.0
(2018-04-11)
New features
Compose file version 2.4
Introduced version 2.4 of the
docker-compose.yml
specification. This version requires Docker Engine 17.12.0 or above.Added support for the
platform
parameter in service definitions. If supplied, the parameter is also used when performing build for the service.
Compose file version 2.2 and up
- Added support for the
cpu_rt_period
andcpu_rt_runtime
parameters in service definitions (2.x only).
Compose file version 2.1 and up
Added support for the
cpu_period
parameter in service definitions (2.x only).Added support for the
isolation
parameter in service build configurations. Additionally, theisolation
parameter in service definitions is used for builds as well if nobuild.isolation
parameter is defined. (2.x only)
All formats
Added support for the
--workdir
flag indocker-compose exec
.Added support for the
--compress
flag indocker-compose build
.docker-compose pull
is now performed in parallel by default. You can opt out using the--no-parallel
flag. The--parallel
flag is now deprecated and will be removed in a future version.Dashes and underscores in project names are no longer stripped out.
docker-compose build
now supports the use of Dockerfile from outside the build context.
Bug fixes
Compose now checks that the volume's configuration matches the remote volume, and errors out if a mismatch is detected.
Fixed a bug that caused Compose to raise unexpected errors when attempting to create several one-off containers in parallel.
Fixed a bug with argument parsing when using
docker-machine config
to generate TLS flags forexec
andrun
commands.Fixed a bug where variable substitution with an empty default value (e.g.
${VAR:-}
) would print an incorrect warning.Improved resilience when encoding of the Compose file doesn't match the system's. Users are encouraged to use UTF-8 when possible.
Fixed a bug where external overlay networks in Swarm would be incorrectly recognized as inexistent by Compose, interrupting otherwise valid operations.
1.20.0
(2018-03-20)
New features
Compose file version 3.6
Introduced version 3.6 of the
docker-compose.yml
specification. This version must be used with Docker Engine 18.02.0 or above.Added support for the
tmpfs.size
property in volume mappings
Compose file version 3.2 and up
- The
--build-arg
option can now be used without specifying a service indocker-compose build
Compose file version 2.3
Added support for
device_cgroup_rules
in service definitionsAdded support for the
tmpfs.size
property in long-form volume mappingsThe
--build-arg
option can now be used without specifying a service indocker-compose build
All formats
Added a
--log-level
option to the top-leveldocker-compose
command. Accepted values aredebug
,info
,warning
,error
,critical
. Default log level isinfo
docker-compose run
now allows users to unset the container's entrypointProxy configuration found in the
~/.docker/config.json
file now populates environment and build args for containers created by ComposeAdded the
--use-aliases
flag todocker-compose run
, indicating that network aliases declared in the service's config should be used for the running containerAdded the
--include-deps
flag todocker-compose pull
docker-compose run
now kills and removes the running container upon receivingSIGHUP
docker-compose ps
now shows the containers' health status if availableAdded the long-form
--detach
option to theexec
,run
andup
commands
Bug fixes
Fixed
.dockerignore
handling, notably with regard to absolute paths and last-line precedence rulesFixed an issue where Compose would make costly DNS lookups when connecting to the Engine when using Docker For Mac
Fixed a bug introduced in 1.19.0 which caused the default certificate path to not be honored by Compose
Fixed a bug where Compose would incorrectly check whether a symlink's destination was accessible when part of a build context
Fixed a bug where
.dockerignore
files containing lines of whitespace caused Compose to error out on WindowsFixed a bug where
--tls*
and--host
options wouldn't be properly honored for interactiverun
andexec
commandsA
seccomp:<filepath>
entry in thesecurity_opt
config now correctly sends the contents of the file to the engineANSI output for
up
anddown
operations should no longer affect the wrong linesImproved support for non-unicode locales
Fixed a crash occurring on Windows when the user's home directory name contained non-ASCII characters
Fixed a bug occurring during builds caused by files with a negative
mtime
values in the build contextFixed an encoding bug when streaming build progress
1.19.0
(2018-02-07)
Breaking changes
- On UNIX platforms, interactive
run
andexec
commands now require thedocker
CLI to be installed on the client by default. To revert to the previous behavior, users may set theCOMPOSE_INTERACTIVE_NO_CLI
environment variable.
New features
Compose file version 3.x
- The output of the
config
command should now mergedeploy
options from several Compose files in a more accurate manner
Compose file version 2.3
- Added support for the
runtime
option in service definitions
Compose file version 2.1 and up
- Added support for the
${VAR:?err}
and${VAR?err}
variable interpolation syntax to indicate mandatory variables
Compose file version 2.x
- Added
priority
key to service network mappings, allowing the user to define in which order the specified service will connect to each network
All formats
Added
--renew-anon-volumes
(shorthand-V
) to theup
command, preventing Compose from recovering volume data from previous containers for anonymous volumesAdded limit for number of simultaneous parallel operations, which should prevent accidental resource exhaustion of the server. Default is 64 and can be configured using the
COMPOSE_PARALLEL_LIMIT
environment variableAdded
--always-recreate-deps
flag to theup
command to force recreating dependent services along with the dependency ownerAdded
COMPOSE_IGNORE_ORPHANS
environment variable to forgo orphan container detection and suppress warningsAdded
COMPOSE_FORCE_WINDOWS_HOST
environment variable to force Compose to parse volume definitions as if the Docker host was a Windows system, even if Compose itself is currently running on UNIXBash completion should now be able to better differentiate between running, stopped and paused services
Bug fixes
Fixed a bug that would cause the
build
command to report a connection error when the build context contained unreadable files or FIFO objects. These file types will now be handled appropriatelyFixed various issues around interactive
run
/exec
sessions.Fixed a bug where setting TLS options with environment and CLI flags simultaneously would result in part of the configuration being ignored
Fixed a bug where the DOCKER_TLS_VERIFY environment variable was being ignored by Compose
Fixed a bug where the
-d
and--timeout
flags inup
were erroneously marked as incompatibleFixed a bug where the recreation of a service would break if the image associated with the previous container had been removed
Fixed a bug where updating a mount's target would break Compose when trying to recreate the associated service
Fixed a bug where
tmpfs
volumes declared using the extended syntax in Compose files using version 3.2 would be erroneously created as anonymous volumes insteadFixed a bug where type conversion errors would print a stacktrace instead of exiting gracefully
Fixed some errors related to unicode handling
Dependent services no longer get recreated along with the dependency owner if their configuration hasn't changed
Added better validation of
labels
fields in Compose files. Label values containing scalar types (number, boolean) now get automatically converted to strings
1.18.0
(2017-12-18)
New features
Compose file version 3.5
Introduced version 3.5 of the
docker-compose.yml
specification. This version requires Docker Engine 17.06.0 or aboveAdded support for the
shm_size
parameter in build configurationsAdded support for the
isolation
parameter in service definitionsAdded support for custom names for network, secret and config definitions
Compose file version 2.3
Added support for
extra_hosts
in build configurationAdded support for the long syntax for volume entries, as previously introduced in the 3.2 format. Using this syntax will create mounts instead of volumes.
Compose file version 2.1 and up
Added support for the
oom_kill_disable
parameter in service definitions (2.x only)Added support for custom names for network definitions (2.x only)
All formats
Values interpolated from the environment will now be converted to the proper type when used in non-string fields.
Added support for
--label
indocker-compose run
Added support for
--timeout
indocker-compose down
Added support for
--memory
indocker-compose build
Setting
stop_grace_period
in service definitions now also sets the container'sstop_timeout
Bug fixes
Fixed an issue where Compose was still handling service hostname according to legacy engine behavior, causing hostnames containing dots to be cut up
Fixed a bug where the
X-Y:Z
syntax for ports was considered invalid by ComposeFixed an issue with CLI logging causing duplicate messages and inelegant output to occur
Fixed an issue that caused
stop_grace_period
to be ignored when using multiple Compose filesFixed a bug that caused
docker-compose images
to crash when using untagged imagesFixed a bug where the valid
${VAR:-}
syntax would cause Compose to error outFixed a bug where
env_file
entries using an UTF-8 BOM were being read incorrectlyFixed a bug where missing secret files would generate an empty directory in their place
Fixed character encoding issues in the CLI's error handlers
Added validation for the
test
field in healthchecksAdded validation for the
subnet
field in IPAM configurationsAdded validation for
volumes
properties when using the long syntax in service definitionsThe CLI now explicit prevents using
-d
and--timeout
together indocker-compose up
1.17.0
(2017-11-01)
New features
Compose file version 3.4
Introduced version 3.4 of the
docker-compose.yml
specification. This version requires to be used with Docker Engine 17.06.0 or above.Added support for
cache_from
,network
andtarget
options in build configurationsAdded support for the
order
parameter in theupdate_config
sectionAdded support for setting a custom name in volume definitions using the
name
parameter
Compose file version 2.3
- Added support for
shm_size
option in build configuration
Compose file version 2.x
- Added support for extension fields (
x-*
). Also available for v3.4 files
All formats
- Added new
--no-start
to theup
command, allowing users to create all resources (networks, volumes, containers) without starting services. Thecreate
command is deprecated in favor of this new option
Bug fixes
Fixed a bug where
extra_hosts
values would be overridden by extension files instead of merging togetherFixed a bug where the validation for v3.2 files would prevent using the
consistency
field in service volume definitionsFixed a bug that would cause a crash when configuration fields expecting unique items would contain duplicates
Fixed a bug where mount overrides with a different mode would create a duplicate entry instead of overriding the original entry
Fixed a bug where build labels declared as a list wouldn't be properly parsed
Fixed a bug where the output of
docker-compose config
would be invalid for some versions if the file contained custom-named external volumesImproved error handling when issuing a build command on Windows using an unsupported file version
Fixed an issue where networks with identical names would sometimes be created when running
up
commands concurrently.
1.16.0
(2017-08-31)
New features
Compose file version 2.3
Introduced version 2.3 of the
docker-compose.yml
specification. This version requires to be used with Docker Engine 17.06.0 or above.Added support for the
target
parameter in build configurationsAdded support for the
start_period
parameter in healthcheck configurations
Compose file version 2.x
Added support for the
blkio_config
parameter in service definitionsAdded support for setting a custom name in volume definitions using the
name
parameter (not available for version 2.0)
All formats
- Added new CLI flag
--no-ansi
to suppress ANSI control characters in output
Bug fixes
Fixed a bug where nested
extends
instructions weren't resolved properly, causing ""file not found"" errorsFixed several issues with
.dockerignore
parsingFixed issues where logs of TTY-enabled services were being printed incorrectly and causing
MemoryError
exceptionsFixed a bug where printing application logs would sometimes be interrupted by a
UnicodeEncodeError
exception on Python 3The
$
character in the output ofdocker-compose config
is now properly escapedFixed a bug where running
docker-compose top
would sometimes fail with an uncaught exceptionFixed a bug where
docker-compose pull
with the--parallel
flag would return a0
exit code when failingFixed an issue where keys in
deploy.resources
were not being validatedFixed an issue where the
logging
options in the output ofdocker-compose config
would be set tonull
, an invalid valueFixed the output of the
docker-compose images
command when an image would come from a private repository using an explicit port numberFixed the output of
docker-compose config
when a port definition used0
as the value for the published port
1.15.0
(2017-07-26)
New features
Compose file version 2.2
- Added support for the
network
parameter in build configurations.
Compose file version 2.1 and up
The
pid
option in a service's definition now supports aservice:<name>
value.Added support for the
storage_opt
parameter in service definitions. This option is not available for the v3 format
All formats
Added
--quiet
flag todocker-compose pull
, suppressing progress outputSome improvements to CLI output
Bug fixes
Volumes specified through the
--volume
flag ofdocker-compose run
now complement volumes declared in the service's definition instead of replacing themFixed a bug where using multiple Compose files would unset the scale value defined inside the Compose file.
Fixed an issue where the
credHelpers
entries in theconfig.json
file were not being honored by ComposeFixed a bug where using multiple Compose files with port declarations would cause failures in Python 3 environments
Fixed a bug where some proxy-related options present in the user's environment would prevent Compose from running
Fixed an issue where the output of
docker-compose config
would be invalid if the original file usedY
orN
valuesFixed an issue preventing
up
operations on a previously created stack on Windows Engine.
1.14.0
(2017-06-19)
New features
Compose file version 3.3
- Introduced version 3.3 of the
docker-compose.yml
specification. This version requires to be used with Docker Engine 17.06.0 or above. Note: thecredential_spec
andconfigs
keys only apply to Swarm services and will be ignored by Compose
Compose file version 2.2
- Added the following parameters in service definitions:
cpu_count
,cpu_percent
,cpus
Compose file version 2.1
- Added support for build labels. This feature is also available in the 2.2 and 3.3 formats.
All formats
Added shorthand
-u
for--user
flag indocker-compose exec
Differences in labels between the Compose file and remote network will now print a warning instead of preventing redeployment.
Bug fixes
Fixed a bug where service's dependencies were being rescaled to their default scale when running a
docker-compose run
commandFixed a bug where
docker-compose rm
with the--stop
flag was not behaving properly when provided with a list of services to removeFixed a bug where
cache_from
in the build section would be ignored when using more than one Compose file.Fixed a bug that prevented binding the same port to different IPs when using more than one Compose file.
Fixed a bug where override files would not be picked up by Compose if they had the
.yaml
extensionFixed a bug on Windows Engine where networks would be incorrectly flagged for recreation
Fixed a bug where services declaring ports would cause crashes on some versions of Python 3
Fixed a bug where the output of
docker-compose config
would sometimes contain invalid port definitions
1.13.0
(2017-05-02)
Breaking changes
docker-compose up
now resets a service's scaling to its default value. You can use the newly introduced--scale
option to specify a custom scale value
New features
Compose file version 2.2
Introduced version 2.2 of the
docker-compose.yml
specification. This version requires to be used with Docker Engine 1.13.0 or aboveAdded support for
init
in service definitions.Added support for
scale
in service definitions. The configuration's value can be overridden using the--scale
flag indocker-compose up
. Thescale
command is disabled for this file format
Compose file version 2.x
- Added support for
options
in theipam
section of network definitions
Bug fixes
Fixed a bug where paths provided to compose via the
-f
option were not being resolved properlyFixed a bug where the
ext_ip::target_port
notation in the ports section was incorrectly marked as invalidFixed an issue where the
exec
command would sometimes not return control to the terminal when using the-d
flagFixed a bug where secrets were missing from the output of the
config
command for v3.2 filesFixed an issue where
docker-compose
would hang if no internet connection was availableFixed an issue where paths containing unicode characters passed via the
-f
flag were causing Compose to crashFixed an issue where the output of
docker-compose config
would be invalid if the Compose file contained external secretsFixed a bug where using
--exit-code-from
withup
would fail if Compose was installed in a Python 3 environmentFixed a bug where recreating containers using a combination of
tmpfs
andvolumes
would result in an invalid config state
1.12.0
(2017-04-04)
New features
Compose file version 3.2
Introduced version 3.2 of the
docker-compose.yml
specificationAdded support for
cache_from
in thebuild
section of servicesAdded support for the new expanded ports syntax in service definitions
Added support for the new expanded volumes syntax in service definitions
Compose file version 2.1
- Added support for
pids_limit
in service definitions
Compose file version 2.0 and up
Added
--volumes
option todocker-compose config
that lists named volumes declared for that projectAdded support for
mem_reservation
in service definitions (2.x only)Added support for
dns_opt
in service definitions (2.x only)
All formats
Added a new
docker-compose images
command that lists images used by the current project's containersAdded a
--stop
(shorthand-s
) option todocker-compose rm
that stops the running containers before removing themAdded a
--resolve-image-digests
option todocker-compose config
that pins the image version for each service to a permanent digestAdded a
--exit-code-from SERVICE
option todocker-compose up
. When used,docker-compose
will exit on any container's exit with the code corresponding to the specified service's exit codeAdded a
--parallel
option todocker-compose pull
that enables images for multiple services to be pulled simultaneouslyAdded a
--build-arg
option todocker-compose build
Added a
--volume <volume_mapping>
(shorthand-v
) option todocker-compose run
to declare runtime volumes to be mountedAdded a
--project-directory PATH
option todocker-compose
that will affect path resolution for the projectWhen using
--abort-on-container-exit
indocker-compose up
, the exit code for the container that caused the abort will be the exit code of thedocker-compose up
commandUsers can now configure which path separator character they want to use to separate the
COMPOSE_FILE
environment value using theCOMPOSE_PATH_SEPARATOR
environment variableAdded support for port range to a single port in port mappings, such as
8000-8010:80
.
Bug fixes
docker-compose run --rm
now removes anonymous volumes after execution, matching the behavior ofdocker run --rm
.Fixed a bug where override files containing port lists would cause a TypeError to be raised
Fixed a bug where the
deploy
key would be missing from the output ofdocker-compose config
Fixed a bug where scaling services up or down would sometimes re-use obsolete containers
Fixed a bug where the output of
docker-compose config
would be invalid if the project declared anonymous volumesVariable interpolation now properly occurs in the
secrets
section of the Compose fileThe
secrets
section now properly appears in the output ofdocker-compose config
Fixed a bug where changes to some networks properties would not be detected against previously created networks
Fixed a bug where
docker-compose
would crash when trying to write into a closed pipeFixed an issue where Compose would not pick up on the value of COMPOSE_TLS_VERSION when used in combination with command-line TLS flags
1.11.2
(2017-02-17)
Bug fixes
Fixed a bug that was preventing secrets configuration from being loaded properly
Fixed a bug where the
docker-compose config
command would fail if the config file contained secrets definitionsFixed an issue where Compose on some linux distributions would pick up and load an outdated version of the requests library
Fixed an issue where socket-type files inside a build folder would cause
docker-compose
to crash when trying to build that serviceFixed an issue where recursive wildcard patterns
**
were not being recognized in.dockerignore
files.
1.11.1
(2017-02-09)
Bug fixes
- Fixed a bug where the 3.1 file format was not being recognized as valid by the Compose parser
1.11.0
(2017-02-08)
New Features
Compose file version 3.1
- Introduced version 3.1 of the
docker-compose.yml
specification. This version requires Docker Engine 1.13.0 or above. It introduces support for secrets. See the documentation for more information
Compose file version 2.0 and up
- Introduced the
docker-compose top
command that displays processes running for the different services managed by Compose.
Bug fixes
Fixed a bug where extending a service defining a healthcheck dictionary would cause
docker-compose
to error out.Fixed an issue where the
pid
entry in a service definition was being ignored when using multiple Compose files.
1.10.1
(2017-02-01)
Bug fixes
Fixed an issue where the presence of older versions of the docker-py package would cause unexpected crashes while running Compose
Fixed an issue where healthcheck dependencies would be lost when using multiple compose files for a project
Fixed a few issues that made the output of the
config
command invalidFixed an issue where adding volume labels to v3 Compose files would result in an error
Fixed an issue on Windows where build context paths containing unicode characters were being improperly encoded
Fixed a bug where Compose would occasionally crash while streaming logs when containers would stop or restart
1.10.0
(2017-01-18)
New Features
Compose file version 3.0
- Introduced version 3.0 of the
docker-compose.yml
specification. This version requires to be used with Docker Engine 1.13 or above and is specifically designed to work with thedocker stack
commands.
Compose file version 2.1 and up
Healthcheck configuration can now be done in the service definition using the
healthcheck
parameterContainers dependencies can now be set up to wait on positive healthchecks when declared using
depends_on
. See the documentation for the updated syntax.Note
This feature will not be ported to version 3 Compose files.
Added support for the
sysctls
parameter in service definitionsAdded support for the
userns_mode
parameter in service definitionsCompose now adds identifying labels to networks and volumes it creates
Compose file version 2.0 and up
- Added support for the
stop_grace_period
option in service definitions.
Bug fixes
Colored output now works properly on Windows.
Fixed a bug where docker-compose run would fail to set up link aliases in interactive mode on Windows.
Networks created by Compose are now always made attachable (Compose files v2.1 and up).
Fixed a bug where falsy values of
COMPOSE_CONVERT_WINDOWS_PATHS
(0
,false
, empty value) were being interpreted as true.Fixed a bug where forward slashes in some .dockerignore patterns weren't being parsed correctly on Windows
1.9.0
(2016-11-16)
Breaking changes
- When using Compose with Docker Toolbox/Machine on Windows, volume paths are
no longer converted from
C:\Users
to/c/Users
-style by default. To re-enable this conversion so that your volumes keep working, set the environment variableCOMPOSE_CONVERT_WINDOWS_PATHS=1
. Users of Docker for Windows are not affected and do not need to set the variable.
New Features
Interactive mode for
docker-compose run
anddocker-compose exec
is now supported on Windows platforms. Thedocker
binary is required to be present on the system for this feature to work.Introduced version 2.1 of the
docker-compose.yml
specification. This version requires to be used with Docker Engine 1.12 or above.- Added support for setting volume labels and network labels in
docker-compose.yml
. - Added support for the
isolation
parameter in service definitions. - Added support for link-local IPs in the service networks definitions.
- Added support for shell-style inline defaults in variable interpolation.
The supported forms are
${FOO-default}
(fall back if FOO is unset) and${FOO:-default}
(fall back if FOO is unset or empty).
- Added support for setting volume labels and network labels in
Added support for the
group_add
andoom_score_adj
parameters in service definitions.Added support for the
internal
andenable_ipv6
parameters in network definitions.Compose now defaults to using the
npipe
protocol on Windows.Overriding a
logging
configuration will now properly merge theoptions
mappings if thedriver
values do not conflict.
Bug fixes
Fixed several bugs related to
npipe
protocol support on Windows.Fixed an issue with Windows paths being incorrectly converted when using Docker on Windows Server.
Fixed a bug where an empty
restart
value would sometimes result in an exception being raised.Fixed an issue where service logs containing unicode characters would sometimes cause an error to occur.
Fixed a bug where unicode values in environment variables would sometimes raise a unicode exception when retrieved.
Fixed an issue where Compose would incorrectly detect a configuration mismatch for overlay networks.
1.8.1
(2016-09-22)
Bug fixes
Fixed a bug where users using a credentials store were not able to access their private images.
Fixed a bug where users using identity tokens to authenticate were not able to access their private images.
Fixed a bug where an
HttpHeaders
entry in the docker configuration file would cause Compose to crash when trying to build an image.Fixed a few bugs related to the handling of Windows paths in volume binding declarations.
Fixed a bug where Compose would sometimes crash while trying to read a streaming response from the engine.
Fixed an issue where Compose would crash when encountering an API error while streaming container logs.
Fixed an issue where Compose would erroneously try to output logs from drivers not handled by the Engine's API.
Fixed a bug where options from the
docker-machine config
command would not be properly interpreted by Compose.Fixed a bug where the connection to the Docker Engine would sometimes fail when running a large number of services simultaneously.
Fixed an issue where Compose would sometimes print a misleading suggestion message when running the
bundle
command.Fixed a bug where connection errors would not be handled properly by Compose during the project initialization phase.
Fixed a bug where a misleading error would appear when encountering a connection timeout.
1.8.0
(2016-06-14)
Breaking Changes
As announced in 1.7.0,
docker-compose rm
now removes containers created bydocker-compose run
by default.Setting
entrypoint
on a service now empties out any default command that was set on the image (i.e. anyCMD
instruction in the Dockerfile used to build it). This makes it consistent with the--entrypoint
flag todocker run
.
New Features
Added
docker-compose bundle
, a command that builds a bundle file to be consumed by the new Docker Stack commands in Docker 1.12.Added
docker-compose push
, a command that pushes service images to a registry.Compose now supports specifying a custom TLS version for interaction with the Docker Engine using the
COMPOSE_TLS_VERSION
environment variable.
Bug fixes
Fixed a bug where Compose would erroneously try to read
.env
at the project's root when it is a directory.docker-compose run -e VAR
now passesVAR
through from the shell to the container, as withdocker run -e VAR
.Improved config merging when multiple compose files are involved for several service sub-keys.
Fixed a bug where volume mappings containing Windows drives would sometimes be parsed incorrectly.
Fixed a bug in Windows environment where volume mappings of the host's root directory would be parsed incorrectly.
Fixed a bug where
docker-compose config
would output an invalid Compose file if external networks were specified.Fixed an issue where unset buildargs would be assigned a string containing
'None'
instead of the expected empty value.Fixed a bug where yes/no prompts on Windows would not show before receiving input.
Fixed a bug where trying to
docker-compose exec
on Windows without the-d
option would exit with a stacktrace. This will still fail for the time being, but should do so gracefully.Fixed a bug where errors during
docker-compose up
would show an unrelated stacktrace at the end of the process.docker-compose create
anddocker-compose start
show more descriptive error messages when something goes wrong.
1.7.1
(2016-05-04)
Bug fixes
Fixed a bug where the output of
docker-compose config
for v1 files would be an invalid configuration file.Fixed a bug where
docker-compose config
would not check the validity of links.Fixed an issue where
docker-compose help
would not output a list of available commands and generic options as expected.Fixed an issue where filtering by service when using
docker-compose logs
would not apply for newly created services.Fixed a bug where unchanged services would sometimes be recreated in in the up phase when using Compose with Python 3.
Fixed an issue where API errors encountered during the up phase would not be recognized as a failure state by Compose.
Fixed a bug where Compose would raise a NameError because of an undefined exception name on non-Windows platforms.
Fixed a bug where the wrong version of
docker-py
would sometimes be installed alongside Compose.Fixed a bug where the host value output by
docker-machine config default
would not be recognized as valid options by thedocker-compose
command line.Fixed an issue where Compose would sometimes exit unexpectedly while reading events broadcasted by a Swarm cluster.
Corrected a statement in the docs about the location of the
.env
file, which is indeed read from the current directory, instead of in the same location as the Compose file.
1.7.0
(2016-04-13)
Breaking Changes
docker-compose logs
no longer follows log output by default. It now matches the behavior ofdocker logs
and exits after the current logs are printed. Use-f
to get the old default behavior.Booleans are no longer allows as values for mappings in the Compose file (for keys
environment
,labels
andextra_hosts
). Previously this was a warning. Boolean values should be quoted so they become string values.
New Features
Compose now looks for a
.env
file in the directory where it's run and reads any environment variables defined inside, if they're not already set in the shell environment. This lets you easily set defaults for variables used in the Compose file, or for any of theCOMPOSE_*
orDOCKER_*
variables.Added a
--remove-orphans
flag to bothdocker-compose up
anddocker-compose down
to remove containers for services that were removed from the Compose file.Added a
--all
flag todocker-compose rm
to include containers created bydocker-compose run
. This will become the default behavior in the next version of Compose.Added support for all the same TLS configuration flags used by the
docker
client:--tls
,--tlscert
,--tlskey
, etc.Compose files now support the
tmpfs
andshm_size
options.Added the
--workdir
flag todocker-compose run
docker-compose logs
now shows logs for new containers that are created after it starts.The
COMPOSE_FILE
environment variable can now contain multiple files, separated by the host system's standard path separator (:
on Mac/Linux,;
on Windows).You can now specify a static IP address when connecting a service to a network with the
ipv4_address
andipv6_address
options.Added
--follow
,--timestamp
, and--tail
flags to thedocker-compose logs
command.docker-compose up
, anddocker-compose start
will now start containers in parallel where possible.docker-compose stop
now stops containers in reverse dependency order instead of all at once.Added the
--build
flag todocker-compose up
to force it to build a new image. It now shows a warning if an image is automatically built when the flag is not used.Added the
docker-compose exec
command for executing a process in a running container.
Bug fixes
docker-compose down
now removes containers created bydocker-compose run
.A more appropriate error is shown when a timeout is hit during
up
when using a tty.Fixed a bug in
docker-compose down
where it would abort if some resources had already been removed.Fixed a bug where changes to network aliases would not trigger a service to be recreated.
Fix a bug where a log message was printed about creating a new volume when it already existed.
Fixed a bug where interrupting
up
would not always shut down containers.Fixed a bug where
log_opt
andlog_driver
were not properly carried over when extending services in the v1 Compose file format.Fixed a bug where empty values for build args would cause file validation to fail.
1.6.2
(2016-02-23)
- Fixed a bug where connecting to a TLS-enabled Docker Engine would fail with a certificate verification error.
1.6.1
(2016-02-23)
Bug fixes
Fixed a bug where recreating a container multiple times would cause the new container to be started without the previous volumes.
Fixed a bug where Compose would set the value of unset environment variables to an empty string, instead of a key without a value.
Provide a better error message when Compose requires a more recent version of the Docker API.
Add a missing config field
network.aliases
which allows setting a network scoped alias for a service.Fixed a bug where
run
would not start services listed independs_on
.Fixed a bug where
networks
andnetwork_mode
where not merged when using extends or multiple Compose files.Fixed a bug with service aliases where the short container id alias was only contained 10 characters, instead of the 12 characters used in previous versions.
Added a missing log message when creating a new named volume.
Fixed a bug where
build.args
was not merged when usingextends
or multiple Compose files.Fixed some bugs with config validation when null values or incorrect types were used instead of a mapping.
Fixed a bug where a
build
section without acontext
would show a stack trace instead of a helpful validation message.Improved compatibility with swarm by only setting a container affinity to the previous instance of a services' container when the service uses an anonymous container volume. Previously the affinity was always set on all containers.
Fixed the validation of some
driver_opts
would cause an error if a number was used instead of a string.Some improvements to the
run.sh
script used by the Compose container install option.Fixed a bug with
up --abort-on-container-exit
where Compose would exit, but would not stop other containers.Corrected the warning message that is printed when a boolean value is used as a value in a mapping.
1.6.0
(2016-01-15)
Major Features
Compose 1.6 introduces a new format for
docker-compose.yml
which lets you define networks and volumes in the Compose file as well as services. It also makes a few changes to the structure of some configuration options.You don't have to use it - your existing Compose files will run on Compose 1.6 exactly as they do today.
Check the upgrade guide for full details.
Support for networking has exited experimental status and is the recommended way to enable communication between containers.
If you use the new file format, your app will use networking. If you aren't ready yet, just leave your Compose file as it is and it'll continue to work just the same.
By default, you don't have to configure any networks. In fact, using networking with Compose involves even less configuration than using links. Consult the networking guide for how to use it.
The experimental flags
--x-networking
and--x-network-driver
, introduced in Compose 1.5, have been removed.You can now pass arguments to a build if you're using the new file format:
build: context: . args: buildno: 1
You can now specify both a
build
and animage
key if you're using the new file format.docker-compose build
will build the image and tag it with the name you've specified, whiledocker-compose pull
will attempt to pull it.There's a new
events
command for monitoring container events from the application, much likedocker events
. This is a good primitive for building tools on top of Compose for performing actions when particular things happen, such as containers starting and stopping.There's a new
depends_on
option for specifying dependencies between services. This enforces the order of startup, and ensures that when you rundocker-compose up SERVICE
on a service with dependencies, those are started as well.
New Features
Added a new command
config
which validates and prints the Compose configuration after interpolating variables, resolving relative paths, and merging multiple files andextends
.Added a new command
create
for creating containers without starting them.Added a new command
down
to stop and remove all the resources created byup
in a single command.Added support for the
cpu_quota
configuration option.Added support for the
stop_signal
configuration option.Commands
start
,restart
,pause
, andunpause
now exit with an error status code if no containers were modified.Added a new
--abort-on-container-exit
flag toup
which causesup
to stop all container and exit once the first container exits.Removed support for
FIG_FILE
,FIG_PROJECT_NAME
, and no longer readsfig.yml
as a default Compose file location.Removed the
migrate-to-labels
command.Removed the
--allow-insecure-ssl
flag.
Bug fixes
Fixed a validation bug that prevented the use of a range of ports in the
expose
field.Fixed a validation bug that prevented the use of arrays in the
entrypoint
field if they contained duplicate entries.Fixed a bug that caused
ulimits
to be ignored when used withextends
.Fixed a bug that prevented ipv6 addresses in
extra_hosts
.Fixed a bug that caused
extends
to be ignored when included from multiple Compose files.Fixed an incorrect warning when a container volume was defined in the Compose file.
Fixed a bug that prevented the force shutdown behavior of
up
andlogs
.Fixed a bug that caused
None
to be printed as the network driver name when the default network driver was used.Fixed a bug where using the string form of
dns
ordns_search
would cause an error.Fixed a bug where a container would be reported as ""Up"" when it was in the restarting state.
Fixed a confusing error message when DOCKER_CERT_PATH was not set properly.
Fixed a bug where attaching to a container would fail if it was using a non-standard logging driver (or none at all).
1.5.2
(2015-12-03)
Fixed a bug which broke the use of
environment
andenv_file
withextends
, and caused environment keys without values to have aNone
value, instead of a value from the host environment.Fixed a regression in 1.5.1 that caused a warning about volumes to be raised incorrectly when containers were recreated.
Fixed a bug which prevented building a
Dockerfile
that usedADD <url>
Fixed a bug with
docker-compose restart
which prevented it from starting stopped containers.Fixed handling of SIGTERM and SIGINT to properly stop containers
Add support for using a url as the value of
build
Improved the validation of the
expose
option
1.5.1
(2015-11-12)
Add the
--force-rm
option tobuild
.Add the
ulimit
option for services in the Compose file.Fixed a bug where
up
would error with ""service needs to be built"" if a service changed from usingimage
to usingbuild
.Fixed a bug that would cause incorrect output of parallel operations on some terminals.
Fixed a bug that prevented a container from being recreated when the mode of a
volumes_from
was changed.Fixed a regression in 1.5.0 where non-utf-8 unicode characters would cause
up
orlogs
to crash.Fixed a regression in 1.5.0 where Compose would use a success exit status code when a command fails due to an HTTP timeout communicating with the docker daemon.
Fixed a regression in 1.5.0 where
name
was being accepted as a valid service option which would override the actual name of the service.When using
--x-networking
Compose no longer sets the hostname to the container name.When using
--x-networking
Compose will only create the default network if at least one container is using the network.When printings logs during
up
orlogs
, flush the output buffer after each line to prevent buffering issues from hiding logs.Recreate a container if one of its dependencies is being created. Previously a container was only recreated if it's dependencies already existed, but were being recreated as well.
Add a warning when a
volume
in the Compose file is being ignored and masked by a container volume from a previous container.Improve the output of
pull
when run without a tty.When using multiple Compose files, validate each before attempting to merge them together. Previously invalid files would result in not helpful errors.
Allow dashes in keys in the
environment
service option.Improve validation error messages by including the filename as part of the error message.
1.5.0
(2015-11-03)
Breaking changes
With the introduction of variable substitution support in the Compose file, any
Compose file that uses an environment variable ($VAR
or ${VAR}
) in the command:
or entrypoint:
field will break.
Previously these values were interpolated inside the container, with a value from the container environment. In Compose 1.5.0, the values will be interpolated on the host, with a value from the host environment.
To migrate a Compose file to 1.5.0, escape the variables with an extra $
(ex: $$VAR
or $${VAR}
). See
https://github.com/docker/compose/blob/8cc8e61/docs/compose-file.md#variable-substitution
Major features
Compose is now available for Windows.
Environment variables can be used in the Compose file. See https://github.com/docker/compose/blob/8cc8e61/docs/compose-file.md#variable-substitution
Multiple compose files can be specified, allowing you to override settings in the default Compose file. See https://github.com/docker/compose/blob/8cc8e61/docs/reference/docker-compose.md for more details.
Compose now produces better error messages when a file contains invalid configuration.
up
now waits for all services to exit before shutting down, rather than shutting down as soon as one container exits.Experimental support for the new docker networking system can be enabled with the
--x-networking
flag. Read more here: https://github.com/docker/docker/blob/8fee1c20/docs/userguide/dockernetworks.md
New features
You can now optionally pass a mode to
volumes_from
. For example,volumes_from: [""servicename:ro""]
.Since Docker now lets you create volumes with names, you can refer to those volumes by name in
docker-compose.yml
. For example,volumes: [""mydatavolume:/data""]
will mount the volume namedmydatavolume
at the path/data
inside the container.If the first component of an entry in
volumes
starts with a.
,/
or~
, it is treated as a path and expansion of relative paths is performed as necessary. Otherwise, it is treated as a volume name and passed straight through to Docker.Read more on named volumes and volume drivers here: https://github.com/docker/docker/blob/244d9c33/docs/userguide/dockervolumes.md
docker-compose build --pull
instructs Compose to pull the base image for each Dockerfile before building.docker-compose pull --ignore-pull-failures
instructs Compose to continue if it fails to pull a single service's image, rather than aborting.You can now specify an IPC namespace in
docker-compose.yml
with theipc
option.Containers created by
docker-compose run
can now be named with the--name
flag.If you install Compose with pip or use it as a library, it now works with Python 3.
image
now supports image digests (in addition to ids and tags). For example,image: ""busybox@sha256:38a203e1986cf79639cfb9b2e1d6e773de84002feea2d4eb006b52004ee8502d""
ports
now supports ranges of ports. For example,ports: - ""3000-3005"" - ""9000-9001:8000-8001""
docker-compose run
now supports a-p|--publish
parameter, much likedocker run -p
, for publishing specific ports to the host.docker-compose pause
anddocker-compose unpause
have been implemented, analogous todocker pause
anddocker unpause
.When using
extends
to copy configuration from another service in the same Compose file, you can omit thefile
option.Compose can be installed and run as a Docker image. This is an experimental feature.
Bug fixes
All values for the
log_driver
option which are supported by the Docker daemon are now supported by Compose.docker-compose build
can now be run successfully against a Swarm cluster.
1.4.2
(2015-09-22)
- Fixed a regression in the 1.4.1 release that would cause
docker-compose up
without the-d
option to exit immediately.
1.4.1
(2015-09-10)
Bug fixes
- Some configuration changes (notably changes to
links
,volumes_from
, andnet
) were not properly triggering a container recreate as part ofdocker-compose up
. docker-compose up <service>
was showing logs for all services instead of just the specified services.- Containers with custom container names were showing up in logs as
service_number
instead of their custom container name. - When scaling a service sometimes containers would be recreated even when the configuration had not changed.
1.4.0
(2015-08-04)
By default,
docker-compose up
now only recreates containers for services whose configuration has changed since they were created. This should result in a dramatic speed-up for many applications.The experimental
--x-smart-recreate
flag which introduced this feature in Compose 1.3.0 has been removed, and a--force-recreate
flag has been added for when you want to recreate everything.Several of Compose's commands -
scale
,stop
,kill
andrm
- now perform actions on multiple containers in parallel, rather than in sequence, which will run much faster on larger applications.You can now specify a custom name for a service's container with
container_name
. Because Docker container names must be unique, this means you can't scale the service beyond one container.You no longer have to specify a
file
option when usingextends
- it will default to the current file.Service names can now contain dots, dashes and underscores.
Compose can now read YAML configuration from standard input, rather than from a file, by specifying
-
as the filename. This makes it easier to generate configuration dynamically:$ echo 'redis: {""image"": ""redis""}' | docker-compose --file - up
There's a new
docker-compose version
command which prints extended information about Compose's bundled dependencies.docker-compose.yml
now supportslog_opt
as well aslog_driver
, allowing you to pass extra configuration to a service's logging driver.docker-compose.yml
now supportsmemswap_limit
, similar todocker run --memory-swap
.When mounting volumes with the
volumes
option, you can now pass in any mode supported by the daemon, not just:ro
or:rw
. For example, SELinux users can pass:z
or:Z
.You can now specify a custom volume driver with the
volume_driver
option indocker-compose.yml
, much likedocker run --volume-driver
.A bug has been fixed where Compose would fail to pull images from private registries serving plain (unsecured) HTTP. The
--allow-insecure-ssl
flag, which was previously used to work around this issue, has been deprecated and now has no effect.A bug has been fixed where
docker-compose build
would fail if the build depended on a private Hub image or an image from a private registry.A bug has been fixed where Compose would crash if there were containers which the Docker daemon had not finished removing.
Two bugs have been fixed where Compose would sometimes fail with a ""Duplicate bind mount"" error, or fail to attach volumes to a container, if there was a volume path specified in
docker-compose.yml
with a trailing slash.
Thanks @mnowster, @dnephin, @ekristen, @funkyfuture, @jeffk and @lukemarsden!
1.3.3
(2015-07-15)
Regression fixes
- When stopping containers gracefully, Compose was setting the timeout to 0, effectively forcing a SIGKILL every time.
- Compose would sometimes crash depending on the formatting of container data returned from the Docker API.
1.3.2
(2015-07-14)
Bug fixes
- When there were one-off containers created by running
docker-compose run
on an older version of Compose,docker-compose run
would fail with a name collision. Compose now shows an error if you have leftover containers of this type lying around, and tells you how to remove them. - Compose was not reading Docker authentication config files created in the new location,
~/docker/config.json
, and authentication against private registries would therefore fail. - When a container had a pseudo-TTY attached, its output in
docker-compose up
would be truncated. docker-compose up --x-smart-recreate
would sometimes fail when an image tag was updated.docker-compose up
would sometimes create two containers with the same numeric suffix.docker-compose rm
anddocker-compose ps
would sometimes list services that aren't part of the current project (though no containers were erroneously removed).- Some
docker-compose
commands would not show an error if invalid service names were passed in.
Thanks @dano, @josephpage, @kevinsimper, @lieryan, @phemmer, @soulrebel and @sschepens!
1.3.1
(2015-06-21)
Bug fixes
docker-compose build
would always attempt to pull the base image before building.docker-compose help migrate-to-labels
failed with an error.- If no network mode was specified, Compose would set it to ""bridge"", rather than allowing the Docker daemon to use its configured default network mode.
1.3.0
(2015-06-18)
Important notes
This release contains breaking changes, and you will need to either remove or migrate your existing containers before running your app - see the upgrading section of the install docs for details.
Compose now requires Docker 1.6.0 or later.
Improvements
Compose now uses container labels, rather than names, to keep track of containers. This makes Compose both faster and easier to integrate with your own tools.
Compose no longer uses ""intermediate containers"" when recreating containers for a service. This makes
docker-compose up
less complex and more resilient to failure.
New features
docker-compose up
has an experimental new behavior: it will only recreate containers for services whose configuration has changed indocker-compose.yml
. This will eventually become the default, but for now you can take it for a spin:$ docker-compose up --x-smart-recreate
When invoked in a subdirectory of a project,
docker-compose
will now climb up through parent directories until it finds adocker-compose.yml
.
Several new configuration keys have been added to docker-compose.yml
:
dockerfile
, likedocker build --file
, lets you specify an alternate Dockerfile to use withbuild
.labels
, likedocker run --labels
, lets you add custom metadata to containers.extra_hosts
, likedocker run --add-host
, lets you add entries to a container's/etc/hosts
file.pid: host
, likedocker run --pid=host
, lets you reuse the same PID namespace as the host machine.cpuset
, likedocker run --cpuset-cpus
, lets you specify which CPUs to allow execution in.read_only
, likedocker run --read-only
, lets you mount a container's filesystem as read-only.security_opt
, likedocker run --security-opt
, lets you specify security options.log_driver
, likedocker run --log-driver
, lets you specify a log driver.
Bug fixes
- The output of
docker-compose run
was sometimes truncated, especially when running under Jenkins. - A service's volumes would sometimes not update after volume configuration was changed in
docker-compose.yml
. - Authenticating against third-party registries would sometimes fail.
docker-compose run --rm
would fail to remove the container if the service had arestart
policy in place.docker-compose scale
would refuse to scale a service beyond 1 container if it exposed a specific port number on the host.- Compose would refuse to create multiple volume entries with the same host path.
Thanks @ahromis, @albers, @aleksandr-vin, @antoineco, @ccverak, @chernjie, @dnephin, @edmorley, @fordhurley, @josephpage, @KyleJamesWalker, @lsowen, @mchasal, @noironetworks, @sdake, @sdurrheimer, @sherter, @stephenlawrence, @thaJeztah, @thieman, @turtlemonvh, @twhiteman, @vdemeester, @xuxinkun and @zwily!
1.2.0
(2015-04-16)
docker-compose.yml
now supports anextends
option, which enables a service to inherit configuration from another service in another configuration file. This is really good for sharing common configuration between apps, or for configuring the same app for different environments. Here's the documentation.When using Compose with a Swarm cluster, containers that depend on one another will be co-scheduled on the same node. This means that most Compose apps will now work out of the box, as long as they don't use
build
.Repeated invocations of
docker-compose up
when using Compose with a Swarm cluster now work reliably.Directories passed to
build
, filenames passed toenv_file
and volume host paths passed tovolumes
are now treated as relative to the directory of the configuration file, not the directory thatdocker-compose
is being run in. In the majority of cases, those are the same, but if you use the-f|--file
argument to specify a configuration file in another directory, this is a breaking change.A service can now share another service's network namespace with
net: container:<service>
.volumes_from
andnet: container:<service>
entries are taken into account when resolving dependencies, sodocker-compose up <service>
will correctly start all dependencies of<service>
.docker-compose run
now accepts a--user
argument to specify a user to run the command as, just likedocker run
.The
up
,stop
andrestart
commands now accept a--timeout
(or-t
) argument to specify how long to wait when attempting to gracefully stop containers, just likedocker stop
.docker-compose rm
now accepts-f
as a shorthand for--force
, just likedocker rm
.
Thanks, @abesto, @albers, @alunduil, @dnephin, @funkyfuture, @gilclark, @IanVS, @KingsleyKelly, @knutwalker, @thaJeztah and @vmalloc!
1.1.0
(2015-02-25)
Fig has been renamed to Docker Compose, or just Compose for short. This has several implications for you:
- The command you type is now
docker-compose
, notfig
. - You should rename your fig.yml to docker-compose.yml.
- If you’re installing via PyPI, the package is now
docker-compose
, so install it withpip install docker-compose
.
Besides that, there’s a lot of new stuff in this release:
We’ve made a few small changes to ensure that Compose will work with Swarm, Docker’s new clustering tool ( https://github.com/docker/swarm). Eventually you'll be able to point Compose at a Swarm cluster instead of a standalone Docker host and it’ll run your containers on the cluster with no extra work from you. As Swarm is still developing, integration is rough and lots of Compose features don't work yet.
docker-compose run
now has a--service-ports
flag for exposing ports on the given service. This is useful for running your webapp with an interactive debugger, for example.You can now link to containers outside your app with the
external_links
option in docker-compose.yml.You can now prevent
docker-compose up
from automatically building images with the--no-build
option. This will make fewer API calls and run faster.If you don’t specify a tag when using the
image
key, Compose will default to thelatest
tag, rather than pulling all tags.docker-compose kill
now supports the-s
flag, allowing you to specify the exact signal you want to send to a service’s containers.docker-compose.yml now has an
env_file
key, analogous todocker run --env-file
, letting you specify multiple environment variables in a separate file. This is great if you have a lot of them, or if you want to keep sensitive information out of version control.docker-compose.yml now supports the
dns_search
,cap_add
,cap_drop
,cpu_shares
andrestart
options, analogous todocker run
’s--dns-search
,--cap-add
,--cap-drop
,--cpu-shares
and--restart
options.Compose now ships with Bash tab completion - see the installation and usage docs at https://github.com/docker/compose/blob/1.1.0/docs/completion.md
A number of bugs have been fixed - see the milestone for details: https://github.com/docker/compose/issues?q=milestone%3A1.1.0+
Thanks @dnephin, @squebe, @jbalonso, @raulcd, @benlangfield, @albers, @ggtools, @bersace, @dtenenba, @petercv, @drewkett, @TFenby, @paulRbr, @Aigeruth and @salehe!
1.0.1
(2014-11-04)
- Added an
--allow-insecure-ssl
option to allowfig up
,fig run
andfig pull
to pull from insecure registries. - Fixed
fig run
not showing output in Jenkins. - Fixed a bug where Fig couldn't build Dockerfiles with ADD statements pointing at URLs.
1.0.0
(2014-10-16)
The highlights:
Fig has joined Docker. Fig will continue to be maintained, but we'll also be incorporating the best bits of Fig into Docker itself.
This means the GitHub repository has moved to https://github.com/docker/fig and our IRC channel is now #docker-fig on Freenode.
Fig can be used with the official Docker OS X installer. Boot2Docker will mount the home directory from your host machine so volumes work as expected.
Fig supports Docker 1.3.
It is now possible to connect to the Docker daemon using TLS by using the
DOCKER_CERT_PATH
andDOCKER_TLS_VERIFY
environment variables.There is a new
fig port
command which outputs the host port binding of a service, in a similar way todocker port
.There is a new
fig pull
command which pulls the latest images for a service.There is a new
fig restart
command which restarts a service's containers.Fig creates multiple containers in service by appending a number to the service name. For example,
db_1
,db_2
. As a convenience, Fig will now give the first container an alias of the service name. For example,db
.This link alias is also a valid hostname and added to
/etc/hosts
so you can connect to linked services using their hostname. For example, instead of resolving the environment variablesDB_PORT_5432_TCP_ADDR
andDB_PORT_5432_TCP_PORT
, you could just use the hostnamedb
and port5432
directly.Volume definitions now support
ro
mode, expanding~
and expanding environment variables..dockerignore
is supported when building.The project name can be set with the
FIG_PROJECT_NAME
environment variable.The
--env
and--entrypoint
options have been added tofig run
.The Fig binary for Linux is now linked against an older version of glibc so it works on CentOS 6 and Debian Wheezy.
Other things:
fig ps
now works on Jenkins and makes fewer API calls to the Docker daemon.--verbose
displays more useful debugging output.- When starting a service where
volumes_from
points to a service without any containers running, that service will now be started. - Lots of docs improvements. Notably, environment variables are documented and official repositories are used throughout.
Thanks @dnephin, @d11wtq, @marksteve, @rubbish, @jbalonso, @timfreund, @alunduil, @mieciu, @shuron, @moss, @suzaku and @chmouel! Whew.
0.5.2
(2014-07-28)
- Added a
--no-cache
option tofig build
, which bypasses the cache just likedocker build --no-cache
. - Fixed the
dns:
fig.yml option, which was causing fig to error out. - Fixed a bug where fig couldn't start under Python 2.6.
- Fixed a log-streaming bug that occasionally caused fig to exit.
Thanks @dnephin and @marksteve!
0.5.1
(2014-07-11)
- If a service has a command defined,
fig run [service]
with no further arguments will run it. - The project name now defaults to the directory containing fig.yml, not the current working directory (if they're different)
volumes_from
now works properly with containers as well as services- Fixed a race condition when recreating containers in
fig up
Thanks @ryanbrainard and @d11wtq!
0.5.0
(2014-07-11)
Fig now starts links when you run
fig run
orfig up
.For example, if you have a
web
service which depends on adb
service,fig run web ...
will start thedb
service.Environment variables can now be resolved from the environment that Fig is running in. Just specify it as a blank variable in your
fig.yml
and, if set, it'll be resolved:environment: RACK_ENV: development SESSION_SECRET:
volumes_from
is now supported infig.yml
. All of the volumes from the specified services and containers will be mounted:volumes_from: - service_name - container_name
A host address can now be specified in
ports
:ports: - ""0.0.0.0:8000:8000"" - ""127.0.0.1:8001:8001""
The
net
andworkdir
options are now supported infig.yml
.The
hostname
option now works in the same way as the Docker CLI, splitting out into adomainname
option.TTY behavior is far more robust, and resizes are supported correctly.
Load YAML files safely.
Thanks to @d11wtq, @ryanbrainard, @rail44, @j0hnsmith, @binarin, @Elemecca, @mozz100 and @marksteve for their help with this release!
0.4.2
(2014-06-18)
- Fix various encoding errors when using
fig run
,fig up
andfig build
.
0.4.1
(2014-05-08)
- Add support for Docker 0.11.0. (Thanks @marksteve!)
- Make project name configurable. (Thanks @jefmathiot!)
- Return correct exit code from
fig run
.
0.4.0
(2014-04-29)
- Support Docker 0.9 and 0.10
- Display progress bars correctly when pulling images (no more ski slopes)
fig up
now stops all services when any container exits- Added support for the
privileged
config option in fig.yml (thanks @kvz!) - Shortened and aligned log prefixes in
fig up
output - Only containers started with
fig run
link back to their own service - Handle UTF-8 correctly when streaming
fig build/run/up
output (thanks @mauvm and @shanejonas!) - Error message improvements
0.3.2
(2014-03-05)
- Added an
--rm
option tofig run
. (Thanks @marksteve!) - Added an
expose
option tofig.yml
.
0.3.1
(2014-03-04)
- Added contribution instructions. (Thanks @kvz!)
- Fixed
fig rm
throwing an error. - Fixed a bug in
fig ps
on Docker 0.8.1 when there is a container with no command.
0.3.0
(2014-03-03)
- We now ship binaries for OS X and Linux. No more having to install with Pip!
- Add
-f
flag to specify alternatefig.yml
files - Add support for custom link names
- Fix a bug where recreating would sometimes hang
- Update docker-py to support Docker 0.8.0.
- Various documentation improvements
- Various error message improvements
Thanks @marksteve, @Gazler and @teozkr!
0.2.2
(2014-02-17)
- Resolve dependencies using Cormen/Tarjan topological sort
- Fix
fig up
not printing log output - Stop containers in reverse order to starting
- Fix scale command not binding ports
Thanks to @barnybug and @dustinlacewell for their work on this release.
0.2.1
(2014-02-04)
- General improvements to error reporting (#77, #79)
0.2.0
(2014-01-31)
- Link services to themselves so run commands can access the running service. (#67)
- Much better documentation.
- Make service dependency resolution more reliable. (#48)
- Load Fig configurations with a
.yaml
extension. (#58)
Big thanks to @cameronmaske, @mrchrisadams and @damianmoore for their help with this release.
0.1.4
(2014-01-27)
- Add a link alias without the project name. This makes the environment variables a little shorter:
REDIS_1_PORT_6379_TCP_ADDR
. (#54)
0.1.3
(2014-01-23)
- Fix ports sometimes being configured incorrectly. (#46)
- Fix log output sometimes not displaying. (#47)
0.1.2
(2014-01-22)
- Add
-T
option tofig run
to disable pseudo-TTY. (#34) - Fix
fig up
requiring the ubuntu image to be pulled to recreate containers. (#33) Thanks @cameronmaske! - Improve reliability, fix arrow keys and fix a race condition in
fig run
. (#34, #39, #40)
0.1.1
(2014-01-17)
- Fix bug where ports were not exposed correctly (#29). Thanks @dustinlacewell!
0.1.0
(2014-01-16)
- Containers are recreated on each
fig up
, ensuring config is up-to-date withfig.yml
(#2) - Add
fig scale
command (#9) - Use
DOCKER_HOST
environment variable to find Docker daemon, for consistency with the official Docker client (was previouslyDOCKER_URL
) (#19) - Truncate long commands in
fig ps
(#18) - Fill out CLI help banners for commands (#15, #16)
- Show a friendlier error when
fig.yml
is missing (#4) - Fix bug with
fig build
logging (#3) - Fix bug where builds would time out if a step took a long time without generating output (#6)
- Fix bug where streaming container output over the Unix socket raised an error (#7)
Big thanks to @tomstuart, @EnTeQuAk, @schickling, @aronasorman and @GeoffreyPlitt.
0.0.2
(2014-01-02)
- Improve documentation
- Try to connect to Docker on
tcp://localdocker:4243
and a UNIX socket in addition tolocalhost
. - Improve
fig up
behavior - Add confirmation prompt to
fig rm
- Add
fig build
command
0.0.1
(2013-12-20)
Initial release.",,,
e711c3e36cafe793779f21f1ab8c8083be583c3034ad6081b2ee983745d68093,"Quickstart
Follow this guide to get started with creating a basic Docker extension. The Quickstart guide automatically generates boilerplate files for you.
Prerequisites
Note
NodeJS and Go are only required when you follow the quickstart guide to create an extension. It uses the
docker extension init
command to automatically generate boilerplate files. This command uses a template based on a ReactJS and Go application.
In Docker Desktop settings, ensure you can install the extension you're developing. You may need to navigate to the Extensions tab in Docker Desktop settings and deselect Allow only extensions distributed through the Docker Marketplace.
Step one: Set up your directory
To set up your directory, use the init
subcommand and provide a name for your extension.
$ docker extension init <my-extension>
The command asks a series of questions about your extension, such as its name, a description, and the name of your Hub repository. This helps the CLI generate a set of boilerplate files for you to get started. It stores the boilerplate files in the my-extension
directory.
The automatically generated extension contains:
- A Go backend service in the
backend
folder that listens on a socket. It has one endpoint/hello
that returns a JSON payload. - A React frontend in the
frontend
folder that can call the backend and output the backend’s response.
For more information and guidelines on building the UI, see the Design and UI styling section.
Step two: Build the extension
To build the extension, move into the newly created directory and run:
$ docker build -t <name-of-your-extension> .
docker build
builds the extension and generates an image named the same as the chosen hub repository. For example, if you typed john/my-extension
as the answer to the following question:
? Hub repository (eg. namespace/repository on hub): john/my-extension`
The docker build
generates an image with name john/my-extension
.
Step three: Install and preview the extension
To install the extension in Docker Desktop, run:
$ docker extension install <name-of-your-extension>
To preview the extension in Docker Desktop, once the installation is complete and you should see a Quickstart item underneath the Extensions menu. Selecting this item opens the extension's frontend.
Tip
During UI development, it’s helpful to use hot reloading to test your changes without rebuilding your entire extension. See Preview whilst developing the UI for more information.
You may also want to inspect the containers that belong to the extension. By default, extension containers are hidden from the Docker Dashboard. You can change this in Settings, see how to show extension containers for more information.
Step four: Submit and publish your extension to the Marketplace
If you want to make your extension available to all Docker Desktop users, you can submit it for publication in the Marketplace. For more information, see Publish.
Clean up
To remove the extension, run:
$ docker extension rm <name-of-your-extension>
What's next
- Build a more advanced frontend for your extension.
- Learn how to test and debug your extension.
- Learn how to setup CI for your extension.
- Learn more about extensions architecture.
- Learn more about designing the UI.",,,
2158e2e815fab6016265d0084c09e50cacb613fd49e35000ac04290c6f7d8882,"Integrate Docker Scout with Circle CI
The following examples runs when triggered in CircleCI. When triggered, it checks out the ""docker/scout-demo-service:latest"" image and tag and then uses Docker Scout to create a CVE report.
Add the following to a .circleci/config.yml file.
First, set up the rest of the workflow. Add the following to the YAML file:
version: 2.1
jobs:
build:
docker:
- image: cimg/base:stable
environment:
IMAGE_TAG: docker/scout-demo-service:latest
This defines the container image the workflow uses and an environment variable for the image.
Add the following to the YAML file to define the steps for the workflow:
steps:
# Checkout the repository files
- checkout
# Set up a separate Docker environment to run `docker` commands in
- setup_remote_docker:
version: 20.10.24
# Install Docker Scout and login to Docker Hub
- run:
name: Install Docker Scout
command: |
env
curl -sSfL https://raw.githubusercontent.com/docker/scout-cli/main/install.sh | sh -s -- -b /home/circleci/bin
echo $DOCKER_HUB_PAT | docker login -u $DOCKER_HUB_USER --password-stdin
# Build the Docker image
- run:
name: Build Docker image
command: docker build -t $IMAGE_TAG .
# Run Docker Scout
- run:
name: Scan image for CVEs
command: |
docker-scout cves $IMAGE_TAG --exit-code --only-severity critical,high
This checks out the repository files and then sets up a separate Docker environment to run commands in.
It installs Docker Scout, logs into Docker Hub, builds the Docker image, and then runs Docker Scout to generate a CVE report. It only shows critical or high-severity vulnerabilities.
Finally, add a name for the workflow and the workflow's jobs:
workflows:
build-docker-image:
jobs:
- build",,,
c176fecd49fc48fd767991ee05de941de5d59efc4dd6879433028c935287735b,"Delete the service running on the swarm
The remaining steps in the tutorial don't use the helloworld
service, so now
you can delete the service from the swarm.
If you haven't already, open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named
manager1
.Run
docker service rm helloworld
to remove thehelloworld
service.$ docker service rm helloworld helloworld
Run
docker service inspect <SERVICE-ID>
to verify that the swarm manager removed the service. The CLI returns a message that the service is not found:$ docker service inspect helloworld [] Status: Error: no such service: helloworld, Code: 1
Even though the service no longer exists, the task containers take a few seconds to clean up. You can use
docker ps
on the nodes to verify when the tasks have been removed.$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES db1651f50347 alpine:latest ""ping docker.com"" 44 minutes ago Up 46 seconds helloworld.5.9lkmos2beppihw95vdwxy1j3w 43bf6e532a92 alpine:latest ""ping docker.com"" 44 minutes ago Up 46 seconds helloworld.3.a71i8rp6fua79ad43ycocl4t2 5a0fb65d8fa7 alpine:latest ""ping docker.com"" 44 minutes ago Up 45 seconds helloworld.2.2jpgensh7d935qdc857pxulfr afb0ba67076f alpine:latest ""ping docker.com"" 44 minutes ago Up 46 seconds helloworld.4.1c47o7tluz7drve4vkm2m5olx 688172d3bfaa alpine:latest ""ping docker.com"" 45 minutes ago Up About a minute helloworld.1.74nbhb3fhud8jfrhigd7s29we $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
Next steps
Next, you'll set up a new service and apply a rolling update.",,,
30c43fe812c739fac34be98001d9e08f46eb67b7119f9eb6ec342503c4cb30f0,"Install Docker Engine on CentOS
To get started with Docker Engine on CentOS, make sure you meet the prerequisites, and then follow the installation steps.
Prerequisites
OS requirements
To install Docker Engine, you need a maintained version of one of the following CentOS versions:
- CentOS 9 (stream)
The centos-extras
repository must be enabled. This repository is enabled by
default. If you have disabled it, you need to re-enable it.
Uninstall old versions
Before you can install Docker Engine, you need to uninstall any conflicting packages.
Your Linux distribution may provide unofficial Docker packages, which may conflict with the official packages provided by Docker. You must uninstall these packages before you install the official version of Docker Engine.
$ sudo dnf remove docker \
docker-client \
docker-client-latest \
docker-common \
docker-latest \
docker-latest-logrotate \
docker-logrotate \
docker-engine
dnf
might report that you have none of these packages installed.
Images, containers, volumes, and networks stored in /var/lib/docker/
aren't
automatically removed when you uninstall Docker.
Installation methods
You can install Docker Engine in different ways, depending on your needs:
You can set up Docker's repositories and install from them, for ease of installation and upgrade tasks. This is the recommended approach.
You can download the RPM package, install it manually, and manage upgrades completely manually. This is useful in situations such as installing Docker on air-gapped systems with no access to the internet.
In testing and development environments, you can use automated convenience scripts to install Docker.
Install using the rpm repository
Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository.
Set up the repository
Install the dnf-plugins-core
package (which provides the commands to manage
your DNF repositories) and set up the repository.
$ sudo dnf -y install dnf-plugins-core
$ sudo dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
Install Docker Engine
Install the Docker packages.
To install the latest version, run:
$ sudo dnf install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
If prompted to accept the GPG key, verify that the fingerprint matches
060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35
, and if so, accept it.This command installs Docker, but it doesn't start Docker. It also creates a
docker
group, however, it doesn't add any users to the group by default.To install a specific version, start by listing the available versions in the repository:
$ dnf list docker-ce --showduplicates | sort -r docker-ce.x86_64 3:28.0.1-1.el9 docker-ce-stable docker-ce.x86_64 3:28.0.0-1.el9 docker-ce-stable <...>
The list returned depends on which repositories are enabled, and is specific to your version of CentOS (indicated by the
.el9
suffix in this example).Install a specific version by its fully qualified package name, which is the package name (
docker-ce
) plus the version string (2nd column), separated by a hyphen (-
). For example,docker-ce-3:28.0.1-1.el9
.Replace
<VERSION_STRING>
with the desired version and then run the following command to install:$ sudo dnf install docker-ce-<VERSION_STRING> docker-ce-cli-<VERSION_STRING> containerd.io docker-buildx-plugin docker-compose-plugin
This command installs Docker, but it doesn't start Docker. It also creates a
docker
group, however, it doesn't add any users to the group by default.Start Docker Engine.
$ sudo systemctl enable --now docker
This configures the Docker systemd service to start automatically when you boot your system. If you don't want Docker to start automatically, use
sudo systemctl start docker
instead.Verify that the installation is successful by running the
hello-world
image:$ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Upgrade Docker Engine
To upgrade Docker Engine, follow the installation instructions, choosing the new version you want to install.
Install from a package
If you can't use Docker's rpm
repository to install Docker Engine, you can
download the .rpm
file for your release and install it manually. You need to
download a new file each time you want to upgrade Docker Engine.
Go to https://download.docker.com/linux/centos/ and choose your version of CentOS. Then browse to
x86_64/stable/Packages/
and download the.rpm
file for the Docker version you want to install.Install Docker Engine, changing the following path to the path where you downloaded the Docker package.
$ sudo dnf install /path/to/package.rpm
Docker is installed but not started. The
docker
group is created, but no users are added to the group.Start Docker Engine.
$ sudo systemctl enable --now docker
This configures the Docker systemd service to start automatically when you boot your system. If you don't want Docker to start automatically, use
sudo systemctl start docker
instead.Verify that the installation is successful by running the
hello-world
image:$ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Upgrade Docker Engine
To upgrade Docker Engine, download the newer package files and repeat the
installation procedure, using dnf upgrade
instead of dnf install
, and point to the new files.
Install using the convenience script
Docker provides a convenience script at
https://get.docker.com/ to install Docker into
development environments non-interactively. The convenience script isn't
recommended for production environments, but it's useful for creating a
provisioning script tailored to your needs. Also refer to the
install using the repository steps to learn
about installation steps to install using the package repository. The source code
for the script is open source, and you can find it in the
docker-install
repository on GitHub.
Always examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script:
- The script requires
root
orsudo
privileges to run. - The script attempts to detect your Linux distribution and version and configure your package management system for you.
- The script doesn't allow you to customize most installation parameters.
- The script installs dependencies and recommendations without asking for confirmation. This may install a large number of packages, depending on the current configuration of your host machine.
- By default, the script installs the latest stable release of Docker, containerd, and runc. When using this script to provision a machine, this may result in unexpected major version upgrades of Docker. Always test upgrades in a test environment before deploying to your production systems.
- The script isn't designed to upgrade an existing Docker installation. When using the script to update an existing installation, dependencies may not be updated to the expected version, resulting in outdated versions.
Tip
Preview script steps before running. You can run the script with the
--dry-run
option to learn what steps the script will run when invoked:$ curl -fsSL https://get.docker.com -o get-docker.sh $ sudo sh ./get-docker.sh --dry-run
This example downloads the script from https://get.docker.com/ and runs it to install the latest stable release of Docker on Linux:
$ curl -fsSL https://get.docker.com -o get-docker.sh
$ sudo sh get-docker.sh
Executing docker install script, commit: 7cae5f8b0decc17d6571f9f52eb840fbc13b2737
<...>
You have now successfully installed and started Docker Engine. The docker
service starts automatically on Debian based distributions. On RPM
based
distributions, such as CentOS, Fedora, RHEL or SLES, you need to start it
manually using the appropriate systemctl
or service
command. As the message
indicates, non-root users can't run Docker commands by default.
Use Docker as a non-privileged user, or install in rootless mode?
The installation script requires
root
orsudo
privileges to install and use Docker. If you want to grant non-root users access to Docker, refer to the post-installation steps for Linux. You can also install Docker withoutroot
privileges, or configured to run in rootless mode. For instructions on running Docker in rootless mode, refer to run the Docker daemon as a non-root user (rootless mode).
Install pre-releases
Docker also provides a convenience script at
https://test.docker.com/ to install pre-releases of
Docker on Linux. This script is equal to the script at get.docker.com
, but
configures your package manager to use the test channel of the Docker package
repository. The test channel includes both stable and pre-releases (beta
versions, release-candidates) of Docker. Use this script to get early access to
new releases, and to evaluate them in a testing environment before they're
released as stable.
To install the latest version of Docker on Linux from the test channel, run:
$ curl -fsSL https://test.docker.com -o test-docker.sh
$ sudo sh test-docker.sh
Upgrade Docker after using the convenience script
If you installed Docker using the convenience script, you should upgrade Docker using your package manager directly. There's no advantage to re-running the convenience script. Re-running it can cause issues if it attempts to re-install repositories which already exist on the host machine.
Uninstall Docker Engine
Uninstall the Docker Engine, CLI, containerd, and Docker Compose packages:
$ sudo dnf remove docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras
Images, containers, volumes, or custom configuration files on your host aren't automatically removed. To delete all images, containers, and volumes:
$ sudo rm -rf /var/lib/docker $ sudo rm -rf /var/lib/containerd
You have to delete any edited configuration files manually.
Next steps
- Continue to Post-installation steps for Linux.",,,
53e2daa4eef5da66ef39b39b7be229e586787725abdce8844db2aba9f1cff487,"GitHub Actions cache
The GitHub Actions cache utilizes the GitHub-provided Action's cache or other cache services supporting the GitHub Actions cache protocol. This is the recommended cache to use inside your GitHub Actions workflows, as long as your use case falls within the size and usage limits set by GitHub.
This cache storage backend is not supported with the default docker
driver.
To use this feature, create a new builder using a different driver. See
Build drivers for more information.
Synopsis
$ docker buildx build --push -t <registry>/<image> \
--cache-to type=gha[,parameters...] \
--cache-from type=gha[,parameters...] .
The following table describes the available CSV parameters that you can pass to
--cache-to
and --cache-from
.
| Name | Option | Type | Default | Description |
|---|---|---|---|---|
url | cache-to ,cache-from | String | $ACTIONS_CACHE_URL | Cache server URL, see authentication. |
url_v2 | cache-to ,cache-from | String | $ACTIONS_CACHE_URL | Cache v2 server URL, see authentication. |
token | cache-to ,cache-from | String | $ACTIONS_RUNTIME_TOKEN | Access token, see authentication. |
scope | cache-to ,cache-from | String | buildkit | Which scope cache object belongs to, see scope |
mode | cache-to | min ,max | min | Cache layers to export, see cache mode. |
ignore-error | cache-to | Boolean | false | Ignore errors caused by failed cache exports. |
timeout | cache-to ,cache-from | String | 10m | Max duration for importing or exporting cache before it's timed out. |
repository | cache-to | String | GitHub repository used for cache storage. | |
ghtoken | cache-to | String | GitHub token required for accessing the GitHub API. |
Authentication
If the url
, url_v2
or token
parameters are left unspecified, the gha
cache backend will fall back to using environment variables. If you invoke the
docker buildx
command manually from an inline step, then the variables must
be manually exposed. Consider using the
crazy-max/ghaction-github-runtime
,
GitHub Action as a helper for exposing the variables.
Scope
Scope is a key used to identify the cache object. By default, it is set to
buildkit
. If you build multiple images, each build will overwrite the cache
of the previous, leaving only the final cache.
To preserve the cache for multiple builds, you can specify this scope attribute with a specific name. In the following example, the cache is set to the image name, to ensure each image gets its own cache:
$ docker buildx build --push -t <registry>/<image> \
--cache-to type=gha,url=...,token=...,scope=image \
--cache-from type=gha,url=...,token=...,scope=image .
$ docker buildx build --push -t <registry>/<image2> \
--cache-to type=gha,url=...,token=...,scope=image2 \
--cache-from type=gha,url=...,token=...,scope=image2 .
GitHub's cache access restrictions, still apply. Only the cache for the current branch, the base branch and the default branch is accessible by a workflow.
Using docker/build-push-action
When using the
docker/build-push-action
, the
url
and token
parameters are automatically populated. No need to manually
specify them, or include any additional workarounds.
For example:
- name: Build and push
uses: docker/build-push-action@v6
with:
context: .
push: true
tags: ""<registry>/<image>:latest""
cache-from: type=gha
cache-to: type=gha,mode=max
Avoid GitHub Actions cache API throttling
GitHub's
usage limits and eviction policy
causes stale cache entries to be removed after a certain period of time. By
default, the gha
cache backend uses the GitHub Actions cache API to check the
status of cache entries.
The GitHub Actions cache API is subject to rate limiting if you make too many
requests in a short period of time, which may happen as a result of cache
lookups during a build using the gha
cache backend.
#31 exporting to GitHub Actions Cache
#31 preparing build cache for export
#31 preparing build cache for export 600.3s done
#31 ERROR: maximum timeout reached
------
> exporting to GitHub Actions Cache:
------
ERROR: failed to solve: maximum timeout reached
make: *** [Makefile:35: release] Error 1
Error: Process completed with exit code 2.
To mitigate this issue, you can supply a GitHub token to BuildKit. This lets BuildKit utilize the standard GitHub API for checking cache keys, thereby reducing the number of requests made to the cache API.
To provide a GitHub token, you can use the ghtoken
parameter, and a
repository
parameter to specify the repository to use for cache storage. The
ghtoken
parameter is a GitHub token with the repo
scope, which is required
to access the GitHub Actions cache API.
The ghtoken
parameter is automatically set to the value of
secrets.GITHUB_TOKEN
when you build with the docker/build-push-action
action. You can also set the ghtoken
parameter manually using the
github-token
input, as shown in the following example:
- name: Build and push
uses: docker/build-push-action@v6
with:
context: .
push: true
tags: ""<registry>/<image>:latest""
cache-from: type=gha
cache-to: type=gha,mode=max
github-token: ${{ secrets.MY_CUSTOM_TOKEN }}
Further reading
For an introduction to caching see Docker build cache.
For more information on the gha
cache backend, see the
BuildKit README.
For more information about using GitHub Actions with Docker, see Introduction to GitHub Actions",,,
c14c248acbd226fa18149545a1e0deec43c937fb38113a00ff38554b6c093de0,"Docker Desktop license agreement
Docker Desktop is licensed under the Docker Subscription Service Agreement. When you download and install Docker Desktop, you will be asked to agree to the updated terms.
Our Docker Subscription Service Agreement states:
- Docker Desktop is free for small businesses (fewer than 250 employees AND less than $10 million in annual revenue), personal use, education, and non-commercial open source projects.
- Otherwise, it requires a paid subscription for professional use.
- Paid subscriptions are also required for government entities.
- The Docker Pro, Team, and Business subscriptions include commercial use of Docker Desktop.
Read the Blog and Docker subscription FAQs to learn how this may affect companies using Docker Desktop.
Note
The licensing and distribution terms for Docker and Moby open-source projects, such as Docker Engine, aren't changing.
Docker Desktop is built using open-source software. For information about the licensing of open-source components in Docker Desktop, select the whale menu > About Docker Desktop > Acknowledgements.
Docker Desktop distributes some components that are licensed under the GNU General Public License. Select here to download the source for these components.
Tip
Explore Docker subscriptions to see what else Docker can offer you.",,,
a3319e7fcbd62ac03f2548bf2c7d687d2e751cf7f927769fe08dfe60fdb73a7f,"Networking with overlay networks
This series of tutorials deals with networking for swarm services. For networking with standalone containers, see Networking with standalone containers. If you need to learn more about Docker networking in general, see the overview.
This page includes the following tutorials. You can run each of them on Linux, Windows, or a Mac, but for the last one, you need a second Docker host running elsewhere.
Use the default overlay network demonstrates how to use the default overlay network that Docker sets up for you automatically when you initialize or join a swarm. This network is not the best choice for production systems.
Use user-defined overlay networks shows how to create and use your own custom overlay networks, to connect services. This is recommended for services running in production.
Use an overlay network for standalone containers shows how to communicate between standalone containers on different Docker daemons using an overlay network.
Prerequisites
These require you to have at least a single-node swarm, which means that
you have started Docker and run docker swarm init
on the host. You can run
the examples on a multi-node swarm as well.
Use the default overlay network
In this example, you start an alpine
service and examine the characteristics
of the network from the point of view of the individual service containers.
This tutorial does not go into operation-system-specific details about how overlay networks are implemented, but focuses on how the overlay functions from the point of view of a service.
Prerequisites
This tutorial requires three physical or virtual Docker hosts which can all communicate with one another. This tutorial assumes that the three hosts are running on the same network with no firewall involved.
These hosts will be referred to as manager
, worker-1
, and worker-2
. The
manager
host will function as both a manager and a worker, which means it can
both run service tasks and manage the swarm. worker-1
and worker-2
will
function as workers only,
If you don't have three hosts handy, an easy solution is to set up three Ubuntu hosts on a cloud provider such as Amazon EC2, all on the same network with all communications allowed to all hosts on that network (using a mechanism such as EC2 security groups), and then to follow the installation instructions for Docker Engine - Community on Ubuntu.
Walkthrough
Create the swarm
At the end of this procedure, all three Docker hosts will be joined to the swarm
and will be connected together using an overlay network called ingress
.
On
manager
. initialize the swarm. If the host only has one network interface, the--advertise-addr
flag is optional.$ docker swarm init --advertise-addr=<IP-ADDRESS-OF-MANAGER>
Make a note of the text that is printed, as this contains the token that you will use to join
worker-1
andworker-2
to the swarm. It is a good idea to store the token in a password manager.On
worker-1
, join the swarm. If the host only has one network interface, the--advertise-addr
flag is optional.$ docker swarm join --token <TOKEN> \ --advertise-addr <IP-ADDRESS-OF-WORKER-1> \ <IP-ADDRESS-OF-MANAGER>:2377
On
worker-2
, join the swarm. If the host only has one network interface, the--advertise-addr
flag is optional.$ docker swarm join --token <TOKEN> \ --advertise-addr <IP-ADDRESS-OF-WORKER-2> \ <IP-ADDRESS-OF-MANAGER>:2377
On
manager
, list all the nodes. This command can only be done from a manager.$ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS d68ace5iraw6whp7llvgjpu48 * ip-172-31-34-146 Ready Active Leader nvp5rwavvb8lhdggo8fcf7plg ip-172-31-35-151 Ready Active ouvx2l7qfcxisoyms8mtkgahw ip-172-31-36-89 Ready Active
You can also use the
--filter
flag to filter by role:$ docker node ls --filter role=manager ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS d68ace5iraw6whp7llvgjpu48 * ip-172-31-34-146 Ready Active Leader $ docker node ls --filter role=worker ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS nvp5rwavvb8lhdggo8fcf7plg ip-172-31-35-151 Ready Active ouvx2l7qfcxisoyms8mtkgahw ip-172-31-36-89 Ready Active
List the Docker networks on
manager
,worker-1
, andworker-2
and notice that each of them now has an overlay network calledingress
and a bridge network calleddocker_gwbridge
. Only the listing formanager
is shown here:$ docker network ls NETWORK ID NAME DRIVER SCOPE 495c570066be bridge bridge local 961c6cae9945 docker_gwbridge bridge local ff35ceda3643 host host local trtnl4tqnc3n ingress overlay swarm c8357deec9cb none null local
The docker_gwbridge
connects the ingress
network to the Docker host's
network interface so that traffic can flow to and from swarm managers and
workers. If you create swarm services and do not specify a network, they are
connected to the ingress
network. It is recommended that you use separate
overlay networks for each application or group of applications which will work
together. In the next procedure, you will create two overlay networks and
connect a service to each of them.
Create the services
On
manager
, create a new overlay network callednginx-net
:$ docker network create -d overlay nginx-net
You don't need to create the overlay network on the other nodes, because it will be automatically created when one of those nodes starts running a service task which requires it.
On
manager
, create a 5-replica Nginx service connected tonginx-net
. The service will publish port 80 to the outside world. All of the service task containers can communicate with each other without opening any ports.Note
Services can only be created on a manager.
$ docker service create \ --name my-nginx \ --publish target=80,published=80 \ --replicas=5 \ --network nginx-net \ nginx
The default publish mode of
ingress
, which is used when you do not specify amode
for the--publish
flag, means that if you browse to port 80 onmanager
,worker-1
, orworker-2
, you will be connected to port 80 on one of the 5 service tasks, even if no tasks are currently running on the node you browse to. If you want to publish the port usinghost
mode, you can addmode=host
to the--publish
output. However, you should also use--mode global
instead of--replicas=5
in this case, since only one service task can bind a given port on a given node.Run
docker service ls
to monitor the progress of service bring-up, which may take a few seconds.Inspect the
nginx-net
network onmanager
,worker-1
, andworker-2
. Remember that you did not need to create it manually onworker-1
andworker-2
because Docker created it for you. The output will be long, but notice theContainers
andPeers
sections.Containers
lists all service tasks (or standalone containers) connected to the overlay network from that host.From
manager
, inspect the service usingdocker service inspect my-nginx
and notice the information about the ports and endpoints used by the service.Create a new network
nginx-net-2
, then update the service to use this network instead ofnginx-net
:$ docker network create -d overlay nginx-net-2
$ docker service update \ --network-add nginx-net-2 \ --network-rm nginx-net \ my-nginx
Run
docker service ls
to verify that the service has been updated and all tasks have been redeployed. Rundocker network inspect nginx-net
to verify that no containers are connected to it. Run the same command fornginx-net-2
and notice that all the service task containers are connected to it.Note
Even though overlay networks are automatically created on swarm worker nodes as needed, they are not automatically removed.
Clean up the service and the networks. From
manager
, run the following commands. The manager will direct the workers to remove the networks automatically.$ docker service rm my-nginx $ docker network rm nginx-net nginx-net-2
Use a user-defined overlay network
Prerequisites
This tutorial assumes the swarm is already set up and you are on a manager.
Walkthrough
Create the user-defined overlay network.
$ docker network create -d overlay my-overlay
Start a service using the overlay network and publishing port 80 to port 8080 on the Docker host.
$ docker service create \ --name my-nginx \ --network my-overlay \ --replicas 1 \ --publish published=8080,target=80 \ nginx:latest
Run
docker network inspect my-overlay
and verify that themy-nginx
service task is connected to it, by looking at theContainers
section.Remove the service and the network.
$ docker service rm my-nginx $ docker network rm my-overlay
Use an overlay network for standalone containers
This example demonstrates DNS container discovery -- specifically, how to communicate between standalone containers on different Docker daemons using an overlay network. Steps are:
- On
host1
, initialize the node as a swarm (manager). - On
host2
, join the node to the swarm (worker). - On
host1
, create an attachable overlay network (test-net
). - On
host1
, run an interactive alpine container (alpine1
) ontest-net
. - On
host2
, run an interactive, and detached, alpine container (alpine2
) ontest-net
. - On
host1
, from within a session ofalpine1
, pingalpine2
.
Prerequisites
For this test, you need two different Docker hosts that can communicate with each other. Each host must have the following ports open between the two Docker hosts:
- TCP port 2377
- TCP and UDP port 7946
- UDP port 4789
One easy way to set this up is to have two VMs (either local or on a cloud provider like AWS), each with Docker installed and running. If you're using AWS or a similar cloud computing platform, the easiest configuration is to use a security group that opens all incoming ports between the two hosts and the SSH port from your client's IP address.
This example refers to the two nodes in our swarm as host1
and host2
. This
example also uses Linux hosts, but the same commands work on Windows.
Walk-through
Set up the swarm.
a. On
host1
, initialize a swarm (and if prompted, use--advertise-addr
to specify the IP address for the interface that communicates with other hosts in the swarm, for instance, the private IP address on AWS):$ docker swarm init Swarm initialized: current node (vz1mm9am11qcmo979tlrlox42) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-5g90q48weqrtqryq4kj6ow0e8xm9wmv9o6vgqc5j320ymybd5c-8ex8j0bc40s6hgvy5ui5gl4gy 172.31.47.252:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
b. On
host2
, join the swarm as instructed above:$ docker swarm join --token <your_token> <your_ip_address>:2377 This node joined a swarm as a worker.
If the node fails to join the swarm, the
docker swarm join
command times out. To resolve, rundocker swarm leave --force
onhost2
, verify your network and firewall settings, and try again.On
host1
, create an attachable overlay network calledtest-net
:$ docker network create --driver=overlay --attachable test-net uqsof8phj3ak0rq9k86zta6ht
Notice the returned NETWORK ID -- you will see it again when you connect to it from
host2
.On
host1
, start an interactive (-it
) container (alpine1
) that connects totest-net
:$ docker run -it --name alpine1 --network test-net alpine / #
On
host2
, list the available networks -- notice thattest-net
does not yet exist:$ docker network ls NETWORK ID NAME DRIVER SCOPE ec299350b504 bridge bridge local 66e77d0d0e9a docker_gwbridge bridge local 9f6ae26ccb82 host host local omvdxqrda80z ingress overlay swarm b65c952a4b2b none null local
On
host2
, start a detached (-d
) and interactive (-it
) container (alpine2
) that connects totest-net
:$ docker run -dit --name alpine2 --network test-net alpine fb635f5ece59563e7b8b99556f816d24e6949a5f6a5b1fbd92ca244db17a4342
Note
Automatic DNS container discovery only works with unique container names.
On
host2
, verify thattest-net
was created (and has the same NETWORK ID astest-net
onhost1
):$ docker network ls NETWORK ID NAME DRIVER SCOPE ... uqsof8phj3ak test-net overlay swarm
On
host1
, pingalpine2
within the interactive terminal ofalpine1
:/ # ping -c 2 alpine2 PING alpine2 (10.0.0.5): 56 data bytes 64 bytes from 10.0.0.5: seq=0 ttl=64 time=0.600 ms 64 bytes from 10.0.0.5: seq=1 ttl=64 time=0.555 ms --- alpine2 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.555/0.577/0.600 ms
The two containers communicate with the overlay network connecting the two hosts. If you run another alpine container on
host2
that is not detached, you can pingalpine1
fromhost2
(and here we add the remove option for automatic container cleanup):$ docker run -it --rm --name alpine3 --network test-net alpine / # ping -c 2 alpine1 / # exit
On
host1
, close thealpine1
session (which also stops the container):/ # exit
Clean up your containers and networks:
You must stop and remove the containers on each host independently because Docker daemons operate independently and these are standalone containers. You only have to remove the network on
host1
because when you stopalpine2
onhost2
,test-net
disappears.a. On
host2
, stopalpine2
, check thattest-net
was removed, then removealpine2
:$ docker container stop alpine2 $ docker network ls $ docker container rm alpine2
a. On
host1
, removealpine1
andtest-net
:$ docker container rm alpine1 $ docker network rm test-net",,,
4c76ac731cf824b51d076097d9ae50c88a2359cedbdccddc2c74f38e5be8dcbb,"Run multiple processes in a container
A container's main running process is the ENTRYPOINT
and/or CMD
at the
end of the Dockerfile
. It's best practice to separate areas of concern by
using one service per container. That service may fork into multiple
processes (for example, Apache web server starts multiple worker processes).
It's ok to have multiple processes, but to get the most benefit out of Docker,
avoid one container being responsible for multiple aspects of your overall
application. You can connect multiple containers using user-defined networks and
shared volumes.
The container's main process is responsible for managing all processes that it
starts. In some cases, the main process isn't well-designed, and doesn't handle
""reaping"" (stopping) child processes gracefully when the container exits. If
your process falls into this category, you can use the --init
option when you
run the container. The --init
flag inserts a tiny init-process into the
container as the main process, and handles reaping of all processes when the
container exits. Handling such processes this way is superior to using a
full-fledged init process such as sysvinit
or systemd
to handle process
lifecycle within your container.
If you need to run more than one service within a container, you can achieve this in a few different ways.
Use a wrapper script
Put all of your commands in a wrapper script, complete with testing and
debugging information. Run the wrapper script as your CMD
. The following is a
naive example. First, the wrapper script:
#!/bin/bash
# Start the first process
./my_first_process &
# Start the second process
./my_second_process &
# Wait for any process to exit
wait -n
# Exit with status of process that exited first
exit $?
Next, the Dockerfile:
# syntax=docker/dockerfile:1
FROM ubuntu:latest
COPY my_first_process my_first_process
COPY my_second_process my_second_process
COPY my_wrapper_script.sh my_wrapper_script.sh
CMD ./my_wrapper_script.sh
Use Bash job controls
If you have one main process that needs to start first and stay running but you temporarily need to run some other processes (perhaps to interact with the main process) then you can use bash's job control. First, the wrapper script:
#!/bin/bash
# turn on bash's job control
set -m
# Start the primary process and put it in the background
./my_main_process &
# Start the helper process
./my_helper_process
# the my_helper_process might need to know how to wait on the
# primary process to start before it does its work and returns
# now we bring the primary process back into the foreground
# and leave it there
fg %1
# syntax=docker/dockerfile:1
FROM ubuntu:latest
COPY my_main_process my_main_process
COPY my_helper_process my_helper_process
COPY my_wrapper_script.sh my_wrapper_script.sh
CMD ./my_wrapper_script.sh
Use a process manager
Use a process manager like supervisord
. This is more involved than the other
options, as it requires you to bundle supervisord
and its configuration into
your image (or base your image on one that includes supervisord
), along with
the different applications it manages. Then you start supervisord
, which
manages your processes for you.
The following Dockerfile example shows this approach. The example assumes that these files exist at the root of the build context:
supervisord.conf
my_first_process
my_second_process
# syntax=docker/dockerfile:1
FROM ubuntu:latest
RUN apt-get update && apt-get install -y supervisor
RUN mkdir -p /var/log/supervisor
COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf
COPY my_first_process my_first_process
COPY my_second_process my_second_process
CMD [""/usr/bin/supervisord""]
If you want to make sure both processes output their stdout
and stderr
to
the container logs, you can add the following to the supervisord.conf
file:
[supervisord]
nodaemon=true
logfile=/dev/null
logfile_maxbytes=0
[program:app]
stdout_logfile=/dev/fd/1
stdout_logfile_maxbytes=0
redirect_stderr=true",,,
683ee8b48ed4db6b8630c21e33a464d5340bf8eec8996123c66450a8889e901d,"Integrate Docker Scout with Sysdig
The Sysdig integration enables Docker Scout to automatically detect the images you're using for your running workloads. Activating this integration gives you real-time insights about your security posture, and lets you compare your builds with what's running in production.
How it works
The Sysdig Agent captures the images of your container workloads. Docker Scout integrates with the Sysdig API to discover the images in your cluster. This integration uses Sysdig's Risk Spotlight feature. For more information, see Risk Spotlight Integrations (Sysdig docs).
Tip
Sysdig offers a free trial for Docker users to try out the new Docker Scout integration.
Each Sysdig integration maps to an environment. When you enable a Sysdig
integration, you specify the environment name for that cluster, such as
production
or staging
. Docker Scout assigns the images in the cluster to
the corresponding environment. This lets you use the environment filters to see
vulnerability status and policy compliance for an environment.
Only images analyzed by Docker Scout can be assigned to an environment. The Sysdig runtime integration doesn't trigger image analysis by itself. To analyze images automatically, enable a registry integration.
Image analysis must not necessarily precede the runtime integration, but the environment assignment only takes place once Docker Scout has analyzed the image.
Prerequisites
- Install the Sysdig Agent in the cluster that you want to integrate, see Install Sysdig Agent (Sysdig docs).
- Enable profiling for Risk Spotlight Integrations in Sysdig, see Profiling (Sysdig docs).
- You must be an organization owner to enable the integration in the Docker Scout Dashboard.
Integrate an environment
Go to the Sysdig integration page on the Docker Scout Dashboard.
In the How to integrate section, enter a configuration name for this integration. Docker Scout uses this label as a display name for the integration.
Select Next.
Enter a Risk Spotlight API token and select the region in the drop-down list.
The Risk Spotlight API token is the Sysdig token that Docker Scout needs to integrate with Sysdig. For more instructions on how to generate a Risk Spotlight token, See Risk Spotlight Integrations (Sysdig docs).
The region corresponds to the
global.sysdig.region
configuration parameter set when deploying the Sysdig Agent.Select Next.
After selecting Next, Docker Scout connects to Sysdig and retrieves the cluster names for your Sysdig account. Cluster names correspond to the
global.clusterConfig.name
configuration parameter set when deploying Sysdig Agents.An error displays if Docker Scout fails to connect to Sysdig using the provided token. If there's an error, you won't be able to continue the integration. Go back and verify that the configuration details are correct.
Select a cluster name in the drop-down list.
Select Next.
Assign an environment name for this cluster.
You can reuse an existing environment or create a new one.
Select Enable integration.
After enabling the integration, Docker Scout automatically detects images running in the cluster, and assigns those images to the environment associated with the cluster. For more information about environments, see Environment monitoring.
Note
Docker Scout only detects images that have been analyzed. To trigger an image analysis, enable a registry integration and push an image to your registry.
If you created a new environment for this integration, the environment appears in Docker Scout when at least one image has been analyzed.
To integrate more clusters, go to the Sysdig integrations page and select the Add button.",,,
7869a8c9c6e5b5a16b526c89661534e8cee56b46fbfc4fb4101fc7ae4d50b246,"Explore the Images view in Docker Desktop
The Images view lets you manage Docker images without having to use the CLI. By default, it displays a list of all Docker images on your local disk.
You can also view Hub images once you have signed in to Docker Hub. This allows you to collaborate with your team and manage your images directly through Docker Desktop.
The Images view lets you perform core operations such as running an image as a container, pulling the latest version of an image from Docker Hub, pushing the image to Docker Hub, and inspecting images.
It also displays metadata about the image such as the:
- Tag
- Image ID
- Date created
- Size of the image.
An In Use tag displays next to images used by running and stopped containers. You can choose what information you want displayed by selecting the More options menu to the right of the search bar, and then use the toggle switches according to your preferences.
The Images on disk status bar displays the number of images and the total disk space used by the images and when this information was last refreshed.
Manage your images
Use the Search field to search for any specific image.
You can sort images by:
- In use
- Unused
- Dangling
Run an image as a container
From the Images view, hover over an image and select Run.
When prompted you can either:
- Select the Optional settings drop-down to specify a name, port, volumes, environment variables and select Run
- Select Run without specifying any optional settings.
Inspect an image
To inspect an image, select the image row. Inspecting an image displays detailed information about the image such as the:
- Image history
- Image ID
- Date the image was created
- Size of the image
- Layers making up the image
- Base images used
- Vulnerabilities found
- Packages inside the image
Docker Scout powers this vulnerability information. For more information about this view, see Image details view
Pull the latest image from Docker Hub
Select the image from the list, select the More options button and select Pull.
Note
The repository must exist on Docker Hub in order to pull the latest version of an image. You must be signed in to pull private images.
Push an image to Docker Hub
Select the image from the list, select the More options button and select Push to Hub.
Note
You can only push an image to Docker Hub if the image belongs to your Docker ID or your organization. That is, the image must contain the correct username/organization in its tag to be able to push it to Docker Hub.
Remove an image
Note
To remove an image used by a running or a stopped container, you must first remove the associated container.
An unused image is an image which is not used by any running or stopped containers. An image becomes dangling when you build a new version of the image with the same tag.
To remove individual images, select the bin icon.
Docker Hub repositories
The Images view also allows you to manage and interact with images in Docker Hub repositories. By default, when you go to Images in Docker Desktop, you see a list of images that exist in your local image store. The Local and Hub repositories tabs near the top toggles between viewing images in your local image store, and images in remote Docker Hub repositories that you have access to.
Switching to the Hub repositories tab prompts you to sign in to your Docker Hub account, if you're not already signed in. When signed in, it shows you a list of images in Docker Hub organizations and repositories that you have access to.
Select an organization from the drop-down to view a list of repositories for that organization.
If you have enabled Docker Scout on the repositories, image analysis results (and health scores if your Docker organization is eligible) appear next to the image tags.
Hovering over an image tag reveals two options:
- Pull: Pull the latest version of the image from Docker Hub.
- View in Hub: Open the Docker Hub page and display detailed information about the image.",,,
ee4a85b85b68f3d09ae19799fdffd4ed9716820629ddebe0ba5af8192c753ebe,"Using USB/IP with Docker Desktop
Note
Available on Docker Desktop for Mac, Linux, and Windows with the Hyper-V backend.
USB/IP enables you to share USB devices over the network, which can then be accessed from within Docker containers. This page focuses on sharing USB devices connected to the machine you run Docker Desktop on. You can repeat the following process to attach and use additional USB devices as needed.
Note
The Docker Desktop VM kernel image comes pre-configured with drivers for many common USB devices, but Docker can't guarantee every possible USB device will work with this setup.
Setup and use
Step one: Run a USB/IP server
To use USB/IP, you need to run a USB/IP server. For this guide, the implementation provided by jiegec/usbip will be used.
Clone the repository.
$ git clone https://github.com/jiegec/usbip $ cd usbip
Run the emulated Human Interface Device (HID) device example.
$ env RUST_LOG=info cargo run --example hid_keyboard
Step two: Start a privileged Docker container
To attach the USB device, start a privileged Docker container with the PID namespace set to host
:
$ docker run --rm -it --privileged --pid=host alpine
Step three: Enter the mount namespace of PID 1
Inside the container, enter the mount namespace of the init
process to gain access to the pre-installed USB/IP tools:
$ nsenter -t 1 -m
Step four: Use USB/IP tools
Now you can use the USB/IP tools as you would on any other system:
List USB devices
To list exportable USB devices from the host:
$ usbip list -r host.docker.internal
Expected output:
Exportable USB devices
======================
- host.docker.internal
0-0-0: unknown vendor : unknown product (0000:0000)
: /sys/bus/0/0/0
: (Defined at Interface level) (00/00/00)
: 0 - unknown class / unknown subclass / unknown protocol (03/00/00)
Attach a USB device
To attach a specific USB device, or the emulated keyboard in this case:
$ usbip attach -r host.docker.internal -d 0-0-0
Verify device attachment
After attaching the emulated keyboard, check the /dev/input
directory for the device node:
$ ls /dev/input/
Example output:
event0 mice
Step five: Use the attached device in another container
While the initial container remains running to keep the USB device operational, you can access the attached device from another container. For example:
Start a new container with the attached device.
$ docker run --rm -it --device ""/dev/input/event0"" alpine
Install a tool like
evtest
to test the emulated keyboard.$ apk add evtest $ evtest /dev/input/event0
Interact with the device, and observe the output.
Example output:
Input driver version is 1.0.1 Input device ID: bus 0x3 vendor 0x0 product 0x0 version 0x111 ... Properties: Testing ... (interrupt to exit) Event: time 1717575532.881540, type 4 (EV_MSC), code 4 (MSC_SCAN), value 7001e Event: time 1717575532.881540, type 1 (EV_KEY), code 2 (KEY_1), value 1 Event: time 1717575532.881540, -------------- SYN_REPORT ------------ ...
Important
The initial container must remain running to maintain the connection to the USB device. Exiting the container will stop the device from working.",,,
d0a1971a70b06399805f9b98a73ff9b09c4dcaafb188eac370834331e769fe3f,"General FAQs for Desktop
Can I use Docker Desktop offline?
Yes, you can use Docker Desktop offline. However, you cannot access features that require an active internet connection. Additionally, any functionality that requires you to sign in won't work while using Docker Desktop offline or in air-gapped environments. This includes:
- The resources in the Learning Center
- Pulling or pushing an image to Docker Hub
- Image Access Management
- Static vulnerability scanning
- Viewing remote images in the Docker Dashboard
- Setting up Dev Environments
- Docker Build when using
BuildKit.
You can work around this by disabling BuildKit. Run
DOCKER_BUILDKIT=0 docker build .
to disable BuildKit. - Kubernetes (Images are download when you enable Kubernetes for the first time)
- Checking for updates
- In-app diagnostics (including the Self-diagnose tool)
- Sending usage statistics
- When
networkMode
is set tomirrored
How do I connect to the remote Docker Engine API?
To connect to the remote Engine API, you might need to provide the location of the Engine API for Docker clients and development tools.
Mac and Windows WSL 2 users can connect to the Docker Engine through a Unix socket: unix:///var/run/docker.sock
.
If you are working with applications like
Apache Maven
that expect settings for DOCKER_HOST
and DOCKER_CERT_PATH
environment
variables, specify these to connect to Docker instances through Unix sockets.
For example:
$ export DOCKER_HOST=unix:///var/run/docker.sock
Docker Desktop Windows users can connect to the Docker Engine through a named pipe: npipe:////./pipe/docker_engine
, or TCP socket at this URL:
tcp://localhost:2375
.
For details, see Docker Engine API.
How do I connect from a container to a service on the host?
The host has a changing IP address, or none if you have no network access.
We recommend that you connect to the special DNS name host.docker.internal
,
which resolves to the internal IP address used by the host.
For more information and examples, see how to connect from a container to a service on the host.
Can I pass through a USB device to a container?
Docker Desktop does not support direct USB device passthrough. However, you can use USB over IP to connect common USB devices to the Docker Desktop VM and in turn be forwarded to a container. For more details, see Using USB/IP with Docker Desktop.
How do I run Docker Desktop without administrator privileges?
Docker Desktop requires administrator privileges only for installation. Once installed, administrator privileges are not needed to run it. However, for non-admin users to run Docker Desktop, it must be installed using a specific installer flag and meet certain prerequisites, which vary by platform.
To run Docker Desktop on Mac without requiring administrator privileges, install via the command line and pass the —user=<userid>
installer flag:
$ /Applications/Docker.app/Contents/MacOS/install --user=<userid>
You can then sign in to your machine with the user ID specified, and launch Docker Desktop.
Note
Before launching Docker Desktop, if a
settings-store.json
file (orsettings.json
for Docker Desktop versions 4.34 and earlier) already exists in the~/Library/Group Containers/group.com.docker/
directory, you will see a Finish setting up Docker Desktop window that prompts for administrator privileges when you select Finish. To avoid this, ensure you delete thesettings-store.json
file (orsettings.json
for Docker Desktop versions 4.34 and earlier) left behind from any previous installations before launching the application.
Note
If you are using the WSL 2 backend, first make sure that you meet the minimum required version for WSL 2. Otherwise, update WSL 2 first.
To run Docker Desktop on Windows without requiring administrator privileges, install via the command line and pass the —always-run-service
installer flag.
$ ""Docker Desktop Installer.exe"" install —always-run-service",,,
cac3edf54eb102c7947dccedfe9c260aba5dd43502d104ccd04eb7849f21030b,"What is Enhanced Container Isolation?
Enhanced Container Isolation (ECI) provides an additional layer of security to prevent malicious workloads running in containers from compromising Docker Desktop or the host.
It uses a variety of advanced techniques to harden container isolation, but without impacting developer productivity.
Enhanced Container Isolation ensures stronger container isolation and also locks in any security configurations that have been created by administrators, for instance through Registry Access Management policies or with Settings Management.
Note
ECI is in addition to other container security techniques used by Docker. For example, reduced Linux Capabilities, seccomp, and AppArmor.
Who is it for?
- For organizations and developers that want to prevent container attacks and reduce vulnerabilities in developer environments.
- For organizations that want to ensure stronger container isolation that is easy and intuitive to implement on developers' machines.
What happens when Enhanced Container Isolation is turned on?
When Enhanced Container Isolation is turned on, the following features and security techniques are enabled:
- All user containers are automatically run in Linux user namespaces which ensures stronger isolation. Each container runs in a dedicated Linux user-namespace.
- The root user in the container maps to an unprivileged user inside the Docker Desktop Linux VM.
- Containers become harder to breach. For example, sensitive system calls are vetted and portions of
/proc
and/sys
are emulated inside the container. - Users can continue using containers as usual, including bind mounting host directories, volumes, etc.
- No change in the way developers run containers, and no special container images are required.
- Privileged containers (e.g.,
--privileged
flag) work, but they are only privileged within the container's Linux user namespace, not in the Docker Desktop VM. Therefore they can't be used to breach the Docker Desktop VM. - Docker-in-Docker and even Kubernetes-in-Docker works, but run unprivileged inside the Docker Desktop Linux VM.
In addition, the following restrictions are imposed:
- Containers can no longer share namespaces with the Docker Desktop VM (e.g.,
--network=host
,--pid=host
are disallowed). - Containers can no longer modify configuration files inside the Docker Desktop VM (e.g., mounting any VM directory into the container is disallowed).
- Containers can no longer access the Docker Engine. For example, mounting the Docker Engine's socket into the container is restricted which prevents malicious containers from gaining control of the Docker Engine. Administrators can relax this for trusted container images.
- Console access to the Docker Desktop VM is forbidden for all users.
These features and restrictions ensure that containers are better secured at runtime, with minimal impact to developer experience and productivity. Developers can continue to use Docker Desktop as usual, but the containers they launch are more strongly isolated.
For more information on how Enhanced Container Isolation work, see How does it work.
Important
ECI protection for Docker builds and Kubernetes in Docker Desktop varies according to the Docker Desktop version. Later versions include more protection than earlier versions. Also, ECI does not yet protect extension containers. For more information on known limitations and workarounds, see FAQs.
How do I enable Enhanced Container Isolation?
As a developer
To enable Enhanced Container Isolation as a developer:
- Ensure your organization has a Docker Business subscription.
- Sign in to your organization in Docker Desktop. This will ensure the ECI feature is available to you in Docker Desktop's Settings menu.
- Stop and remove all existing containers.
- Navigate to Settings > General in Docker Desktop.
- Next to Use Enhanced Container Isolation, select the checkbox.
- Select Apply and restart to save your settings.
Important
Enhanced Container Isolation does not protect containers created prior to enabling ECI. For more information on known limitations and workarounds, see FAQs.
As an administrator
Prerequisite
You first need to enforce sign-in to ensure that all Docker Desktop developers authenticate with your organization. Since Settings Management requires a Docker Business subscription, enforced sign-in guarantees that only authenticated users have access and that the feature consistently takes effect across all users, even though it may still work without enforced sign-in.
Setup
Create and configure the admin-settings.json
file and specify:
{
""configurationFileVersion"": 2,
""enhancedContainerIsolation"": {
""value"": true,
""locked"": true
}
}
Setting ""value"": true
ensures ECI is enabled by default. By
setting ""locked"": true
, ECI can't be disabled by
developers. If you want to give developers the ability to disable the feature,
set ""locked"": false
.
In addition, you can also configure Docker socket mount permissions for containers.
For this to take effect:
- On a new install, developers need to launch Docker Desktop and authenticate to their organization.
- On an existing install, developers need to quit Docker Desktop through the Docker menu, and then relaunch Docker Desktop. If they are already signed in, they don’t need to sign in again for the changes to take effect.
Important
Selecting Restart from the Docker menu isn't enough as it only restarts some components of Docker Desktop.
What do users see when this setting is enforced by an administrator?
Tip
You can now also configure these settings in the Docker Admin Console.
When Enhanced Container Isolation is enabled, users see:
- Use Enhanced Container Isolation toggled on in Settings > General.
- Containers run within a Linux user namespace.
To check, run:
$ docker run --rm alpine cat /proc/self/uid_map
The following output displays:
0 100000 65536
This indicates that the container's root user (0) maps to unprivileged user (100000) in the Docker Desktop VM, and that the mapping extends for a range of 64K user-IDs. If a container process were to escape the container, it would find itself without privileges at the VM level. The user-ID mapping varies with each new container, as each container gets an exclusive range of host User-IDs for isolation. User-ID mapping is automatically managed by Docker Desktop. For further details, see How Enhanced Container Isolation works.
In contrast, without ECI the Linux user namespace is not used for containers, the following displays:
0 0 4294967295
This means that the root user in the container (0) is in fact the root user in the Docker Desktop VM (0) which reduces container isolation.
Since Enhanced Container Isolation
uses the Sysbox container runtime embedded in the Docker Desktop Linux VM, another way to determine if a container is running with Enhanced Container Isolation is by using docker inspect
:
$ docker inspect --format='{{.HostConfig.Runtime}}' my_container
It outputs:
sysbox-runc
Without Enhanced Container Isolation, docker inspect
outputs runc
, which is the standard OCI runtime.",,,
ac26dca885f1b6f0a216ee63e16c7002f973158d2e4e6e1fdc1dc96702b75e35,"Roles and permissions
Organization and company owners can assign roles to individuals giving them different permissions in the organization. This guide outlines Docker's organization roles and their permission scopes.
Roles
When you invite users to your organization, you assign them a role. A role is a collection of permissions. Roles define whether users can create repositories, pull images, create teams, and configure organization settings.
The following roles are available to assign:
- Member: Non-administrative role. Members can view other members that are in the same organization.
- Editor: Partial administrative access to the organization. Editors can create, edit, and delete repositories. They can also edit an existing team's access permissions.
- Organization owner: Full organization administrative access. Organization owners can manage organization repositories, teams, members, settings, and billing.
- Company owner: In addition to the permissions of an organization owner, company owners can configure settings for their associated organizations.
Owners can manage roles for members of an organization using Docker Hub or the Admin Console:
- Update a member role in Docker Hub
- Update an organization's members or company in the Admin Console
- Learn more about organizations and companies
Permissions
Note
Company owners have the same access as organization owners for all associated organizations. For more information, see Company overview.
The following sections describe the permissions for each role.
Content and registry permissions
The following table outlines content and registry permissions for member, editor, and organization owner roles. These permissions and roles apply to the entire organization, including all the repositories in the namespace for the organization.
| Permission | Member | Editor | Organization owner |
|---|---|---|---|
| Explore images and extensions | ✅ | ✅ | ✅ |
| Star, favorite, vote, and comment on content | ✅ | ✅ | ✅ |
| Pull images | ✅ | ✅ | ✅ |
| Create and publish an extension | ✅ | ✅ | ✅ |
| Become a Verified, Official, or Open Source publisher | ❌ | ❌ | ✅ |
| Observe content engagement as a publisher | ❌ | ❌ | ✅ |
| Create public and private repositories | ❌ | ✅ | ✅ |
| Edit and delete repositories | ❌ | ✅ | ✅ |
| Manage tags | ❌ | ✅ | ✅ |
| View repository activity | ❌ | ❌ | ✅ |
| Set up Automated builds | ❌ | ❌ | ✅ |
| Edit build settings | ❌ | ❌ | ✅ |
| View teams | ✅ | ✅ | ✅ |
| Assign team permissions to repositories | ❌ | ✅ | ✅ |
When you add members to a team, you can manage their repository permissions. For team repository permissions, see Create and manage a team permissions reference.
The following diagram provides an example of how permissions may work for a user. In this example, the first permission check is for the role: member or editor. Editors have administrative permissions for repositories across the namespace of the organization. Members may have administrative permissions for a repository if they're a member of a team that grants those permissions.
Organization management permissions
The following table outlines organization management permissions for member, editor, organization owner, and company owner roles.
| Permission | Member | Editor | Organization owner | Company owner |
|---|---|---|---|---|
| Create teams | ❌ | ❌ | ✅ | ✅ |
| Manage teams (including delete) | ❌ | ❌ | ✅ | ✅ |
| Configure the organization's settings (including linked services) | ❌ | ❌ | ✅ | ✅ |
| Add organizations to a company | ❌ | ❌ | ✅ | ✅ |
| Invite members | ❌ | ❌ | ✅ | ✅ |
| Manage members | ❌ | ❌ | ✅ | ✅ |
| Manage member roles and permissions | ❌ | ❌ | ✅ | ✅ |
| View member activity | ❌ | ❌ | ✅ | ✅ |
| Export and reporting | ❌ | ❌ | ✅ | ✅ |
| Image Access Management | ❌ | ❌ | ✅ | ✅ |
| Registry Access Management | ❌ | ❌ | ✅ | ✅ |
| Set up Single Sign-On (SSO) and SCIM | ❌ | ❌ | ✅ * | ✅ |
| Require Docker Desktop sign-in | ❌ | ❌ | ✅ * | ✅ |
| Manage billing information (for example, billing address) | ❌ | ❌ | ✅ | ✅ |
| Manage payment methods (for example, credit card or invoice) | ❌ | ❌ | ✅ | ✅ |
| View billing history | ❌ | ❌ | ✅ | ✅ |
| Manage subscriptions | ❌ | ❌ | ✅ | ✅ |
| Manage seats | ❌ | ❌ | ✅ | ✅ |
| Upgrade and downgrade plans | ❌ | ❌ | ✅ | ✅ |
* If not part of a company
Docker Scout permissions
The following table outlines Docker Scout management permissions for member, editor, and organization owner roles.
| Permission | Member | Editor | Organization owner |
|---|---|---|---|
| View and compare analysis results | ✅ | ✅ | ✅ |
| Upload analysis records | ✅ | ✅ | ✅ |
| Activate and deactivate Docker Scout for a repository | ❌ | ✅ | ✅ |
| Create environments | ❌ | ❌ | ✅ |
| Manage registry integrations | ❌ | ❌ | ✅ |
Docker Build Cloud permissions
The following table outlines Docker Build Cloud management permissions for member, editor, and organization owner roles.
| Permission | Member | Editor | Organization owner |
|---|---|---|---|
| Use a cloud builder | ✅ | ✅ | ✅ |
| Create and remove builders | ✅ | ✅ | ✅ |
| Configure builder settings | ✅ | ✅ | ✅ |
| Buy minutes | ❌ | ❌ | ✅ |
| Manage subscription | ❌ | ❌ | ✅ |",,,
f6c10b5143dd26f8bf6c45af9e7172b86916813f4d79b751fe9cc6533c4d1894,"Docker-Sponsored Open Source Program
Docker-Sponsored Open Source images are published and maintained by open-source projects sponsored by Docker through the program.
Images that are part of this program have a special badge on Docker Hub making it easier for users to identify projects that Docker has verified as trusted, secure, and active open-source projects.
The Docker-Sponsored Open Source (DSOS) Program provides several features and benefits to non-commercial open source developers.
The program grants the following perks to eligible projects:
- Repository logo
- Verified Docker-Sponsored Open Source badge
- Insights and analytics
- Access to Docker Scout for software supply chain management
- Removal of rate limiting for developers
- Improved discoverability on Docker Hub
These benefits are valid for one year and publishers can renew annually if the project still meets the program requirements. Program members and all users pulling public images from the project namespace get access to unlimited pulls and unlimited egress.
Repository logo
DSOS organizations can upload custom images for individual repositories on Docker Hub. This lets you override the default organization-level logo on a per-repository basis.
Only a user with administrative access (owner or team member with administrator permission) over the repository can change the repository logo.
Image requirements
- The supported filetypes for the logo image are JPEG and PNG.
- The minimum allowed image size in pixels is 120×120.
- The maximum allowed image size in pixels is 1000×1000.
- The maximum allowed image file size is 5MB.
Set the repository logo
- Sign in to Docker Hub.
- Go to the page of the repository that you want to change the logo for.
- Select the upload logo button, represented by a camera icon ( ) overlaying the current repository logo.
- In the dialog that opens, select the PNG image that you want to upload to set it as the logo for the repository.
Remove the logo
Select the Clear button ( ) to remove a logo.
Removing the logo makes the repository default to using the organization logo, if set, or the following default logo if not.
Verified Docker-Sponsored Open Source badge
Docker verifies that developers can trust images with this badge on Docker Hub as an active open source project.
Insights and analytics
The insights and analytics service provides usage metrics for how the community uses Docker images, granting insight into user behavior.
The usage metrics show the number of image pulls by tag or by digest, and breakdowns by geolocation, cloud provider, client, and more.
You can select the time span for which you want to view analytics data. You can also export the data in either a summary or raw format.
Docker Scout
DSOS projects can enable Docker Scout on up to 100 repositories for free. Docker Scout provides automatic image analysis, policy evaluation for improved supply chain management, integrations with third-party systems like CI platforms and source code management, and more.
You can enable Docker Scout on a per-repository basis. For information about how to use this product, refer to the Docker Scout documentation.
Who's eligible for the Docker-Sponsored Open Source program?
To qualify for the program, a publisher must share the project namespace in public repositories, meet the Open Source Initiative definition, and be in active development with no pathway to commercialization.
Find out more by heading to the Docker-Sponsored Open Source Program application page.",,,
95c044f65fcf54c5b24e204ad5b775a3daab035ff33b01e513b691d9ed16b9a1,"Docker Engine 20.10 release notes
This document describes the latest changes, additions, known issues, and fixes for Docker Engine version 20.10.
20.10.24
2023-04-04Updates
- Update Go runtime to 1.19.7.
- Update Docker Buildx to v0.10.4.
- Update containerd to v1.6.20.
- Update runc to v1.1.5.
Bug fixes and enhancements
- Fixed a number of issues that can cause Swarm encrypted overlay networks
to fail to uphold their guarantees, addressing
CVE-2023-28841,
CVE-2023-28840, and
CVE-2023-28842.
- A lack of kernel support for encrypted overlay networks now reports as an error.
- Encrypted overlay networks are eagerly set up, rather than waiting for multiple nodes to attach.
- Encrypted overlay networks are now usable on Red Hat Enterprise Linux 9
through the use of the
xt_bpf
kernel module. - Users of Swarm overlay networks should review GHSA-vwm3-crmr-xfxw to ensure that unintentional exposure has not occurred.
- Upgrade github.com/containerd/fifo to v1.1.0 to fix a potential panic moby/moby#45216.
- Fix missing Bash completion for installed cli-plugins docker/cli#4091.
20.10.23
2023-01-19This release of Docker Engine contains updated versions of Docker Compose, Docker Buildx, containerd, and some minor bug fixes and enhancements.
Updates
- Update Docker Compose to v2.15.1.
- Update Docker Buildx to v0.10.0.
- Update containerd (
containerd.io
package) to v1.6.15. - Update the package versioning format for
docker-compose-cli
to allow distribution version updates docker/docker-ce-packaging#822. - Update Go runtime to 1.18.10,
Bug fixes and enhancements
Fix an issue where
docker build
would fail when using--add-host=host.docker.internal:host-gateway
with BuildKit enabled moby/moby#44650.Revert seccomp: block socket calls to
AF_VSOCK
in default profile moby/moby#44712.This change, while favorable from a security standpoint, caused a change in behavior for some use-cases. As such, we are reverting it to ensure stability and compatibility for the affected users.
However, users of
AF_VSOCK
in containers should recognize that this (special) address family is not currently namespaced in any version of the Linux kernel, and may result in unexpected behavior, like containers communicating directly with host hypervisors.Future releases, will filter
AF_VSOCK
. Users who need to allow containers to communicate over the unnamespacedAF_VSOCK
will need to turn off seccomp confinement or set a custom seccomp profile.
20.10.22
2022-12-16This release of Docker Engine contains updated versions of Docker Compose, Docker Scan, containerd, and some minor bug fixes and enhancements.
Updates
- Update Docker Compose to v2.14.1.
- Update Docker Scan to v0.23.0.
- Update containerd (
containerd.io
package) to v1.6.13, to include a fix for CVE-2022-23471. - Update Go runtime to 1.18.9, to include fixes for CVE-2022-41716, CVE-2022-41717, and CVE-2022-41720.
Bug fixes and enhancements
- Improve error message when attempting to pull an unsupported image format or OCI artifact moby/moby#44413, moby/moby#44569.
- Fix an issue where the host's ephemeral port-range was ignored when selecting random ports for containers moby/moby#44476.
- Fix
ssh: parse error in message type 27
errors duringdocker build
on hosts using OpenSSH 8.9 or above moby/moby#3862. - seccomp: block socket calls to
AF_VSOCK
in default profile moby/moby#44564.
20.10.21
2022-10-25This release of Docker Engine contains updated versions of Docker Compose, Docker Scan, containerd, added packages for Ubuntu 22.10, and some minor bug fixes and enhancements.
New
- Provide packages for Ubuntu 22.10 (Kinetic Kudu).
- Add support for
allow-nondistributable-artifacts
towards Docker Hub moby/moby#44313.
Updates
- Update Docker Compose to v2.12.2.
- Update Docker Scan to v0.21.0.
- Update containerd (
containerd.io
package) to v1.6.9. - Update bundled BuildKit version to fix
output clipped, log limit 1MiB reached
errors moby/moby#44339.
Bug fixes and enhancements
- Remove experimental gate for
--platform
in bash completion docker/cli#3824. - Fix an
Invalid standard handle identifier
panic when registering the Docker Engine as a service from a legacy CLI on Windows moby/moby#44326. - Fix running Git commands in Cygwin on Windows moby/moby#44332.
20.10.20
2022-10-18This release of Docker Engine contains partial mitigations for a Git vulnerability
(
CVE-2022-39253),
and has updated handling of image:tag@digest
image references.
The Git vulnerability allows a maliciously crafted Git repository, when used as a build context, to copy arbitrary filesystem paths into resulting containers/images; this can occur in both the daemon, and in API clients, depending on the versions and tools in use.
The mitigations available in this release and in other consumers of the daemon API
are partial and only protect users who build a Git URL context (e.g. git+protocol://
).
As the vulnerability could still be exploited by manually run Git commands that interact
with and check out submodules, users should immediately upgrade to a patched version of
Git to protect against this vulnerability. Further details are available from the GitHub
blog (
""Git security vulnerabilities announced"").
Updates
- Update Docker Compose to v2.12.0.
- Updated handling of
image:tag@digest
references. When pulling an image using theimage:tag@digest
(""pull by digest""), image resolution happens through the content-addressable digest and theimage
andtag
are not used. While this is expected, this could lead to confusing behavior, and could potentially be exploited through social engineering to run an image that is already present in the local image store. Docker now checks if the digest matches the repository name used to pull the image, and otherwise will produce an error. - Updated handling of
image:tag@digest
references. Refer to the ""Daemon"" section above for details.
Bug fixes and enhancements
- Added a mitigation for CVE-2022-39253, when using the classic Builder with a Git URL as the build context.
- Added a mitigation to the classic Builder and updated BuildKit to v0.8.3-31-gc0149372, for CVE-2022-39253.
20.10.19
2022-10-14This release of Docker Engine comes with some bug-fixes, and an updated version of Docker Compose.
Updates
- Update Docker Compose to v2.11.2.
- Update Go runtime to 1.18.7, which contains fixes for CVE-2022-2879, CVE-2022-2880, and CVE-2022-41715.
Bug fixes and enhancements
- Fix an issue that could result in a panic during
docker builder prune
ordocker system prune
moby/moby#44122. - Fix a bug where using
docker volume prune
would remove volumes that were still in use if the daemon was running with ""live restore"" and was restarted moby/moby#44238.
20.10.18
2022-09-09This release of Docker Engine comes with a fix for a low-severity security issue,
some minor bug fixes, and updated versions of Docker Compose, Docker Buildx,
containerd
, and runc
.
Updates
- Update Docker Buildx to v0.9.1.
- Update Docker Compose to v2.10.2.
- Update containerd (
containerd.io
package) to v1.6.8. - Update runc version to v1.1.4.
- Update Go runtime to 1.18.6, which contains fixes for CVE-2022-27664 and CVE-2022-32190.
Bug fixes and enhancements
- Add Bash completion for Docker Compose docker/cli#3752.
- Fix an issue where file-capabilities were not preserved during build moby/moby#43876.
- Fix an issue that could result in a panic caused by a concurrent map read and map write moby/moby#44067.
- Fix a security vulnerability relating to supplementary group permissions, which could allow a container process to bypass primary group restrictions within the container CVE-2022-36109, GHSA-rc4r-wh2q-q6c4.
- seccomp: add support for Landlock syscalls in default policy moby/moby#43991.
- seccomp: update default policy to support new syscalls introduced in kernel 5.12 - 5.16 moby/moby#43991.
- Fix an issue where cache lookup for image manifests would fail, resulting in a redundant round-trip to the image registry moby/moby#44109.
- Fix an issue where
exec
processes and healthchecks were not terminated when they timed out moby/moby#44018.
20.10.17
2022-06-06This release of Docker Engine comes with updated versions of Docker Compose and the
containerd
, and runc
components, as well as some minor bug fixes.
Updates
- Update Docker Compose to v2.6.0.
- Update containerd (
containerd.io
package) to v1.6.6, which contains a fix for CVE-2022-31030 - Update runc version to v1.1.2, which contains a fix for CVE-2022-29162.
- Update Go runtime to 1.17.11, which contains fixes for CVE-2022-30634, CVE-2022-30629, CVE-2022-30580 and CVE-2022-29804
Bug fixes and enhancements
- Remove asterisk from docker commands in zsh completion script docker/cli#3648.
- Fix Windows port conflict with published ports in host mode for overlay moby/moby#43644.
- Ensure performance tuning is always applied to libnetwork sandboxes moby/moby#43683.
20.10.16
2022-05-12This release of Docker Engine fixes a regression in the Docker CLI builds for
macOS, fixes an issue with docker stats
when using containerd 1.5 and up,
and updates the Go runtime to include a fix for
CVE-2022-29526.
Updates
- Update golang.org/x/sys dependency which contains a fix for CVE-2022-29526.
- Updated the
golang.org/x/sys
build-time dependency which contains a fix for CVE-2022-29526. - Updated Go runtime to 1.17.10, which contains a fix for CVE-2022-29526.
Bug fixes and enhancements
- Fixed a regression in binaries for macOS introduced in 20.10.15, which resulted in a panic docker/cli#43426.
- Fixed an issue where
docker stats
was showing empty stats when running with containerd 1.5.0 or up moby/moby#43567. - Used ""weak"" dependencies for the
docker scan
CLI plugin, to prevent a ""conflicting requests"" error when users performed an off-line installation from downloaded RPM packages docker/docker-ce-packaging#659.
20.10.15
2022-05-05This release of Docker Engine comes with updated versions of the compose
,
buildx
, containerd
, and runc
components, as well as some minor bug fixes.
Updates
- Update Docker Compose to v2.5.0.
- Update Docker Buildx to v0.8.2.
- Update Go runtime to 1.17.9.
- Update containerd (
containerd.io
package) to v1.6.4. - Update runc version to v1.1.1.
Bug fixes and enhancements
- Use a RWMutex for stateCounter to prevent potential locking congestion moby/moby#43426.
- Prevent an issue where the daemon was unable to find an available IP-range in some conditions moby/moby#43360
- Add packages for CentOS 9 stream and Fedora 36.
Known issues
- We've identified an issue with the macOS CLI binaries in the 20.10.15 release. This issue has been resolved in the 20.10.16 release.
20.10.14
2022-03-23This release of Docker Engine updates the default inheritable capabilities for
containers to address
CVE-2022-24769,
a new version of the containerd.io
runtime is also included to address the same
issue.
Updates
- Update the default inheritable capabilities.
- Update the default inheritable capabilities for containers used during build.
- Update containerd (
containerd.io
package) to v1.5.11. - Update
docker buildx
to v0.8.1.
20.10.13
2022-03-10This release of Docker Engine contains some bug-fixes and packaging changes,
updates to the docker scan
and docker buildx
commands, an updated version of
the Go runtime, and new versions of the containerd.io
runtime.
Together with this release, we now also provide .deb
and .rpm
packages of
Docker Compose V2, which can be installed using the (optional) docker-compose-plugin
package.
New
- Provide
.deb
and.rpm
packages for Docker Compose V2. Docker Compose v2.3.3 can now be installed on Linux using thedocker-compose-plugin
packages, which provides thedocker compose
subcommand on the Docker CLI. The Docker Compose plugin can also be installed and run standalone to be used as a drop-in replacement fordocker-compose
(Docker Compose V1) docker/docker-ce-packaging#638. Thecompose-cli-plugin
package can also be used on older version of the Docker CLI with support for CLI plugins (Docker CLI 18.09 and up). - Provide packages for the upcoming Ubuntu 22.04 ""Jammy Jellyfish"" LTS release docker/docker-ce-packaging#645, docker/containerd-packaging#271.
Updates
- Updated the bundled version of buildx to v0.8.0.
- Update
docker buildx
to v0.8.0. - Update
docker scan
(docker-scan-plugin
) to v0.17.0. - Update containerd (
containerd.io
package) to v1.5.10. - Update the bundled runc version to v1.0.3.
- Update Golang runtime to Go 1.16.15.
- Updates the fluentd log driver to prevent a potential daemon crash, and prevent
containers from hanging when using the
fluentd-async-connect=true
and the remote server is unreachable moby/moby#43147.
Bug fixes and enhancements
- Fix a race condition when updating the container's state moby/moby#43166.
- Update the etcd dependency to prevent the daemon from incorrectly holding file locks moby/moby#43259
- Fix detection of user-namespaces when configuring the default
net.ipv4.ping_group_range
sysctl moby/moby#43084. - Retry downloading image-manifests if a connection failure happens during image pull moby/moby#43333.
- Various fixes in command-line reference and API documentation.
- Prevent an OOM when using the ""local"" logging driver with containers that produce a large amount of log messages moby/moby#43165.
20.10.12
2021-12-13
This release of Docker Engine contains changes in packaging only, and provides
updates to the docker scan
and docker buildx
commands. Versions of docker scan
before v0.11.0 are not able to detect the
Log4j 2 CVE-2021-44228.
We are shipping an updated version of docker scan
in this release to help you
scan your images for this vulnerability.
Note
The
docker scan
command on Linux is currently only supported on x86 platforms. We do not yet provide a package for other hardware architectures on Linux.
The docker scan
feature is provided as a separate package and, depending on your
upgrade or installation method, 'docker scan' may not be updated automatically to
the latest version. Use the instructions below to update docker scan
to the latest
version. You can also use these instructions to install, or upgrade the docker scan
package without upgrading the Docker Engine:
On .deb
based distributions, such as Ubuntu and Debian:
$ apt-get update && apt-get install docker-scan-plugin
On rpm-based distributions, such as CentOS or Fedora:
$ yum install docker-scan-plugin
After upgrading, verify you have the latest version of docker scan
installed:
$ docker scan --accept-license --version
Version: v0.12.0
Git commit: 1074dd0
Provider: Snyk (1.790.0 (standalone))
Read our blog post on CVE-2021-44228
to learn how to use the docker scan
command to check if images are vulnerable.
Packaging
20.10.11
2021-11-17
Important
Due to net/http changes in Go 1.16, HTTP proxies configured through the
$HTTP_PROXY
environment variable are no longer used for TLS (https://
) connections. Make sure you also set an$HTTPS_PROXY
environment variable for handling requests tohttps://
URLs. Refer to Configure the daemon to use a proxy to learn how to configure the Docker Daemon to use a proxy server.
Distribution
- Handle ambiguous OCI manifest parsing to mitigate CVE-2021-41190 / GHSA-mc8v-mgrf-8f4m. See GHSA-xmmx-7jpf-fx42 for details.
Windows
- Fix panic.log file having read-only attribute set moby/moby#42987.
Packaging
- Update containerd to v1.4.12 to mitigate CVE-2021-41190.
- Update Golang runtime to Go 1.16.10.
20.10.10
2021-10-25
Important
Due to net/http changes in Go 1.16, HTTP proxies configured through the
$HTTP_PROXY
environment variable are no longer used for TLS (https://
) connections. Make sure you also set an$HTTPS_PROXY
environment variable for handling requests tohttps://
URLs. Refer to the HTTP/HTTPS proxy section to learn how to configure the Docker Daemon to use a proxy server.
Builder
- Fix platform-matching logic to fix
docker build
using not finding images in the local image cache on Arm machines when using BuildKit moby/moby#42954
Runtime
- Add support for
clone3
syscall in the default seccomp policy to support running containers based on recent versions of Fedora and Ubuntu. moby/moby/#42836. - Windows: update hcsshim library to fix a bug in sparse file handling in container layers, which was exposed by recent changes in Windows moby/moby#42944.
- Fix some situations where
docker stop
could hang forever moby/moby#42956.
Swarm
- Fix an issue where updating a service did not roll back on failure moby/moby#42875.
Packaging
- Add packages for Ubuntu 21.10 ""Impish Indri"" and Fedora 35.
- Update
docker scan
to v0.9.0 - Update Golang runtime to Go 1.16.9.
20.10.9
2021-10-04
This release is a security release with security fixes in the CLI, runtime, as well as updated versions of the containerd.io package.
Important
Due to net/http changes in Go 1.16, HTTP proxies configured through the
$HTTP_PROXY
environment variable are no longer used for TLS (https://
) connections. Make sure you also set an$HTTPS_PROXY
environment variable for handling requests tohttps://
URLs. Refer to the HTTP/HTTPS proxy section to learn how to configure the Docker Daemon to use a proxy server.
Client
- CVE-2021-41092 Ensure default auth config has address field set, to prevent credentials being sent to the default registry.
Runtime
- CVE-2021-41089
Create parent directories inside a chroot during
docker cp
to prevent a specially crafted container from changing permissions of existing files in the host’s filesystem. - CVE-2021-41091
Lock down file permissions to prevent unprivileged users from discovering and
executing programs in
/var/lib/docker
.
Packaging
Known issue
The
ctr
binary shipping with the static packages of this release is not statically linked, and will not run in Docker images using alpine as a base image. Users can install thelibc6-compat
package, or download a previous version of thectr
binary as a workaround. Refer to the containerd ticket related to this issue for more details: containerd/containerd#5824.
- Update Golang runtime to Go 1.16.8, which contains fixes for CVE-2021-36221 and CVE-2021-39293
- Update static binaries and containerd.io rpm and deb packages to containerd v1.4.11 and runc v1.0.2 to address CVE-2021-41103.
- Update the bundled buildx version to v0.6.3 for rpm and deb packages.
20.10.8
2021-08-03
Important
Due to net/http changes in Go 1.16, HTTP proxies configured through the
$HTTP_PROXY
environment variable are no longer used for TLS (https://
) connections. Make sure you also set an$HTTPS_PROXY
environment variable for handling requests tohttps://
URLs. Refer to the HTTP/HTTPS proxy section to learn how to configure the Docker Daemon to use a proxy server.
Deprecation
- Deprecate support for encrypted TLS private keys. Legacy PEM encryption as specified in RFC 1423 is insecure by design. Because it does not authenticate the ciphertext, it is vulnerable to padding oracle attacks that can let an attacker recover the plaintext. Support for encrypted TLS private keys is now marked as deprecated, and will be removed in an upcoming release. docker/cli#3219
- Deprecate Kubernetes stack support. Following the deprecation of
Compose on Kubernetes,
support for Kubernetes in the
stack
andcontext
commands in the Docker CLI is now marked as deprecated, and will be removed in an upcoming release docker/cli#3174.
Client
- Fix
Invalid standard handle identifier
errors on Windows docker/cli#3132.
Rootless
- Avoid
can't open lock file /run/xtables.lock: Permission denied
error on SELinux hosts moby/moby#42462. - Disable overlay2 when running with SELinux to prevent permission denied errors moby/moby#42462.
- Fix
x509: certificate signed by unknown authority
error on openSUSE Tumbleweed moby/moby#42462.
Runtime
- Print a warning when using the
--platform
option to pull a single-arch image that does not match the specified architecture moby/moby#42633. - Fix incorrect
Your kernel does not support swap memory limit
warning when running with cgroups v2 moby/moby#42479. - Windows: Fix a situation where containers were not stopped if
HcsShutdownComputeSystem
returned anERROR_PROC_NOT_FOUND
error moby/moby#42613
Swarm
- Fix a possibility where overlapping IP addresses could exist as a result of the node failing to clean up its old loadbalancer IPs moby/moby#42538
- Fix a deadlock in log broker (""dispatcher is stopped"") moby/moby#42537
Packaging
Known issue
The
ctr
binary shipping with the static packages of this release is not statically linked, and will not run in Docker images using alpine as a base image. Users can install thelibc6-compat
package, or download a previous version of thectr
binary as a workaround. Refer to the containerd ticket related to this issue for more details: containerd/containerd#5824.
- Remove packaging for Ubuntu 16.04 ""Xenial"" and Fedora 32, as they reached EOL docker/docker-ce-packaging#560
- Update Golang runtime to Go 1.16.6
- Update the bundled buildx version to v0.6.1 for rpm and deb packages docker/docker-ce-packaging#562
- Update static binaries and containerd.io rpm and deb packages to containerd v1.4.9 and runc v1.0.1: docker/containerd-packaging#241, docker/containerd-packaging#245, docker/containerd-packaging#247.
20.10.7
2021-06-02
Client
- Suppress warnings for deprecated cgroups docker/cli#3099.
- Prevent sending
SIGURG
signals to container on Linux and macOS. The Go runtime (starting with Go 1.14) usesSIGURG
signals internally as an interrupt to support preemptable syscalls. In situations where the Docker CLI was attached to a container, these interrupts were forwarded to the container. This fix changes the Docker CLI to ignoreSIGURG
signals docker/cli#3107, moby/moby#42421.
Builder
- Update BuildKit to version v0.8.3-3-g244e8cde
moby/moby#42448:
- Transform relative mountpoints for exec mounts in the executor to work around a breaking change in runc v1.0.0-rc94 and up. moby/buildkit#2137.
- Add retry on image push 5xx errors. moby/buildkit#2043.
- Fix build-cache not being invalidated when renaming a file that is copied using
a
COPY
command with a wildcard. Note that this change invalidates existing build caches for copy commands that use a wildcard. moby/buildkit#2018. - Fix build-cache not being invalidated when using mounts moby/buildkit#2076.
- Fix build failures when
FROM
image is not cached when using legacy schema 1 images moby/moby#42382.
Logging
- Update the hcsshim SDK to make daemon logs on Windows less verbose moby/moby#42292.
Rootless
- Fix capabilities not being honored when an image was built on a daemon with user-namespaces enabled moby/moby#42352.
Networking
- Update libnetwork to fix publishing ports on environments with kernel boot
parameter
ipv6.disable=1
, and to fix a deadlock causing internal DNS lookups to fail moby/moby#42413.
Contrib
- Update rootlesskit to v0.14.2 to fix a timeout when starting the userland proxy
with the
slirp4netns
port driver moby/moby#42294. - Fix ""Device or resource busy"" errors when running docker-in-docker on a rootless daemon moby/moby#42342.
Packaging
- Update containerd to v1.4.6, runc v1.0.0-rc95 to address CVE-2021-30465 moby/moby#42398, moby/moby#42395, docker/containerd-packaging#234
- Update containerd to v1.4.5, runc v1.0.0-rc94 moby/moby#42372, moby/moby#42388, docker/containerd-packaging#232.
- Update Docker Scan plugin packages (
docker-scan-plugin
) to v0.8 docker/docker-ce-packaging#545.
20.10.6
2021-04-12
Client
- Apple Silicon (darwin/arm64) support for Docker CLI docker/cli#3042
- config: print deprecation warning when falling back to pre-v1.7.0 config file
~/.dockercfg
. Support for this file will be removed in a future release docker/cli#3000
Builder
- Fix classic builder silently ignoring unsupported Dockerfile options and prompt to enable BuildKit instead moby/moby#42197
Logging
- json-file: fix sporadic unexpected EOF errors moby/moby#42174
Networking
- Fix a regression in docker 20.10, causing IPv6 addresses no longer to be bound by default when mapping ports moby/moby#42205
- Fix implicit IPv6 port-mappings not included in API response. Before docker 20.10, published ports were accessible through both IPv4 and IPv6 by default, but the API only included information about the IPv4 (0.0.0.0) mapping moby/moby#42205
- Fix a regression in docker 20.10, causing the docker-proxy to not be terminated in all cases moby/moby#42205
- Fix iptables forwarding rules not being cleaned up upon container removal moby/moby#42205
Packaging
- Update containerd to v1.4.4 for static binaries. The containerd.io package on apt/yum repos already had this update out of band. Includes a fix for CVE-2021-21334. moby/moby#42124
- Packages for Debian/Raspbian 11 Bullseye, Ubuntu 21.04 Hirsute Hippo and Fedora 34 docker/docker-ce-packaging#521 docker/docker-ce-packaging#522 docker/docker-ce-packaging#533
- Provide the
Docker Scan CLI plugin on Linux amd64 via a
docker-scan-plugin
package as a recommended dependency for thedocker-ce-cli
package docker/docker-ce-packaging#537 - Include VPNKit binary for arm64 moby/moby#42141
Plugins
- Fix docker plugin create making plugins that were incompatible with older versions of Docker moby/moby#42256
Rootless
- Update RootlessKit to v0.14.1 (see also v0.14.0 v0.13.2) moby/moby#42186 moby/moby#42232
- dockerd-rootless-setuptool.sh: create CLI context ""rootless"" moby/moby#42109
- dockerd-rootless.sh: prohibit running as root moby/moby#42072
- Fix ""operation not permitted"" when bind mounting existing mounts moby/moby#42233
- overlay2: fix ""createDirWithOverlayOpaque(...) ... input/output error"" moby/moby#42235
- overlay2: support ""userxattr"" option (kernel 5.11) moby/moby#42168
- btrfs: allow unprivileged user to delete subvolumes (kernel >= 4.18) moby/moby#42253
- cgroup2: Move cgroup v2 out of experimental moby/moby#42263
20.10.5
2021-03-02
Client
- Revert
docker/cli#2960 to fix hanging in
docker start --attach
and remove spuriousUnsupported signal: <nil>. Discarding
messages. docker/cli#2987.
20.10.4
2021-02-26
Builder
- Fix incorrect cache match for inline cache import with empty layers moby/moby#42061
- Update BuildKit to v0.8.2
moby/moby#42061
- resolver: avoid error caching on token fetch
- fileop: fix checksum to contain indexes of inputs preventing certain cache misses
- Fix reference count issues on typed errors with mount references (fixing
invalid mutable ref
errors) - git: set token only for main remote access allowing cloning submodules with different credentials
- Ensure blobs get deleted in /var/lib/docker/buildkit/content/blobs/sha256 after pull. To clean up old state run
builder prune
moby/moby#42065 - Fix parallel pull synchronization regression moby/moby#42049
- Ensure libnetwork state files do not leak moby/moby#41972
Client
- Fix a panic on
docker login
if no config file is present docker/cli#2959 - Fix
WARNING: Error loading config file: .dockercfg: $HOME is not defined
docker/cli#2958
Runtime
- docker info: silence unhandleable warnings moby/moby#41958
- Avoid creating parent directories for XGlobalHeader moby/moby#42017
- Use 0755 permissions when creating missing directories moby/moby#42017
- Fallback to manifest list when no platform matches in image config moby/moby#42045 moby/moby#41873
- Fix a daemon panic on setups with a custom default runtime configured moby/moby#41974
- Fix a panic when daemon configuration is empty moby/moby#41976
- Fix daemon panic when starting container with invalid device cgroup rule moby/moby#42001
- Fix userns-remap option when username & UID match moby/moby#42013
- static: update runc binary to v1.0.0-rc93 moby/moby#42014
Logger
- Honor
labels-regex
config even iflabels
is not set moby/moby#42046 - Handle long log messages correctly preventing awslogs in non-blocking mode to split events bigger than 16kB mobymoby#41975
Rootless
- Prevent the service hanging when stopping by setting systemd KillMode to mixed moby/moby#41956
- dockerd-rootless.sh: add typo guard moby/moby#42070
- Update rootlesskit to v0.13.1 to fix handling of IPv6 addresses moby/moby#42025
- allow mknodding FIFO inside userns moby/moby#41957
Security
- profiles: seccomp: update to Linux 5.11 syscall list moby/moby#41971
Swarm
- Fix issue with heartbeat not persisting upon restart moby/moby#42060
- Fix potential stalled tasks moby/moby#42060
- Fix
--update-order
and--rollback-order
flags when only--update-order
or--rollback-order
is provided docker/cli#2963 - Fix
docker service rollback
returning a non-zero exit code in some situations docker/cli#2964 - Fix inconsistent progress-bar direction on
docker service rollback
docker/cli#2964
20.10.3
2021-02-01
Security
- CVE-2021-21285 Prevent an invalid image from crashing docker daemon
- CVE-2021-21284 Lock down file permissions to prevent remapped root from accessing docker state
- Ensure AppArmor and SELinux profiles are applied when building with BuildKit
Client
- Check contexts before importing them to reduce risk of extracted files escaping context store
- Windows: prevent executing certain binaries from current directory docker/cli#2950
20.10.2
2021-01-04
Runtime
- Fix a daemon start up hang when restoring containers with restart policies but that keep failing to start moby/moby#41729
- overlay2: fix an off-by-one error preventing to build or run containers when data-root is 24-bytes long moby/moby#41830
- systemd: send
sd_notify STOPPING=1
when shutting down moby/moby#41832
Networking
- Fix IPv6 port forwarding moby/moby#41805 moby/libnetwork#2604
Swarm
- Fix filtering for
replicated-job
andglobal-job
service modes moby/moby#41806
Packaging
- buildx updated to v0.5.1 docker/docker-ce-packaging#516
20.10.1
2020-12-14
Builder
- buildkit: updated to v0.8.1 with various bugfixes moby/moby#41793
Packaging
- Revert a change in the systemd unit that could prevent docker from starting due to a startup order conflict docker/docker-ce-packaging#514
- buildx updated to v0.5.0 docker/docker-ce-packaging#515
20.10.0
2020-12-08
Deprecation / Removal
For an overview of all deprecated features, refer to the Deprecated Engine Features page.
- Warnings and deprecation notice when
docker pull
-ing from non-compliant registries not supporting pull-by-digest docker/cli#2872 - Sterner warnings and deprecation notice for unauthenticated tcp access moby/moby#41285
- Deprecate KernelMemory (
docker run --kernel-memory
) moby/moby#41254 docker/cli#2652 - Deprecate
aufs
storage driver docker/cli#1484 - Deprecate host-discovery and overlay networks with external k/v stores moby/moby#40614 moby/moby#40510
- Deprecate Dockerfile legacy 'ENV name value' syntax, use
ENV name=value
instead docker/cli#2743 - Remove deprecated ""filter"" parameter for API v1.41 and up moby/moby#40491
- Disable distribution manifest v2 schema 1 on push moby/moby#41295
- Remove hack MalformedHostHeaderOverride breaking old docker clients (<= 1.12) in which case, set
DOCKER_API_VERSION
moby/moby#39076 - Remove ""docker engine"" subcommands docker/cli#2207
- Remove experimental ""deploy"" from ""dab"" files docker/cli#2216
- Remove deprecated
docker search --automated
and--stars
flags docker/cli#2338 - No longer allow reserved namespaces in engine labels docker/cli#2326
API
- Update API version to v1.41
- Do not require ""experimental"" for metrics API moby/moby#40427
GET /events
now returnsprune
events after pruning resources have completed moby/moby#41259- Prune events are returned for
container
,network
,volume
,image
, andbuilder
, and have areclaimed
attribute, indicating the amount of space reclaimed (in bytes)
- Prune events are returned for
- Add
one-shot
stats option to not prime the stats moby/moby#40478 - Adding OS version info to the system info's API (
/info
) moby/moby#38349 - Add DefaultAddressPools to docker info moby/moby#40714
- Add API support for PidsLimit on services moby/moby#39882
Builder
- buildkit,dockerfile: Support for
RUN --mount
options without needing to specify experimental dockerfile#syntax
directive. moby/buildkit#1717 - dockerfile:
ARG
command now supports defining multiple build args on the same line similarly toENV
moby/buildkit#1692 - dockerfile:
--chown
flag inADD
now allows parameter expansion moby/buildkit#1473 - buildkit: Fetching authorization tokens has been moved to client-side (if the client supports it). Passwords do not leak into the build daemon anymore and users can see from build output when credentials or tokens are accessed. moby/buildkit#1660
- buildkit: Connection errors while communicating with the registry for push and pull now trigger a retry moby/buildkit#1791
- buildkit: Git source now supports token authentication via build secrets moby/moby#41234 docker/cli#2656 moby/buildkit#1533
- buildkit: Building from git source now supports forwarding SSH socket for authentication moby/buildkit#1782
- buildkit: Avoid builds that generate excessive logs to cause a crash or slow down the build. Clipping is performed if needed. moby/buildkit#1754
- buildkit: Change default Seccomp profile to the one provided by Docker moby/buildkit#1807
- buildkit: Support for exposing SSH agent socket on Windows has been improved moby/buildkit#1695
- buildkit: Disable truncating by default when using --progress=plain moby/buildkit#1435
- buildkit: Allow better handling client sessions dropping while it is being shared by multiple builds moby/buildkit#1551
- buildkit: secrets: allow providing secrets with env
moby/moby#41234
docker/cli#2656
moby/buildkit#1534
- Support
--secret id=foo,env=MY_ENV
as an alternative for storing a secret value to a file. --secret id=GIT_AUTH_TOKEN
will load env if it exists and the file does not.
- Support
- buildkit: Support for mirrors fallbacks, insecure TLS and custom TLS config moby/moby#40814
- buildkit: remotecache: Only visit each item once when walking results
moby/moby#41234
moby/buildkit#1577
- Improves performance and CPU use on bigger graphs
- buildkit: Check remote when local image platform doesn't match moby/moby#40629
- buildkit: image export: Use correct media type when creating new layer blobs moby/moby#41234 moby/buildkit#1541
- buildkit: progressui: fix logs time formatting moby/moby#41234 docker/cli#2656 moby/buildkit#1549
- buildkit: mitigate containerd issue on parallel push moby/moby#41234 moby/buildkit#1548
- buildkit: inline cache: fix handling of duplicate blobs
moby/moby#41234
moby/buildkit#1568
- Fixes https://github.com/moby/buildkit/issues/1388 cache-from working unreliably
- Fixes https://github.com/moby/moby/issues/41219 Image built from cached layers is missing data
- Allow ssh:// for remote context URLs moby/moby#40179
- builder: remove legacy build's session handling (was experimental) moby/moby#39983
Client
- Add swarm jobs support to CLI docker/cli#2262
- Add
-a/--all-tags
to docker push docker/cli#2220 - Add support for Kubernetes username/password auth docker/cli#2308
- Add
--pull=missing|always|never
torun
andcreate
commands docker/cli#1498 - Add
--env-file
flag todocker exec
for parsing environment variables from a file docker/cli#2602 - Add shorthand
-n
for--tail
option docker/cli#2646 - Add log-driver and options to service inspect ""pretty"" format docker/cli#1950
- docker run: specify cgroup namespace mode with
--cgroupns
docker/cli#2024 docker manifest rm
command to remove manifest list draft from local storage docker/cli#2449- Add ""context"" to ""docker version"" and ""docker info"" docker/cli#2500
- Propagate platform flag to container create API docker/cli#2551
- The
docker ps --format
flag now has a.State
placeholder to print the container's state without additional details about uptime and health check docker/cli#2000 - Add support for docker-compose schema v3.9 docker/cli#2073
- Add support for docker push
--quiet
docker/cli#2197 - Hide flags that are not supported by BuildKit, if BuildKit is enabled docker/cli#2123
- Update flag description for
docker rm -v
to clarify the option only removes anonymous (unnamed) volumes docker/cli#2289 - Improve tasks printing for docker services docker/cli#2341
- docker info: list CLI plugins alphabetically docker/cli#2236
- Fix order of processing of
--label-add/--label-rm
,--container-label-add/--container-label-rm
, and--env-add/--env-rm
flags ondocker service update
to allow replacing existing values docker/cli#2668 - Fix
docker rm --force
returning a non-zero exit code if one or more containers did not exist docker/cli#2678 - Improve memory stats display by using
total_inactive_file
instead ofcache
docker/cli#2415 - Mitigate against YAML files that has excessive aliasing docker/cli#2117
- Allow using advanced syntax when setting a config or secret with only the source field docker/cli#2243
- Fix reading config files containing
username
andpassword
auth even ifauth
is empty docker/cli#2122 - docker cp: prevent NPE when failing to stat destination docker/cli#2221
- config: preserve ownership and permissions on configfile docker/cli#2228
Logging
- Support reading
docker logs
with all logging drivers (best effort) moby/moby#40543 - Add
splunk-index-acknowledgment
log option to work with Splunk HECs with index acknowledgment enabled moby/moby#39987 - Add partial metadata to journald logs moby/moby#41407
- Reduce allocations for logfile reader moby/moby#40796
- Fluentd: add fluentd-async, fluentd-request-ack, and deprecate fluentd-async-connect moby/moby#39086
Runtime
- Support cgroup2 moby/moby#40174 moby/moby#40657 moby/moby#40662
- cgroup2: use ""systemd"" cgroup driver by default when available moby/moby#40846
- new storage driver: fuse-overlayfs moby/moby#40483
- Update containerd binary to v1.4.3 moby/moby#41732
docker push
now defaults tolatest
tag instead of all tags moby/moby#40302- Added ability to change the number of reconnect attempts during connection loss while pulling an image by adding max-download-attempts to the config file moby/moby#39949
- Add support for containerd v2 shim by using the now default
io.containerd.runc.v2
runtime moby/moby#41182 - cgroup v1: change the default runtime to io.containerd.runc.v2. Requires containerd v1.3.0 or later. v1.3.5 or later is recommended moby/moby#41210
- Start containers in their own cgroup namespaces moby/moby#38377
- Enable DNS Lookups for CIFS Volumes moby/moby#39250
- Use MemAvailable instead of MemFree to estimate actual available memory moby/moby#39481
- The
--device
flag indocker run
will now be honored when the container is started in privileged mode moby/moby#40291 - Enforce reserved internal labels moby/moby#40394
- Raise minimum memory limit to 6M, to account for higher memory use by runtimes during container startup moby/moby#41168
- vendor runc v1.0.0-rc92 moby/moby#41344 moby/moby#41317
- info: add warnings about missing blkio cgroup support moby/moby#41083
- Accept platform spec on container create moby/moby#40725
- Fix handling of looking up user- and group-names with spaces moby/moby#41377
Networking
- Support host.docker.internal in dockerd on Linux moby/moby#40007
- Include IPv6 address of linked containers in /etc/hosts moby/moby#39837
--ip6tables
enables IPv6 iptables rules (only if experimental) moby/moby#41622- Add alias for hostname if hostname != container name moby/moby#39204
- Better selection of DNS server (with systemd) moby/moby#41022
- Add docker interfaces to firewalld docker zone
moby/moby#41189
moby/libnetwork#2548
- Fixes DNS issue on CentOS8 docker/for-linux#957
- Fixes Port Forwarding on RHEL 8 with Firewalld running with FirewallBackend=nftables moby/libnetwork#2496
- Fix an issue reporting 'failed to get network during CreateEndpoint' moby/moby#41189 moby/libnetwork#2554
- Log error instead of disabling IPv6 router advertisement failed moby/moby#41189 moby/libnetwork#2563
- No longer ignore
--default-address-pool
option in certain cases moby/moby#40711 - Produce an error with invalid address pool moby/moby#40808 moby/libnetwork#2538
- Fix
DOCKER-USER
chain not created when IPTableEnable=false moby/moby#40808 moby/libnetwork#2471 - Fix panic on startup in systemd environments moby/moby#40808 moby/libnetwork#2544
- Fix issue preventing containers to communicate over macvlan internal network moby/moby#40596 moby/libnetwork#2407
- Fix InhibitIPv4 nil panic moby/moby#40596
- Fix VFP leak in Windows overlay network deletion moby/moby#40596 moby/libnetwork#2524
Packaging
- docker.service: Add multi-user.target to After= in unit file moby/moby#41297
- docker.service: Allow socket activation moby/moby#37470
- seccomp: Remove dependency in dockerd on libseccomp moby/moby#41395
Rootless
- rootless: graduate from experimental moby/moby#40759
- Add dockerd-rootless-setuptool.sh moby/moby#40950
- Support
--exec-opt native.cgroupdriver=systemd
moby/moby#40486
Security
- Fix CVE-2019-14271 loading of nsswitch based config inside chroot under Glibc moby/moby#39612
- seccomp: Whitelist
clock_adjtime
.CAP_SYS_TIME
is still required for time adjustment moby/moby#40929 - seccomp: Add openat2 and faccessat2 to default seccomp profile moby/moby#41353
- seccomp: allow 'rseq' syscall in default seccomp profile moby/moby#41158
- seccomp: allow syscall membarrier moby/moby#40731
- seccomp: whitelist io-uring related system calls moby/moby#39415
- Add default sysctls to allow ping sockets and privileged ports with no capabilities moby/moby#41030
- Fix seccomp profile for clone syscall moby/moby#39308
Swarm
- Add support for swarm jobs moby/moby#40307
- Add capabilities support to stack/service commands docker/cli#2687 docker/cli#2709 moby/moby#39173 moby/moby#41249
- Add support for sending down service Running and Desired task counts moby/moby#39231
- service: support
--mount type=bind,bind-nonrecursive
moby/moby#38788 - Support ulimits on Swarm services. moby/moby#41284 docker/cli#2712
- Fixed an issue where service logs could leak goroutines on the worker moby/moby#40426",,,
e3df7ab91d3d0314aed7dd46da27a183b564684c2bcb6b0eaa5b37354e077a35,"Add or update a payment method
This page describes how to add or update a payment method for your personal account or for an organization.
You can add a payment method or update your account's existing payment method at any time.
Important
If you want to remove all payment methods, you must first downgrade your subscription to a free plan. See Downgrade.
The following payment methods are supported:
- Visa
- MasterCard
- American Express
- Discover
- JCB
- Diners
- UnionPay
All currency, for example the amount listed on your billing invoice, is in United States dollar (USD).
Important
Starting July 1, 2024, Docker will begin collecting sales tax on subscription fees in compliance with state regulations for customers in the United States. For our global customers subject to VAT, the implementation will start rolling out on July 1, 2024. Note that while the roll out begins on this date, VAT charges may not apply to all applicable subscriptions immediately.
To ensure that tax assessments are correct, make sure that your billing information and VAT/Tax ID, if applicable, are updated. If you're exempt from sales tax, see Register a tax certificate.
Manage payment method
Personal account
To add a payment method:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- Select Payment methods from the left-hand menu.
- Select Add payment method.
- Enter your new payment information.
- Select Add.
- Optional. You can set a new default payment method by selecting the Set as default action.
- Optional. You can remove non-default payment methods by selecting the Delete action.
To add a payment method:
- Sign in to Docker Hub.
- Select your avatar in the top-right corner.
- From the drop-down menu select Billing.
- Select the Payment methods and billing history link.
- In the Payment method section, select Add payment method.
- Enter your new payment information, then select Add.
- Select the Actions icon, then select Make default to ensure that your new payment method applies to all purchases and subscriptions.
- Optional. You can remove non-default payment methods by selecting the Actions icon. Then, select Delete.
Organization
Note
You must be an organization owner to make changes to the payment information.
To add a payment method:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- Choose your organization from the top-left drop-down.
- Select Payment methods from the left-hand menu.
- Select Add payment method.
- Enter your new payment information.
- Select Add.
- Optional. You can set a new default payment method by selecting the Set as default action.
- Optional. You can remove non-default payment methods by selecting the Delete action.
To add a payment method:
- Sign in to Docker Hub.
- Select your avatar in the top-right corner.
- From the drop-down menu select Billing.
- Select the organization account you want to update.
- Select the Payment methods and billing history link.
- In the Payment Method section, select Add payment method.
- Enter your new payment information, then select Add.
- Select the Actions icon, then select Make default to ensure that your new payment method applies to all purchases and subscriptions.
- Optional. You can remove non-default payment methods by selecting the Actions icon. Then, select Delete.
Failed payments
If your subscription payment fails, there is a grace period of 15 days, including the due date. Docker retries to collect the payment 3 times using the following schedule:
- 3 days after the due date
- 5 days after the previous attempt
- 7 days after the previous attempt
Docker also sends an email notification Action Required - Credit Card Payment Failed
with an attached unpaid invoice after each failed payment attempt.
Once the grace period is over and the invoice is still not paid, the subscription downgrades to a free plan and all paid features are disabled.
Redeem a coupon
You can redeem a coupon for any paid Docker subscription.
A coupon can be used when you:
- Sign up to a new paid subscription from a free subscription
- Upgrade an existing paid subscription
You are asked to enter your coupon code when you confirm or enter your payment method.
If you use a coupon to pay for a subscription, when the coupon expires, your payment method is charged the full cost of your subscription. If you don't have a saved payment method, your account downgrades to a free subscription.",,,
7650e68f747681d4904082572280c8591db716483ab6516927643e5ea06572f4,"Docker Engine 17.12 release notes
Table of contents
17.12.1-ce
2018-02-27
Client
- Fix
node-generic-resource
typo moby/moby#35970 and moby/moby#36125
- Return errors from daemon on stack deploy configs create/update docker/cli#757
Logging
- awslogs: fix batch size calculation for large logs moby/moby#35726
- Support a proxy in splunk log driver moby/moby#36220
Networking
- Fix ingress network when upgrading from 17.09 to 17.12 moby/moby#36003
- Add verbose info to partial overlay ID moby/moby#35989
- Fix IPv6 networking being deconfigured if live-restore is being enabled docker/libnetwork#2043
- Fix watchMiss thread context docker/libnetwork#2051
Packaging
- Set TasksMax in docker.service docker/docker-ce-packaging#78
Runtime
- Bump Golang to 1.9.4
- Bump containerd to 1.0.1
- Fix dockerd not being able to reconnect to containerd when it is restarted moby/moby#36173
- Fix containerd events from being processed twice moby/moby#35891
- Fix vfs graph driver failure to initialize because of failure to setup fs quota moby/moby#35827
- Fix regression of health check not using container's working directory moby/moby#35845
- Honor
DOCKER_RAMDISK
with containerd 1.0 moby/moby#35957 - Update runc to fix hang during start and exec moby/moby#36097
- Windows: Vendor of Microsoft/hcsshim @v.0.6.8 partial fix for import layer failing moby/moby#35924
- Do not make graphdriver homes private mounts moby/moby#36047
- Use rslave propagation for mounts from daemon root moby/moby#36055
- Set daemon root to use shared mount propagation moby/moby#36096
- Validate that mounted paths exist when container is started, not just during creation moby/moby#35833
- Add
REMOVE
andORPHANED
to TaskState moby/moby#36146
- Fix issue where network inspect does not show Created time for networks in swarm scope moby/moby#36095
- Nullify container read write layer upon release moby/moby#36130 and moby/moby#36343
Swarm
- Remove watchMiss from swarm mode docker/libnetwork#2047
Known Issues
- Health check no longer uses the container's working directory moby/moby#35843
- Errors not returned from client in stack deploy configs moby/moby#757
- Docker cannot use memory limit when using systemd options moby/moby#35123
17.12.0-ce
2017-12-27
Known Issues
- AWS logs batch size calculation moby/moby#35726
- Health check no longer uses the container's working directory moby/moby#35843
- Errors not returned from client in stack deploy configs moby/moby#757
- Daemon aborts when project quota fails moby/moby#35827
- Docker cannot use memory limit when using systemd options moby/moby#35123
Builder
- Fix build cache hash for broken symlink moby/moby#34271
- Fix long stream sync moby/moby#35404
- Fix dockerfile parser failing silently on long tokens moby/moby#35429
Client
- Remove secret/config duplication in cli/compose docker/cli#671
- Add
--local
flag todocker trust sign
docker/cli#575 - Add
docker trust inspect
docker/cli#694
- Add
name
field to secrets and configs to allow interpolation in Compose files docker/cli#668 - Add
--isolation
for setting swarm service isolation mode docker/cli#426
- Remove deprecated ""daemon"" subcommand docker/cli#689
- Fix behaviour of
rmi -f
with unexpected errors docker/cli#654
- Integrated Generic resource in service create docker/cli#429
- Fix external networks in stacks docker/cli#743
- Remove support for referencing images by image shortid docker/cli#753 and moby/moby#35790
- Use commit-sha instead of tag for containerd moby/moby#35770
Documentation
- Update API version history for 1.35 moby/moby#35724
Logging
- Logentries driver line-only=true []byte output fix moby/moby#35612
- Logentries line-only logopt fix to maintain backwards compatibility moby/moby#35628
- Add
--until
flag for docker logs moby/moby#32914 - Add gelf log driver plugin to Windows build moby/moby#35073
- Set timeout on splunk batch send moby/moby#35496
- Update Graylog2/go-gelf moby/moby#35765
Networking
- Move load balancer sandbox creation/deletion into libnetwork moby/moby#35422
- Only chown network files within container metadata moby/moby#34224
- Restore error type in FindNetwork moby/moby#35634
- Fix consumes MIME type for NetworkConnect moby/moby#35542
- Added support for persisting Windows network driver specific options moby/moby#35563
- Fix timeout on netlink sockets and watchmiss leak moby/moby#35677
- New daemon config for networking diagnosis moby/moby#35677
- Clean up node management logic docker/libnetwork#2036
- Allocate VIPs when endpoints are restored docker/swarmkit#2474
Runtime
- Update to containerd v1.0.0 moby/moby#35707
- Have VFS graphdriver use accelerated in-kernel copy moby/moby#35537
- Introduce
workingdir
option for docker exec moby/moby#35661 - Bump Go to 1.9.2 moby/moby#33892 docker/cli#716
/dev
should not be readonly with--readonly
flag moby/moby#35344
- Add custom build-time Graphdrivers priority list moby/moby#35522
- LCOW: CLI changes to add platform flag - pull, run, create and build docker/cli#474
- Fix width/height on Windows for
docker exec
moby/moby#35631 - Detect overlay2 support on pre-4.0 kernels moby/moby#35527
- Devicemapper: remove container rootfs mountPath after umount moby/moby#34573
- Disallow overlay/overlay2 on top of NFS moby/moby#35483
- Fix potential panic during plugin set. moby/moby#35632
- Fix some issues with locking on the container moby/moby#35501
- Fixup some issues with plugin refcounting moby/moby#35265
- Add missing lock in ProcessEvent moby/moby#35516
- Add vfs quota support moby/moby#35231
- Skip empty directories on prior graphdriver detection moby/moby#35528
- Skip xfs quota tests when running in user namespace moby/moby#35526
- Added SubSecondPrecision to config option. moby/moby#35529
- Update fsnotify to fix deadlock in removing watch moby/moby#35453
- Fix ""duplicate mount point"" when
--tmpfs /dev/shm
is used moby/moby#35467 - Fix honoring tmpfs-size for user
/dev/shm
mount moby/moby#35316 - Fix EBUSY errors under overlayfs and v4.13+ kernels moby/moby#34948
- Container: protect health monitor channel moby/moby#35482
- Container: protect the health status with mutex moby/moby#35517
- Container: update real-time resources moby/moby#33731
- Create labels when volume exists only remotely moby/moby#34896
- Fix leaking container/exec state moby/moby#35484
- Disallow using legacy (v1) registries moby/moby#35751 and docker/cli#747
- Windows: Fix case insensitive filename matching against builder cache moby/moby#35793
- Fix race conditions around process handling and error checks moby/moby#35809
- Ensure containers are stopped on daemon startup moby/moby#35805
- Follow containerd namespace conventions moby/moby#35812
Swarm Mode
- Added support for swarm service isolation mode moby/moby#34424
- Fix task clean up for tasks that are complete docker/swarmkit#2477
Packaging
- Add Packaging for Fedora 27 docker/docker-ce-packaging#59
- Change default versioning scheme to 0.0.0-dev unless specified for packaging docker/docker-ce-packaging#67
- Pass Version to engine static builds docker/docker-ce-packaging#70
- Added support for aarch64 on Debian (stretch/jessie) and Ubuntu Zesty or newer docker/docker-ce-packaging#35",,,
d7dcbdf5f815e171366df53b15d9004c75e013cb0a2df9ace5969588156a262e,"Settings and feedback for Docker Extensions
Settings
Turn on or turn off extensions
Docker Extensions is switched on by default. To change your settings:
- Navigate to Settings.
- Select the Extensions tab.
- Next to Enable Docker Extensions, select or clear the checkbox to set your desired state.
- In the bottom-right corner, select Apply & Restart.
Note
If you are an organization owner, you can turn off extensions for your users. Open the
settings-store.json
file, and set""extensionsEnabled""
tofalse
. Thesettings-store.json
file (orsettings.json
for Docker Desktop versions 4.34 and earlier) is located at:
~/Library/Group Containers/group.com.docker/settings-store.json
on MacC:\Users\[USERNAME]\AppData\Roaming\Docker\settings-store.json
on WindowsThis can also be done with Hardened Docker Desktop
Turn on or turn off extensions not available in the Marketplace
You can install extensions through the Marketplace or through the Extensions SDK tools. You can choose to only allow published extensions. These are extensions that have been reviewed and published in the Extensions Marketplace.
- Navigate to Settings.
- Select the Extensions tab.
- Next to Allow only extensions distributed through the Docker Marketplace, select or clear the checkbox to set your desired state.
- In the bottom-right corner, select Apply & Restart.
See containers created by extensions
By default, containers created by extensions are hidden from the list of containers in the Docker Desktop Dashboard and the Docker CLI. To make them visible update your settings:
- Navigate to Settings.
- Select the Extensions tab.
- Next to Show Docker Extensions system containers, select or clear the checkbox to set your desired state.
- In the bottom-right corner, select Apply & Restart.
Note
Enabling extensions doesn't use computer resources (CPU / Memory) by itself.
Specific extensions might use computer resources, depending on the features and implementation of each extension, but there is no reserved resources or usage cost associated with enabling extensions.
Submit feedback
Feedback can be given to an extension author through a dedicated Slack channel or GitHub. To submit feedback about a particular extension:
- Navigate to the Docker Desktop Dashboard and select the Manage tab. This displays a list of extensions you've installed.
- Select the extension you want to provide feedback on.
- Scroll down to the bottom of the extension's description and, depending on the
extension, select:
- Support
- Slack
- Issues. You'll be sent to a page outside of Docker Desktop to submit your feedback.
If an extension doesn't provide a way for you to give feedback, contact us and we'll pass on the feedback for you. To provide feedback, select the Give feedback to the right of Extensions Marketplace.",,,
43361c403e609f1c7ce0a70142a6a860ecee365a11413ac2c1912f33f16a2daa,"Network drivers
Docker's networking subsystem is pluggable, using drivers. Several drivers exist by default, and provide core networking functionality:
bridge
: The default network driver. If you don't specify a driver, this is the type of network you are creating. Bridge networks are commonly used when your application runs in a container that needs to communicate with other containers on the same host. See Bridge network driver.host
: Remove network isolation between the container and the Docker host, and use the host's networking directly. See Host network driver.overlay
: Overlay networks connect multiple Docker daemons together and enable Swarm services and containers to communicate across nodes. This strategy removes the need to do OS-level routing. See Overlay network driver.ipvlan
: IPvlan networks give users total control over both IPv4 and IPv6 addressing. The VLAN driver builds on top of that in giving operators complete control of layer 2 VLAN tagging and even IPvlan L3 routing for users interested in underlay network integration. See IPvlan network driver.macvlan
: Macvlan networks allow you to assign a MAC address to a container, making it appear as a physical device on your network. The Docker daemon routes traffic to containers by their MAC addresses. Using themacvlan
driver is sometimes the best choice when dealing with legacy applications that expect to be directly connected to the physical network, rather than routed through the Docker host's network stack. See Macvlan network driver.none
: Completely isolate a container from the host and other containers.none
is not available for Swarm services. See None network driver.Network plugins: You can install and use third-party network plugins with Docker.
Network driver summary
- The default bridge network is good for running containers that don't require special networking capabilities.
- User-defined bridge networks enable containers on the same Docker host to communicate with each other. A user-defined network typically defines an isolated network for multiple containers belonging to a common project or component.
- Host network shares the host's network with the container. When you use this driver, the container's network isn't isolated from the host.
- Overlay networks are best when you need containers running on different Docker hosts to communicate, or when multiple applications work together using Swarm services.
- Macvlan networks are best when you are migrating from a VM setup or need your containers to look like physical hosts on your network, each with a unique MAC address.
- IPvlan is similar to Macvlan, but doesn't assign unique MAC addresses to containers. Consider using IPvlan when there's a restriction on the number of MAC addresses that can be assigned to a network interface or port.
- Third-party network plugins allow you to integrate Docker with specialized network stacks.
Networking tutorials
Now that you understand the basics about Docker networks, deepen your understanding using the following tutorials:",,,
51b13ae6173e3c65649a9c510b61fcd06f558d061879aa83eef8f0423dc58d97,"Interacting with Kubernetes from an extension
The Extensions SDK does not provide any API methods to directly interact with the Docker Desktop managed Kubernetes cluster or any other created using other tools such as KinD. However, this page provides a way for you to use other SDK APIs to interact indirectly with a Kubernetes cluster from your extension.
To request an API that directly interacts with Docker Desktop-managed Kubernetes, you can upvote this issue in the Extensions SDK GitHub repository.
Prerequisites
Turn on Kubernetes
You can use the built-in Kubernetes in Docker Desktop to start a Kubernetes single-node cluster.
A kubeconfig
file is used to configure access to Kubernetes when used in conjunction with the kubectl
command-line tool, or other clients.
Docker Desktop conveniently provides the user with a local preconfigured kubeconfig
file and kubectl
command within the user’s home area. It is a convenient way to fast-tracking access for those looking to leverage Kubernetes from Docker Desktop.
Ship the kubectl
as part of the extension
If your extension needs to interact with Kubernetes clusters, it is recommended that you include the kubectl
command line tool as part of your extension. By doing this, users who install your extension get kubectl
installed on their host.
To find out how to ship the kubectl
command line tool for multiple platforms as part of your Docker Extension image, see
Build multi-arch extensions.
Examples
The following code snippets have been put together in the
Kubernetes Sample Extension. It shows how to interact with a Kubernetes cluster by shipping the kubectl
command-line tool.
Check the Kubernetes API server is reachable
Once the kubectl
command-line tool is added to the extension image in the Dockerfile
, and defined in the metadata.json
, the Extensions framework deploys kubectl
to the users' host when the extension is installed.
You can use the JS API ddClient.extension.host?.cli.exec
to issue kubectl
commands to, for instance, check whether the Kubernetes API server is reachable given a specific context:
const output = await ddClient.extension.host?.cli.exec(""kubectl"", [
""cluster-info"",
""--request-timeout"",
""2s"",
""--context"",
""docker-desktop"",
]);
List Kubernetes contexts
const output = await ddClient.extension.host?.cli.exec(""kubectl"", [
""config"",
""view"",
""-o"",
""jsonpath='{.contexts}'"",
]);
List Kubernetes namespaces
const output = await ddClient.extension.host?.cli.exec(""kubectl"", [
""get"",
""namespaces"",
""--no-headers"",
""-o"",
'custom-columns="":metadata.name""',
""--context"",
""docker-desktop"",
]);
Persisting the kubeconfig file
Below there are different ways to persist and read the kubeconfig
file from the host filesystem. Users can add, edit, or remove Kubernetes context to the kubeconfig
file at any time.
Warning
The
kubeconfig
file is very sensitive and if found can give an attacker administrative access to the Kubernetes Cluster.
Extension's backend container
If you need your extension to persist the kubeconfig
file after it's been read, you can have a backend container that exposes an HTTP POST endpoint to store the content of the file either in memory or somewhere within the container filesystem. This way, if the user navigates out of the extension to another part of Docker Desktop and then comes back, you don't need to read the kubeconfig
file again.
export const updateKubeconfig = async () => {
const kubeConfig = await ddClient.extension.host?.cli.exec(""kubectl"", [
""config"",
""view"",
""--raw"",
""--minify"",
""--context"",
""docker-desktop"",
]);
if (kubeConfig?.stderr) {
console.log(""error"", kubeConfig?.stderr);
return false;
}
// call backend container to store the kubeconfig retrieved into the container's memory or filesystem
try {
await ddClient.extension.vm?.service?.post(""/store-kube-config"", {
data: kubeConfig?.stdout,
});
} catch (err) {
console.log(""error"", JSON.stringify(err));
}
};
Docker volume
Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. You can make use of them to persist the kubeconfig
file.
By persisting the kubeconfig
in a volume you won't need to read the kubeconfig
file again when the extension pane closes. This makes it ideal for persisting data when navigating out of the extension to other parts of Docker Desktop.
const kubeConfig = await ddClient.extension.host?.cli.exec(""kubectl"", [
""config"",
""view"",
""--raw"",
""--minify"",
""--context"",
""docker-desktop"",
]);
if (kubeConfig?.stderr) {
console.log(""error"", kubeConfig?.stderr);
return false;
}
await ddClient.docker.cli.exec(""run"", [
""--rm"",
""-v"",
""my-vol:/tmp"",
""alpine"",
""/bin/sh"",
""-c"",
`""touch /tmp/.kube/config && echo '${kubeConfig?.stdout}' > /tmp/.kube/config""`,
]);
Extension's localStorage
localStorage
is one of the mechanisms of a browser's web storage. It allows users to save data as key-value pairs in the browser for later use.
localStorage
does not clear data when the browser (the extension pane) closes. This makes it ideal for persisting data when navigating out of the extension to other parts of Docker Desktop.
localStorage.setItem(""kubeconfig"", kubeConfig);
localStorage.getItem(""kubeconfig"");",,,
23ebc488a006ee411f130a0a5446727859326085323f83d77653ad4dc197eaf7,"Register a tax certificate
If you're a customer in the United States and you're exempt from sales tax, you can register a valid tax exemption certificate with Docker's Support team. If you're a global customer subject to VAT, make sure that you provide your VAT number.
Important
Starting July 1, 2024, Docker will begin collecting sales tax on subscription fees in compliance with state regulations for customers in the United States. For our global customers subject to VAT, the implementation will start rolling out on July 1, 2024. Note that while the roll out begins on this date, VAT charges may not apply to all applicable subscriptions immediately.
To ensure that tax assessments are correct, make sure that your billing information and VAT/Tax ID, if applicable, are updated. If you're exempt from sales tax, see Register a tax certificate.
Prerequisites
Before you submit your tax exemption certificate, ensure the following.
- Your customer name matches the name on the exemption certificate
- Your tax exemption certificate specifies Docker Inc as the Seller or Vendor and all applicable information is filled out
- Your certificate is signed and dated, and the expiration date hasn't passed
- You have a valid Docker ID/namespace(s) of the accounts that you want to apply the tax exemption certificate to
Contact information
You can use the following for Docker's contact information on your tax exemption certificate.
Docker, Inc.
3790 El Camino Real #1052
Palo Alto, CA 94306
(415) 941-0376
Register a tax certificate
Submit a Docker Support ticket to initiate the process to register a tax certificate.
Enter the required information.
In the Additional Information field, list the Docker ID/namespace(s) of the accounts that you want to apply the tax exemption certificate to.
Tip
You can list multiple namespaces that share the same tax exemption certificate, if applicable.
Add the tax certificate from your system by dragging and dropping them onto the file area, or select the Browse Files button to open a file dialog.
Select Submit.
Docker's support team will reach out to you if any additional information is required. You'll receive an e-mail confirmation from Docker once your tax exemption status is applied to your account.",,,
b5322e4dc4a300e692302ee49798e308f2610bb39378b3e999d0492a22dfb1df,"Organization access tokens
Warning
Organization access tokens (OATs) are not intended to be used with Docker Desktop, and are incompatible.
OATs are also currently incompatible with the following services:
- Docker Scout
If you use Docker Desktop or one of these services, you must use personal access tokens instead.
An organization access token (OAT) is like a personal access token (PAT), but an OAT is associated with an organization and not a single user account. Use an OAT instead of a PAT to let business-critical tasks access Docker Hub repositories without connecting the token to single user. You must have a Docker Team or Business subscription to use OATs.
OATs provide the following advantages:
- You can investigate when the OAT was last used and then disable or delete it if you find any suspicious activity.
- You can limit what each OAT has access to, which limits the impact if an OAT is compromised.
- All company or organization owners can manage OATs. If one owner leaves the organization, the remaining owners can still manage the OATs.
- OATs have their own Docker Hub usage limits that don't count towards your personal account's limits.
If you have existing service accounts, Docker recommends that you replace the service accounts with OATs. OATs offer the following advantages over service accounts:
- Access permissions are easier to manage with OATs. You can assign access permissions to OATs, while service accounts require using teams for access permissions.
- OATs are easier to manage. OATs are centrally managed in the Admin Console. For service accounts, you may need to sign in to that service account to manage it. If using single sign-on enforcement and the service account is not in your IdP, you may not be able to sign in to the service account to manage it.
- OATs are not associated with a single user. If a user with access to the service account leaves your organization, you may lose access to the service account. OATs can be managed by any company or organization owner.
Create an organization access token
Important
Treat access tokens like a password and keep them secret. Store your tokens securely in a credential manager for example.
Company or organization owners can create up to 10 organization access tokens (OATs) for organizations with a Team subscription and up to 100 OATs for organizations with a Business subscription. Expired tokens count towards the total amount of tokens.
To create an OAT:
Sign in to the Admin Console.
Select the organization you want to create an access token for.
Under Security and access, select Access tokens.
Select Generate access token.
Add a label and optional description for your token. Use something that indicates the use case or purpose of the token.
Select the expiration date for the token.
Select the repository access for the token.
The access permissions are scopes that set restrictions in your repositories. For example, for Read & Write permissions, an automation pipeline can build an image and then push it to a repository. However, it can't delete the repository. You can select one of the following options:
- Public repositories (read only)
- All repositories: You can select read access, or read and write access.
- Select repositories: You can select up to 50 repositories, and then select read access, or read and write access for each repository.
Select Generate token and then copy the token that appears on the screen and save it. You won't be able to retrieve the token once you exit the screen.
Use an organization access token
You can use an organization access token when you sign in using Docker CLI.
Sign in from your Docker CLI client with the following command, replacing
YOUR_ORG
with your organization name:
$ docker login --username <YOUR_ORG>
When prompted for a password, enter your organization access token instead of a password.
Modify existing tokens
You can rename, update the description, update the repository access, deactivate, or delete a token as needed.
Sign in to the Admin Console.
Select the organization you want to modify an access token for.
Under Security and access, select Access tokens.
Select the actions menu on the far right of a token row, then select Deactivate, Edit, or Delete to modify the token. For Inactive tokens, you can only select Delete.
If editing a token, select Save after specifying your modifications.",,,
14a2763e2d18e0f0f0594f4a77089d93aaa096667143e202076406958d33ff87,"Docker Engine
Docker Engine is an open source containerization technology for building and containerizing your applications. Docker Engine acts as a client-server application with:
- A server with a long-running daemon process
dockerd
. - APIs which specify interfaces that programs can use to talk to and instruct the Docker daemon.
- A command line interface (CLI) client
docker
.
The CLI uses Docker APIs to control or interact with the Docker daemon through scripting or direct CLI commands. Many other Docker applications use the underlying API and CLI. The daemon creates and manages Docker objects, such as images, containers, networks, and volumes.
For more details, see Docker Architecture.
Licensing
The Docker Engine is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.
However, for commercial use of Docker Engine obtained via Docker Desktop within larger enterprises (exceeding 250 employees OR with annual revenue surpassing $10 million USD), a paid subscription is required.",,,
97db19e3a76f46a2ad15d5e2a75ba76e03b6074947b8be4b93f483ac77743a74,"Docker Engine 17.05 release notes
Table of contents
17.05.0-ce
2017-05-04
Builder
- Add multi-stage build support #31257 #32063
- Allow using build-time args (
ARG
) inFROM
#31352 - Add an option for specifying build target #32496
- Accept
-f -
to read Dockerfile fromstdin
, but use local context for building #31236 - The values of default build time arguments (e.g
HTTP_PROXY
) are no longer displayed in docker image history unless a correspondingARG
instruction is written in the Dockerfile. #31584
- Fix setting command if a custom shell is used in a parent image #32236
- Fix
docker build --label
when the label includes single quotes and a space #31750
Client
- Add
--mount
flag todocker run
anddocker create
#32251 - Add
--type=secret
todocker inspect
#32124 - Add
--format
option todocker secret ls
#31552 - Add
--filter
option todocker secret ls
#30810 - Add
--filter scope=<swarm|local>
todocker network ls
#31529 - Add
--cpus
support todocker update
#31148 - Add label filter to
docker system prune
and otherprune
commands #30740 docker stack rm
now accepts multiple stacks as input #32110- Improve
docker version --format
option when the client has downgraded the API version #31022 - Prompt when using an encrypted client certificate to connect to a docker daemon #31364
- Display created tags on successful
docker build
#32077 - Cleanup compose convert error messages #32087
Contrib
- Add support for building docker debs for Ubuntu 17.04 Zesty on amd64 #32435
Daemon
- Fix
--api-cors-header
being ignored if--api-enable-cors
is not set #32174 - Cleanup docker tmp dir on start #31741
- Deprecate
--graph
flag in favor or--data-root
#28696
Logging
- Add support for logging driver plugins #28403
- Add support for showing logs of individual tasks to
docker service logs
, and add/task/{id}/logs
REST endpoint #32015 - Add
--log-opt env-regex
option to match environment variables using a regular expression #27565
Networking
- Allow user to replace, and customize the ingress network #31714
- Fix UDP traffic in containers not working after the container is restarted #32505
- Fix files being written to
/var/lib/docker
if a different data-root is set #32505
Runtime
- Ensure health probe is stopped when a container exits #32274
Swarm Mode
- Add update/rollback order for services (
--update-order
/--rollback-order
) #30261 - Add support for synchronous
service create
andservice update
#31144 - Add support for ""grace periods"" on healthchecks through the
HEALTHCHECK --start-period
and--health-start-period
flag todocker service create
,docker service update
,docker create
, anddocker run
to support containers with an initial startup time #28938
docker service create
now omits fields that are not specified by the user, when possible. This will allow defaults to be applied inside the manager #32284docker service inspect
now shows default values for fields that are not specified by the user #32284- Move
docker service logs
out of experimental #32462 - Add support for Credential Spec and SELinux to services to the API #32339
- Add
--entrypoint
flag todocker service create
anddocker service update
#29228 - Add
--network-add
and--network-rm
todocker service update
#32062 - Add
--credential-spec
flag todocker service create
anddocker service update
#32339 - Add
--filter mode=<global|replicated>
todocker service ls
#31538 - Resolve network IDs on the client side, instead of in the daemon when creating services #32062
- Add
--format
option todocker node ls
#30424 - Add
--prune
option todocker stack deploy
to remove services that are no longer defined in the docker-compose file #31302 - Add
PORTS
column fordocker service ls
when usingingress
mode #30813
- Fix unnecessary re-deploying of tasks when environment-variables are used #32364
- Fix
docker stack deploy
not supportingendpoint_mode
when deploying from a docker compose file #32333 - Proceed with startup if cluster component cannot be created to allow recovering from a broken swarm setup #31631
Security
- Allow setting SELinux type or MCS labels when using
--ipc=container:
or--ipc=host
#30652",,,
d73dcddf4fbb815a10a15c17abb96109a3541da249dfca935b9f49bf8d9ec8eb,"Build context
The docker build
and docker buildx build
commands build Docker images from
a
Dockerfile and a context.
What is a build context?
The build context is the set of files that your build can access. The positional argument that you pass to the build command specifies the context that you want to use for the build:
$ docker build [OPTIONS] PATH | URL | -
^^^^^^^^^^^^^^
You can pass any of the following inputs as the context for a build:
- The relative or absolute path to a local directory
- A remote URL of a Git repository, tarball, or plain-text file
- A plain-text file or tarball piped to the
docker build
command through standard input
Filesystem contexts
When your build context is a local directory, a remote Git repository, or a tar
file, then that becomes the set of files that the builder can access during the
build. Build instructions such as COPY
and ADD
can refer to any of the
files and directories in the context.
A filesystem build context is processed recursively:
- When you specify a local directory or a tarball, all subdirectories are included
- When you specify a remote Git repository, the repository and all submodules are included
For more information about the different types of filesystem contexts that you can use with your builds, see:
Text file contexts
When your build context is a plain-text file, the builder interprets the file as a Dockerfile. With this approach, the build doesn't use a filesystem context.
For more information, see empty build context.
Local context
To use a local build context, you can specify a relative or absolute filepath
to the docker build
command. The following example shows a build command that
uses the current directory (.
) as a build context:
$ docker build .
...
#16 [internal] load build context
#16 sha256:23ca2f94460dcbaf5b3c3edbaaa933281a4e0ea3d92fe295193e4df44dc68f85
#16 transferring context: 13.16MB 2.2s done
...
This makes files and directories in the current working directory available to the builder. The builder loads the files it needs from the build context when needed.
You can also use local tarballs as build context, by piping the tarball
contents to the docker build
command. See
Tarballs.
Local directories
Consider the following directory structure:
.
├── index.ts
├── src/
├── Dockerfile
├── package.json
└── package-lock.json
Dockerfile instructions can reference and include these files in the build if you pass this directory as a context.
# syntax=docker/dockerfile:1
FROM node:latest
WORKDIR /src
COPY package.json package-lock.json .
RUN npm ci
COPY index.ts src .
$ docker build .
Local context with Dockerfile from stdin
Use the following syntax to build an image using files on your local filesystem, while using a Dockerfile from stdin.
$ docker build -f- <PATH>
The syntax uses the -f (or --file) option to specify the Dockerfile to use, and it uses a hyphen (-) as filename to instruct Docker to read the Dockerfile from stdin.
The following example uses the current directory (.) as the build context, and builds an image using a Dockerfile passed through stdin using a here-document.
# create a directory to work in
mkdir example
cd example
# create an example file
touch somefile.txt
# build an image using the current directory as context
# and a Dockerfile passed through stdin
docker build -t myimage:latest -f- . <<EOF
FROM busybox
COPY somefile.txt ./
RUN cat /somefile.txt
EOF
Local tarballs
When you pipe a tarball to the build command, the build uses the contents of the tarball as a filesystem context.
For example, given the following project directory:
.
├── Dockerfile
├── Makefile
├── README.md
├── main.c
├── scripts
├── src
└── test.Dockerfile
You can create a tarball of the directory and pipe it to the build for use as a context:
$ tar czf foo.tar.gz *
$ docker build - < foo.tar.gz
The build resolves the Dockerfile from the tarball context. You can use the
--file
flag to specify the name and location of the Dockerfile relative to
the root of the tarball. The following command builds using test.Dockerfile
in the tarball:
$ docker build --file test.Dockerfile - < foo.tar.gz
Remote context
You can specify the address of a remote Git repository, tarball, or plain-text file as your build context.
- For Git repositories, the builder automatically clones the repository. See Git repositories.
- For tarballs, the builder downloads and extracts the contents of the tarball. See Tarballs.
If the remote tarball is a text file, the builder receives no filesystem context, and instead assumes that the remote file is a Dockerfile. See Empty build context.
Git repositories
When you pass a URL pointing to the location of a Git repository as an argument
to docker build
, the builder uses the repository as the build context.
The builder performs a shallow clone of the repository, downloading only the HEAD commit, not the entire history.
The builder recursively clones the repository and any submodules it contains.
$ docker build https://github.com/user/myrepo.git
By default, the builder clones the latest commit on the default branch of the repository that you specify.
URL fragments
You can append URL fragments to the Git repository address to make the builder clone a specific branch, tag, and subdirectory of a repository.
The format of the URL fragment is #ref:dir
, where:
ref
is the name of the branch, tag, or commit hashdir
is a subdirectory inside the repository
For example, the following command uses the container
branch,
and the docker
subdirectory in that branch, as the build context:
$ docker build https://github.com/user/myrepo.git#container:docker
The following table represents all the valid suffixes with their build contexts:
| Build Syntax Suffix | Commit Used | Build Context Used |
|---|---|---|
myrepo.git | refs/heads/<default branch> | / |
myrepo.git#mytag | refs/tags/mytag | / |
myrepo.git#mybranch | refs/heads/mybranch | / |
myrepo.git#pull/42/head | refs/pull/42/head | / |
myrepo.git#:myfolder | refs/heads/<default branch> | /myfolder |
myrepo.git#master:myfolder | refs/heads/master | /myfolder |
myrepo.git#mytag:myfolder | refs/tags/mytag | /myfolder |
myrepo.git#mybranch:myfolder | refs/heads/mybranch | /myfolder |
When you use a commit hash as the ref
in the URL fragment, use the full,
40-character string SHA-1 hash of the commit. A short hash, for example a hash
truncated to 7 characters, is not supported.
# ✅ The following works:
docker build github.com/docker/buildx#d4f088e689b41353d74f1a0bfcd6d7c0b213aed2
# ❌ The following doesn't work because the commit hash is truncated:
docker build github.com/docker/buildx#d4f088e
Keep .git
directory
By default, BuildKit doesn't keep the .git
directory when using Git contexts.
You can configure BuildKit to keep the directory by setting the
BUILDKIT_CONTEXT_KEEP_GIT_DIR
build argument.
This can be useful to if you want to retrieve Git information during your build:
# syntax=docker/dockerfile:1
FROM alpine
WORKDIR /src
RUN --mount=target=. \
make REVISION=$(git rev-parse HEAD) build
$ docker build \
--build-arg BUILDKIT_CONTEXT_KEEP_GIT_DIR=1
https://github.com/user/myrepo.git#main
Private repositories
When you specify a Git context that's also a private repository, the builder needs you to provide the necessary authentication credentials. You can use either SSH or token-based authentication.
Buildx automatically detects and uses SSH credentials if the Git context you
specify is an SSH or Git address. By default, this uses $SSH_AUTH_SOCK
.
You can configure the SSH credentials to use with the
--ssh
flag.
$ docker buildx build --ssh default git@github.com:user/private.git
If you want to use token-based authentication instead, you can pass the token
using the
--secret
flag.
$ GIT_AUTH_TOKEN=<token> docker buildx build \
--secret id=GIT_AUTH_TOKEN \
https://github.com/user/private.git
Note
Don't use
--build-arg
for secrets.
Remote context with Dockerfile from stdin
Use the following syntax to build an image using files on your local filesystem, while using a Dockerfile from stdin.
$ docker build -f- <URL>
The syntax uses the -f (or --file) option to specify the Dockerfile to use, and it uses a hyphen (-) as filename to instruct Docker to read the Dockerfile from stdin.
This can be useful in situations where you want to build an image from a repository that doesn't contain a Dockerfile. Or if you want to build with a custom Dockerfile, without maintaining your own fork of the repository.
The following example builds an image using a Dockerfile from stdin, and adds
the hello.c
file from the
hello-world
repository on GitHub.
docker build -t myimage:latest -f- https://github.com/docker-library/hello-world.git <<EOF
FROM busybox
COPY hello.c ./
EOF
Remote tarballs
If you pass the URL to a remote tarball, the URL itself is sent to the builder.
$ docker build http://server/context.tar.gz
#1 [internal] load remote build context
#1 DONE 0.2s
#2 copy /context /
#2 DONE 0.1s
...
The download operation will be performed on the host where the BuildKit daemon
is running. Note that if you're using a remote Docker context or a remote
builder, that's not necessarily the same machine as where you issue the build
command. BuildKit fetches the context.tar.gz
and uses it as the build
context. Tarball contexts must be tar archives conforming to the standard tar
Unix format and can be compressed with any one of the xz
, bzip2
, gzip
or
identity
(no compression) formats.
Empty context
When you use a text file as the build context, the builder interprets the file as a Dockerfile. Using a text file as context means that the build has no filesystem context.
You can build with an empty build context when your Dockerfile doesn't depend on any local files.
How to build without a context
You can pass the text file using a standard input stream, or by pointing at the URL of a remote text file.
$ docker build - < Dockerfile
Get-Content Dockerfile | docker build -
docker build -t myimage:latest - <<EOF
FROM busybox
RUN echo ""hello world""
EOF
$ docker build https://raw.githubusercontent.com/dvdksn/clockbox/main/Dockerfile
When you build without a filesystem context, Dockerfile instructions such as
COPY
can't refer to local files:
$ ls
main.c
$ docker build -<<< $'FROM scratch\nCOPY main.c .'
[+] Building 0.0s (4/4) FINISHED
=> [internal] load build definition from Dockerfile 0.0s
=> => transferring dockerfile: 64B 0.0s
=> [internal] load .dockerignore 0.0s
=> => transferring context: 2B 0.0s
=> [internal] load build context 0.0s
=> => transferring context: 2B 0.0s
=> ERROR [1/1] COPY main.c . 0.0s
------
> [1/1] COPY main.c .:
------
Dockerfile:2
--------------------
1 | FROM scratch
2 | >>> COPY main.c .
3 |
--------------------
ERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref 7ab2bb61-0c28-432e-abf5-a4c3440bc6b6::4lgfpdf54n5uqxnv9v6ymg7ih: ""/main.c"": not found
.dockerignore files
You can use a .dockerignore
file to exclude files or directories from the
build context.
# .dockerignore
node_modules
bar
This helps avoid sending unwanted files and directories to the builder, improving build speed, especially when using a remote builder.
Filename and location
When you run a build command, the build client looks for a file named
.dockerignore
in the root directory of the context. If this file exists, the
files and directories that match patterns in the files are removed from the
build context before it's sent to the builder.
If you use multiple Dockerfiles, you can use different ignore-files for each Dockerfile. You do so using a special naming convention for the ignore-files. Place your ignore-file in the same directory as the Dockerfile, and prefix the ignore-file with the name of the Dockerfile, as shown in the following example.
.
├── index.ts
├── src/
├── docker
│ ├── build.Dockerfile
│ ├── build.Dockerfile.dockerignore
│ ├── lint.Dockerfile
│ ├── lint.Dockerfile.dockerignore
│ ├── test.Dockerfile
│ └── test.Dockerfile.dockerignore
├── package.json
└── package-lock.json
A Dockerfile-specific ignore-file takes precedence over the .dockerignore
file at the root of the build context if both exist.
Syntax
The .dockerignore
file is a newline-separated list of patterns similar to the
file globs of Unix shells. Leading and trailing slashes in ignore patterns are
disregarded. The following patterns all exclude a file or directory named bar
in the subdirectory foo
under the root of the build context:
/foo/bar/
/foo/bar
foo/bar/
foo/bar
If a line in .dockerignore
file starts with #
in column 1, then this line
is considered as a comment and is ignored before interpreted by the CLI.
#/this/is/a/comment
If you're interested in learning the precise details of the .dockerignore
pattern matching logic, check out the
moby/patternmatcher repository
on GitHub, which contains the source code.
Matching
The following code snippet shows an example .dockerignore
file.
# comment
*/temp*
*/*/temp*
temp?
This file causes the following build behavior:
| Rule | Behavior |
|---|---|
# comment | Ignored. |
*/temp* | Exclude files and directories whose names start with temp in any immediate subdirectory of the root. For example, the plain file /somedir/temporary.txt is excluded, as is the directory /somedir/temp . |
*/*/temp* | Exclude files and directories starting with temp from any subdirectory that is two levels below the root. For example, /somedir/subdir/temporary.txt is excluded. |
temp? | Exclude files and directories in the root directory whose names are a one-character extension of temp . For example, /tempa and /tempb are excluded. |
Matching is done using Go's
filepath.Match
function rules.
A preprocessing step uses Go's
filepath.Clean
function
to trim whitespace and remove .
and ..
.
Lines that are blank after preprocessing are ignored.
Note
For historical reasons, the pattern
.
is ignored.
Beyond Go's filepath.Match
rules, Docker also supports a special wildcard
string **
that matches any number of directories (including zero). For
example, **/*.go
excludes all files that end with .go
found anywhere in the
build context.
You can use the .dockerignore
file to exclude the Dockerfile
and
.dockerignore
files. These files are still sent to the builder as they're
needed for running the build. But you can't copy the files into the image using
ADD
, COPY
, or bind mounts.
Negating matches
You can prepend lines with a !
(exclamation mark) to make exceptions to
exclusions. The following is an example .dockerignore
file that uses this
mechanism:
*.md
!README.md
All markdown files right under the context directory except README.md
are
excluded from the context. Note that markdown files under subdirectories are
still included.
The placement of !
exception rules influences the behavior: the last line of
the .dockerignore
that matches a particular file determines whether it's
included or excluded. Consider the following example:
*.md
!README*.md
README-secret.md
No markdown files are included in the context except README files other than
README-secret.md
.
Now consider this example:
*.md
README-secret.md
!README*.md
All of the README files are included. The middle line has no effect because
!README*.md
matches README-secret.md
and comes last.
Named contexts
In addition to the default build context (the positional argument to the
docker build
command), you can also pass additional named contexts to builds.
Named contexts are specified using the --build-context
flag, followed by a
name-value pair. This lets you include files and directories from multiple
sources during the build, while keeping them logically separated.
$ docker build --build-context docs=./docs .
In this example:
- The named
docs
context points to the./docs
directory. - The default context (
.
) points to the current working directory.
Using named contexts in a Dockerfile
Dockerfile instructions can reference named contexts as if they are stages in a multi-stage build.
For example, the following Dockerfile:
- Uses a
COPY
instruction to copy files from the default context into the current build stage. - Bind mounts the files in a named context to process the files as part of the build.
# syntax=docker/dockerfile:1
FROM buildbase
WORKDIR /app
# Copy all files from the default context into /app/src in the build container
COPY . /app/src
RUN make bin
# Mount the files from the named ""docs"" context to build the documentation
RUN --mount=from=docs,target=/app/docs \
make manpages
Use cases for named contexts
Using named contexts allows for greater flexibility and efficiency when building Docker images. Here are some scenarios where using named contexts can be useful:
Example: combine local and remote sources
You can define separate named contexts for different types of sources. For example, consider a project where the application source code is local, but the deployment scripts are stored in a Git repository:
$ docker build --build-context scripts=https://github.com/user/deployment-scripts.git .
In the Dockerfile, you can use these contexts independently:
# syntax=docker/dockerfile:1
FROM alpine:latest
# Copy application code from the main context
COPY . /opt/app
# Run deployment scripts using the remote ""scripts"" context
RUN --mount=from=scripts,target=/scripts /scripts/main.sh
Example: dynamic builds with custom dependencies
In some scenarios, you might need to dynamically inject configuration files or dependencies into the build from external sources. Named contexts make this straightforward by allowing you to mount different configurations without modifying the default build context.
$ docker build --build-context config=./configs/prod .
Example Dockerfile:
# syntax=docker/dockerfile:1
FROM nginx:alpine
# Use the ""config"" context for environment-specific configurations
COPY --from=config nginx.conf /etc/nginx/nginx.conf
Example: pin or override images
You can refer to named contexts in a Dockerfile the same way you can refer to an image. That means you can change an image reference in your Dockerfile by overriding it with a named context. For example, given the following Dockerfile:
FROM alpine:3.21
If you want to force image reference to resolve to a different version, without changing the Dockerfile, you can pass a context with the same name to the build. For example:
docker buildx build --build-context alpine:3.21=docker-image://alpine:edge .
The docker-image://
prefix marks the context as an image reference. The
reference can be a local image or an image in your registry.
Named contexts with Bake
Bake is a tool built into docker build
that
lets you manage your build configuration with a configuration file. Bake fully
supports named contexts.
To define named contexts in a Bake file:
target ""app"" {
contexts = {
docs = ""./docs""
}
}
This is equivalent to the following CLI invocation:
$ docker build --build-context docs=./docs .
Linking targets with named contexts
In addition to making complex builds more manageable, Bake also provides
additional features on top of what you can do with docker build
on the CLI.
You can use named contexts to create build pipelines, where one target depends
on and builds on top of another. For example, consider a Docker build setup
where you have two Dockerfiles:
base.Dockerfile
: for building a base imageapp.Dockerfile
: for building an application image
The app.Dockerfile
uses the image produced by base.Dockerfile
as it's base
image:
FROM mybaseimage
Normally, you would have to build the base image first, and then either load it
to Docker Engine's local image store or push it to a registry. With Bake, you
can reference other targets directly, creating a dependency between the app
target and the base
target.
target ""base"" {
dockerfile = ""base.Dockerfile""
}
target ""app"" {
dockerfile = ""app.Dockerfile""
contexts = {
# the target: prefix indicates that 'base' is a Bake target
mybaseimage = ""target:base""
}
}
With this configuration, references to mybaseimage
in app.Dockerfile
use
the results from building the base
target. Building the app
target will
also trigger a rebuild of mybaseimage
, if necessary:
$ docker buildx bake app
Further reading
For more information about working with named contexts, see:",,,
a27d0c0427b14990017781e5282d6b3fadf3971fb13f96b77d796deaa709f9c3,"Docker Engine 26.0 release notes
This page describes the latest changes, additions, known issues, and fixes for Docker Engine version 26.0.
For more information about:
- Deprecated and removed features, see Deprecated Engine Features.
- Changes to the Engine API, see Engine API version history.
26.0.2
2024-04-18For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 26.0.2 milestone
- moby/moby, 26.0.2 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
Security
This release contains a security fix for CVE-2024-32473, an unexpected configuration of IPv6 on IPv4-only interfaces.
Bug fixes and enhancements
CVE-2024-32473: Ensure IPv6 is disabled on interfaces only allocated an IPv4 address by the engine. moby#GHSA-x84c-p2g9-rqv9
26.0.1
2024-04-11For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 26.0.1 milestone
- moby/moby, 26.0.1 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
Bug fixes and enhancements
- Fix a regression that meant network interface specific
--sysctl
options prevented container startup. moby/moby#47646 - Remove erroneous
platform
from imageconfig
OCI descriptor indocker save
output. moby/moby#47694 - containerd image store: OCI archives produced by
docker save
will now have a non-emptymediaType
field inindex.json
moby/moby#47701 - Fix a regression that prevented the internal resolver from forwarding requests from IPvlan L3 networks to external resolvers. moby/moby#47705
- Prevent the use of external resolvers in IPvlan and Macvlan networks created with no parent interface specified. moby/moby#47705
Packaging updates
- Update Go runtime to 1.21.9 moby/moby#47671, docker/cli#4987
- Update Compose to v1.26.1 , docker/docker-ce-packaging#1009
- Update containerd to v1.7.15 (static binaries only) moby/moby#47692
26.0.0
2024-03-20For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 26.0.0 milestone
- moby/moby, 26.0.0 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
Security
This release contains a security fix for CVE-2024-29018, a potential data exfiltration from 'internal' networks via authoritative DNS servers.
New
- Add
Subpath
field to theVolumeOptions
making it possible to mount a subpath of a volume. moby/moby#45687 - Add
volume-subpath
support to the mount flag (--mount type=volume,...,volume-subpath=<subpath>
). docker/cli#4331 - Accept
=
separators and[ipv6]
in compose files fordocker stack deploy
. docker/cli#4860 - rootless: Add support for enabling host loopback by setting the
DOCKERD_ROOTLESS_ROOTLESSKIT_DISABLE_HOST_LOOPBACK
environment variable tofalse
(defaults totrue
). This lets containers connect to the host by using IP address10.0.2.2
. moby/moby#47352 - containerd image store:
docker image ls
no longer creates duplicates entries for multi-platform images. moby/moby#45967 - containerd image store: Send Prometheus metrics. moby/moby#47555
Bug fixes and enhancements
CVE-2024-29018: Do not forward requests to external DNS servers for a container that is only connected to an 'internal' network. Previously, requests were forwarded if the host's DNS server was running on a loopback address, like systemd's 127.0.0.53. moby/moby#47589
Ensure that a generated MAC address is not restored when a container is restarted, but a configured MAC address is preserved. moby/moby#47233
Warning
Containers created using Docker Engine 25.0.0 may have duplicate MAC addresses, they must be re-created. Containers created using version 25.0.0 or 25.0.1 with user-defined MAC addresses will get generated MAC addresses when they are started using 25.0.2. They must also be re-created.
Always attempt to enable IPv6 on a container's loopback interface, and only include IPv6 in
/etc/hosts
if successful. moby/moby#47062Note
By default, IPv6 will remain enabled on a container's loopback interface when the container is not connected to an IPv6-enabled network. For example, containers that are only connected to an IPv4-only network now have the
::1
address on their loopback interface.To disable IPv6 in a container, use option
--sysctl net.ipv6.conf.all.disable_ipv6=1
in thecreate
orrun
command, or the equivalentsysctls
option in the service configuration section of a Compose file.If IPv6 is not available in a container because it has been explicitly disabled for the container, or the host's networking stack does not have IPv6 enabled (or for any other reason) the container's
/etc/hosts
file will not include IPv6 entries.Fix
ADD
Dockerfile instruction failing withlsetxattr <file>: operation not supported
when unpacking archive with xattrs onto a filesystem that doesn't support them. moby/moby#47175Fix
docker container start
failing when used with--checkpoint
. moby/moby#47456Restore IP connectivity between the host and containers on an internal bridge network. moby/moby#47356
Do not enforce new validation rules for existing swarm networks. moby/moby#47361
Restore DNS names for containers in the default ""nat"" network on Windows. moby/moby#47375
Print hint when invoking
docker image ls
with ambiguous argument. docker/cli#4849Cleanup
@docker_cli_[UUID]
files on OpenBSD. docker/cli#4862Add explicit deprecation notice message when using remote TCP connections without TLS. docker/cli#4928, moby/moby#47556
Use IPv6 nameservers from the host's
resolv.conf
as upstream resolvers for Docker Engine's internal DNS, rather than listing them in the container'sresolv.conf
. moby/moby#47512containerd image store: Isolate images with different containerd namespaces when
--userns-remap
option is used. moby/moby#46786containerd image store: Fix image pull not emitting
Pulling fs layer
status. moby/moby#47432
API
- To preserve backwards compatibility, read-only mounts are not recursive by default when using older clients (API version < v1.44). moby/moby#47391
GET /images/{id}/json
omits theCreated
field (previously it was0001-01-01T00:00:00Z
) if theCreated
field is missing from the image config. moby/moby#47451- Populate a missing
Created
field inGET /images/{id}/json
with0001-01-01T00:00:00Z
for API version <= 1.43. moby/moby#47387 - The
is_automated
field in thePOST /images/search
endpoint results is alwaysfalse
now. Consequently, searching foris-automated=true
will yield no results, whileis-automated=false
will be a no-op. moby/moby#47465 - Remove
Container
andContainerConfig
fields from theGET /images/{name}/json
response. moby/moby#47430
Packaging updates
- Update BuildKit to v0.13.1. moby/moby#47582
- Update Buildx to v0.13.1. docker/docker-ce-packaging#1000
- Update Compose to v2.25.0. docker/docker-ce-packaging#1002
- Update Go runtime to 1.21.8. moby/moby#47502
- Update RootlessKit to v2.0.2. moby/moby#47508
- Update containerd to v1.7.13 (static binaries only) moby/moby#47278
- Update runc binary to v1.1.12 moby/moby#47268
- Update OTel to v0.46.1 / v1.21.0 moby/moby#47245
Removed
Remove
Container
andContainerConfig
fields from theGET /images/{name}/json
response. moby/moby#47430Deprecate the ability to accept remote TCP connections without TLS. Deprecation notice docker/cli#4928 moby/moby#47556.
Remove deprecated API versions (API < v1.24) moby/moby#47155
Disable pulling of deprecated image formats by default. These image formats are deprecated, and support will be removed in a future version. moby/moby#47459
image: remove deprecated IDFromDigest moby/moby#47198
Remove the deprecated
github.com/docker/docker/pkg/loopback
package. moby/moby#47128pkg/system: remove deprecated
ErrNotSupportedOperatingSystem
,IsOSSupported
moby/moby#47129pkg/homedir: remove deprecated Key() and GetShortcutString() moby/moby#47130
pkg/containerfs: remove deprecated ResolveScopedPath moby/moby#47131
The daemon flag
--oom-score-adjust
was deprecated in v24.0 and is now removed. moby/moby#46113Remove deprecated aliases from the api/types package. These types were deprecated in v25.0.0, which provided temporary aliases. moby/moby#47148 These aliases are now removed:
types.Info
,types.Commit
,types.PluginsInfo
,types.NetworkAddressPool
,types.Runtime
,types.SecurityOpt
,types.KeyValue
,types.DecodeSecurityOptions
,types.CheckpointCreateOptions
,types.CheckpointListOptions
,types.CheckpointDeleteOptions
,types.Checkpoint
,types.ImageDeleteResponseItem
,types.ImageSummary
,types.ImageMetadata
,types.ServiceUpdateResponse
,types.ServiceCreateResponse
,types.ResizeOptions
,types.ContainerAttachOptions
,types.ContainerCommitOptions
,types.ContainerRemoveOptions
,types.ContainerStartOptions
,types.ContainerListOptions
,types.ContainerLogsOptions
cli/command/container: remove deprecated
NewStartOptions()
docker/cli#4811cli/command: remove deprecated
DockerCliOption
,InitializeOpt
docker/cli#4810",,,
07283b7d7b7dc1029ef2f6edf8e69f0094d409b65ec422df4f57ad6437708d8f,"Integrate Docker Scout with GitHub Actions
The following example shows how to set up a Docker Scout workflow with GitHub Actions. Triggered by a pull request, the action builds the image and uses Docker Scout to compare the new version to the version of that image running in production.
This workflow uses the
docker/scout-action GitHub Action to
run the docker scout compare
command to visualize how images for pull request
stack up against the image you run in production.
Prerequisites
- This example assumes that you have an existing image repository, in Docker Hub or in another registry, where you've enabled Docker Scout.
- This example makes use of
environments, to compare
the image built in the pull request with a different version of the same image
in an environment called
production
.
Steps
First, set up the GitHub Action workflow to build an image. This isn't specific to Docker Scout here, but you'll need to build an image to have something to compare with.
Add the following to a GitHub Actions YAML file:
name: Docker
on:
push:
tags: [""*""]
branches:
- ""main""
pull_request:
branches: [""**""]
env:
# Hostname of your registry
REGISTRY: docker.io
# Image repository, without hostname and tag
IMAGE_NAME: ${{ github.repository }}
SHA: ${{ github.event.pull_request.head.sha || github.event.after }}
jobs:
build:
runs-on: ubuntu-latest
permissions:
pull-requests: write
steps:
# Authenticate to the container registry
- name: Authenticate to registry ${{ env.REGISTRY }}
uses: docker/login-action@v3
with:
registry: ${{ env.REGISTRY }}
username: ${{ secrets.REGISTRY_USER }}
password: ${{ secrets.REGISTRY_TOKEN }}
- name: Setup Docker buildx
uses: docker/setup-buildx-action@v3
# Extract metadata (tags, labels) for Docker
- name: Extract Docker metadata
id: meta
uses: docker/metadata-action@v5
with:
images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
labels: |
org.opencontainers.image.revision=${{ env.SHA }}
tags: |
type=edge,branch=$repo.default_branch
type=semver,pattern=v{{version}}
type=sha,prefix=,suffix=,format=short
# Build and push Docker image with Buildx
# (don't push on PR, load instead)
- name: Build and push Docker image
id: build-and-push
uses: docker/build-push-action@v6
with:
sbom: ${{ github.event_name != 'pull_request' }}
provenance: ${{ github.event_name != 'pull_request' }}
push: ${{ github.event_name != 'pull_request' }}
load: ${{ github.event_name == 'pull_request' }}
tags: ${{ steps.meta.outputs.tags }}
labels: ${{ steps.meta.outputs.labels }}
cache-from: type=gha
cache-to: type=gha,mode=max
This creates workflow steps to:
- Set up Docker buildx.
- Authenticate to the registry.
- Extract metadata from Git reference and GitHub events.
- Build and push the Docker image to the registry.
Note
This CI workflow runs a local analysis and evaluation of your image. To evaluate the image locally, you must ensure that the image is loaded the local image store of your runner.
This comparison doesn't work if you push the image to a registry, or if you build an image that can't be loaded to the runner's local image store. For example, multi-platform images or images with SBOM or provenance attestation can't be loaded to the local image store.
With this setup out of the way, you can add the following steps to run the image comparison:
# You can skip this step if Docker Hub is your registry
# and you already authenticated before
- name: Authenticate to Docker
uses: docker/login-action@v3
with:
username: ${{ secrets.DOCKER_USER }}
password: ${{ secrets.DOCKER_PAT }}
# Compare the image built in the pull request with the one in production
- name: Docker Scout
id: docker-scout
if: ${{ github.event_name == 'pull_request' }}
uses: docker/scout-action@v1
with:
command: compare
image: ${{ steps.meta.outputs.tags }}
to-env: production
ignore-unchanged: true
only-severities: critical,high
github-token: ${{ secrets.GITHUB_TOKEN }}
The compare command analyzes the image and evaluates policy compliance, and
cross-references the results with the corresponding image in the production
environment. This example only includes critical and high-severity
vulnerabilities, and excludes vulnerabilities that exist in both images,
showing only what's changed.
The GitHub Action outputs the comparison results in a pull request comment by default.
Expand the Policies section to view the difference in policy compliance between the two images. Note that while the new image in this example isn't fully compliant, the output shows that the standing for the new image has improved compared to the baseline.",,,
6a1aa29959f63997c0edd14804ed6fb3e33c9d58c8ea6490097b60f3c003734a,"Install Docker Engine on Fedora
To get started with Docker Engine on Fedora, make sure you meet the prerequisites, and then follow the installation steps.
Prerequisites
OS requirements
To install Docker Engine, you need a maintained version of one of the following Fedora versions:
- Fedora 40
- Fedora 41
Uninstall old versions
Before you can install Docker Engine, you need to uninstall any conflicting packages.
Your Linux distribution may provide unofficial Docker packages, which may conflict with the official packages provided by Docker. You must uninstall these packages before you install the official version of Docker Engine.
$ sudo dnf remove docker \
docker-client \
docker-client-latest \
docker-common \
docker-latest \
docker-latest-logrotate \
docker-logrotate \
docker-selinux \
docker-engine-selinux \
docker-engine
dnf
might report that you have none of these packages installed.
Images, containers, volumes, and networks stored in /var/lib/docker/
aren't
automatically removed when you uninstall Docker.
Installation methods
You can install Docker Engine in different ways, depending on your needs:
You can set up Docker's repositories and install from them, for ease of installation and upgrade tasks. This is the recommended approach.
You can download the RPM package, install it manually, and manage upgrades completely manually. This is useful in situations such as installing Docker on air-gapped systems with no access to the internet.
In testing and development environments, you can use automated convenience scripts to install Docker.
Install using the rpm repository
Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository.
Set up the repository
Install the dnf-plugins-core
package (which provides the commands to manage
your DNF repositories) and set up the repository.
$ sudo dnf -y install dnf-plugins-core
$ sudo dnf-3 config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo
Install Docker Engine
Install the Docker packages.
To install the latest version, run:
$ sudo dnf install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
If prompted to accept the GPG key, verify that the fingerprint matches
060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35
, and if so, accept it.This command installs Docker, but it doesn't start Docker. It also creates a
docker
group, however, it doesn't add any users to the group by default.To install a specific version, start by listing the available versions in the repository:
$ dnf list docker-ce --showduplicates | sort -r docker-ce.x86_64 3:28.0.1-1.fc41 docker-ce-stable docker-ce.x86_64 3:28.0.0-1.fc41 docker-ce-stable <...>
The list returned depends on which repositories are enabled, and is specific to your version of Fedora (indicated by the
.fc40
suffix in this example).Install a specific version by its fully qualified package name, which is the package name (
docker-ce
) plus the version string (2nd column), separated by a hyphen (-
). For example,docker-ce-3:28.0.1-1.fc41
.Replace
<VERSION_STRING>
with the desired version and then run the following command to install:$ sudo dnf install docker-ce-<VERSION_STRING> docker-ce-cli-<VERSION_STRING> containerd.io docker-buildx-plugin docker-compose-plugin
This command installs Docker, but it doesn't start Docker. It also creates a
docker
group, however, it doesn't add any users to the group by default.Start Docker Engine.
$ sudo systemctl enable --now docker
This configures the Docker systemd service to start automatically when you boot your system. If you don't want Docker to start automatically, use
sudo systemctl start docker
instead.Verify that the installation is successful by running the
hello-world
image:$ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Upgrade Docker Engine
To upgrade Docker Engine, follow the installation instructions, choosing the new version you want to install.
Install from a package
If you can't use Docker's rpm
repository to install Docker Engine, you can
download the .rpm
file for your release and install it manually. You need to
download a new file each time you want to upgrade Docker Engine.
Go to https://download.docker.com/linux/fedora/ and choose your version of Fedora. Then browse to
x86_64/stable/Packages/
and download the.rpm
file for the Docker version you want to install.Install Docker Engine, changing the following path to the path where you downloaded the Docker package.
$ sudo dnf install /path/to/package.rpm
Docker is installed but not started. The
docker
group is created, but no users are added to the group.Start Docker Engine.
$ sudo systemctl enable --now docker
This configures the Docker systemd service to start automatically when you boot your system. If you don't want Docker to start automatically, use
sudo systemctl start docker
instead.Verify that the installation is successful by running the
hello-world
image:$ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Upgrade Docker Engine
To upgrade Docker Engine, download the newer package files and repeat the
installation procedure, using dnf upgrade
instead of dnf install
, and point to the new files.
Install using the convenience script
Docker provides a convenience script at
https://get.docker.com/ to install Docker into
development environments non-interactively. The convenience script isn't
recommended for production environments, but it's useful for creating a
provisioning script tailored to your needs. Also refer to the
install using the repository steps to learn
about installation steps to install using the package repository. The source code
for the script is open source, and you can find it in the
docker-install
repository on GitHub.
Always examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script:
- The script requires
root
orsudo
privileges to run. - The script attempts to detect your Linux distribution and version and configure your package management system for you.
- The script doesn't allow you to customize most installation parameters.
- The script installs dependencies and recommendations without asking for confirmation. This may install a large number of packages, depending on the current configuration of your host machine.
- By default, the script installs the latest stable release of Docker, containerd, and runc. When using this script to provision a machine, this may result in unexpected major version upgrades of Docker. Always test upgrades in a test environment before deploying to your production systems.
- The script isn't designed to upgrade an existing Docker installation. When using the script to update an existing installation, dependencies may not be updated to the expected version, resulting in outdated versions.
Tip
Preview script steps before running. You can run the script with the
--dry-run
option to learn what steps the script will run when invoked:$ curl -fsSL https://get.docker.com -o get-docker.sh $ sudo sh ./get-docker.sh --dry-run
This example downloads the script from https://get.docker.com/ and runs it to install the latest stable release of Docker on Linux:
$ curl -fsSL https://get.docker.com -o get-docker.sh
$ sudo sh get-docker.sh
Executing docker install script, commit: 7cae5f8b0decc17d6571f9f52eb840fbc13b2737
<...>
You have now successfully installed and started Docker Engine. The docker
service starts automatically on Debian based distributions. On RPM
based
distributions, such as CentOS, Fedora, RHEL or SLES, you need to start it
manually using the appropriate systemctl
or service
command. As the message
indicates, non-root users can't run Docker commands by default.
Use Docker as a non-privileged user, or install in rootless mode?
The installation script requires
root
orsudo
privileges to install and use Docker. If you want to grant non-root users access to Docker, refer to the post-installation steps for Linux. You can also install Docker withoutroot
privileges, or configured to run in rootless mode. For instructions on running Docker in rootless mode, refer to run the Docker daemon as a non-root user (rootless mode).
Install pre-releases
Docker also provides a convenience script at
https://test.docker.com/ to install pre-releases of
Docker on Linux. This script is equal to the script at get.docker.com
, but
configures your package manager to use the test channel of the Docker package
repository. The test channel includes both stable and pre-releases (beta
versions, release-candidates) of Docker. Use this script to get early access to
new releases, and to evaluate them in a testing environment before they're
released as stable.
To install the latest version of Docker on Linux from the test channel, run:
$ curl -fsSL https://test.docker.com -o test-docker.sh
$ sudo sh test-docker.sh
Upgrade Docker after using the convenience script
If you installed Docker using the convenience script, you should upgrade Docker using your package manager directly. There's no advantage to re-running the convenience script. Re-running it can cause issues if it attempts to re-install repositories which already exist on the host machine.
Uninstall Docker Engine
Uninstall the Docker Engine, CLI, containerd, and Docker Compose packages:
$ sudo dnf remove docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras
Images, containers, volumes, or custom configuration files on your host aren't automatically removed. To delete all images, containers, and volumes:
$ sudo rm -rf /var/lib/docker $ sudo rm -rf /var/lib/containerd
You have to delete any edited configuration files manually.
Next steps
- Continue to Post-installation steps for Linux.",,,
ecee4b3a4c0ee3328d9e551fce47f9432cb62527523a35458acf1dff4cd43e49,"Docker Engine 17.06 release notes
Table of contents
17.06.2-ce
2017-09-05
Client
- Enable TCP keepalive in the client to prevent loss of connection docker/cli#415
Runtime
- Devmapper: ensure UdevWait is called after calls to setCookie moby/moby#33732
- Aufs: ensure diff layers are correctly removed to prevent leftover files from using up storage moby/moby#34587
Swarm mode
- Ignore PullOptions for running tasks docker/swarmkit#2351
17.06.1-ce
2017-08-15
Builder
- Fix a regression, where
ADD
from remote URL's extracted archives #89 - Fix handling of remote ""git@"" notation #100
- Fix copy
--from
conflict with force pull #86
Client
- Make pruning volumes optional when running
docker system prune
, and add a--volumes
flag #109 - Show progress of replicated tasks before they are assigned #97
- Fix
docker wait
hanging if the container does not exist #106 - If
docker swarm ca
is called without the--rotate
flag, warn if other flags are passed #110 - Fix API version negotiation not working if the daemon returns an error #115
- Print an error if ""until"" filter is combined with ""--volumes"" on system prune #154
Logging
- Fix stderr logging for
journald
andsyslog
#95 - Fix log readers can block writes indefinitely #98
- Fix
awslogs
driver repeating last event #151
Networking
- Fix issue with driver options not received by network drivers #127
Plugins
- Make plugin removes more resilient to failure #91
Runtime
- Prevent a
goroutine
leak whenhealthcheck
gets stopped #90 - Do not error on relabel when relabel not supported #92
- Limit max backoff delay to 2 seconds for GRPC connection #94
- Fix issue preventing containers to run when memory cgroup was specified due to bug in certain kernels #102
- Fix container not responding to SIGKILL when paused #102
- Improve error message if an image for an incompatible OS is loaded #108
- Fix a handle leak in
go-winio
#112 - Fix issue upon upgrade, preventing docker from showing running containers when
--live-restore
is enabled #117 - Fix bug where services using secrets would fail to start on daemons using the
userns-remap
feature #121 - Fix error handling with
not-exist
errors on remove #142 - Fix REST API Swagger representation cannot be loaded with SwaggerUI #156
Security
- Redact secret data on secret creation #99
Swarm mode
- Do not add duplicate platform information to service spec #107
- Cluster update and memory issue fixes #114
- Changing get network request to return predefined network in swarm #150
17.06.0-ce
2017-06-28
Note
of the
ADD
instruction of Dockerfile when referencing a remote.tar.gz
file. The issue will be fixed in Docker 17.06.1.
Note
for IBM Z using the s390x architecture.
Note
registries. If you require interaction with registries that have not yet migrated to the v2 protocol, set the
--disable-legacy-registry=false
daemon option. Interaction with v1 registries will be removed in Docker 17.12.
Builder
- Add
--iidfile
option to docker build. It allows specifying a location where to save the resulting image ID - Allow specifying any remote ref in git checkout URLs #32502
Client
- Add
--format
option todocker stack ls
#31557 - Add support for labels in compose initiated builds #32632 #32972
- Add
--format
option todocker history
#30962 - Add
--format
option todocker system df
#31482 - Allow specifying Nameservers and Search Domains in stack files #32059
- Add support for
read_only
service todocker stack deploy
#docker/cli/73
- Display Swarm cluster and node TLS information #docker/cli/44
- Add support for placement preference to
docker stack deploy
#docker/cli/35 - Add new
ca
subcommand todocker swarm
to allow managing a swarm CA #docker/cli/48 - Add credential-spec to compose #docker/cli/71
- Add support for csv format options to
--network
and--network-add
#docker/cli/62 #33130
- Fix stack compose bind-mount volumes on Windows #docker/cli/136
- Correctly handle a Docker daemon without registry info #docker/cli/126
- Allow
--detach
and--quiet
flags when using --rollback #docker/cli/144 - Remove deprecated
--email
flag fromdocker login
#docker/cli/143
- Adjusted
docker stats
memory output #docker/cli/80
Distribution
- Select digest over tag when both are provided during a pull #33214
Logging
- Add monitored resource type metadata for GCP logging driver #32930
- Add multiline processing to the AWS CloudWatch logs driver #30891
Networking
- Add Support swarm-mode services with node-local networks such as macvlan, ipvlan, bridge, host #32981
- Pass driver-options to network drivers on service creation #32981
- Isolate Swarm Control-plane traffic from Application data traffic using --data-path-addr #32717
- Several improvements to Service Discovery #docker/libnetwork/1796
Packaging
- Rely on
container-selinux
on Centos/Fedora/RHEL when available #32437
Runtime
- Add build & engine info prometheus metrics #32792
- Update containerd to d24f39e203aa6be4944f06dd0fe38a618a36c764 #33007
- Update runc to 992a5be178a62e026f4069f443c6164912adbf09 #33007
- Add option to auto-configure blkdev for devmapper #31104
- Add log driver list to
docker info
#32540 - Add API endpoint to allow retrieving an image manifest #32061
- Do not remove container from memory on error with
forceremove
#31012
- Add support for metric plugins #32874
- Return an error when an invalid filter is given to
prune
commands #33023
- Add daemon option to allow pushing foreign layers #33151
- Fix an issue preventing containerd to be restarted after it died #32986
- Upgrade to Go 1.8.3 #33387
- Prevent a containerd crash when journald is restarted #containerd/930
- Fix healthcheck failures due to invalid environment variables #33249
- Prevent a directory to be created in lieu of the daemon socket when a container mounting it is to be restarted during a shutdown #30348
- Prevent a container to be restarted upon stop if its stop signal is set to
SIGKILL
#33335 - Ensure log drivers get passed the same filename to both StartLogging and StopLogging endpoints #33583
- Remove daemon data structure dump on
SIGUSR1
to avoid a panic #33598
Security
- Allow personality with UNAME26 bit set in default seccomp profile #32965
Swarm Mode
- Add an option to allow specifying a different interface for the data traffic (as opposed to control traffic) #32717
- Allow specifying a secret location within the container #32571
- Add support for secrets on Windows #32208
- Add TLS Info to swarm info and node info endpoint #32875
- Add support for services to carry arbitrary config objects #32336, #docker/cli/45, #33169
- Add API to rotate swarm CA certificate #32993
- Placement now also take platform in account #33144
- Fix possible hang when joining fails #docker-ce/19
- Fix an issue preventing external CA to be accepted #33341
- Fix possible orchestration panic in mixed version clusters #swarmkit/2233
- Avoid assigning duplicate IPs during initialization #swarmkit/2237
Deprecation
- Disable legacy registry (v1) by default #33629",,,
4190ce5c46f7b0bb85fbb91d0bf315ce69d709f6b140d1a6c3028cdfb5414d76,"Create a simple extension
To start creating your extension, you first need a directory with files which range from the extension’s source code to the required extension-specific files. This page provides information on how to set up a minimal frontend extension based on plain HTML.
Before you start, make sure you have installed the latest version of Docker Desktop.
Tip
If you want to start a codebase for your new extension, our Quickstart guide and
docker extension init <my-extension>
provides a better base for your extension.
Extension folder structure
In the minimal-frontend
sample folder, you can find a ready-to-go example that represents a UI Extension built on HTML. We will go through this code example in this tutorial.
Although you can start from an empty directory, it is highly recommended that you start from the template below and change it accordingly to suit your needs.
.
├── Dockerfile # (1)
├── metadata.json # (2)
└── ui # (3)
└── index.html
- Contains everything required to build the extension and run it in Docker Desktop.
- A file that provides information about the extension such as the name, description, and version.
- The source folder that contains all your HTML, CSS and JS files. There can also be other static assets such as logos and icons. For more information and guidelines on building the UI, see the Design and UI styling section.
Create a Dockerfile
At a minimum, your Dockerfile needs:
- Labels which provide extra information about the extension, icon and screenshots.
- The source code which in this case is an
index.html
that sits within theui
folder. - The
metadata.json
file.
# syntax=docker/dockerfile:1
FROM scratch
LABEL org.opencontainers.image.title=""Minimal frontend"" \
org.opencontainers.image.description=""A sample extension to show how easy it's to get started with Desktop Extensions."" \
org.opencontainers.image.vendor=""Awesome Inc."" \
com.docker.desktop.extension.api.version=""0.3.3"" \
com.docker.desktop.extension.icon=""https://www.docker.com/wp-content/uploads/2022/03/Moby-logo.png""
COPY ui ./ui
COPY metadata.json .
Configure the metadata file
A metadata.json
file is required at the root of the image filesystem.
{
""ui"": {
""dashboard-tab"": {
""title"": ""Minimal frontend"",
""root"": ""/ui"",
""src"": ""index.html""
}
}
}
For more information on the metadata.json
, see
Metadata.
Build the extension and install it
Now that you have configured the extension, you need to build the extension image that Docker Desktop will use to install it.
$ docker build --tag=awesome-inc/my-extension:latest .
This built an image tagged awesome-inc/my-extension:latest
, you can run docker inspect awesome-inc/my-extension:latest
to see more details about it.
Finally, you can install the extension and see it appearing in the Docker Desktop Dashboard.
$ docker extension install awesome-inc/my-extension:latest
Preview the extension
To preview the extension in Docker Desktop, close and open the Docker Desktop Dashboard once the installation is complete.
The left-hand menu displays a new tab with the name of your extension.
What's next?
- Build a more advanced frontend extension.
- Learn how to test and debug your extension.
- Learn how to setup CI for your extension.
- Learn more about extensions architecture.",,,
0e14404f1dd12a59b98f04a60d959d4ae8c1b678960b04e8bfeeae00fa22b373,"Security
Table of contents
Docker provides security guardrails for both administrators and developers.
If you're an administrator, you can enforce sign-in across Docker products for your developers, and scale, manage, and secure your instances of Docker Desktop with DevOps security controls like Enhanced Container Isolation and Registry Access Management.
For both administrators and developers, Docker provides security-specific products such as Docker Scout, for securing your software supply chain with proactive image vulnerability monitoring and remediation strategies.
For administrators
Explore the security features Docker offers to satisfy your company's security policies.
For developers
See how you can protect your local environments, infrastructure, and networks without impeding productivity.",,,
8d83dd89ecfc4632f78fb5699374f8415e7ab4f5c69925a1a67e70ad65c9f23d,"Deactivate an account
You can deactivate an account at any time. This section describes the prerequisites and steps to deactivate a user account. For information on deactivating an organization, see Deactivating an organization.
Warning
All Docker products and services that use your Docker account will be inaccessible after deactivating your account.
Prerequisites
Before deactivating your Docker account, ensure you meet the following requirements:
For owners, you must leave your organization or company before deactivating your Docker account. To do this:
- Sign in to the Docker Admin Console.
- Select the organization you need to leave from the Choose profile page.
- Find your username in the Members tab.
- Select the More options menu and then select Leave organization.
If you are the sole owner of an organization, you must assign the owner role to another member of the organization and then remove yourself from the organization, or deactivate the organization. Similarly, if you are the sole owner of a company, either add someone else as a company owner and then remove yourself, or deactivate the company.
If you have an active Docker subscription, downgrade it to a Docker Personal subscription.
Download any images and tags you want to keep. Use
docker pull -a <image>:<tag>
.Unlink your GitHub and Bitbucket accounts.
Deactivate
Once you have completed all the previous steps, you can deactivate your account.
Warning
This cannot be undone. Be sure you've gathered all the data you need from your account before deactivating it.
- Sign in to Docker Home.
- Select your avatar to open the drop-down menu.
- Select Account settings.
- Select Deactivate.
- Select Deactivate account.
- To confirm, select Deactivate account.",,,
d1c248255d7cbf7ebf974d52d0fddbdf37bcbc33b29c9eb20bcac91bb5d4481c,"Configure logging drivers
Docker includes multiple logging mechanisms to help you get information from running containers and services. These mechanisms are called logging drivers. Each Docker daemon has a default logging driver, which each container uses unless you configure it to use a different logging driver, or log driver for short.
As a default, Docker uses the
json-file
logging driver, which
caches container logs as JSON internally. In addition to using the logging drivers
included with Docker, you can also implement and use
logging driver plugins.
Tip
Use the
local
logging driver to prevent disk-exhaustion. By default, no log-rotation is performed. As a result, log-files stored by the defaultjson-file
logging driver logging driver can cause a significant amount of disk space to be used for containers that generate much output, which can lead to disk space exhaustion.Docker keeps the json-file logging driver (without log-rotation) as a default to remain backwards compatible with older versions of Docker, and for situations where Docker is used as runtime for Kubernetes.
For other situations, the
local
logging driver is recommended as it performs log-rotation by default, and uses a more efficient file format. Refer to the Configure the default logging driver section below to learn how to configure thelocal
logging driver as a default, and the local file logging driver page for more details about thelocal
logging driver.
Configure the default logging driver
To configure the Docker daemon to default to a specific logging driver, set the
value of log-driver
to the name of the logging driver in the daemon.json
configuration file. Refer to the ""daemon configuration file"" section in the
dockerd
reference manual
for details.
The default logging driver is json-file
. The following example sets the default
logging driver to the
local
log driver:
{
""log-driver"": ""local""
}
If the logging driver has configurable options, you can set them in the
daemon.json
file as a JSON object with the key log-opts
. The following
example sets four configurable options on the json-file
logging driver:
{
""log-driver"": ""json-file"",
""log-opts"": {
""max-size"": ""10m"",
""max-file"": ""3"",
""labels"": ""production_status"",
""env"": ""os,customer""
}
}
Restart Docker for the changes to take effect for newly created containers. Existing containers don't use the new logging configuration automatically.
Note
log-opts
configuration options in thedaemon.json
configuration file must be provided as strings. Boolean and numeric values (such as the value formax-file
in the example above) must therefore be enclosed in quotes (""
).
If you don't specify a logging driver, the default is json-file
.
To find the current default logging driver for the Docker daemon, run
docker info
and search for Logging Driver
. You can use the following
command on Linux, macOS, or PowerShell on Windows:
$ docker info --format '{{.LoggingDriver}}'
json-file
Note
Changing the default logging driver or logging driver options in the daemon configuration only affects containers that are created after the configuration is changed. Existing containers retain the logging driver options that were used when they were created. To update the logging driver for a container, the container has to be re-created with the desired options. Refer to the configure the logging driver for a container section below to learn how to find the logging-driver configuration of a container.
Configure the logging driver for a container
When you start a container, you can configure it to use a different logging
driver than the Docker daemon's default, using the --log-driver
flag. If the
logging driver has configurable options, you can set them using one or more
instances of the --log-opt <NAME>=<VALUE>
flag. Even if the container uses the
default logging driver, it can use different configurable options.
The following example starts an Alpine container with the none
logging driver.
$ docker run -it --log-driver none alpine ash
To find the current logging driver for a running container, if the daemon
is using the json-file
logging driver, run the following docker inspect
command, substituting the container name or ID for <CONTAINER>
:
$ docker inspect -f '{{.HostConfig.LogConfig.Type}}' <CONTAINER>
json-file
Configure the delivery mode of log messages from container to log driver
Docker provides two modes for delivering messages from the container to the log driver:
- (default) direct, blocking delivery from container to driver
- non-blocking delivery that stores log messages in an intermediate per-container buffer for consumption by driver
The non-blocking
message delivery mode prevents applications from blocking due
to logging back pressure. Applications are likely to fail in unexpected ways when
STDERR or STDOUT streams block.
Warning
When the buffer is full, new messages will not be enqueued. Dropping messages is often preferred to blocking the log-writing process of an application.
The mode
log option controls whether to use the blocking
(default) or
non-blocking
message delivery.
The max-buffer-size
controls the size of the buffer used for
intermediate message storage when mode
is set to non-blocking
.
The default is 1m
meaning 1 MB (1 million bytes).
See
function FromHumanSize()
in the go-units
package for the allowed format strings,
some examples are 1KiB
for 1024 bytes, 2g
for 2 billion bytes.
The following example starts an Alpine container with log output in non-blocking mode and a 4 megabyte buffer:
$ docker run -it --log-opt mode=non-blocking --log-opt max-buffer-size=4m alpine ping 127.0.0.1
Use environment variables or labels with logging drivers
Some logging drivers add the value of a container's --env|-e
or --label
flags to the container's logs. This example starts a container using the Docker
daemon's default logging driver (in the following example, json-file
) but
sets the environment variable os=ubuntu
.
$ docker run -dit --label production_status=testing -e os=ubuntu alpine sh
If the logging driver supports it, this adds additional fields to the logging
output. The following output is generated by the json-file
logging driver:
""attrs"":{""production_status"":""testing"",""os"":""ubuntu""}
Supported logging drivers
The following logging drivers are supported. See the link to each driver's documentation for its configurable options, if applicable. If you are using logging driver plugins, you may see more options.
| Driver | Description |
|---|---|
none | No logs are available for the container and docker logs does not return any output. |
local | Logs are stored in a custom format designed for minimal overhead. |
json-file | The logs are formatted as JSON. The default logging driver for Docker. |
syslog | Writes logging messages to the syslog facility. The syslog daemon must be running on the host machine. |
journald | Writes log messages to journald . The journald daemon must be running on the host machine. |
gelf | Writes log messages to a Graylog Extended Log Format (GELF) endpoint such as Graylog or Logstash. |
fluentd | Writes log messages to fluentd (forward input). The fluentd daemon must be running on the host machine. |
awslogs | Writes log messages to Amazon CloudWatch Logs. |
splunk | Writes log messages to splunk using the HTTP Event Collector. |
etwlogs | Writes log messages as Event Tracing for Windows (ETW) events. Only available on Windows platforms. |
gcplogs | Writes log messages to Google Cloud Platform (GCP) Logging. |
Limitations of logging drivers
- Reading log information requires decompressing rotated log files, which causes a temporary increase in disk usage (until the log entries from the rotated files are read) and an increased CPU usage while decompressing.
- The capacity of the host storage where the Docker data directory resides determines the maximum size of the log file information.",,,
bdc827c3f0e84b74aa3060b8117027850f051d712db610b720e709a36a9172a0,"Using lifecycle hooks with Compose
Services lifecycle hooks
When Docker Compose runs a container, it uses two elements, ENTRYPOINT and COMMAND, to manage what happens when the container starts and stops.
However, it can sometimes be easier to handle these tasks separately with lifecycle hooks - commands that run right after the container starts or just before it stops.
Lifecycle hooks are particularly useful because they can have special privileges (like running as the root user), even when the container itself runs with lower privileges for security. This means that certain tasks requiring higher permissions can be done without compromising the overall security of the container.
Post-start hooks
Post-start hooks are commands that run after the container has started, but there's no
set time for when exactly they will execute. The hook execution timing is not assured during
the execution of the container's entrypoint
.
In the example provided:
- The hook is used to change the ownership of a volume to a non-root user (because volumes are created with root ownership by default).
- After the container starts, the
chown
command changes the ownership of the/data
directory to user1001
.
services:
app:
image: backend
user: 1001
volumes:
- data:/data
post_start:
- command: chown -R /data 1001:1001
user: root
volumes:
data: {} # a Docker volume is created with root ownership
Pre-stop hooks
Pre-stop hooks are commands that run before the container is stopped by a specific
command (like docker compose down
or stopping it manually with Ctrl+C
).
These hooks won't run if the container stops by itself or gets killed suddenly.
In the following example, before the container stops, the ./data_flush.sh
script is
run to perform any necessary cleanup.
services:
app:
image: backend
pre_stop:
- command: ./data_flush.sh",,,
911bf2315c18fcc2749306732a63e67124ee490da0b3463cae346d3803247feb,"Volumes
Volumes are persistent data stores for containers, created and managed by
Docker. You can create a volume explicitly using the docker volume create
command, or Docker can create a volume during container or service creation.
When you create a volume, it's stored within a directory on the Docker host. When you mount the volume into a container, this directory is what's mounted into the container. This is similar to the way that bind mounts work, except that volumes are managed by Docker and are isolated from the core functionality of the host machine.
When to use volumes
Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure and OS of the host machine, volumes are completely managed by Docker. Volumes are a good choice for the following use cases:
- Volumes are easier to back up or migrate than bind mounts.
- You can manage volumes using Docker CLI commands or the Docker API.
- Volumes work on both Linux and Windows containers.
- Volumes can be more safely shared among multiple containers.
- New volumes can have their content pre-populated by a container or build.
- When your application requires high-performance I/O.
Volumes are not a good choice if you need to access the files from the host, as the volume is completely managed by Docker. Use bind mounts if you need to access files or directories from both containers and the host.
Volumes are often a better choice than writing data directly to a container, because a volume doesn't increase the size of the containers using it. Using a volume is also faster; writing into a container's writable layer requires a storage driver to manage the filesystem. The storage driver provides a union filesystem, using the Linux kernel. This extra abstraction reduces performance as compared to using volumes, which write directly to the host filesystem.
If your container generates non-persistent state data, consider using a tmpfs mount to avoid storing the data anywhere permanently, and to increase the container's performance by avoiding writing into the container's writable layer.
Volumes use rprivate
(recursive private) bind propagation, and bind propagation isn't
configurable for volumes.
A volume's lifecycle
A volume's contents exist outside the lifecycle of a given container. When a container is destroyed, the writable layer is destroyed with it. Using a volume ensures that the data is persisted even if the container using it is removed.
A given volume can be mounted into multiple containers simultaneously. When no
running container is using a volume, the volume is still available to Docker
and isn't removed automatically. You can remove unused volumes using docker volume prune
.
Mounting a volume over existing data
If you mount a non-empty volume into a directory in the container in which
files or directories exist, the pre-existing files are obscured by the mount.
This is similar to if you were to save files into /mnt
on a Linux host, and
then mounted a USB drive into /mnt
. The contents of /mnt
would be obscured
by the contents of the USB drive until the USB drive was unmounted.
With containers, there's no straightforward way of removing a mount to reveal the obscured files again. Your best option is to recreate the container without the mount.
If you mount an empty volume into a directory in the container in which files or directories exist, these files or directories are propagated (copied) into the volume by default. Similarly, if you start a container and specify a volume which does not already exist, an empty volume is created for you. This is a good way to pre-populate data that another container needs.
To prevent Docker from copying a container's pre-existing files into an empty
volume, use the volume-nocopy
option, see
Options for --mount.
Named and anonymous volumes
A volume may be named or anonymous. Anonymous volumes are given a random name
that's guaranteed to be unique within a given Docker host. Just like named
volumes, anonymous volumes persist even if you remove the container that uses
them, except if you use the --rm
flag when creating the container, in which
case the anonymous volume associated with the container is destroyed. See
Remove anonymous volumes.
If you create multiple containers consecutively that each use anonymous volumes, each container creates its own volume. Anonymous volumes aren't reused or shared between containers automatically. To share an anonymous volume between two or more containers, you must mount the anonymous volume using the random volume ID.
Syntax
To mount a volume with the docker run
command, you can use either the
--mount
or --volume
flag.
$ docker run --mount type=volume,src=<volume-name>,dst=<mount-path>
$ docker run --volume <volume-name>:<mount-path>
In general, --mount
is preferred. The main difference is that the --mount
flag is more explicit and supports all the available options.
You must use --mount
if you want to:
- Specify volume driver options
- Mount a volume subdirectory
- Mount a volume into a Swarm service
Options for --mount
The --mount
flag consists of multiple key-value pairs, separated by commas
and each consisting of a <key>=<value>
tuple. The order of the keys isn't
significant.
$ docker run --mount type=volume[,src=<volume-name>],dst=<mount-path>[,<key>=<value>...]
Valid options for --mount type=volume
include:
| Option | Description |
|---|---|
source , src | The source of the mount. For named volumes, this is the name of the volume. For anonymous volumes, this field is omitted. |
destination , dst , target | The path where the file or directory is mounted in the container. |
volume-subpath | A path to a subdirectory within the volume to mount into the container. The subdirectory must exist in the volume before the volume is mounted to a container. See Mount a volume subdirectory. |
readonly , ro | If present, causes the volume to be mounted into the container as read-only. |
volume-nocopy | If present, data at the destination isn't copied into the volume if the volume is empty. By default, content at the target destination gets copied into a mounted volume if empty. |
volume-opt | Can be specified more than once, takes a key-value pair consisting of the option name and its value. |
$ docker run --mount type=volume,src=myvolume,dst=/data,ro,volume-subpath=/foo
Options for --volume
The --volume
or -v
flag consists of three fields, separated by colon
characters (:
). The fields must be in the correct order.
$ docker run -v [<volume-name>:]<mount-path>[:opts]
In the case of named volumes, the first field is the name of the volume, and is unique on a given host machine. For anonymous volumes, the first field is omitted. The second field is the path where the file or directory is mounted in the container.
The third field is optional, and is a comma-separated list of options. Valid
options for --volume
with a data volume include:
| Option | Description |
|---|---|
readonly , ro | If present, causes the volume to be mounted into the container as read-only. |
volume-nocopy | If present, data at the destination isn't copied into the volume if the volume is empty. By default, content at the target destination gets copied into a mounted volume if empty. |
$ docker run -v myvolume:/data:ro
Create and manage volumes
Unlike a bind mount, you can create and manage volumes outside the scope of any container.
Create a volume:
$ docker volume create my-vol
List volumes:
$ docker volume ls
local my-vol
Inspect a volume:
$ docker volume inspect my-vol
[
{
""Driver"": ""local"",
""Labels"": {},
""Mountpoint"": ""/var/lib/docker/volumes/my-vol/_data"",
""Name"": ""my-vol"",
""Options"": {},
""Scope"": ""local""
}
]
Remove a volume:
$ docker volume rm my-vol
Start a container with a volume
If you start a container with a volume that doesn't yet exist, Docker creates
the volume for you. The following example mounts the volume myvol2
into
/app/
in the container.
The following -v
and --mount
examples produce the same result. You can't
run them both unless you remove the devtest
container and the myvol2
volume
after running the first one.
$ docker run -d \
--name devtest \
--mount source=myvol2,target=/app \
nginx:latest
$ docker run -d \
--name devtest \
-v myvol2:/app \
nginx:latest
Use docker inspect devtest
to verify that Docker created the volume and it mounted
correctly. Look for the Mounts
section:
""Mounts"": [
{
""Type"": ""volume"",
""Name"": ""myvol2"",
""Source"": ""/var/lib/docker/volumes/myvol2/_data"",
""Destination"": ""/app"",
""Driver"": ""local"",
""Mode"": """",
""RW"": true,
""Propagation"": """"
}
],
This shows that the mount is a volume, it shows the correct source and destination, and that the mount is read-write.
Stop the container and remove the volume. Note volume removal is a separate step.
$ docker container stop devtest
$ docker container rm devtest
$ docker volume rm myvol2
Use a volume with Docker Compose
The following example shows a single Docker Compose service with a volume:
services:
frontend:
image: node:lts
volumes:
- myapp:/home/node/app
volumes:
myapp:
Running docker compose up
for the first time creates a volume. Docker reuses the same volume when you run the command subsequently.
You can create a volume directly outside of Compose using docker volume create
and
then reference it inside compose.yaml
as follows:
services:
frontend:
image: node:lts
volumes:
- myapp:/home/node/app
volumes:
myapp:
external: true
For more information about using volumes with Compose, refer to the Volumes section in the Compose specification.
Start a service with volumes
When you start a service and define a volume, each service container uses its own
local volume. None of the containers can share this data if you use the local
volume driver. However, some volume drivers do support shared storage.
The following example starts an nginx
service with four replicas, each of which
uses a local volume called myvol2
.
$ docker service create -d \
--replicas=4 \
--name devtest-service \
--mount source=myvol2,target=/app \
nginx:latest
Use docker service ps devtest-service
to verify that the service is running:
$ docker service ps devtest-service
ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS
4d7oz1j85wwn devtest-service.1 nginx:latest moby Running Running 14 seconds ago
You can remove the service to stop the running tasks:
$ docker service rm devtest-service
Removing the service doesn't remove any volumes created by the service. Volume removal is a separate step.
Populate a volume using a container
If you start a container which creates a new volume, and the container
has files or directories in the directory to be mounted such as /app/
,
Docker copies the directory's contents into the volume. The container then
mounts and uses the volume, and other containers which use the volume also
have access to the pre-populated content.
To show this, the following example starts an nginx
container and
populates the new volume nginx-vol
with the contents of the container's
/usr/share/nginx/html
directory. This is where Nginx stores its default HTML
content.
The --mount
and -v
examples have the same end result.
$ docker run -d \
--name=nginxtest \
--mount source=nginx-vol,destination=/usr/share/nginx/html \
nginx:latest
$ docker run -d \
--name=nginxtest \
-v nginx-vol:/usr/share/nginx/html \
nginx:latest
After running either of these examples, run the following commands to clean up the containers and volumes. Note volume removal is a separate step.
$ docker container stop nginxtest
$ docker container rm nginxtest
$ docker volume rm nginx-vol
Use a read-only volume
For some development applications, the container needs to write into the bind
mount so that changes are propagated back to the Docker host. At other times,
the container only needs read access to the data. Multiple
containers can mount the same volume. You can simultaneously mount a
single volume as read-write
for some containers and as read-only
for others.
The following example changes the previous one. It mounts the directory as a read-only
volume, by adding ro
to the (empty by default) list of options, after the
mount point within the container. Where multiple options are present, you can separate
them using commas.
The --mount
and -v
examples have the same result.
$ docker run -d \
--name=nginxtest \
--mount source=nginx-vol,destination=/usr/share/nginx/html,readonly \
nginx:latest
$ docker run -d \
--name=nginxtest \
-v nginx-vol:/usr/share/nginx/html:ro \
nginx:latest
Use docker inspect nginxtest
to verify that Docker created the read-only mount
correctly. Look for the Mounts
section:
""Mounts"": [
{
""Type"": ""volume"",
""Name"": ""nginx-vol"",
""Source"": ""/var/lib/docker/volumes/nginx-vol/_data"",
""Destination"": ""/usr/share/nginx/html"",
""Driver"": ""local"",
""Mode"": """",
""RW"": false,
""Propagation"": """"
}
],
Stop and remove the container, and remove the volume. Volume removal is a separate step.
$ docker container stop nginxtest
$ docker container rm nginxtest
$ docker volume rm nginx-vol
Mount a volume subdirectory
When you mount a volume to a container, you can specify a subdirectory of the
volume to use, with the volume-subpath
parameter for the --mount
flag. The
subdirectory that you specify must exist in the volume before you attempt to
mount it into a container; if it doesn't exist, the mount fails.
Specifying volume-subpath
is useful if you only want to share a specific
portion of a volume with a container. Say for example that you have multiple
containers running and you want to store logs from each container in a shared
volume. You can create a subdirectory for each container in the shared volume,
and mount the subdirectory to the container.
The following example creates a logs
volume and initiates the subdirectories
app1
and app2
in the volume. It then starts two containers and mounts one
of the subdirectories of the logs
volume to each container. This example
assumes that the processes in the containers write their logs to
/var/log/app1
and /var/log/app2
.
$ docker volume create logs
$ docker run --rm \
--mount src=logs,dst=/logs \
alpine mkdir -p /logs/app1 /logs/app2
$ docker run -d \
--name=app1 \
--mount src=logs,dst=/var/log/app1/,volume-subpath=app1 \
app1:latest
$ docker run -d \
--name=app2 \
--mount src=logs,dst=/var/log/app2,volume-subpath=app2 \
app2:latest
With this setup, the containers write their logs to separate subdirectories of
the logs
volume. The containers can't access the other container's logs.
Share data between machines
When building fault-tolerant applications, you may need to configure multiple replicas of the same service to have access to the same files.
There are several ways to achieve this when developing your applications. One is to add logic to your application to store files on a cloud object storage system like Amazon S3. Another is to create volumes with a driver that supports writing files to an external storage system like NFS or Amazon S3.
Volume drivers let you abstract the underlying storage system from the application logic. For example, if your services use a volume with an NFS driver, you can update the services to use a different driver. For example, to store data in the cloud, without changing the application logic.
Use a volume driver
When you create a volume using docker volume create
, or when you start a
container which uses a not-yet-created volume, you can specify a volume driver.
The following examples use the vieux/sshfs
volume driver, first when creating
a standalone volume, and then when starting a container which creates a new
volume.
Note
If your volume driver accepts a comma-separated list as an option, you must escape the value from the outer CSV parser. To escape a
volume-opt
, surround it with double quotes (""
) and surround the entire mount parameter with single quotes ('
).For example, the
local
driver accepts mount options as a comma-separated list in theo
parameter. This example shows the correct way to escape the list.$ docker service create \ --mount 'type=volume,src=<VOLUME-NAME>,dst=<CONTAINER-PATH>,volume-driver=local,volume-opt=type=nfs,volume-opt=device=<nfs-server>:<nfs-path>,""volume-opt=o=addr=<nfs-address>,vers=4,soft,timeo=180,bg,tcp,rw""' --name myservice \ <IMAGE>
Initial setup
The following example assumes that you have two nodes, the first of which is a Docker host and can connect to the second node using SSH.
On the Docker host, install the vieux/sshfs
plugin:
$ docker plugin install --grant-all-permissions vieux/sshfs
Create a volume using a volume driver
This example specifies an SSH password, but if the two hosts have shared keys
configured, you can exclude the password. Each volume driver may have zero or more
configurable options, you specify each of them using an -o
flag.
$ docker volume create --driver vieux/sshfs \
-o sshcmd=test@node2:/home/test \
-o password=testpassword \
sshvolume
Start a container which creates a volume using a volume driver
The following example specifies an SSH password. However, if the two hosts have shared keys configured, you can exclude the password. Each volume driver may have zero or more configurable options.
Note
If the volume driver requires you to pass any options, you must use the
--mount
flag to mount the volume, and not-v
.
$ docker run -d \
--name sshfs-container \
--mount type=volume,volume-driver=vieux/sshfs,src=sshvolume,target=/app,volume-opt=sshcmd=test@node2:/home/test,volume-opt=password=testpassword \
nginx:latest
Create a service which creates an NFS volume
The following example shows how you can create an NFS volume when creating a service.
It uses 10.0.0.10
as the NFS server and /var/docker-nfs
as the exported directory on the NFS server.
Note that the volume driver specified is local
.
NFSv3
$ docker service create -d \
--name nfs-service \
--mount 'type=volume,source=nfsvolume,target=/app,volume-driver=local,volume-opt=type=nfs,volume-opt=device=:/var/docker-nfs,volume-opt=o=addr=10.0.0.10' \
nginx:latest
NFSv4
$ docker service create -d \
--name nfs-service \
--mount 'type=volume,source=nfsvolume,target=/app,volume-driver=local,volume-opt=type=nfs,volume-opt=device=:/var/docker-nfs,""volume-opt=o=addr=10.0.0.10,rw,nfsvers=4,async""' \
nginx:latest
Create CIFS/Samba volumes
You can mount a Samba share directly in Docker without configuring a mount point on your host.
$ docker volume create \
--driver local \
--opt type=cifs \
--opt device=//uxxxxx.your-server.de/backup \
--opt o=addr=uxxxxx.your-server.de,username=uxxxxxxx,password=*****,file_mode=0777,dir_mode=0777 \
--name cif-volume
The addr
option is required if you specify a hostname instead of an IP.
This lets Docker perform the hostname lookup.
Block storage devices
You can mount a block storage device, such as an external drive or a drive partition, to a container. The following example shows how to create and use a file as a block storage device, and how to mount the block device as a container volume.
Important
The following procedure is only an example. The solution illustrated here isn't recommended as a general practice. Don't attempt this approach unless you're confident about what you're doing.
How mounting block devices works
Under the hood, the --mount
flag using the local
storage driver invokes the
Linux mount
syscall and forwards the options you pass to it unaltered.
Docker doesn't implement any additional functionality on top of the native mount features supported by the Linux kernel.
If you're familiar with the
Linux mount
command,
you can think of the --mount
options as forwarded to the mount
command in the following manner:
$ mount -t <mount.volume-opt.type> <mount.volume-opt.device> <mount.dst> -o <mount.volume-opts.o>
To explain this further, consider the following mount
command example.
This command mounts the /dev/loop5
device to the path /external-drive
on the system.
$ mount -t ext4 /dev/loop5 /external-drive
The following docker run
command achieves a similar result, from the point of view of the container being run.
Running a container with this --mount
option sets up the mount in the same way as if you had executed the
mount
command from the previous example.
$ docker run \
--mount='type=volume,dst=/external-drive,volume-driver=local,volume-opt=device=/dev/loop5,volume-opt=type=ext4'
You can't run the mount
command inside the container directly,
because the container is unable to access the /dev/loop5
device.
That's why the docker run
command uses the --mount
option.
Example: Mounting a block device in a container
The following steps create an ext4
filesystem and mounts it into a container.
The filesystem support of your system depends on the version of the Linux kernel you are using.
Create a file and allocate some space to it:
$ fallocate -l 1G disk.raw
Build a filesystem onto the
disk.raw
file:$ mkfs.ext4 disk.raw
Create a loop device:
$ losetup -f --show disk.raw /dev/loop5
Note
losetup
creates an ephemeral loop device that's removed after system reboot, or manually removed withlosetup -d
.Run a container that mounts the loop device as a volume:
$ docker run -it --rm \ --mount='type=volume,dst=/external-drive,volume-driver=local,volume-opt=device=/dev/loop5,volume-opt=type=ext4' \ ubuntu bash
When the container starts, the path
/external-drive
mounts thedisk.raw
file from the host filesystem as a block device.When you're done, and the device is unmounted from the container, detach the loop device to remove the device from the host system:
$ losetup -d /dev/loop5
Back up, restore, or migrate data volumes
Volumes are useful for backups, restores, and migrations.
Use the --volumes-from
flag to create a new container that mounts that volume.
Back up a volume
For example, create a new container named dbstore
:
$ docker run -v /dbdata --name dbstore ubuntu /bin/bash
In the next command:
- Launch a new container and mount the volume from the
dbstore
container - Mount a local host directory as
/backup
- Pass a command that tars the contents of the
dbdata
volume to abackup.tar
file inside the/backup
directory.
$ docker run --rm --volumes-from dbstore -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /dbdata
When the command completes and the container stops, it creates a backup of
the dbdata
volume.
Restore volume from a backup
With the backup just created, you can restore it to the same container, or to another container that you created elsewhere.
For example, create a new container named dbstore2
:
$ docker run -v /dbdata --name dbstore2 ubuntu /bin/bash
Then, un-tar the backup file in the new container’s data volume:
$ docker run --rm --volumes-from dbstore2 -v $(pwd):/backup ubuntu bash -c ""cd /dbdata && tar xvf /backup/backup.tar --strip 1""
You can use these techniques to automate backup, migration, and restore testing using your preferred tools.
Remove volumes
A Docker data volume persists after you delete a container. There are two types of volumes to consider:
- Named volumes have a specific source from outside the container, for example,
awesome:/bar
. - Anonymous volumes have no specific source. Therefore, when the container is deleted, you can instruct the Docker Engine daemon to remove them.
Remove anonymous volumes
To automatically remove anonymous volumes, use the --rm
option. For example,
this command creates an anonymous /foo
volume. When you remove the container,
the Docker Engine removes the /foo
volume but not the awesome
volume.
$ docker run --rm -v /foo -v awesome:/bar busybox top
Note
If another container binds the volumes with
--volumes-from
, the volume definitions are copied and the anonymous volume also stays after the first container is removed.
Remove all volumes
To remove all unused volumes and free up space:
$ docker volume prune
Next steps
- Learn about bind mounts.
- Learn about tmpfs mounts.
- Learn about storage drivers.
- Learn about third-party volume driver plugins.",,,
6f50d9c4fc0ce4688e4d7194c9fc0f9728520563e96e0aafdb81702f8e344c7f,"Repository information
Each repository can include a description, an overview, and categories to help users understand its purpose and usage. Adding clear repository information ensures that others can find your images and use them effectively.
You can only modify the repository information of repositories that aren't archived. If a repository is archived, you must unarchive it to modify the information. For more details, see Unarchive a repository.
Repository description
The description appears in search results when using the docker search
command
and in the search results on Docker Hub.
Consider the following repository description best practices.
- Summarize the purpose. Clearly state what the image does in a concise and specific manner. Make it clear if it's for a particular application, tool, or platform, or has a distinct use case.
- Highlight key features or benefits. Briefly mention the primary benefits or unique features that differentiate the image. Examples include high performance, ease of use, lightweight build, or compatibility with specific frameworks or operating systems.
- Include relevant keywords. Use keywords that users may search for to increase visibility, such as technology stacks, use cases, or environments.
- Keep it concise. The description can be a maximum of 100 characters. Aim to stay within one or two sentences for the description to ensure it's easy to read in search results. Users should quickly understand the image's value.
- Focus on the audience. Consider your target audience (developers, system administrators, etc.) and write the description to address their needs directly.
Following these practices can help make the description more engaging and effective in search results, driving more relevant traffic to your repository.
Add or update a repository description
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears.
Select the pencil icon under the description field.
Specify a description.
The description can be up to 100 characters long.
Select Update.
Repository overview
An overview describes what your image does and how to run it. It displays in the
public view of your repository when the repository has at least one image. If
automated builds are enabled, the overview will be synced from the source code
repository's README.md
file on each successful build.
Consider the following repository overview best practices.
- Describe what the image is, the features it offers, and why it should be used. Can include examples of usage or the team behind the project.
- Explain how to get started with running a container using the image. You can include a minimal example of how to use the image in a Dockerfile.
- List the key image variants and tags to use them, as well as use cases for the variants.
- Link to documentation or support sites, communities, or mailing lists for additional resources.
- Provide contact information for the image maintainers.
- Include the license for the image and where to find more details if needed.
Add or update a repository overview
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears.
Under Repository overview, select Edit or Add overview.
The Write and Preview tabs appear.
Under Write, specify your repository overview.
You can use basic Markdown and use the Preview tab to preview the formatting.
Select Update.
Repository categories
You can tag Docker Hub repositories with categories, representing the primary intended use cases for your images. This lets users more easily find and explore content for the problem domain that they're interested in.
Available categories
The Docker Hub content team maintains a curated list of categories.
The categories include:
- API Management: Tools for creating, publishing, analyzing, and securing APIs.
- Content Management System: Software applications to create and manage digital content through templates, procedures, and standard formats.
- Data Science: Tools and software to support analyzing data and generating actionable insights.
- Databases & Storage: Systems for storing, retrieving, and managing data.
- Languages & Frameworks: Programming language runtimes and frameworks.
- Integrations & Delivery: Tools for Continuous Integration (CI) and Continuous Delivery (CD).
- Internet of Things: Tools supporting Internet of Things (IoT) applications.
- Machine Learning & AI: Tools and frameworks optimized for artificial intelligence and machine learning projects, such as pre-installed libraries and frameworks for data analysis, model training, and deployment.
- Message Queues: Message queuing systems optimized for reliable, scalable, and efficient message handling.
- Monitoring & Observability: Tools to track software and system performance through metrics, logs, and traces, as well as observability to explore the system’s state and diagnose issues.
- Networking: Repositories that support data exchange and connecting computers and other devices to share resources.
- Operating Systems: Software that manages all other programs on a computer and serves as an intermediary between users and the computer hardware, while overseeing applications and system resources.
- Security: Tools to protect a computer system or network from theft, unauthorized access, or damage to their hardware, software, or electronic data, as well as from service disruption.
- Web Servers: Software to serve web pages, HTML files, and other assets to users or other systems.
- Web Analytics: Tools to collect, measure, analyze, and report on web data and website visitor engagement.
Auto-generated categories
Note
Auto-generated categories only apply to Docker Verified Publishers and Docker-Sponsored Open Source program participants.
For repositories that pre-date the Categories feature in Docker Hub, categories have been automatically generated and applied, using OpenAI, based on the repository title and description.
As an owner of a repository that has been auto-categorized, you can manually edit the categories if you think they're inaccurate. See Manage categories for a repository.
The auto-generated categorization was a one-time effort to help seed categories onto repositories created before the feature existed. Categories are not assigned to new repositories automatically.
Manage categories for a repository
You can tag a repository with up to three categories.
To edit the categories of a repository:
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears.
Select the pencil icon under the description field.
Select the categories you want to apply.
Select Update.
If you're missing a category, use the Give feedback link to let us know what categories you'd like to see.",,,
fc948127ba7588043ca4d785d0237b35e87eaa686636deaa43a013242c8083ad,"Overview of Hardened Docker Desktop
Hardened Docker Desktop is a group of security features, designed to improve the security of developer environments with minimal impact on developer experience or productivity.
It lets you enforce strict security settings, preventing developers and their containers from bypassing these controls, either intentionally or unintentionally. Additionally, you can enhance container isolation, to mitigate potential security threats such as malicious payloads breaching the Docker Desktop Linux VM and the underlying host.
Hardened Docker Desktop moves the ownership boundary for Docker Desktop configuration to the organization, meaning that any security controls you set cannot be altered by the user of Docker Desktop.
It is for security conscious organizations who:
- Don’t give their users root or administrator access on their machines
- Would like Docker Desktop to be within their organization’s centralized control
- Have certain compliance obligations
How does it help my organization?
Hardened Desktop features work independently but collectively to create a defense-in-depth strategy, safeguarding developer workstations against potential attacks across various functional layers, such as configuring Docker Desktop, pulling container images, and running container images. This multi-layered defense approach ensures comprehensive security. It helps mitigate against threats such as:
- Malware and supply chain attacks: Registry Access Management and Image Access Management prevent developers from accessing certain container registries and image types, significantly lowering the risk of malicious payloads. Additionally, Enhanced Container Isolation (ECI) restricts the impact of containers with malicious payloads by running them without root privileges inside a Linux user namespace.
- Lateral movement: Air-gapped containers lets you configure network access restrictions for containers, thereby preventing malicious containers from performing lateral movement within the organization's network.
- Insider threats: Settings Management configures and locks various Docker Desktop settings so you can enforce company policies and prevent developers from introducing insecure configurations, intentionally or unintentionally.",,,
7166a5ed891f982dc2ba4186b913c05200f07510e4d22a5e8a38c79c49f570d4,"Docker's product release lifecycle
This page details Docker's product release lifecycle and how Docker defines each stage. It also provides information on the product retirement process. Features and products may progress through some or all of these phases.
Note
Our Subscription Service Agreement governs your use of Docker and covers details of eligibility, content, use, payments and billing, and warranties. This document is not a contract and all use of Docker’s services are subject to Docker’s Subscription Service Agreement.
Lifecycle stage
| Lifecycle stage | Customer availability | Support availability | Limitations | Retirement |
|---|---|---|---|---|
| Experimental | Limited availability | Community support | Software may have limitations, bugs and/or stability concerns | Can be discontinued without notice |
| Beta | All or those involved in a Beta Feedback Program | Community support | Software may have limitations, bugs and/or stability concerns | Can be discontinued without notice |
| Early Access (EA) | All or those involved in an Early Access Feedback Program | Full | Software may have limitations, bugs and/or stability concerns. These limitations will be documented. | Follows the retirement process |
| General Availability (GA) | All | Full | Few or no limitations for supported use cases | Follows the retirement process |
Experimental
Experimental offerings are features that Docker is currently experimenting with. Customers who access experimental features have the opportunity to test, validate, and provide feedback on future functionality. This helps us focus our efforts on what provides the most value to our customers.
Customer availability: Availability of experimental features is limited. A portion of users may have access to none, one or many experimental features.
Support: Support for experimental features is best effort via Community support channels and forums.
Limitations: Experimental features may have potentially significant limitations such as functional limitations, performance limitations, and API limitations. Features and programmatic interfaces may change at any time without notice.
Retirement: During the experimental period, Docker will determine whether to continue an offering through its lifecycle. We reserve the right to change the scope of or discontinue an Experimental product or feature at any point in time without notice, as outlined in our Subscription Service Agreement.
Beta
Beta offerings are initial releases of potential future products or features. Customers who participate in our beta programs have the opportunity to test, validate, and provide feedback on future functionality. This helps us focus our efforts on what provides the most value to our customers.
Customer availability: Participation in beta releases is by invitation or via use of clearly identified beta features in product. Beta invitations may be public or private.
Support: Support for beta features is best effort via Community support channels and forums.
Limitations: Beta releases may have potentially significant limitations such as functional limitations, performance limitations, and API limitations. Features and programmatic interfaces may change at any time without notice.
Retirement: During the beta period, Docker will determine whether to continue an offering through its lifecycle. We reserve the right to change the scope of or discontinue a Beta product or feature at any point in time without notice, as outlined in our Subscription Service Agreement.
Early Access (EA)
Early Access offerings are products or features that may have potential feature limitations and are enabled for specific user groups as part of an incremental roll-out strategy. They are ready to be released to the world, pending some fine tuning.
Customer availability: Early Access functionality can be rolled out to all customers or specific segments of users in addition to or in place of existing functionality.
Support: Early Access offerings are provided with the same level of support as General Availability features and products.
Limitations: Early Access releases may have potentially significant limitations such as functional limitations, performance limitations, and API limitations, though these limitations will be documented. Breaking changes to features and programmatic interfaces will follow the retirement process outlined below.
Retirement: In the event we retire an Early Access product before General Availability, we will strive to follow the retirement process outlined below.
General Availability (GA)
General Availability offerings are fully functional products or features that are openly accessible to all Docker customers.
Customer availability: All Docker users have access to GA offerings according to their subscription levels.
Limitations: General Availability features and products will have few or no limitations for supported use cases.
Support: All GA offerings are fully supported, as described in our support page.
Retirement: General Availability offerings follow the retirement process outlined below.
Retirement process
The decision to retire or deprecate features follows a rigorous process including understanding the demand, use, impact of feature retirement and, most importantly, customer feedback. Our goal is to invest resources in areas that will add the most value for the most customers
Docker is committed to being clear, transparent, and proactive when interacting with our customers, especially about changes to our platform. To that end, we will make best efforts to follow these guidelines when retiring functionality:
- Advance notice: For retirement of major features or products, we will attempt to notify customers at least 6 months in advance.
- Viable alternatives: Docker will strive to provide viable alternatives to our customers when retiring functionality. These may be alternative offerings from Docker or recommended alternatives from 3rd party providers. Where possible and appropriate, Docker will automatically migrate customers to alternatives for retired functionality.
- Continued support: Docker commits to providing continued support for functionality until its retirement date.
We may need to accelerate the timeline for retirement of functionality in extenuating circumstances, such as essential changes necessary to protect the integrity of our platform or the security of our customers and others. In these cases, it is important that those changes occur as quickly as possible.
Similarly, integrated third party software or services may need to be retired due to the third-party decision to change or retire their solution. In these situations, the pace of retirement will be out of our control.
However, even under these circumstances, we will provide as much advance notice as possible.",,,
6dbe634daddf4236b920cd9fcdd5f48a60543fea98351a47d11c1c2378eecff0,"Bake
Bake is a feature of Docker Buildx that lets you define your build configuration using a declarative file, as opposed to specifying a complex CLI expression. It also lets you run multiple builds concurrently with a single invocation.
A Bake file can be written in HCL, JSON, or YAML formats, where the YAML format is an extension of a Docker Compose file. Here's an example Bake file in HCL format:
group ""default"" {
targets = [""frontend"", ""backend""]
}
target ""frontend"" {
context = ""./frontend""
dockerfile = ""frontend.Dockerfile""
args = {
NODE_VERSION = ""22""
}
tags = [""myapp/frontend:latest""]
}
target ""backend"" {
context = ""./backend""
dockerfile = ""backend.Dockerfile""
args = {
GO_VERSION = ""1.23""
}
tags = [""myapp/backend:latest""]
}
The group
block defines a group of targets that can be built concurrently.
Each target
block defines a build target with its own configuration, such as
the build context, Dockerfile, and tags.
To invoke a build using the above Bake file, you can run:
$ docker buildx bake
This executes the default
group, which builds the frontend
and backend
targets concurrently.
Get started
To learn how to get started with Bake, head over to the Bake introduction.",,,
63c851c3d3b1cb3ab111e22df3e2cefd61d048b477b244e32510513e7dd74417,"Use Compose in production
When you define your app with Compose in development, you can use this definition to run your application in different environments such as CI, staging, and production.
The easiest way to deploy an application is to run it on a single server, similar to how you would run your development environment. If you want to scale up your application, you can run Compose apps on a Swarm cluster.
Modify your Compose file for production
You may need to make changes to your app configuration to make it ready for production. These changes might include:
- Removing any volume bindings for application code, so that code stays inside the container and can't be changed from outside
- Binding to different ports on the host
- Setting environment variables differently, such as reducing the verbosity of logging, or to specify settings for external services such as an email server
- Specifying a restart policy like
restart: always
to avoid downtime - Adding extra services such as a log aggregator
For this reason, consider defining an additional Compose file, for example
compose.production.yaml
, which specifies production-appropriate
configuration. This configuration file only needs to include the changes you want to make from the original Compose file. The additional Compose file
is then applied over the original compose.yaml
to create a new configuration.
Once you have a second configuration file, you can use it with the
-f
option:
$ docker compose -f compose.yaml -f compose.production.yaml up -d
See Using multiple compose files for a more complete example, and other options.
Deploying changes
When you make changes to your app code, remember to rebuild your image and
recreate your app's containers. To redeploy a service called
web
, use:
$ docker compose build web
$ docker compose up --no-deps -d web
This first command rebuilds the image for web
and then stops, destroys, and recreates
just the web
service. The --no-deps
flag prevents Compose from also
recreating any services which web
depends on.
Running Compose on a single server
You can use Compose to deploy an app to a remote Docker host by setting the
DOCKER_HOST
, DOCKER_TLS_VERIFY
, and DOCKER_CERT_PATH
environment variables
appropriately. For more information, see
pre-defined environment variables.
Once you've set up your environment variables, all the normal docker compose
commands work with no further configuration.",,,
7462d1e1f422e827e022e1bbf3aabad77baf6e32e1fc4b913d6fbf17e5bd438a,"Registry Access Management
With Registry Access Management (RAM), administrators can ensure that their developers using Docker Desktop only access allowed registries. This is done through the Registry Access Management dashboard in Docker Hub or the Docker Admin Console.
Registry Access Management supports both cloud and on-prem registries. This feature operates at the DNS level and therefore is compatible with all registries. You can add any hostname or domain name you’d like to include in the list of allowed registries. However, if the registry redirects to other domains such as s3.amazon.com
, then you must add those domains to the list.
Example registries administrators can allow include:
- Docker Hub. This is enabled by default.
- Amazon ECR
- GitHub Container Registry
- Google Container Registry
- GitLab Container Registry
- Nexus
- Artifactory
Prerequisites
You need to enforce sign-in. For Registry Access Management to take effect, Docker Desktop users must authenticate to your organization. Enforcing sign-in ensures that your Docker Desktop developers always authenticate to your organization, even though they can authenticate without it and the feature will take effect. Enforcing sign-in guarantees the feature always takes effect.
Configure Registry Access Management permissions
To configure Registry Access Management permissions, perform the following steps:
Sign in to Docker Hub.
Select Organizations, your organization, Settings, and then select Registry Access.
Enable Registry Access Management to set the permissions for your registry.
Note
When enabled, the Docker Hub registry is set by default, however you can also restrict this registry for your developers.
Select Add registry and enter your registry details in the applicable fields, and then select Create to add the registry to your list. There is no limit on the number of registries you can add.
Verify that the registry appears in your list and select Save changes.
Once you add a registry, it can take up to 24 hours for the changes to be enforced on your developers’ machines.
If you want to apply the changes sooner, you must force a Docker signout on your developers’ machine and have the developers re-authenticate for Docker Desktop. See the Caveats section below to learn more about limitations when using this feature.
Important
Starting with Docker Desktop version 4.36, you can enforce sign-in for multiple organizations. If a developer belongs to multiple organizations with different RAM policies, only the RAM policy for the first organization listed in the
registry.json
file,.plist
file, or registry key is enforced.
Tip
Since RAM sets policies about where content can be fetched from, the ADD instruction of the Dockerfile, when the parameter of the ADD instruction is a URL, is also subject to registry restrictions. It's recommended that you add the domains of URL parameters to the list of allowed registry addresses under the Registry Access Management settings of your organization.
To configure Registry Access Management permissions, perform the following steps:
Sign in to the Admin Console.
Select your organization in the left navigation drop-down menu, and then select Registry access.
Enable Registry Access Management to set the permissions for your registry.
Note
When enabled, the Docker Hub registry is set by default, however you can also restrict this registry for your developers.
Select Add registry and enter your registry details in the applicable fields, and then select Create to add the registry to your list. There is no limit on the number of registries you can add.
Verify that the registry appears in your list and select Save changes.
Once you add a registry, it can take up to 24 hours for the changes to be enforced on your developers’ machines.
If you want to apply the changes sooner, you must force a Docker signout on your developers’ machine and have the developers re-authenticate for Docker Desktop. See the Caveats section below to learn more about limitations when using this feature.
Important
Starting with Docker Desktop version 4.36, you can enforce sign-in for multiple organizations. If a developer belongs to multiple organizations with different RAM policies, only the RAM policy for the first organization listed in the
registry.json
file,.plist
file, or registry key is enforced.
Tip
Since RAM sets policies about where content can be fetched from, the ADD instruction of the Dockerfile, when the parameter of the ADD instruction is a URL, is also subject to registry restrictions. It's recommended that you add the domains of URL parameters to the list of allowed registry addresses under the Registry Access Management settings of your organization.
Verify the restrictions
The new Registry Access Management policy takes effect after the developer successfully authenticates to Docker Desktop using their organization credentials. If a developer attempts to pull an image from a disallowed registry via the Docker CLI, they receive an error message that the organization has disallowed this registry.
Caveats
There are certain limitations when using Registry Access Management:
- Windows image pulls and image builds are not restricted by default. For Registry Access Management to take effect on Windows Container mode, you must allow the Windows Docker daemon to use Docker Desktop's internal proxy by selecting the Use proxy for Windows Docker daemon setting.
- Builds such as
docker buildx
using a Kubernetes driver are not restricted - Builds such as
docker buildx
using a custom docker-container driver are not restricted - Blocking is DNS-based; you must use a registry's access control mechanisms to distinguish between “push” and “pull”
- WSL 2 requires at least a 5.4 series Linux kernel (this does not apply to earlier Linux kernel series)
- Under the WSL 2 network, traffic from all Linux distributions is restricted (this will be resolved in the updated 5.15 series Linux kernel)
- Images pulled by Docker Desktop when Docker Debug or Kubernetes is enabled, are not restricted by default even if Docker Hub is blocked by RAM.
Also, Registry Access Management operates on the level of hosts, not IP addresses. Developers can bypass this restriction within their domain resolution, for example by running Docker against a local proxy or modifying their operating system's sts
file. Blocking these forms of manipulation is outside the remit of Docker Desktop.",,,
289e4b495348dbe6b51efd95a12f8b0351dcb758f923aa9c1ebd2991ad45fefc,"Add SBOM and provenance attestations with GitHub Actions
Software Bill of Material (SBOM) and provenance attestations add metadata about the contents of your image, and how it was built.
Attestations are supported with version 4 and later of the
docker/build-push-action
.
Default provenance
The docker/build-push-action
GitHub Action automatically adds provenance
attestations to your image, with the following conditions:
- If the GitHub repository is public, provenance attestations with
mode=max
are automatically added to the image. - If the GitHub repository is private, provenance attestations with
mode=min
are automatically added to the image. - If you're using the
docker
exporter, or you're loading the build results to the runner withload: true
, no attestations are added to the image. These output formats don't support attestations.
Warning
If you're using
docker/build-push-action
to build images for code in a public GitHub repository, the provenance attestations attached to your image by default contains the values of build arguments. If you're misusing build arguments to pass secrets to your build, such as user credentials or authentication tokens, those secrets are exposed in the provenance attestation. Refactor your build to pass those secrets using secret mounts instead. Also remember to rotate any secrets you may have exposed.
Max-level provenance
It's recommended that you build your images with max-level provenance
attestations. Private repositories only add min-level provenance by default,
but you can manually override the provenance level by setting the provenance
input on the docker/build-push-action
GitHub Action to mode=max
.
Note that adding attestations to an image means you must push the image to a registry directly, as opposed to loading the image to the local image store of the runner. This is because the local image store doesn't support loading images with attestations.
name: ci
on:
push:
env:
IMAGE_NAME: user/app
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Extract metadata
id: meta
uses: docker/metadata-action@v5
with:
images: ${{ env.IMAGE_NAME }}
- name: Build and push image
uses: docker/build-push-action@v6
with:
push: true
provenance: mode=max
tags: ${{ steps.meta.outputs.tags }}
SBOM
SBOM attestations aren't automatically added to the image. To add SBOM
attestations, set the sbom
input of the docker/build-push-action
to true.
Note that adding attestations to an image means you must push the image to a registry directly, as opposed to loading the image to the local image store of the runner. This is because the local image store doesn't support loading images with attestations.
name: ci
on:
push:
env:
IMAGE_NAME: user/app
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Extract metadata
id: meta
uses: docker/metadata-action@v5
with:
images: ${{ env.IMAGE_NAME }}
- name: Build and push image
uses: docker/build-push-action@v6
with:
sbom: true
push: true
tags: ${{ steps.meta.outputs.tags }}",,,
a24096b39447f929ec00c2189ca2f1e11ea9a02826a1e3b55cd1bb3d4991f114,"History and development of Docker Compose
This page provides:
- A brief history of the development of the Docker Compose CLI
- A clear explanation of the major versions and file formats that make up Compose V1 and Compose V2
- The main differences between Compose V1 and Compose V2
Introduction
The image above shows that the currently supported version of the Docker Compose CLI is Compose V2 which is defined by the Compose Specification.
It also provides a quick snapshot of the differences in file formats, command-line syntax, and top-level elements. This is covered in more detail in the following sections.
Docker Compose CLI versioning
Version one of the Docker Compose command-line binary was first released in 2014. It was written in Python, and is invoked with docker-compose
.
Typically, Compose V1 projects include a top-level version
element in the compose.yaml
file, with values ranging from 2.0
to 3.8
, which refer to the specific
file formats.
Version two of the Docker Compose command-line binary was announced in 2020, is written in Go, and is invoked with docker compose
.
Compose V2 ignores the version
top-level element in the compose.yaml
file.
Compose file format versioning
The Docker Compose CLIs are defined by specific file formats.
Three major versions of the Compose file format for Compose V1 were released:
- Compose file format 1 with Compose 1.0.0 in 2014
- Compose file format 2.x with Compose 1.6.0 in 2016
- Compose file format 3.x with Compose 1.10.0 in 2017
Compose file format 1 is substantially different to all the following formats as it lacks a top-level services
key.
Its usage is historical and files written in this format don't run with Compose V2.
Compose file format 2.x and 3.x are very similar to each other, but the latter introduced many new options targeted at Swarm deployments.
To address confusion around Compose CLI versioning, Compose file format versioning, and feature parity depending on whether Swarm mode was in use, file format 2.x and 3.x were merged into the Compose Specification.
Compose V2 uses the Compose Specification for project definition. Unlike the prior file formats, the Compose Specification is rolling and makes the version
top-level element optional. Compose V2 also makes use of optional specifications -
Deploy,
Develop and
Build.
To make migration easier, Compose V2 has backwards compatibility for certain elements that have been deprecated or changed between Compose file format 2.x/3.x and the Compose Specification.",,,
68f11129a67c05a0cc855190200721121aeff168f9b22472f0432d1b813b7dd6,"Integrate Docker Scout with Microsoft Azure DevOps Pipelines
The following examples runs in an Azure DevOps-connected repository containing a Docker image's definition and contents. Triggered by a commit to the main branch, the pipeline builds the image and uses Docker Scout to create a CVE report.
First, set up the rest of the workflow and set up the variables available to all pipeline steps. Add the following to an azure-pipelines.yml file:
trigger:
- main
resources:
- repo: self
variables:
tag: ""$(Build.BuildId)""
image: ""vonwig/nodejs-service""
This sets up the workflow to use a particular container image for the application and tag each new image build with the build ID.
Add the following to the YAML file:
stages:
- stage: Build
displayName: Build image
jobs:
- job: Build
displayName: Build
pool:
vmImage: ubuntu-latest
steps:
- task: Docker@2
displayName: Build an image
inputs:
command: build
dockerfile: ""$(Build.SourcesDirectory)/Dockerfile""
repository: $(image)
tags: |
$(tag)
- task: CmdLine@2
displayName: Find CVEs on image
inputs:
script: |
# Install the Docker Scout CLI
curl -sSfL https://raw.githubusercontent.com/docker/scout-cli/main/install.sh | sh -s --
# Login to Docker Hub required for Docker Scout CLI
echo $(DOCKER_HUB_PAT) | docker login -u $(DOCKER_HUB_USER) --password-stdin
# Get a CVE report for the built image and fail the pipeline when critical or high CVEs are detected
docker scout cves $(image):$(tag) --exit-code --only-severity critical,high
This creates the flow mentioned previously. It builds and tags the image using
the checked-out Dockerfile, downloads the Docker Scout CLI, and then runs the
cves
command against the new tag to generate a CVE report. It only shows
critical or high-severity vulnerabilities.",,,
3d8fdf6a1a3b27ad1476308a14ea528b9cd42753a0da10e53545d5c0b92dce0e,"Integrate Docker Scout with Artifactory
Integrating Docker Scout with JFrog Artifactory lets you run image analysis automatically on images in Artifactory registries.
Local image analysis
You can analyze Artifactory images for vulnerabilities locally using Docker Desktop or the Docker CLI. You first need to authenticate with JFrog Artifactory using the
docker login
command. For example:
docker login {URL}
Tip
For cloud-hosted Artifactory you can find the credentials for your Artifactory repository by selecting it in the Artifactory UI and then the Set Me Up button.
Remote image analysis
To automatically analyze images running in remote environments you need to deploy the Docker Scout Artifactory agent. The agent is a standalone service that analyzes images and uploads the result to Docker Scout. You can view the results using the Docker Scout Dashboard.
How the agent works
The Docker Scout Artifactory agent is available as an image on Docker Hub. The agent works by continuously polling Artifactory for new images. When it finds a new image, it performs the following steps:
- Pull the image from Artifactory
- Analyze the image
- Upload the analysis result to Docker Scout
The agent records the Software Bill of Materials (SBOM) for the image, and the SBOMs for all of its base images. The recorded SBOMs include both Operating System (OS)-level and application-level programs or dependencies that the image contains.
Additionally, the agent sends the following metadata about the image to Docker Scout:
- The source repository URL and commit SHA for the image
- Build instructions
- Build date
- Tags and digest
- Target platforms
- Layer sizes
The agent never transacts the image itself, nor any data inside the image, such as code, binaries, and layer blobs.
The agent doesn't detect and analyze pre-existing images. It only analyzes images that appear in the registry while the agent is running.
Deploy the agent
This section describes the steps for deploying the Artifactory agent.
Prerequisites
Before you deploy the agent, ensure that you meet the prerequisites:
- The server where you host the agent can access the following resources over
the network:
- Your JFrog Artifactory instance
hub.docker.com
, port 443, for authenticating with Dockerapi.dso.docker.com
, port 443, for transacting data to Docker Scout
- The registries are Docker V2 registries. V1 registries aren't supported.
The agent supports all versions of JFrog Artifactory and JFrog Container Registry.
Create the configuration file
You configure the agent using a JSON file. The agent expects the configuration
file to be in /opt/artifactory-agent/data/config.json
on startup.
The configuration file includes the following properties:
| Property | Description |
|---|---|
agent_id | Unique identifier for the agent. |
docker.organization_name | Name of the Docker organization. |
docker.username | Username of the admin user in the Docker organization. |
docker.pat | Personal access token of the admin user with read and write permissions. |
artifactory.base_url | Base URL of the Artifactory instance. |
artifactory.username | Username of the Artifactory user with read permissions that the agent will use. |
artifactory.password | Password or API token for the Artifactory user. |
artifactory.image_filters | Optional: List of repositories and images to analyze. |
If you don't specify any repositories in artifactory.image_filters
, the agent
runs image analysis on all images in your Artifactory instance.
The following snippet shows a sample configuration:
{
""agent_id"": ""acme-prod-agent"",
""docker"": {
""organization_name"": ""acme"",
""username"": ""mobythewhale"",
""pat"": ""dckr_pat__dsaCAs_xL3kNyupAa7dwO1alwg""
},
""artifactory"": [
{
""base_url"": ""https://acme.jfrog.io"",
""username"": ""acmeagent"",
""password"": ""hayKMvFKkFp42RAwKz2K"",
""image_filters"": [
{
""repository"": ""dev-local"",
""images"": [""internal/repo1"", ""internal/repo2""]
},
{
""repository"": ""prod-local"",
""images"": [""staging/repo1"", ""prod/repo1""]
}
]
}
]
}
Create a configuration file and save it somewhere on the server where you plan
to run the agent. For example, /var/opt/artifactory-agent/config.json
.
Run the agent
The following example shows how to run the Docker Scout Artifactory agent using
docker run
. This command creates a bind mount for the directory containing the
JSON configuration file created earlier at /opt/artifactory-agent/data
inside
the container. Make sure the mount path you use is the directory containing the
config.json
file.
Important
Use the
v1
tag of the Artifactory agent image. Don't use thelatest
tag as doing so may incur breaking changes.
$ docker run \
--mount type=bind,src=/var/opt/artifactory-agent,target=/opt/artifactory-agent/data \
docker/artifactory-agent:v1
Analyzing pre-existing data
By default the agent detects and analyzes images as they're created and
updated. If you want to use the agent to analyze pre-existing images, you
can use backfill mode. Use the --backfill-from=TIME
command line option,
where TIME
is an ISO 8601 formatted time, to run the agent in backfill mode.
If you use this option, the agent analyzes all images pushed between that
time and the current time when the agent starts, then exits.
For example:
$ docker run \
--mount type=bind,src=/var/opt/artifactory-agent,target=/opt/artifactory-agent/data \
docker/artifactory-agent:v1 --backfill-from=2022-04-10T10:00:00Z
When running a backfill multiple times, the agent won't analyze images that
it's already analyzed. To force re-analysis, provide the --force
command
line flag.
View analysis results
You can view the image analysis results in the Docker Scout Dashboard.
Go to Images page in the Docker Scout Dashboard.
This page displays the Docker Scout-enabled repositories in your organization.
Select the image in the list.
Select the tag.
When you have selected a tag, you're taken to the vulnerability report for that tag. Here, you can select if you want to view all vulnerabilities in the image, or vulnerabilities introduced in a specific layer. You can also filter vulnerabilities by severity, and whether or not there's a fix version available.",,,
8ef3a97fa0c1160f0596a621181cd26ecfc8dfef2102f685d982fc0dad822346,"Optimize for building in the cloud
Docker Build Cloud runs your builds remotely, and not on the machine where you invoke the build. This means that file transfers between the client and builder happen over the network.
Transferring files over the network has a higher latency and lower bandwidth than local transfers. Docker Build Cloud has several features to mitigate this:
- It uses attached storage volumes for build cache, which makes reading and writing cache very fast.
- Loading build results back to the client only pulls the layers that were changed compared to previous builds.
Despite these optimizations, building remotely can still yield slow context transfers and image loads, for large projects or if the network connection is slow. Here are some ways that you can optimize your builds to make the transfer more efficient:
For more information on how to optimize your builds, see Building best practices.
Dockerignore files
Using a
.dockerignore
file,
you can be explicit about which local files you don’t want to include in the
build context. Files caught by the glob patterns you specify in your
ignore-file aren't transferred to the remote builder.
Some examples of things you might want to add to your .dockerignore
file are:
.git
— skip sending the version control history in the build context. Note that this means you won’t be able to run Git commands in your build steps, such asgit rev-parse
.- Directories containing build artifacts, such as binaries. Build artifacts created locally during development.
- Vendor directories for package managers, such as
node_modules
.
In general, the contents of your .dockerignore
file should be similar to what
you have in your .gitignore
.
Slim base images
Selecting smaller images for your FROM
instructions in your Dockerfile can
help reduce the size of the final image. The
Alpine image
is a good example of a minimal Docker image that provides all of the OS
utilities you would expect from a Linux container.
There’s also the
special scratch
image,
which contains nothing at all. Useful for creating images of statically linked
binaries, for example.
Multi-stage builds
Multi-stage builds can make your build run faster, because stages can run in parallel. It can also make your end-result smaller. Write your Dockerfile in such a way that the final runtime stage uses the smallest possible base image, with only the resources that your program requires to run.
It’s also possible to
copy resources from other images or stages,
using the Dockerfile COPY --from
instruction. This technique can reduce the
number of layers, and the size of those layers, in the final stage.
Fetch remote files in build
When possible, you should fetch files from a remote location in the build, rather than bundling the files into the build context. Downloading files on the Docker Build Cloud server directly is better, because it will likely be faster than transferring the files with the build context.
You can fetch remote files during the build using the
Dockerfile ADD
instruction,
or in your RUN
instructions with tools like wget
and rsync
.
Multi-threaded tools
Some tools that you use in your build instructions may not utilize multiple
cores by default. One such example is make
which uses a single thread by
default, unless you specify the make --jobs=<n>
option. For build steps
involving such tools, try checking if you can optimize the execution with
parallelization.",,,
4187728571ab74966685785540dbabefbb50872ec6eee84d4cd5a183160cd4af,"Manage swarm security with public key infrastructure (PKI)
The Swarm mode public key infrastructure (PKI) system built into Docker makes it simple to securely deploy a container orchestration system. The nodes in a swarm use mutual Transport Layer Security (TLS) to authenticate, authorize, and encrypt the communications with other nodes in the swarm.
When you create a swarm by running docker swarm init
, Docker designates itself
as a manager node. By default, the manager node generates a new root Certificate
Authority (CA) along with a key pair, which are used to secure communications
with other nodes that join the swarm. If you prefer, you can specify your own
externally-generated root CA, using the --external-ca
flag of the
docker swarm init command.
The manager node also generates two tokens to use when you join additional nodes to the swarm: one worker token and one manager token. Each token includes the digest of the root CA's certificate and a randomly generated secret. When a node joins the swarm, the joining node uses the digest to validate the root CA certificate from the remote manager. The remote manager uses the secret to ensure the joining node is an approved node.
Each time a new node joins the swarm, the manager issues a certificate to the node. The certificate contains a randomly generated node ID to identify the node under the certificate common name (CN) and the role under the organizational unit (OU). The node ID serves as the cryptographically secure node identity for the lifetime of the node in the current swarm.
The diagram below illustrates how manager nodes and worker nodes encrypt communications using a minimum of TLS 1.2.
The example below shows the information from a certificate from a worker node:
Certificate:
Data:
Version: 3 (0x2)
Serial Number:
3b:1c:06:91:73:fb:16:ff:69:c3:f7:a2:fe:96:c1:73:e2:80:97:3b
Signature Algorithm: ecdsa-with-SHA256
Issuer: CN=swarm-ca
Validity
Not Before: Aug 30 02:39:00 2016 GMT
Not After : Nov 28 03:39:00 2016 GMT
Subject: O=ec2adilxf4ngv7ev8fwsi61i7, OU=swarm-worker, CN=dw02poa4vqvzxi5c10gm4pq2g
...snip...
By default, each node in the swarm renews its certificate every three months.
You can configure this interval by running the docker swarm update --cert-expiry <TIME PERIOD>
command. The minimum rotation value is 1 hour.
Refer to the
docker swarm update CLI
reference for details.
Rotating the CA certificate
Note
Mirantis Kubernetes Engine (MKE), formerly known as Docker UCP, provides an external certificate manager service for the swarm. If you run swarm on MKE, you shouldn't rotate the CA certificates manually. Instead, contact Mirantis support if you need to rotate a certificate.
In the event that a cluster CA key or a manager node is compromised, you can rotate the swarm root CA so that none of the nodes trust certificates signed by the old root CA anymore.
Run docker swarm ca --rotate
to generate a new CA certificate and key. If you
prefer, you can pass the --ca-cert
and --external-ca
flags to specify the
root certificate and to use a root CA external to the swarm. Alternately,
you can pass the --ca-cert
and --ca-key
flags to specify the exact
certificate and key you would like the swarm to use.
When you issue the docker swarm ca --rotate
command, the following things
happen in sequence:
Docker generates a cross-signed certificate. This means that a version of the new root CA certificate is signed with the old root CA certificate. This cross-signed certificate is used as an intermediate certificate for all new node certificates. This ensures that nodes that still trust the old root CA can still validate a certificate signed by the new CA.
Docker also tells all nodes to immediately renew their TLS certificates. This process may take several minutes, depending on the number of nodes in the swarm.
After every node in the swarm has a new TLS certificate signed by the new CA, Docker forgets about the old CA certificate and key material, and tells all the nodes to trust the new CA certificate only.
This also causes a change in the swarm's join tokens. The previous join tokens are no longer valid.
From this point on, all new node certificates issued are signed with the new root CA, and do not contain any intermediates.",,,
a48b535bbff0915917398a1edb22635921ce14f48ff96885d84a1907ad023bb2,"Trusted content
Docker's trusted content programs ensure that container images meet the highest standards for security, quality, and reliability. These programs provide opportunities for publishers and contributors to share their images with millions of developers worldwide while gaining valuable insights into their content's usage. By participating, you can enhance your content's visibility, build credibility, and access tools to optimize its impact within the container ecosystem.
In this section, learn about:
- Docker Official Images: Learn how to contribute, propose, and maintain Docker Official Images to serve as reliable foundations for containerized applications.
- Docker-Sponsored Open Source (DSOS) Program: Discover how open source projects can gain perks like verified badges, insights, and access to Docker Scout, enhancing visibility and trust on Docker Hub.
- Docker Verified Publisher (DVP) Program: Explore how to join the DVP program to showcase trusted, high-quality images with a verified badge, gain priority in search results, access insights, and enhance security through vulnerability analysis.
- Insights and analytics: Access detailed metrics on image and extension usage, including pull counts, geolocation, and client data, to understand user behavior and optimize your content.",,,
f2e3db1bfc0a9922e9a0c5c407e03f135c5436e0a081faeaaacafd555471bac6,"Configure Settings Management with the Admin Console
This page contains information for administrators on how to configure Settings Management with the Docker Admin Console. You can specify and lock configuration parameters to create a standardized Docker Desktop environment across your Docker company or organization.
Prerequisites
- Download and install Docker Desktop 4.36.0 or later.
- Verify your domain.
- Enforce sign-in. The Settings Management feature requires a Docker Business subscription, therefore your Docker Desktop users must authenticate to your organization for configurations to take effect.
Create a settings policy
Within the Docker Admin Console navigate to the company or organization you want to define a settings policy for.
Under the Security and access section, select Desktop Settings Management.
In the top-right corner, select Create a settings policy.
Give your settings policy a name and an optional description.
Tip
If you have already configured Settings Management with an
admin-settings.json
file for an organization, you can upload it using the Upload existing settings button which then automatically populates the form for you.Settings policies deployed via the Docker Admin Console take precedence over manually deployed
admin-settings.json
files.Assign the setting policy to all your users within the company or organization, or specific users.
Note
If a settings policy is assigned to all users, it sets the policy as the global default policy. You can only have one global settings policy at a time. If a user already has a user-specific settings policy assigned, the user-specific policy takes precedence over a global policy.
Tip
Before setting a global settings policy, it is recommended that you first test it as a user-specific policy to make sure you're happy with the changes before proceeding.
Configure the settings for the policy. Go through each setting and select your chosen setting state. You can choose:
- User-defined. Your developers are able to control and change this setting.
- Always enabled. This means the setting is turned on and your users won't be able to edit this setting from Docker Desktop or the CLI.
- Enabled. The setting is turned on and users can edit this setting from Docker Desktop or the CLI.
- Always disabled. This means the setting is turned off and your users won't be able to edit this setting from Docker Desktop or the CLI.
- Disabled. The setting is turned off and users can edit this setting from Docker Desktop or the CLI.
Select Create
For the settings policy to take effect:
On a new install, users need to launch Docker Desktop and authenticate to their organization.
On an existing install, users need to quit Docker Desktop through the Docker menu, and then re-launch Docker Desktop. If they are already signed in, they don't need to sign in again for the changes to take effect.
Important
Selecting Restart from the Docker menu isn't enough as it only restarts some components of Docker Desktop.
To avoid disrupting your users' workflows, Docker doesn't automatically require that users re-launch once a change has been made.
Note
Settings are synced to Docker Desktop and the CLI when a user is signed in and starts Docker Desktop, and then every 60 minutes.
If your settings policy needs to be rolled back, either delete the policy or edit the policy to set individual settings to User-defined.
Settings policy actions
From the Actions menu on the Desktop Settings Management page in the Docker Admin Console, you can:
- Edit or delete an existing settings policy.
- Export a settings policy as an
admin-settings.json
file. - Promote a policy that is applied to a select group of users, to be the new global default policy for all users.",,,
97d743b6c9548e99b7bb7601d8b2fb2e8e37f5c16d0b0a0df17338239a393c6c,"Using secrets with GitHub Actions
A build secret is sensitive information, such as a password or API token, consumed as part of the build process. Docker Build supports two forms of secrets:
- Secret mounts add secrets as files in the build container
(under
/run/secrets
by default). - SSH mounts add SSH agent sockets or keys into the build container.
This page shows how to use secrets with GitHub Actions. For an introduction to secrets in general, see Build secrets.
Secret mounts
In the following example uses and exposes the
GITHUB_TOKEN
secret
as provided by GitHub in your workflow.
First, create a Dockerfile
that uses the secret:
# syntax=docker/dockerfile:1
FROM alpine
RUN --mount=type=secret,id=github_token,env=GITHUB_TOKEN ...
In this example, the secret name is github_token
. The following workflow
exposes this secret using the secrets
input:
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Set up QEMU
uses: docker/setup-qemu-action@v3
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build
uses: docker/build-push-action@v6
with:
platforms: linux/amd64,linux/arm64
tags: user/app:latest
secrets: |
""github_token=${{ secrets.GITHUB_TOKEN }}""
Note
You can also expose a secret file to the build with the
secret-files
input:secret-files: | ""MY_SECRET=./secret.txt""
If you're using GitHub secrets and need to handle multi-line value, you will need to place the key-value pair between quotes:
secrets: |
""MYSECRET=${{ secrets.GPG_KEY }}""
GIT_AUTH_TOKEN=abcdefghi,jklmno=0123456789
""MYSECRET=aaaaaaaa
bbbbbbb
ccccccccc""
FOO=bar
""EMPTYLINE=aaaa
bbbb
ccc""
""JSON_SECRET={""""key1"""":""""value1"""",""""key2"""":""""value2""""}""
| Key | Value |
|---|---|
MYSECRET | *********************** |
GIT_AUTH_TOKEN | abcdefghi,jklmno=0123456789 |
MYSECRET | aaaaaaaa\nbbbbbbb\nccccccccc |
FOO | bar |
EMPTYLINE | aaaa\n\nbbbb\nccc |
JSON_SECRET | {""key1"":""value1"",""key2"":""value2""} |
Note
Double escapes are needed for quote signs.
SSH mounts
SSH mounts let you authenticate with SSH servers.
For example to perform a git clone
,
or to fetch application packages from a private repository.
The following Dockerfile example uses an SSH mount to fetch Go modules from a private GitHub repository.
# syntax=docker/dockerfile:1
ARG GO_VERSION=""1.23""
FROM golang:${GO_VERSION}-alpine AS base
ENV CGO_ENABLED=0
ENV GOPRIVATE=""github.com/foo/*""
RUN apk add --no-cache file git rsync openssh-client
RUN mkdir -p -m 0700 ~/.ssh && ssh-keyscan github.com >> ~/.ssh/known_hosts
WORKDIR /src
FROM base AS vendor
# this step configure git and checks the ssh key is loaded
RUN --mount=type=ssh <<EOT
set -e
echo ""Setting Git SSH protocol""
git config --global url.""git@github.com:"".insteadOf ""https://github.com/""
(
set +e
ssh -T git@github.com
if [ ! ""$?"" = ""1"" ]; then
echo ""No GitHub SSH key loaded exiting...""
exit 1
fi
)
EOT
# this one download go modules
RUN --mount=type=bind,target=. \
--mount=type=cache,target=/go/pkg/mod \
--mount=type=ssh \
go mod download -x
FROM vendor AS build
RUN --mount=type=bind,target=. \
--mount=type=cache,target=/go/pkg/mod \
--mount=type=cache,target=/root/.cache \
go build ...
To build this Dockerfile, you must specify an SSH mount that the builder can
use in the steps with --mount=type=ssh
.
The following GitHub Action workflow uses the MrSquaare/ssh-setup-action
third-party action to bootstrap SSH setup on the GitHub runner. The action
creates a private key defined by the GitHub Action secret SSH_GITHUB_PPK
and
adds it to the SSH agent socket file at SSH_AUTH_SOCK
. The SSH mount in the
build step assume SSH_AUTH_SOCK
by default, so there's no need to specify the
ID or path for the SSH agent socket explicitly.
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Set up SSH
uses: MrSquaare/ssh-setup-action@2d028b70b5e397cf8314c6eaea229a6c3e34977a # v3.1.0
with:
host: github.com
private-key: ${{ secrets.SSH_GITHUB_PPK }}
private-key-name: github-ppk
- name: Build and push
uses: docker/build-push-action@v6
with:
ssh: default
push: true
tags: user/app:latest
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Set up SSH
uses: MrSquaare/ssh-setup-action@2d028b70b5e397cf8314c6eaea229a6c3e34977a # v3.1.0
with:
host: github.com
private-key: ${{ secrets.SSH_GITHUB_PPK }}
private-key-name: github-ppk
- name: Build
uses: docker/bake-action@v6
with:
set: |
*.ssh=default",,,
f2bde83652530bb58c77eefa6cb43a46bf7ca41566e516bb3907f198a2bb96ea,"Alternative container runtimes
Docker Engine uses containerd for managing the container lifecycle, which includes creating, starting, and stopping containers. By default, containerd uses runc as its container runtime.
What runtimes can I use?
You can use any runtime that implements the containerd shim API. Such runtimes ship with a containerd shim, and you can use them without any additional configuration. See Use containerd shims.
Examples of runtimes that implement their own containerd shims include:
You can also use runtimes designed as drop-in replacements for runc. Such runtimes depend on the runc containerd shim for invoking the runtime binary. You must manually register such runtimes in the daemon configuration.
youki is one example of a runtime that can function as a runc drop-in replacement. Refer to the youki example explaining the setup.
Use containerd shims
containerd shims let you use alternative runtimes without having to change the
configuration of the Docker daemon. To use a containerd shim, install the shim
binary on PATH
on the system where the Docker daemon is running.
To use a shim with docker run
, specify the fully qualified name of the
runtime as the value to the --runtime
flag:
$ docker run --runtime io.containerd.kata.v2 hello-world
Use a containerd shim without installing on PATH
You can use a shim without installing it on PATH
, in which case you need to
register the shim in the daemon configuration as follows:
{
""runtimes"": {
""foo"": {
""runtimeType"": ""/path/to/containerd-shim-foobar-v1""
}
}
}
To use the shim, specify the name that you assigned to it:
$ docker run --runtime foo hello-world
Configure shims
If you need to pass additional configuration for a containerd shim, you can
use the runtimes
option in the daemon configuration file.
Edit the daemon configuration file by adding a
runtimes
entry for the shim you want to configure.- Specify the fully qualified name for the runtime in
runtimeType
key - Add your runtime configuration under the
options
key
{ ""runtimes"": { ""gvisor"": { ""runtimeType"": ""io.containerd.runsc.v1"", ""options"": { ""TypeUrl"": ""io.containerd.runsc.v1.options"", ""ConfigPath"": ""/etc/containerd/runsc.toml"" } } } }
- Specify the fully qualified name for the runtime in
Reload the daemon's configuration.
# systemctl reload docker
Use the customized runtime using the
--runtime
flag fordocker run
.$ docker run --runtime gvisor hello-world
For more information about the configuration options for containerd shims, see Configure containerd shims.
Examples
The following examples show you how to set up and use alternative container runtimes with Docker Engine.
youki
youki is a container runtime written in Rust. youki claims to be faster and use less memory than runc, making it a good choice for resource-constrained environments.
youki functions as a drop-in replacement for runc, meaning it relies on the runc shim to invoke the runtime binary. When you register runtimes acting as runc replacements, you configure the path to the runtime executable, and optionally a set of runtime arguments. For more information, see Configure runc drop-in replacements.
To add youki as a container runtime:
Install youki and its dependencies.
For instructions, refer to the official setup guide.
Register youki as a runtime for Docker by editing the Docker daemon configuration file, located at
/etc/docker/daemon.json
by default.The
path
key should specify the path to wherever you installed youki.# cat > /etc/docker/daemon.json <<EOF { ""runtimes"": { ""youki"": { ""path"": ""/usr/local/bin/youki"" } } } EOF
Reload the daemon's configuration.
# systemctl reload docker
Now you can run containers that use youki as a runtime.
$ docker run --rm --runtime youki hello-world
Wasmtime
Wasmtime is a Bytecode Alliance project, and a Wasm runtime that lets you run Wasm containers. Running Wasm containers with Docker provides two layers of security. You get all the benefits from container isolation, plus the added sandboxing provided by the Wasm runtime environment.
To add Wasmtime as a container runtime, follow these steps:
Turn on the containerd image store feature in the daemon configuration file.
{ ""features"": { ""containerd-snapshotter"": true } }
Restart the Docker daemon.
# systemctl restart docker
Install the Wasmtime containerd shim on
PATH
.The following command Dockerfile builds the Wasmtime binary from source and exports it to
./containerd-shim-wasmtime-v1
.$ docker build --output . - <<EOF FROM rust:latest as build RUN cargo install \ --git https://github.com/containerd/runwasi.git \ --bin containerd-shim-wasmtime-v1 \ --root /out \ containerd-shim-wasmtime FROM scratch COPY --from=build /out/bin / EOF
Put the binary in a directory on
PATH
.$ mv ./containerd-shim-wasmtime-v1 /usr/local/bin
Now you can run containers that use Wasmtime as a runtime.
$ docker run --rm \
--runtime io.containerd.wasmtime.v1 \
--platform wasi/wasm32 \
michaelirwin244/wasm-example
Related information
- To learn more about the configuration options for container runtimes, see Configure container runtimes.
- You can configure which runtime that the daemon should use as its default. Refer to Configure the default container runtime.",,,
452026709960f47108a1d822f012a72c64fbb7e688590eb11ec57ca494807e02,"Use the default Compose Bridge transformation
Compose Bridge supplies an out-of-the box transformation for your Compose configuration file. Based on an arbitrary compose.yaml
file, Compose Bridge produces:
- A Namespace so all your resources are isolated and don't conflict with resources from other deployments.
- A ConfigMap with an entry for each and every config resource in your Compose application.
- Deployments for application services. This ensures that the specified number of instances of your application are maintained in the Kubernetes cluster.
- Services for ports exposed by your services, used for service-to-service communication.
- Services for ports published by your services, with type
LoadBalancer
so that Docker Desktop will also expose the same port on the host. - Network policies to replicate the networking topology defined in your
compose.yaml
file. - PersistentVolumeClaims for your volumes, using
hostpath
storage class so that Docker Desktop manages volume creation. - Secrets with your secret encoded. This is designed for local use in a testing environment.
It also supplies a Kustomize overlay dedicated to Docker Desktop with:
Loadbalancer
for services which need to expose ports on host.- A
PersistentVolumeClaim
to use the Docker Desktop storage provisionerdesktop-storage-provisioner
to handle volume provisioning more effectively. - A Kustomize file to link all the resources together.
Use the default Compose Bridge transformation
To use the default transformation run the following command:
$ compose-bridge convert
Compose looks for a compose.yaml
file inside the current directory and then converts it.
The following output is displayed
$ compose-bridge convert -f compose.yaml
Kubernetes resource api-deployment.yaml created
Kubernetes resource db-deployment.yaml created
Kubernetes resource web-deployment.yaml created
Kubernetes resource api-expose.yaml created
Kubernetes resource db-expose.yaml created
Kubernetes resource web-expose.yaml created
Kubernetes resource 0-avatars-namespace.yaml created
Kubernetes resource default-network-policy.yaml created
Kubernetes resource private-network-policy.yaml created
Kubernetes resource public-network-policy.yaml created
Kubernetes resource db-db_data-persistentVolumeClaim.yaml created
Kubernetes resource api-service.yaml created
Kubernetes resource web-service.yaml created
Kubernetes resource kustomization.yaml created
Kubernetes resource db-db_data-persistentVolumeClaim.yaml created
Kubernetes resource api-service.yaml created
Kubernetes resource web-service.yaml created
Kubernetes resource kustomization.yaml created
These files are then stored within your project in the /out
folder.
The Kubernetes manifests can then be used to run the application on Kubernetes using
the standard deployment command kubectl apply -k out/overlays/desktop/
.
Note
Make sure you have enabled Kubernetes in Docker Desktop before you deploy your Compose Bridge transformations.
If you want to convert a compose.yaml
file that is located in another directory, you can run:
$ compose-bridge convert -f <path-to-file>/compose.yaml
To see all available flags, run:
$ compose-bridge convert --help
Tip
You can now convert and deploy your Compose project to a Kubernetes cluster from the Compose file viewer.
Make sure you are signed in to your Docker account, navigate to your container in the Containers view, and in the top-right corner select View configurations and then Convert and Deploy to Kubernetes.",,,
c1be3f054565072d5bb2433c66bfc37f92a5e8cf1b930a0c1a5bdcf6d8b5eb22,"Networking overview
Container networking refers to the ability for containers to connect to and communicate with each other, or to non-Docker workloads.
Containers have networking enabled by default, and they can make outgoing
connections. A container has no information about what kind of network it's
attached to, or whether their peers are also Docker workloads or not. A
container only sees a network interface with an IP address, a gateway, a
routing table, DNS services, and other networking details. That is, unless the
container uses the none
network driver.
This page describes networking from the point of view of the container,
and the concepts around container networking.
This page doesn't describe OS-specific details about how Docker networks work.
For information about how Docker manipulates iptables
rules on Linux,
see
Packet filtering and firewalls.
User-defined networks
You can create custom, user-defined networks, and connect multiple containers to the same network. Once connected to a user-defined network, containers can communicate with each other using container IP addresses or container names.
The following example creates a network using the bridge
network driver and
running a container in the created network:
$ docker network create -d bridge my-net
$ docker run --network=my-net -itd --name=container3 busybox
Drivers
The following network drivers are available by default, and provide core networking functionality:
| Driver | Description |
|---|---|
bridge | The default network driver. |
host | Remove network isolation between the container and the Docker host. |
none | Completely isolate a container from the host and other containers. |
overlay | Overlay networks connect multiple Docker daemons together. |
ipvlan | IPvlan networks provide full control over both IPv4 and IPv6 addressing. |
macvlan | Assign a MAC address to a container. |
For more information about the different drivers, see Network drivers overview.
Connecting to multiple networks
A container can be connected to multiple networks.
For example, a frontend container may be connected to a bridge network
with external access, and a
--internal
network
to communicate with containers running backend services that do not need
external network access.
A container may also be connected to different types of network. For example,
an ipvlan
network to provide internet access, and a bridge
network for
access to local services.
When sending packets, if the destination is an address in a directly connected
network, packets are sent to that network. Otherwise, packets are sent to
a default gateway for routing to their destination. In the example above,
the ipvlan
network's gateway must be the default gateway.
The default gateway is selected by Docker, and may change whenever a
container's network connections change.
To make Docker choose a specific default gateway when creating the container
or connecting a new network, set a gateway priority. See option gw-priority
for the
docker run
and
docker network connect
commands.
The default gw-priority
is 0
and the gateway in the network with the
highest priority is the default gateway. So, when a network should always
be the default gateway, it is enough to set its gw-priority
to 1
.
$ docker run --network name=gwnet,gw-priority=1 --network anet1 --name myctr myimage
$ docker network connect anet2 myctr
Container networks
In addition to user-defined networks, you can attach a container to another
container's networking stack directly, using the --network container:<name|id>
flag format.
The following flags aren't supported for containers using the container:
networking mode:
--add-host
--hostname
--dns
--dns-search
--dns-option
--mac-address
--publish
--publish-all
--expose
The following example runs a Redis container, with Redis binding to
localhost
, then running the redis-cli
command and connecting to the Redis
server over the localhost
interface.
$ docker run -d --name redis example/redis --bind 127.0.0.1
$ docker run --rm -it --network container:redis example/redis-cli -h 127.0.0.1
Published ports
By default, when you create or run a container using docker create
or docker run
,
containers on bridge networks don't expose any ports to the outside world.
Use the --publish
or -p
flag to make a port available to services
outside the bridge network.
This creates a firewall rule in the host,
mapping a container port to a port on the Docker host to the outside world.
Here are some examples:
| Flag value | Description |
|---|---|
-p 8080:80 | Map port 8080 on the Docker host to TCP port 80 in the container. |
-p 192.168.1.100:8080:80 | Map port 8080 on the Docker host IP 192.168.1.100 to TCP port 80 in the container. |
-p 8080:80/udp | Map port 8080 on the Docker host to UDP port 80 in the container. |
-p 8080:80/tcp -p 8080:80/udp | Map TCP port 8080 on the Docker host to TCP port 80 in the container, and map UDP port 8080 on the Docker host to UDP port 80 in the container. |
Important
Publishing container ports is insecure by default. Meaning, when you publish a container's ports it becomes available not only to the Docker host, but to the outside world as well.
If you include the localhost IP address (
127.0.0.1
, or::1
) with the publish flag, only the Docker host and its containers can access the published container port.$ docker run -p 127.0.0.1:8080:80 -p '[::1]:8080:80' nginx
Warning
Hosts within the same L2 segment (for example, hosts connected to the same network switch) can reach ports published to localhost. For more information, see moby/moby#45610
If you want to make a container accessible to other containers, it isn't necessary to publish the container's ports. You can enable inter-container communication by connecting the containers to the same network, usually a bridge network.
Ports on the host's IPv6 addresses will map to the container's IPv4 address
if no host IP is given in a port mapping, the bridge network is IPv4-only,
and --userland-proxy=true
(default).
For more information about port mapping, including how to disable it and use direct routing to containers, see packet filtering and firewalls.
IP address and hostname
When creating a network, IPv4 address allocation is enabled by default, it
can be disabled using --ipv4=false
. IPv6 address allocation can be enabled
using --ipv6
.
$ docker network create --ipv6 --ipv4=false v6net
By default, the container gets an IP address for every Docker network it attaches to. A container receives an IP address out of the IP subnet of the network. The Docker daemon performs dynamic subnetting and IP address allocation for containers. Each network also has a default subnet mask and gateway.
You can connect a running container to multiple networks,
either by passing the --network
flag multiple times when creating the container,
or using the docker network connect
command for already running containers.
In both cases, you can use the --ip
or --ip6
flags to specify the container's IP address on that particular network.
In the same way, a container's hostname defaults to be the container's ID in Docker.
You can override the hostname using --hostname
.
When connecting to an existing network using docker network connect
,
you can use the --alias
flag to specify an additional network alias for the container on that network.
DNS services
Containers use the same DNS servers as the host by default, but you can
override this with --dns
.
By default, containers inherit the DNS settings as defined in the
/etc/resolv.conf
configuration file.
Containers that attach to the default bridge
network receive a copy of this file.
Containers that attach to a
custom network
use Docker's embedded DNS server.
The embedded DNS server forwards external DNS lookups to the DNS servers configured on the host.
You can configure DNS resolution on a per-container basis, using flags for the
docker run
or docker create
command used to start the container.
The following table describes the available docker run
flags related to DNS
configuration.
| Flag | Description |
|---|---|
--dns | The IP address of a DNS server. To specify multiple DNS servers, use multiple --dns flags. DNS requests will be forwarded from the container's network namespace so, for example, --dns=127.0.0.1 refers to the container's own loopback address. |
--dns-search | A DNS search domain to search non-fully qualified hostnames. To specify multiple DNS search prefixes, use multiple --dns-search flags. |
--dns-opt | A key-value pair representing a DNS option and its value. See your operating system's documentation for resolv.conf for valid options. |
--hostname | The hostname a container uses for itself. Defaults to the container's ID if not specified. |
Custom hosts
Your container will have lines in /etc/hosts
which define the hostname of the
container itself, as well as localhost
and a few other common things. Custom
hosts, defined in /etc/hosts
on the host machine, aren't inherited by
containers. To pass additional hosts into a container, refer to
add entries to
container hosts file in the
docker run
reference documentation.
Proxy server
If your container needs to use a proxy server, see Use a proxy server.",,,
7f719a8d52e664f1b986bd6374f298ac9e0d89f9f35fa727f5b01f420fea1ac8,"Configuring your GitHub Actions builder
This page contains instructions on configuring your BuildKit instances when using our Setup Buildx Action.
Version pinning
By default, the action will attempt to use the latest version of Buildx available on the GitHub Runner (the build client) and the latest release of BuildKit (the build server).
To pin to a specific version of Buildx, use the version
input. For example,
to pin to Buildx v0.10.0:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
with:
version: v0.10.0
To pin to a specific version of BuildKit, use the image
option in the
driver-opts
input. For example, to pin to BuildKit v0.11.0:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
with:
driver-opts: image=moby/buildkit:v0.11.0
BuildKit container logs
To display BuildKit container logs when using the docker-container
driver,
you must either
enable step debug logging,
or set the --debug
buildkitd flag in the
Docker Setup Buildx action:
name: ci
on:
push:
jobs:
buildx:
runs-on: ubuntu-latest
steps:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
with:
buildkitd-flags: --debug
- name: Build
uses: docker/build-push-action@v6
Logs will be available at the end of a job:
BuildKit Daemon configuration
You can provide a
BuildKit configuration
to your builder if you're using the
docker-container
driver
(default) with the config
or buildkitd-config-inline
inputs:
Registry mirror
You can configure a registry mirror using an inline block directly in your
workflow with the buildkitd-config-inline
input:
name: ci
on:
push:
jobs:
buildx:
runs-on: ubuntu-latest
steps:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
with:
buildkitd-config-inline: |
[registry.""docker.io""]
mirrors = [""mirror.gcr.io""]
For more information about using a registry mirror, see Registry mirror.
Max parallelism
You can limit the parallelism of the BuildKit solver which is particularly useful for low-powered machines.
You can use the buildkitd-config-inline
input like the previous example, or you can use
a dedicated BuildKit config file from your repository if you want with the
config
input:
# .github/buildkitd.toml
[worker.oci]
max-parallelism = 4
name: ci
on:
push:
jobs:
buildx:
runs-on: ubuntu-latest
steps:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
with:
config: .github/buildkitd.toml
Append additional nodes to the builder
Buildx supports running builds on multiple machines. This is useful for building multi-platform images on native nodes for more complicated cases that aren't handled by QEMU. Building on native nodes generally has better performance, and allows you to distribute the build across multiple machines.
You can append nodes to the builder you're creating using the append
option.
It takes input in the form of a YAML string document to remove limitations
intrinsically linked to GitHub Actions: you can only use strings in the input
fields:
| Name | Type | Description |
|---|---|---|
name | String | Name of the node. If empty, it's the name of the builder it belongs to, with an index number suffix. This is useful to set it if you want to modify/remove a node in an underlying step of you workflow. |
endpoint | String | Docker context or endpoint of the node to add to the builder |
driver-opts | List | List of additional driver-specific options |
buildkitd-flags | String | Flags for buildkitd daemon |
platforms | String | Fixed platforms for the node. If not empty, values take priority over the detected ones. |
Here is an example using remote nodes with the
remote
driver
and
TLS authentication:
name: ci
on:
push:
jobs:
buildx:
runs-on: ubuntu-latest
steps:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
with:
driver: remote
endpoint: tcp://oneprovider:1234
append: |
- endpoint: tcp://graviton2:1234
platforms: linux/arm64
- endpoint: tcp://linuxone:1234
platforms: linux/s390x
env:
BUILDER_NODE_0_AUTH_TLS_CACERT: ${{ secrets.ONEPROVIDER_CA }}
BUILDER_NODE_0_AUTH_TLS_CERT: ${{ secrets.ONEPROVIDER_CERT }}
BUILDER_NODE_0_AUTH_TLS_KEY: ${{ secrets.ONEPROVIDER_KEY }}
BUILDER_NODE_1_AUTH_TLS_CACERT: ${{ secrets.GRAVITON2_CA }}
BUILDER_NODE_1_AUTH_TLS_CERT: ${{ secrets.GRAVITON2_CERT }}
BUILDER_NODE_1_AUTH_TLS_KEY: ${{ secrets.GRAVITON2_KEY }}
BUILDER_NODE_2_AUTH_TLS_CACERT: ${{ secrets.LINUXONE_CA }}
BUILDER_NODE_2_AUTH_TLS_CERT: ${{ secrets.LINUXONE_CERT }}
BUILDER_NODE_2_AUTH_TLS_KEY: ${{ secrets.LINUXONE_KEY }}
Authentication for remote builders
The following examples show how to handle authentication for remote builders, using SSH or TLS.
SSH authentication
To be able to connect to an SSH endpoint using the
docker-container
driver,
you have to set up the SSH private key and configuration on the GitHub Runner:
name: ci
on:
push:
jobs:
buildx:
runs-on: ubuntu-latest
steps:
- name: Set up SSH
uses: MrSquaare/ssh-setup-action@2d028b70b5e397cf8314c6eaea229a6c3e34977a # v3.1.0
with:
host: graviton2
private-key: ${{ secrets.SSH_PRIVATE_KEY }}
private-key-name: aws_graviton2
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
with:
endpoint: ssh://me@graviton2
TLS authentication
You can also
set up a remote BuildKit instance
using the remote driver. To ease the integration in your workflow, you can use
an environment variables that sets up authentication using the BuildKit client
certificates for the tcp://
:
BUILDER_NODE_<idx>_AUTH_TLS_CACERT
BUILDER_NODE_<idx>_AUTH_TLS_CERT
BUILDER_NODE_<idx>_AUTH_TLS_KEY
The <idx>
placeholder is the position of the node in the list of nodes.
name: ci
on:
push:
jobs:
buildx:
runs-on: ubuntu-latest
steps:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
with:
driver: remote
endpoint: tcp://graviton2:1234
env:
BUILDER_NODE_0_AUTH_TLS_CACERT: ${{ secrets.GRAVITON2_CA }}
BUILDER_NODE_0_AUTH_TLS_CERT: ${{ secrets.GRAVITON2_CERT }}
BUILDER_NODE_0_AUTH_TLS_KEY: ${{ secrets.GRAVITON2_KEY }}
Standalone mode
If you don't have the Docker CLI installed on the GitHub Runner, the Buildx
binary gets invoked directly, instead of calling it as a Docker CLI plugin. This
can be useful if you want to use the kubernetes
driver in your self-hosted
runner:
name: ci
on:
push:
jobs:
buildx:
runs-on: ubuntu-latest
steps:
- name: Checkout
uses: actions/checkout@v4
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
with:
driver: kubernetes
- name: Build
run: |
buildx build .
Isolated builders
The following example shows how you can select different builders for different jobs.
An example scenario where this might be useful is when you are using a monorepo, and you want to pinpoint different packages to specific builders. For example, some packages may be particularly resource-intensive to build and require more compute. Or they require a builder equipped with a particular capability or hardware.
For more information about remote builder, see
remote
driver
and the
append builder nodes example.
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Set up builder1
uses: docker/setup-buildx-action@v3
id: builder1
- name: Set up builder2
uses: docker/setup-buildx-action@v3
id: builder2
- name: Build against builder1
uses: docker/build-push-action@v6
with:
builder: ${{ steps.builder1.outputs.name }}
target: mytarget1
- name: Build against builder2
uses: docker/build-push-action@v6
with:
builder: ${{ steps.builder2.outputs.name }}
target: mytarget2",,,
aead152a5b938f078a277091b4641748f75e62f11997ff7065e8ad5646e0b811,"Configure BuildKit
If you create a docker-container
or kubernetes
builder with Buildx, you can
apply a custom
BuildKit configuration by passing the
--config
flag to
the docker buildx create
command.
Registry mirror
You can define a registry mirror to use for your builds. Doing so redirects
BuildKit to pull images from a different hostname. The following steps exemplify
defining a mirror for docker.io
(Docker Hub) to mirror.gcr.io
.
Create a TOML at
/etc/buildkitd.toml
with the following content:debug = true [registry.""docker.io""] mirrors = [""mirror.gcr.io""]
Note
debug = true
turns on debug requests in the BuildKit daemon, which logs a message that shows when a mirror is being used.Create a
docker-container
builder that uses this BuildKit configuration:$ docker buildx create --use --bootstrap \ --name mybuilder \ --driver docker-container \ --config /etc/buildkitd.toml
Build an image:
docker buildx build --load . -f - <<EOF FROM alpine RUN echo ""hello world"" EOF
The BuildKit logs for this builder now shows that it uses the GCR mirror. You
can tell by the fact that the response messages include the x-goog-*
HTTP
headers.
$ docker logs buildx_buildkit_mybuilder0
...
time=""2022-02-06T17:47:48Z"" level=debug msg=""do request"" request.header.accept=""application/vnd.docker.container.image.v1+json, */*"" request.header.user-agent=containerd/1.5.8+unknown request.method=GET spanID=9460e5b6e64cec91 traceID=b162d3040ddf86d6614e79c66a01a577
time=""2022-02-06T17:47:48Z"" level=debug msg=""fetch response received"" response.header.accept-ranges=bytes response.header.age=1356 response.header.alt-svc=""h3=\"":443\""; ma=2592000,h3-29=\"":443\""; ma=2592000,h3-Q050=\"":443\""; ma=2592000,h3-Q046=\"":443\""; ma=2592000,h3-Q043=\"":443\""; ma=2592000,quic=\"":443\""; ma=2592000; v=\""46,43\"""" response.header.cache-control=""public, max-age=3600"" response.header.content-length=1469 response.header.content-type=application/octet-stream response.header.date=""Sun, 06 Feb 2022 17:25:17 GMT"" response.header.etag=""\""774380abda8f4eae9a149e5d5d3efc83\"""" response.header.expires=""Sun, 06 Feb 2022 18:25:17 GMT"" response.header.last-modified=""Wed, 24 Nov 2021 21:07:57 GMT"" response.header.server=UploadServer response.header.x-goog-generation=1637788077652182 response.header.x-goog-hash=""crc32c=V3DSrg=="" response.header.x-goog-hash.1=""md5=d0OAq9qPTq6aFJ5dXT78gw=="" response.header.x-goog-metageneration=1 response.header.x-goog-storage-class=STANDARD response.header.x-goog-stored-content-encoding=identity response.header.x-goog-stored-content-length=1469 response.header.x-guploader-uploadid=ADPycduqQipVAXc3tzXmTzKQ2gTT6CV736B2J628smtD1iDytEyiYCgvvdD8zz9BT1J1sASUq9pW_ctUyC4B-v2jvhIxnZTlKg response.status=""200 OK"" spanID=9460e5b6e64cec91 traceID=b162d3040ddf86d6614e79c66a01a577
time=""2022-02-06T17:47:48Z"" level=debug msg=""fetch response received"" response.header.accept-ranges=bytes response.header.age=760 response.header.alt-svc=""h3=\"":443\""; ma=2592000,h3-29=\"":443\""; ma=2592000,h3-Q050=\"":443\""; ma=2592000,h3-Q046=\"":443\""; ma=2592000,h3-Q043=\"":443\""; ma=2592000,quic=\"":443\""; ma=2592000; v=\""46,43\"""" response.header.cache-control=""public, max-age=3600"" response.header.content-length=1471 response.header.content-type=application/octet-stream response.header.date=""Sun, 06 Feb 2022 17:35:13 GMT"" response.header.etag=""\""35d688bd15327daafcdb4d4395e616a8\"""" response.header.expires=""Sun, 06 Feb 2022 18:35:13 GMT"" response.header.last-modified=""Wed, 24 Nov 2021 21:07:12 GMT"" response.header.server=UploadServer response.header.x-goog-generation=1637788032100793 response.header.x-goog-hash=""crc32c=aWgRjA=="" response.header.x-goog-hash.1=""md5=NdaIvRUyfar8201DleYWqA=="" response.header.x-goog-metageneration=1 response.header.x-goog-storage-class=STANDARD response.header.x-goog-stored-content-encoding=identity response.header.x-goog-stored-content-length=1471 response.header.x-guploader-uploadid=ADPycdtR-gJYwC7yHquIkJWFFG8FovDySvtmRnZBqlO3yVDanBXh_VqKYt400yhuf0XbQ3ZMB9IZV2vlcyHezn_Pu3a1SMMtiw response.status=""200 OK"" spanID=9460e5b6e64cec91 traceID=b162d3040ddf86d6614e79c66a01a577
time=""2022-02-06T17:47:48Z"" level=debug msg=fetch spanID=9460e5b6e64cec91 traceID=b162d3040ddf86d6614e79c66a01a577
time=""2022-02-06T17:47:48Z"" level=debug msg=fetch spanID=9460e5b6e64cec91 traceID=b162d3040ddf86d6614e79c66a01a577
time=""2022-02-06T17:47:48Z"" level=debug msg=fetch spanID=9460e5b6e64cec91 traceID=b162d3040ddf86d6614e79c66a01a577
time=""2022-02-06T17:47:48Z"" level=debug msg=fetch spanID=9460e5b6e64cec91 traceID=b162d3040ddf86d6614e79c66a01a577
time=""2022-02-06T17:47:48Z"" level=debug msg=""do request"" request.header.accept=""application/vnd.docker.image.rootfs.diff.tar.gzip, */*"" request.header.user-agent=containerd/1.5.8+unknown request.method=GET spanID=9460e5b6e64cec91 traceID=b162d3040ddf86d6614e79c66a01a577
time=""2022-02-06T17:47:48Z"" level=debug msg=""fetch response received"" response.header.accept-ranges=bytes response.header.age=1356 response.header.alt-svc=""h3=\"":443\""; ma=2592000,h3-29=\"":443\""; ma=2592000,h3-Q050=\"":443\""; ma=2592000,h3-Q046=\"":443\""; ma=2592000,h3-Q043=\"":443\""; ma=2592000,quic=\"":443\""; ma=2592000; v=\""46,43\"""" response.header.cache-control=""public, max-age=3600"" response.header.content-length=2818413 response.header.content-type=application/octet-stream response.header.date=""Sun, 06 Feb 2022 17:25:17 GMT"" response.header.etag=""\""1d55e7be5a77c4a908ad11bc33ebea1c\"""" response.header.expires=""Sun, 06 Feb 2022 18:25:17 GMT"" response.header.last-modified=""Wed, 24 Nov 2021 21:07:06 GMT"" response.header.server=UploadServer response.header.x-goog-generation=1637788026431708 response.header.x-goog-hash=""crc32c=ZojF+g=="" response.header.x-goog-hash.1=""md5=HVXnvlp3xKkIrRG8M+vqHA=="" response.header.x-goog-metageneration=1 response.header.x-goog-storage-class=STANDARD response.header.x-goog-stored-content-encoding=identity response.header.x-goog-stored-content-length=2818413 response.header.x-guploader-uploadid=ADPycdsebqxiTBJqZ0bv9zBigjFxgQydD2ESZSkKchpE0ILlN9Ibko3C5r4fJTJ4UR9ddp-UBd-2v_4eRpZ8Yo2llW_j4k8WhQ response.status=""200 OK"" spanID=9460e5b6e64cec91 traceID=b162d3040ddf86d6614e79c66a01a577
...
Setting registry certificates
If you specify registry certificates in the BuildKit configuration, the daemon
copies the files into the container under /etc/buildkit/certs
. The following
steps show adding a self-signed registry certificate to the BuildKit
configuration.
Add the following configuration to
/etc/buildkitd.toml
:# /etc/buildkitd.toml debug = true [registry.""myregistry.com""] ca=[""/etc/certs/myregistry.pem""] [[registry.""myregistry.com"".keypair]] key=""/etc/certs/myregistry_key.pem"" cert=""/etc/certs/myregistry_cert.pem""
This tells the builder to push images to the
myregistry.com
registry using the certificates in the specified location (/etc/certs
).Create a
docker-container
builder that uses this configuration:$ docker buildx create --use --bootstrap \ --name mybuilder \ --driver docker-container \ --config /etc/buildkitd.toml
Inspect the builder's configuration file (
/etc/buildkit/buildkitd.toml
), it shows that the certificate configuration is now configured in the builder.$ docker exec -it buildx_buildkit_mybuilder0 cat /etc/buildkit/buildkitd.toml
debug = true [registry] [registry.""myregistry.com""] ca = [""/etc/buildkit/certs/myregistry.com/myregistry.pem""] [[registry.""myregistry.com"".keypair]] cert = ""/etc/buildkit/certs/myregistry.com/myregistry_cert.pem"" key = ""/etc/buildkit/certs/myregistry.com/myregistry_key.pem""
Verify that the certificates are inside the container:
$ docker exec -it buildx_buildkit_mybuilder0 ls /etc/buildkit/certs/myregistry.com/ myregistry.pem myregistry_cert.pem myregistry_key.pem
Now you can push to the registry using this builder, and it will authenticate using the certificates:
$ docker buildx build --push --tag myregistry.com/myimage:latest .
CNI networking
CNI networking for builders can be useful for dealing with network port contention during concurrent builds. CNI is not yet available in the default BuildKit image. But you can create your own image that includes CNI support.
The following Dockerfile example shows a custom BuildKit image with CNI support. It uses the CNI config for integration tests in BuildKit as an example. Feel free to include your own CNI configuration.
# syntax=docker/dockerfile:1
ARG BUILDKIT_VERSION=v0.20.0
ARG CNI_VERSION=v1.0.1
FROM --platform=$BUILDPLATFORM alpine AS cni-plugins
RUN apk add --no-cache curl
ARG CNI_VERSION
ARG TARGETOS
ARG TARGETARCH
WORKDIR /opt/cni/bin
RUN curl -Ls https://github.com/containernetworking/plugins/releases/download/$CNI_VERSION/cni-plugins-$TARGETOS-$TARGETARCH-$CNI_VERSION.tgz | tar xzv
FROM moby/buildkit:${BUILDKIT_VERSION}
ARG BUILDKIT_VERSION
RUN apk add --no-cache iptables
COPY --from=cni-plugins /opt/cni/bin /opt/cni/bin
ADD https://raw.githubusercontent.com/moby/buildkit/${BUILDKIT_VERSION}/hack/fixtures/cni.json /etc/buildkit/cni.json
Now you can build this image, and create a builder instance from it using
the --driver-opt image
option:
$ docker buildx build --tag buildkit-cni:local --load .
$ docker buildx create --use --bootstrap \
--name mybuilder \
--driver docker-container \
--driver-opt ""image=buildkit-cni:local"" \
--buildkitd-flags ""--oci-worker-net=cni""
Resource limiting
Max parallelism
You can limit the parallelism of the BuildKit solver, which is particularly useful
for low-powered machines, using a
BuildKit configuration
while creating a builder with the
--config
flags.
# /etc/buildkitd.toml
[worker.oci]
max-parallelism = 4
Now you can
create a docker-container
builder
that will use this BuildKit configuration to limit parallelism.
$ docker buildx create --use \
--name mybuilder \
--driver docker-container \
--config /etc/buildkitd.toml
TCP connection limit
TCP connections are limited to 4 simultaneous connections per registry for pulling and pushing images, plus one additional connection dedicated to metadata requests. This connection limit prevents your build from getting stuck while pulling images. The dedicated metadata connection helps reduce the overall build time.
More information: moby/buildkit#2259",,,
140799ea6d5b4bd649a931514d22a01ccce2b6339c7f4df811a1082dc66be0ae,"Image Access Management
Image Access Management gives you control over which types of images, such as Docker Official Images, Docker Verified Publisher Images, or community images, your developers can pull from Docker Hub.
For example, a developer, who is part of an organization, building a new containerized application could accidentally use an untrusted, community image as a component of their application. This image could be malicious and pose a security risk to the company. Using Image Access Management, the organization owner can ensure that the developer can only access trusted content like Docker Official Images, Docker Verified Publisher Images, or the organization’s own images, preventing such a risk.
Prerequisites
You first need to enforce sign-in to ensure that all Docker Desktop developers authenticate with your organization. Since Image Access Management requires a Docker Business subscription, enforced sign-in guarantees that only authenticated users have access and that the feature consistently takes effect across all users, even though it may still work without enforced sign-in.
Configure
- Sign in to Docker Hub.
- Select Organizations, your organization, Settings, and then select Image Access.
- Enable Image Access Management to set the permissions for the following categories of images you can manage:
- Organization Images: Images from your organization are always allowed by default. These images can be public or private created by members within your organization.
- Docker Official Images: A curated set of Docker repositories hosted on Hub. They provide OS repositories, best practices for Dockerfiles, drop-in solutions, and applies security updates on time.
- Docker Verified Publisher Images: Images published by Docker partners that are part of the Verified Publisher program and are qualified to be included in the developer secure supply chain.
- Community Images: These images are disabled by default when Image Access Management is enabled because various users contribute them and they may pose security risks. This category includes Docker-Sponsored Open Source images.
Note
Image Access Management is turned off by default. However, owners in your organization have access to all images regardless of the settings.
- Select the category restrictions for your images by selecting Allowed. Once the restrictions are applied, your members can view the organization permissions page in a read-only format.
Verify the restrictions
The new Image Access Management policy takes effect after the developer successfully authenticates to Docker Desktop using their organization credentials. If a developer attempts to pull a disallowed image type using Docker, they receive an error message.
- Sign in to the Admin Console.
- Select your organization in the left navigation drop-down menu, and then select Image access.
- Enable Image Access Management to set the permissions for the following categories of images you can manage:
- Organization Images: Images from your organization are always allowed by default. These images can be public or private created by members within your organization.
- Docker Official Images: A curated set of Docker repositories hosted on Hub. They provide OS repositories, best practices for Dockerfiles, drop-in solutions, and applies security updates on time.
- Docker Verified Publisher Images: Images published by Docker partners that are part of the Verified Publisher program and are qualified to be included in the developer secure supply chain.
- Community Images: These images are disabled by default when Image Access Management is enabled because various users contribute them and they may pose security risks. This category includes Docker-Sponsored Open Source images.
Note
Image Access Management is turned off by default. However, owners in your organization have access to all images regardless of the settings.
- Select the category restrictions for your images by selecting Allowed. Once the restrictions are applied, your members can view the organization permissions page in a read-only format.
Verify the restrictions
The new Image Access Management policy takes effect after the developer successfully authenticates to Docker Desktop using their organization credentials. If a developer attempts to pull a disallowed image type using Docker, they receive an error message.",,,
fb3dfdb6be6c39e5add8862fa7bbf7a7c895c2360b71a1e170244547689aa3d4,"Explore networking features on Docker Desktop
Docker Desktop provides several networking features to make it easier to use.
Networking features for all platforms
VPN Passthrough
Docker Desktop networking can work when attached to a VPN. To do this, Docker Desktop intercepts traffic from the containers and injects it into the host as if it originated from the Docker application.
Port mapping
When you run a container with the -p
argument, for example:
$ docker run -p 80:80 -d nginx
Docker Desktop makes whatever is running on port 80 in the container, in
this case, nginx
, available on port 80 of localhost
. In this example, the
host and container ports are the same. If, for example, you already have something running on port 80 of
your host machine, you can connect the container to a different port:
$ docker run -p 8000:80 -d nginx
Now, connections to localhost:8000
are sent to port 80 in the container. The
syntax for -p
is HOST_PORT:CLIENT_PORT
.
HTTP/HTTPS Proxy support
See Proxies
SOCKS5 proxy support
Note
Requires a Business subscription.
SOCKS (Socket Secure) is a protocol that facilitates the routing of network packets between a client and a server through a proxy server. It provides a way to enhance privacy, security, and network performance for users and applications.
You can enable SOCKS proxy support to allow outgoing requests, such as pulling images, and access Linux container backend IPs from the host.
To enable and set up SOCKS proxy support:
- Navigate to the Resources tab in Settings.
- From the dropdown menu select Proxies.
- Switch on the Manual proxy configuration toggle.
- In the Secure Web Server HTTPS box, paste your
socks5://host:port
URL.
Networking features for Mac and Linux
SSH agent forwarding
Docker Desktop on Mac and Linux allows you to use the host’s SSH agent inside a container. To do this:
Bind mount the SSH agent socket by adding the following parameter to your
docker run
command:$--mount type=bind,src=/run/host-services/ssh-auth.sock,target=/run/host-services/ssh-auth.sock
Add the
SSH_AUTH_SOCK
environment variable in your container:$ -e SSH_AUTH_SOCK=""/run/host-services/ssh-auth.sock""
To enable the SSH agent in Docker Compose, add the following flags to your service:
services:
web:
image: nginx:alpine
volumes:
- type: bind
source: /run/host-services/ssh-auth.sock
target: /run/host-services/ssh-auth.sock
environment:
- SSH_AUTH_SOCK=/run/host-services/ssh-auth.sock
Known limitations
Changing internal IP addresses
The internal IP addresses used by Docker can be changed from Settings. After changing IPs, it is necessary to reset the Kubernetes cluster and to leave any active Swarm.
There is no docker0 bridge on the host
Because of the way networking is implemented in Docker Desktop, you cannot
see a docker0
interface on the host. This interface is actually within the
virtual machine.
I cannot ping my containers
Docker Desktop can't route traffic to Linux containers. However if you're a Windows user, you can ping the Windows containers.
Per-container IP addressing is not possible
This is because the Docker bridge
network is not reachable from the host.
However if you are a Windows user, per-container IP addressing is possible with Windows containers.
Use cases and workarounds
I want to connect from a container to a service on the host
The host has a changing IP address, or none if you have no network access.
We recommend that you connect to the special DNS name host.docker.internal
,
which resolves to the internal IP address used by the host.
You can also reach the gateway using gateway.docker.internal
.
If you have installed Python on your machine, use the following instructions as an example to connect from a container to a service on the host:
Run the following command to start a simple HTTP server on port 8000.
python -m http.server 8000
If you have installed Python 2.x, run
python -m SimpleHTTPServer 8000
.Now, run a container, install
curl
, and try to connect to the host using the following commands:$ docker run --rm -it alpine sh # apk add curl # curl http://host.docker.internal:8000 # exit
I want to connect to a container from the host
Port forwarding works for localhost
. --publish
, -p
, or -P
all work.
Ports exposed from Linux are forwarded to the host.
We recommend you publish a port, or to connect from another container. This is what you need to do even on Linux if the container is on an overlay network, not a bridge network, as these are not routed.
For example, to run an nginx
webserver:
$ docker run -d -p 80:80 --name webserver nginx
To clarify the syntax, the following two commands both publish container's port 80
to host's port 8000
:
$ docker run --publish 8000:80 --name webserver nginx
$ docker run -p 8000:80 --name webserver nginx
To publish all ports, use the -P
flag. For example, the following command
starts a container (in detached mode) and the -P
flag publishes all exposed ports of the
container to random ports on the host.
$ docker run -d -P --name webserver nginx
Alternatively, you can also use host networking to give the container direct access to the network stack of the host.
See the
run command for more details on
publish options used with docker run
.",,,
d8a2a27055d893cd71e82558bed3a0bfbb5056606ee67d2b5ca79d34cbde7cb9,"Personal settings for repositories
For your account, you can set personal settings for repositories, including default repository privacy and autobuild notifications.
Default repository privacy
When creating a new repository in Docker Hub, you are able to specify the repository visibility. You can also change the visibility at any time in Docker Hub.
The default setting is useful if you use the docker push
command to push to a
repository that doesn't exist yet. In this case, Docker Hub automatically
creates the repository with your default repository privacy.
Configure default repository privacy
Sign in to Docker Hub.
Select Repositories.
Near the top-right corner, select the settings icon and then Repository Settings.
Select the Default privacy for any new repository created.
- Public: All new repositories appear in Docker Hub search results and can be pulled by everyone.
- Private: All new repositories don't appear in Docker Hub search results and are only accessible to you and collaborators. In addition, if the repository is created in an organization's namespace, then the repository is accessible to those with applicable roles or permissions.
Select Save.
Autobuild notifications
You can send notifications to your email for all your repositories using autobuilds.
Configure autobuild notifications
Sign in to Docker Hub.
Select Repositories.
Near the top-right corner, select the settings icon and then Repository Settings.
Select the Notifications
Select the notifications to receive by email.
- Off: No notifications.
- Only failures: Only notifications about failed builds.
- Everything: Notifications for successful and failed builds.
Select Save.",,,
6b8ee31b9c8e646f0d4917ce3b1e21c831e8f3d81bf69183a337bfaff3ae2786,"Integrating Docker Scout with other systems
By default, Docker Scout integrates with your Docker organization and your Docker Scout-enabled repositories on Docker Hub. You can integrate Docker Scout with additional third-party systems to get access to even more insights, including real-time information about you running workloads.
Integration categories
You'll get different insights depending on where and how you choose to integrate Docker Scout.
Container registries
Integrating Docker Scout with third-party container registries enables Docker Scout to run image analysis on those repositories, so that you can get insights into the composition of those images even if they aren't hosted on Docker Hub.
The following container registry integrations are available:
Continuous Integration
Integrating Docker Scout with Continuous Integration (CI) systems is a great way to get instant, automatic feedback about your security posture in your inner loop. Analysis running in CI also gets the benefit of additional context that's useful for getting even more insights.
The following CI integrations are available:
Environment monitoring
Environment monitoring refers to integrating Docker Scout with your deployments. This can give you information in real-time about your running container workloads.
Integrating with environments lets you compare production workloads to other versions, in your image repositories or in your other environments.
The following environment monitoring integrations are available
For more information about environment integrations, see Environments.
Code quality
Integrating Docker Scout with code analysis tools enables quality checks directly on source code, helping you keep track of bugs, security issues, test coverage, and more. In addition to image analysis and environment monitoring, code quality gates let you shift left your supply chain management with Docker Scout.
Once you enable a code quality integration, Docker Scout includes the code quality assessments as policy evaluation results for the repositories where you've enabled the integration.
The following code quality integrations are available:
Source code management
Integrate Docker Scout with your version control system to get guided remediation advice on how to address issues detected by Docker Scout image analysis, directly in your repositories.
The following source code management integrations are available:
- GitHub Beta
Team collaboration
Integrations in this category let you integrate Docker Scout with collaboration platforms for broadcasting notifications about your software supply chain in real-time to team communication platforms.
The following team collaboration integrations are available:",,,
d7f5611667e4ebab3c6e46d2449f00efa17e1115c5a026fcef55b98557f5f22a,"Automated builds
Docker Hub can automatically build images from source code in an external repository and automatically push the built image to your Docker repositories.
When you set up automated builds, also called autobuilds, you create a list of branches and tags that you want to build into Docker images. When you push code to a source-code branch, for example in GitHub, for one of those listed image tags, the push uses a webhook to trigger a new build, which produces a Docker image. The built image is then pushed to Docker Hub.
Note
You can still use
docker push
to push pre-built images to repositories with automated builds configured.
If you have automated tests configured, these run after building, but before pushing to the registry. You can use these tests to create a continuous integration workflow where a build that fails its tests doesn't push the built image. Automated tests don't push images to the registry on their own. Learn about automated image testing.
Depending on your
subscription,
you may get concurrent builds, which means that N
autobuilds can be run at the
same time. N
is configured according to your subscription. Once N+1
builds
are running, any additional builds go into a queue to be run later.
The maximum number of pending builds in the queue is 30 and Docker Hub discards further requests. The number of concurrent builds for Pro is 5 and for Team and Business is 15. Automated builds can handle images of up to 10 GB in size.",,,
153897b42dbf63be1bb31f2d4b5850dbcb8a7cdc5c64640590341f6dac025103,"Docker Engine 17.09 release notes
Table of contents
17.09.1-ce
2017-12-07
Builder
- Fix config leakage on shared parent stage moby/moby#33753
- Warn on empty continuation lines only, not on comment-only lines moby/moby#35004
Client
- Set API version on Client even when Ping fails docker/cli#546
Networking
- Overlay fix for transient IP reuse docker/libnetwork#2016
- Fix reapTime logic in NetworkDB and handle DNS cleanup for attachable container docker/libnetwork#2017
- Disable hostname lookup on chain exists check docker/libnetwork#2019
- Fix lint issues docker/libnetwork#2020
- Restore error type in FindNetwork moby/moby#35634
Runtime
- Protect
health monitor
Go channel moby/moby#35482 - Fix leaking container/exec state moby/moby#35484
- Add /proc/scsi to masked paths (patch to work around CVE-2017-16539 moby/moby/#35399
- Vendor tar-split: fix to prevent memory exhaustion issue that could crash Docker daemon moby/moby/#35424 Fixes CVE-2017-14992
- Fix P/Z HubPullSuite tests moby/moby#34837
- Windows: Add support for version filtering on pull moby/moby#35090
- Windows: Stop filtering Windows manifest lists by version moby/moby#35117
- Use rslave instead of rprivate in chroot archive moby/moby/#35217
- Remove container rootfs mountPath after unmount moby/moby#34573
- Fix honoring tmpfs size of user /dev/shm mount moby/moby#35316
- Don't abort when setting may_detach_mounts (log the error instead) moby/moby#35172
- Fix version comparison when negotiating the API version moby/moby#35008
Swarm mode
- Increase gRPC request timeout when sending snapshots docker/swarmkit#2404
- Fix node filtering when there is no log driver docker/swarmkit#2442
- Add an error on attempt to change cluster name docker/swarmkit/#2454
- Delete node attachments when node is removed docker/swarmkit/#2456
- Provide custom gRPC dialer to override default proxy dialer docker/swarmkit/#2457
- Avoids recursive readlock on swarm info moby/moby#35388
17.09.0-ce
2017-09-26
Builder
- Add
--chown
flag toADD/COPY
commands in Dockerfile moby/moby#34263
- Fix cloning unneeded files while building from git repositories moby/moby#33704
Client
- Allow extension fields in the v3.4 version of the compose format docker/cli#452
- Make compose file allow to specify names for non-external volume docker/cli#306
- Support
--compose-file -
as stdin docker/cli#347 - Support
start_period
for healthcheck in Docker Compose docker/cli#475
- Add support for
stop-signal
in docker stack commands docker/cli#388 - Add support for update order in compose deployments docker/cli#360
- Add ulimits to unsupported compose fields docker/cli#482
- Add
--format
todocker-search
docker/cli#440
- Show images digests when
{{.Digest}}
is in format docker/cli#439 - Print output of
docker stack rm
onstdout
instead ofstderr
docker/cli#491
- Fix
docker history --format {{json .}}
printing human-readable timestamps instead of ISO8601 when--human=true
docker/cli#438 - Fix idempotence of
docker stack deploy
when secrets or configs are used docker/cli#509 - Fix presentation of random host ports docker/cli#404
- Fix redundant service restarts when service created with multiple secrets moby/moby#34746
Logging
- Fix Splunk logger not transmitting log data when tag is empty and raw-mode is used moby/moby#34520
Networking
- Add the control plane MTU option in the daemon config moby/moby#34103
- Add service virtual IP to sandbox's loopback address docker/libnetwork#1877
Runtime
- Graphdriver: promote overlay2 over aufs moby/moby#34430
- LCOW: Additional flags for VHD boot moby/moby#34451
- LCOW: Don't block export moby/moby#34448
- LCOW: Dynamic sandbox management moby/moby#34170
- LCOW: Force Hyper-V Isolation moby/moby#34468
- LCOW: Move toolsScratchPath to /tmp moby/moby#34396
- LCOW: Remove hard-coding moby/moby#34398
- LCOW: WORKDIR correct handling moby/moby#34405
- Windows: named pipe mounts moby/moby#33852
- Fix ""permission denied"" errors when accessing volume with SELinux enforcing mode moby/moby#34684
- Fix layers size reported as
0
indocker system df
moby/moby#34826 - Fix some ""device or resource busy"" errors when removing containers on RHEL 7.4 based kernels moby/moby#34886
Swarm mode
- Include whether the managers in the swarm are autolocked as part of
docker info
docker/cli#471
- Add 'docker service rollback' subcommand docker/cli#205
- Fix managers failing to join if the gRPC snapshot is larger than 4MB docker/swarmkit#2375
- Fix ""permission denied"" errors for configuration file in SELinux-enabled containers moby/moby#34732
- Fix services failing to deploy on ARM nodes moby/moby#34021
Packaging
- Build scripts for ppc64el on Ubuntu docker/docker-ce-packaging#43
Deprecation
- Remove deprecated
--enable-api-cors
daemon flag moby/moby#34821",,,
ea0cfc9b4295dcaaed9a0b47251816e619395c3902681b0b034f261009c2cd11,"Install Docker Engine on SLES (s390x)
Note
The installation instructions on this page refer to packages for SLES on the s390x architecture (IBM Z). Other architectures, including x86_64, aren't supported for SLES.
To get started with Docker Engine on SLES, make sure you meet the prerequisites, and then follow the installation steps.
Prerequisites
OS requirements
To install Docker Engine, you need a maintained version of one of the following SLES versions:
- SLES 15-SP4 on s390x (IBM Z)
- SLES 15-SP5 on s390x (IBM Z)
You must enable the
SCC SUSE
repositories.
You must add the
OpenSUSE SELinux
repository. This repository is not added by default. Run the following commands to add it:
$ opensuse_repo=""https://download.opensuse.org/repositories/security:/SELinux/openSUSE_Factory/security:SELinux.repo""
$ sudo zypper addrepo $opensuse_repo
Uninstall old versions
Before you can install Docker Engine, you need to uninstall any conflicting packages.
Your Linux distribution may provide unofficial Docker packages, which may conflict with the official packages provided by Docker. You must uninstall these packages before you install the official version of Docker Engine.
$ sudo zypper remove docker \
docker-client \
docker-client-latest \
docker-common \
docker-latest \
docker-latest-logrotate \
docker-logrotate \
docker-engine \
runc
zypper
might report that you have none of these packages installed.
Images, containers, volumes, and networks stored in /var/lib/docker/
aren't
automatically removed when you uninstall Docker.
Installation methods
You can install Docker Engine in different ways, depending on your needs:
You can set up Docker's repositories and install from them, for ease of installation and upgrade tasks. This is the recommended approach.
You can download the RPM package, install it manually, and manage upgrades completely manually. This is useful in situations such as installing Docker on air-gapped systems with no access to the internet.
In testing and development environments, you can use automated convenience scripts to install Docker.
Install using the rpm repository
Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository.
Set up the repository
Set up the repository.
$ sudo zypper addrepo https://download.docker.com/linux/sles/docker-ce.repo
Install Docker Engine
Install the Docker packages.
To install the latest version, run:
$ sudo zypper install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
If prompted to accept the GPG key, verify that the fingerprint matches
060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35
, and if so, accept it.This command installs Docker, but it doesn't start Docker. It also creates a
docker
group, however, it doesn't add any users to the group by default.To install a specific version, start by listing the available versions in the repository:
$ sudo zypper search -s --match-exact docker-ce | sort -r v | docker-ce | package | 3:28.0.1-1 | s390x | Docker CE Stable - s390x v | docker-ce | package | 3:28.0.0-1 | s390x | Docker CE Stable - s390x
The list returned depends on which repositories are enabled, and is specific to your version of SLES.
Install a specific version by its fully qualified package name, which is the package name (
docker-ce
) plus the version string (2nd column), separated by a hyphen (-
). For example,docker-ce-3:28.0.1
.Replace
<VERSION_STRING>
with the desired version and then run the following command to install:$ sudo zypper install docker-ce-<VERSION_STRING> docker-ce-cli-<VERSION_STRING> containerd.io docker-buildx-plugin docker-compose-plugin
This command installs Docker, but it doesn't start Docker. It also creates a
docker
group, however, it doesn't add any users to the group by default.Start Docker Engine.
$ sudo systemctl enable --now docker
This configures the Docker systemd service to start automatically when you boot your system. If you don't want Docker to start automatically, use
sudo systemctl start docker
instead.Verify that the installation is successful by running the
hello-world
image:$ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Upgrade Docker Engine
To upgrade Docker Engine, follow the installation instructions, choosing the new version you want to install.
Install from a package
If you can't use Docker's rpm
repository to install Docker Engine, you can
download the .rpm
file for your release and install it manually. You need to
download a new file each time you want to upgrade Docker Engine.
Go to https://download.docker.com/linux/sles/ and choose your version of SLES. Then browse to
s390x/stable/Packages/
and download the.rpm
file for the Docker version you want to install.Install Docker Engine, changing the following path to the path where you downloaded the Docker package.
$ sudo zypper install /path/to/package.rpm
Docker is installed but not started. The
docker
group is created, but no users are added to the group.Start Docker Engine.
$ sudo systemctl enable --now docker
This configures the Docker systemd service to start automatically when you boot your system. If you don't want Docker to start automatically, use
sudo systemctl start docker
instead.Verify that the installation is successful by running the
hello-world
image:$ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Upgrade Docker Engine
To upgrade Docker Engine, download the newer package files and repeat the
installation procedure, using zypper -y upgrade
instead of zypper -y install
, and point to the new files.
Install using the convenience script
Docker provides a convenience script at
https://get.docker.com/ to install Docker into
development environments non-interactively. The convenience script isn't
recommended for production environments, but it's useful for creating a
provisioning script tailored to your needs. Also refer to the
install using the repository steps to learn
about installation steps to install using the package repository. The source code
for the script is open source, and you can find it in the
docker-install
repository on GitHub.
Always examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script:
- The script requires
root
orsudo
privileges to run. - The script attempts to detect your Linux distribution and version and configure your package management system for you.
- The script doesn't allow you to customize most installation parameters.
- The script installs dependencies and recommendations without asking for confirmation. This may install a large number of packages, depending on the current configuration of your host machine.
- By default, the script installs the latest stable release of Docker, containerd, and runc. When using this script to provision a machine, this may result in unexpected major version upgrades of Docker. Always test upgrades in a test environment before deploying to your production systems.
- The script isn't designed to upgrade an existing Docker installation. When using the script to update an existing installation, dependencies may not be updated to the expected version, resulting in outdated versions.
Tip
Preview script steps before running. You can run the script with the
--dry-run
option to learn what steps the script will run when invoked:$ curl -fsSL https://get.docker.com -o get-docker.sh $ sudo sh ./get-docker.sh --dry-run
This example downloads the script from https://get.docker.com/ and runs it to install the latest stable release of Docker on Linux:
$ curl -fsSL https://get.docker.com -o get-docker.sh
$ sudo sh get-docker.sh
Executing docker install script, commit: 7cae5f8b0decc17d6571f9f52eb840fbc13b2737
<...>
You have now successfully installed and started Docker Engine. The docker
service starts automatically on Debian based distributions. On RPM
based
distributions, such as CentOS, Fedora, RHEL or SLES, you need to start it
manually using the appropriate systemctl
or service
command. As the message
indicates, non-root users can't run Docker commands by default.
Use Docker as a non-privileged user, or install in rootless mode?
The installation script requires
root
orsudo
privileges to install and use Docker. If you want to grant non-root users access to Docker, refer to the post-installation steps for Linux. You can also install Docker withoutroot
privileges, or configured to run in rootless mode. For instructions on running Docker in rootless mode, refer to run the Docker daemon as a non-root user (rootless mode).
Install pre-releases
Docker also provides a convenience script at
https://test.docker.com/ to install pre-releases of
Docker on Linux. This script is equal to the script at get.docker.com
, but
configures your package manager to use the test channel of the Docker package
repository. The test channel includes both stable and pre-releases (beta
versions, release-candidates) of Docker. Use this script to get early access to
new releases, and to evaluate them in a testing environment before they're
released as stable.
To install the latest version of Docker on Linux from the test channel, run:
$ curl -fsSL https://test.docker.com -o test-docker.sh
$ sudo sh test-docker.sh
Upgrade Docker after using the convenience script
If you installed Docker using the convenience script, you should upgrade Docker using your package manager directly. There's no advantage to re-running the convenience script. Re-running it can cause issues if it attempts to re-install repositories which already exist on the host machine.
Uninstall Docker Engine
Uninstall the Docker Engine, CLI, containerd, and Docker Compose packages:
$ sudo zypper remove docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras
Images, containers, volumes, or custom configuration files on your host aren't automatically removed. To delete all images, containers, and volumes:
$ sudo rm -rf /var/lib/docker $ sudo rm -rf /var/lib/containerd
You have to delete any edited configuration files manually.
Next steps
- Continue to Post-installation steps for Linux.",,,
79ca1025aa474624989e01c5dfb5cdddfc8bf4c3264df0c3873c807856c873bc,"Use the MSI installer
The MSI package supports various MDM (Mobile Device Management) solutions, making it ideal for bulk installations and eliminating the need for manual setups by individual users. With this package, IT administrators can ensure standardized, policy-driven installations of Docker Desktop, enhancing efficiency and software management across their organizations.
Install interactively
In the Docker Admin Console, navigate to your organization.
Under Docker Desktop, select the Deploy page.
From the Windows OS tab, select the Download MSI installer button.
Once downloaded, double-click
Docker Desktop Installer.msi
to run the installer.Once you've accepted the license agreement, you can choose the install location. By default, Docker Desktop is installed at
C:\Program Files\Docker\Docker
.Configure the Docker Desktop installation. You can:
Create a desktop shortcut
Set the Docker Desktop service startup type to automatic
Disable Windows Container usage
Select the engine for Docker Desktop. Either WSL or Hyper-V. If your system only supports one of the two options, you won't be able to select which backend to use.
Follow the instructions on the installation wizard to authorize the installer and proceed with the install.
When the installation is successful, select Finish to complete the installation process.
If your administrator account is different to your user account, you must add the user to the docker-users group:
- Run Computer Management as an administrator.
- Navigate to Local Users and Groups > Groups > docker-users.
- Right-click to add the user to the group.
- Sign out and sign back in for the changes to take effect.
Note
When installing Docker Desktop with the MSI, in-app updates are automatically disabled. This feature ensures your organization maintains the required Docker Desktop version. For Docker Desktop installed with the .exe installer, in-app updates remain supported.
Docker Desktop notifies you when an update is available. To update Docker Desktop, download the latest installer from the Docker Admin Console. Navigate to the Deploy page > under Docker Desktop.
To keep up to date with new releases, check the release notes page.
Install from the command line
This section covers command line installations of Docker Desktop using PowerShell. It provides common installation commands that you can run. You can also add additional arguments which are outlined in configuration options.
When installing Docker Desktop, you can choose between interactive or non-interactive installations.
Interactive installations, without specifying /quiet
or /qn
, display the user interface and let you select your own properties.
When installing via the user interface it's possible to:
- Choose the destination folder
- Create a desktop shortcut
- Configure the Docker Desktop service startup type
- Disable Windows Containers
- Choose between the WSL or Hyper-V engine
Non-interactive installations are silent and any additional configuration must be passed as arguments.
Common installation commands
Important
Admin rights are required to run any of the following commands.
Installing interactively with verbose logging
msiexec /i ""DockerDesktop.msi"" /L*V "".\msi.log""
Installing interactively without verbose logging
msiexec /i ""DockerDesktop.msi""
Installing non-interactively with verbose logging
msiexec /i ""DockerDesktop.msi"" /L*V "".\msi.log"" /quiet
Installing non-interactively and suppressing reboots
msiexec /i ""DockerDesktop.msi"" /L*V "".\msi.log"" /quiet /norestart
Installing non-interactively with admin settings
msiexec /i ""DockerDesktop.msi"" /L*V "".\msi.log"" /quiet /norestart ADMINSETTINGS=""{""configurationFileVersion"":2,""enhancedContainerIsolation"":{""value"":true,""locked"":false}}"" ALLOWEDORG=""docker""
Installing with the passive display option
You can use the /passive
display option instead of /quiet
when you want to perform a non-interactive installation but show a progress dialog.
In passive mode the installer doesn't display any prompts or error messages to the user and the installation cannot be cancelled.
For example:
msiexec /i ""DockerDesktop.msi"" /L*V "".\msi.log"" /passive /norestart
Tip
Some useful tips to remember when creating a value that expects a JSON string as it’s value:
- The property expects a JSON formatted string
- The string should be wrapped in double quotes
- The string shouldn't contain any whitespace
- Property names are expected to be in double quotes
Common uninstall commands
When uninstalling Docker Desktop, you need to use the same .msi
file that was originally used to install the application.
If you no longer have the original .msi
file, you need to use the product code associated with the installation. To find the product code, run:
Get-WmiObject Win32_Product | Select-Object IdentifyingNumber, Name | Where-Object {$_.Name -eq ""Docker Desktop""}
It should return output similar to the following:
IdentifyingNumber Name
----------------- ----
{10FC87E2-9145-4D7D-B493-2E99E8D8E103} Docker Desktop
Note
This command can take some time to return, depending on the number of installed applications.
IdentifyingNumber
is the applications product code and can be used to uninstall Docker Desktop. For example:
msiexec /x {10FC87E2-9145-4D7D-B493-2E99E8D8E103} /L*V "".\msi.log"" /quiet
Uninstalling interactively with verbose logging
msiexec /x ""DockerDesktop.msi"" /L*V "".\msi.log""
Uninstalling interactively without verbose logging
msiexec /x ""DockerDesktop.msi""
Uninstalling non-interactively with verbose logging
msiexec /x ""DockerDesktop.msi"" /L*V "".\msi.log"" /quiet
Uninstalling non-interactively without verbose logging
msiexec /x ""DockerDesktop.msi"" /quiet
Configuration options
Important
In addition to the following custom properties, the Docker Desktop MSI installer also supports the standard Windows Installer command line options.
| Property | Description | Default |
|---|---|---|
ENABLEDESKTOPSHORTCUT | Creates a desktop shortcut. | 1 |
INSTALLFOLDER | Specifies a custom location where Docker Desktop will be installed. | C:\Program Files\Docker |
ADMINSETTINGS | Automatically creates an admin-settings.json file which is used to
control certain Docker Desktop settings on client machines within organizations. It must be used together with the ALLOWEDORG property. | None |
ALLOWEDORG | Requires the user to sign in and be part of the specified Docker Hub organization when running the application. This creates a registry key called allowedOrgs in HKLM\Software\Policies\Docker\Docker Desktop . | None |
ALWAYSRUNSERVICE | Lets users switch to Windows containers without needing admin rights | 0 |
DISABLEWINDOWSCONTAINERS | Disables the Windows containers integration | 0 |
ENGINE | Sets the Docker Engine that's used to run containers. This can be either wsl , hyperv , or windows | wsl |
PROXYENABLEKERBEROSNTLM | When set to 1, enables support for Kerberos and NTLM proxy authentication. Available with Docker Desktop 4.33 and later | 0 |
PROXYHTTPMODE | Sets the HTTP Proxy mode. This can be either system or manual | system |
OVERRIDEPROXYHTTP | Sets the URL of the HTTP proxy that must be used for outgoing HTTP requests. | None |
OVERRIDEPROXYHTTPS | Sets the URL of the HTTP proxy that must be used for outgoing HTTPS requests. | None |
OVERRIDEPROXYEXCLUDE | Bypasses proxy settings for the hosts and domains. Uses a comma-separated list. | None |
HYPERVDEFAULTDATAROOT | Specifies the default location for the Hyper-V VM disk. | None |
WINDOWSCONTAINERSDEFAULTDATAROOT | Specifies the default location for Windows containers. | None |
WSLDEFAULTDATAROOT | Specifies the default location for the WSL distribution disk. | None |
DISABLEANALYTICS | When set to 1, analytics collection will be disabled for the MSI. For more information, see Analytics. | 0 |
Additionally, you can also use /norestart
or /forcerestart
to control reboot behaviour.
By default, the installer reboots the machine after a successful installation. When ran silently, the reboot is automatic and the user is not prompted.
Analytics
The MSI installer collects anonymous usage statistics to better understand user behaviour and to improve the user experience by identifying and addressing issues or optimizing popular features.
How to opt-out
When you install Docker Desktop from the default installer GUI, select the Disable analytics checkbox located on the bottom-left corner of the Welcome dialog.
When you install Docker Desktop from the command line, use the DISABLEANALYTICS
property.
msiexec /i ""win\msi\bin\en-US\DockerDesktop.msi"" /L*V "".\msi.log"" DISABLEANALYTICS=1
Persistence
If you decide to disable analytics for an installation, your choice is persisted in the registry and honoured across future upgrades and uninstalls.
However, the key is removed when Docker Desktop is uninstalled and must be configured again via one of the previous methods.
The registry key is as follows:
SOFTWARE\Docker Inc.\Docker Desktop\DisableMsiAnalytics
When analytics is disabled, this key has a value of 1
.",,,
7675ca07ac146efeace27314905a8b269a057627f6f46c93a080958b236e4db3,"Ask Gordon
Ask Gordon is your personal AI assistant embedded in Docker Desktop and the Docker CLI. It's designed to streamline your workflow and help you make the most of the Docker ecosystem.
What is Ask Gordon?
Ask Gordon is a suite of AI-powered capabilities integrated into Docker's tools. These features, currently in Beta, are not enabled by default, and are not production-ready. You may also encounter the term ""Docker AI"" as a broader reference to this technology.
The goal of Ask Gordon is to make Docker's tools for managing images and containers more intuitive and accessible. It provides contextual assistance tailored to your local environment, including Dockerfiles, containers, and applications.
Ask Gordon integrates directly with Docker's tools to help you perform specific tasks. It understands your local setup, such as your local source code and images. For example, you can ask Gordon to help you identify vulnerabilities in your project or how to optimize a Dockerfile in your local repository. This tight integration ensures responses are practical and actionable.
Note
Ask Gordon is powered by Large Language Models (LLMs). Like all LLM-based tools, its responses may sometimes be inaccurate. Always verify the information provided.
What data does Gordon access?
When you use Ask Gordon, the data it accesses depends on the context of your query:
- Local files: If you use the
docker ai
command, Ask Gordon can access files and directories in the current working directory where the command is executed. In Docker Desktop, if you ask about a specific file or directory in the Ask Gordon view, you'll be prompted to select the relevant context. - Local images: Gordon integrates with Docker Desktop and can view all images in your local image store. This includes images you've built or pulled from a registry.
To provide accurate responses, Ask Gordon may send relevant files, directories, or image metadata to the Gordon backend along with your query. This data transfer occurs over the network but is never stored persistently or shared with third parties. It is used exclusively to process your request and formulate a response. For more information about privacy terms and conditions for Docker AI, review Gordon's Supplemental Terms.
All data transferred is encrypted in transit.
How your data is collected and used
Docker collects anonymized data from your interactions with Ask Gordon to enhance the service. This includes the following:
- Your queries: Questions you ask Gordon.
- Responses: Answers provided by Gordon.
- Feedback: Thumbs-up and thumbs-down ratings.
To ensure privacy and security:
- Data is anonymized and cannot be traced back to you or your account.
- Docker does not use this data to train AI models or share it with third parties.
By using Ask Gordon, you help improve Docker AI's reliability and accuracy, making it more effective for all users.
If you have concerns about data collection or usage, you can disable the feature at any time.
Setup
To use this feature, you must have:
- Docker Desktop version 4.38 or later.
Ask Gordon is not enabled by default. To enable the feature:
- Sign in to your Docker account.
- Enable the feature in the Docker Desktop settings.
Sign in
- Open Docker Desktop.
- Select the Sign in button.
- Complete the sign-in process in your web browser.
Enable the feature
After signing in to your Docker Account, enable the Docker AI feature:
Open the Settings view in Docker Desktop.
Navigate to Features in development.
Check the Enable Docker AI checkbox.
The Docker AI terms of service agreement is displayed. You must agree to the terms before you can enable the feature. Review the terms and select Accept and enable to continue.
Select Apply & restart.
Using Ask Gordon
The primary interfaces to Docker's AI capabilities are through the Ask
Gordon view in Docker Desktop, or if you prefer to use the CLI: the docker ai
CLI command.
If you've used an AI chatbot before, these interfaces will be pretty familiar to you. You can chat with the Docker AI to get help with your Docker tasks.
Contextual help
Once you've enabled the Docker AI features, you'll also find references to Ask Gordon in various other places throughout the Docker Desktop user interface. Whenever you encounter a button with the ""sparkles"" (✨) icon in the user interface, you can use the button to get contextual support from Ask Gordon.
Example workflows
Ask Gordon is a general-purpose AI assistant created to help you with all your Docker-related tasks and workflows. If you need some inspiration, here are a few ways things you can try:
For more examples, try asking Gordon directly. For example:
$ docker ai ""What can you do?""
Troubleshoot a crashed container
If you try to start a container with an invalid configuration or command, you can use Ask Gordon to troubleshoot the error. For example, try starting a Postgres container without specifying a database password:
$ docker run postgres
Error: Database is uninitialized and superuser password is not specified.
You must specify POSTGRES_PASSWORD to a non-empty value for the
superuser. For example, ""-e POSTGRES_PASSWORD=password"" on ""docker run"".
You may also use ""POSTGRES_HOST_AUTH_METHOD=trust"" to allow all
connections without a password. This is *not* recommended.
See PostgreSQL documentation about ""trust"":
https://www.postgresql.org/docs/current/auth-trust.html
In the Containers view in Docker Desktop, select the ✨ icon next to the container's name, or inspect the container and open the Ask Gordon tab.
Get help with running a container
If you want to run a specific image but you're not sure how, Gordon might be able to help you get set up:
- Pull an image from Docker Hub (for example,
postgres
). - Open the Images view in Docker Desktop and select the image.
- Select the Run button.
In the Run a new container dialog that opens, you should see a message about Ask Gordon.
The linked text in the hint is a suggested prompt to start a conversation with Ask Gordon.
Improve a Dockerfile
Gordon can analyze your Dockerfile and suggest improvements. To have Gordon
evaluate your Dockerfile using the docker ai
command:
Navigate to your project directory:
$ cd path/to/my/project
Use the
docker ai
command to rate your Dockerfile:$ docker ai rate my Dockerfile
Gordon will analyze your Dockerfile and identify opportunities for improvement across several dimensions:
- Build cache optimization
- Security
- Image size efficiency
- Best practices compliance
- Maintainability
- Reproducibility
- Portability
- Resource efficiency
Disable Ask Gordon
If you've enabled Ask Gordon and you want to disable it again:
- Open the Settings view in Docker Desktop.
- Navigate to Features in development.
- Clear the Enable Docker AI checkbox.
- Select Apply & restart.
If you want to disable Ask Gordon for your entire Docker organization, using
Settings Management,
add the following property to your admin-settings.json
file:
{
""enableDockerAI"": {
""value"": false,
""locked"": true
}
}
Alternatively, you can disable all Beta features by setting allowBetaFeatures
to false:
{
""allowBetaFeatures"": {
""value"": false,
""locked"": true
}
}
Feedback
We value your input on Ask Gordon and encourage you to share your experience. Your feedback helps us improve and refine Ask Gordon for all users. If you encounter issues, have suggestions, or simply want to share what you like, here's how you can get in touch:
Thumbs-up and thumbs-down buttons
Rate Ask Gordon's responses using the thumbs-up or thumbs-down buttons in the response.
Feedback survey
You can access the Ask Gordon survey by following the Give feedback link in the Ask Gordon view in Docker Desktop, or from the CLI by running the
docker ai feedback
command.
Thank you for helping us improve Ask Gordon.",,,
acc28e1ce7ac9690cc2c84d1635665d6285872d29f715ef18a0f6149c991dcb8,"Extend your Compose file
Docker Compose's
extends
attribute
lets you share common configurations among different files, or even different
projects entirely.
Extending services is useful if you have several services that reuse a common
set of configuration options. With extends
you can define a common set of
service options in one place and refer to it from anywhere. You can refer to
another Compose file and select a service you want to also use in your own
application, with the ability to override some attributes for your own needs.
Important
When you use multiple Compose files, you must make sure all paths in the files are relative to the base Compose file (i.e. the Compose file in your main-project folder). This is required because extend files need not be valid Compose files. Extend files can contain small fragments of configuration. Tracking which fragment of a service is relative to which path is difficult and confusing, so to keep paths easier to understand, all paths must be defined relative to the base file.
How it works
Extending services from another file
Take the following example:
services:
web:
extends:
file: common-services.yml
service: webapp
This instructs Compose to re-use only the properties of the webapp
service
defined in the common-services.yml
file. The webapp
service itself is not part of the final project.
If common-services.yml
looks like this:
services:
webapp:
build: .
ports:
- ""8000:8000""
volumes:
- ""/data""
You get exactly the same result as if you wrote
compose.yaml
with the same build
, ports
, and volumes
configuration
values defined directly under web
.
To include the service webapp
in the final project when extending services from another file, you need to explicitly include both services in your current Compose file. For example (note this is a non-normative example):
services:
web:
build: alpine
command: echo
extends:
file: common-services.yml
service: webapp
webapp:
extends:
file: common-services.yml
service: webapp
Alternatively, you can use include.
Extending services within the same file
If you define services in the same Compose file and extend one service from another, both the original service and the extended service will be part of your final configuration. For example:
services:
web:
build: alpine
extends: webapp
webapp:
environment:
- DEBUG=1
Extending services within the same file and from another file
You can go further and define, or re-define, configuration locally in
compose.yaml
:
services:
web:
extends:
file: common-services.yml
service: webapp
environment:
- DEBUG=1
cpu_shares: 5
important_web:
extends: web
cpu_shares: 10
Additional example
Extending an individual service is useful when you have multiple services that have a common configuration. The example below is a Compose app with two services, a web application and a queue worker. Both services use the same codebase and share many configuration options.
The common.yaml
file defines the common configuration:
services:
app:
build: .
environment:
CONFIG_FILE_PATH: /code/config
API_KEY: xxxyyy
cpu_shares: 5
The compose.yaml
defines the concrete services which use the common
configuration:
services:
webapp:
extends:
file: common.yaml
service: app
command: /code/run_web_app
ports:
- 8080:8080
depends_on:
- queue
- db
queue_worker:
extends:
file: common.yaml
service: app
command: /code/run_worker
depends_on:
- queue
Exceptions and limitations
volumes_from
and depends_on
are never shared between services using
extends
. These exceptions exist to avoid implicit dependencies; you always
define volumes_from
locally. This ensures dependencies between services are
clearly visible when reading the current file. Defining these locally also
ensures that changes to the referenced file don't break anything.
extends
is useful if you only need a single service to be shared and you are
familiar with the file you're extending to, so you can tweak the
configuration. But this isn’t an acceptable solution when you want to re-use
someone else's unfamiliar configurations and you don’t know about its own
dependencies.
Relative paths
When using extends
with a file
attribute which points to another folder, relative paths
declared by the service being extended are converted so they still point to the
same file when used by the extending service. This is illustrated in the following example:
Base Compose file:
services:
webapp:
image: example
extends:
file: ../commons/compose.yaml
service: base
The commons/compose.yaml
file:
services:
base:
env_file: ./container.env
The resulting service refers to the original container.env
file
within the commons
directory. This can be confirmed with docker compose config
which inspects the actual model:
services:
webapp:
image: example
env_file:
- ../commons/container.env",,,
1345bd2852f0f3cde8af5518f5db4336bd5999875cd9a3d53ca31dbd616f17a7,"Annotations
Annotations provide descriptive metadata for images. Use annotations to record arbitrary information and attach it to your image, which helps consumers and tools understand the origin, contents, and how to use the image.
Annotations are similar to, and in some sense overlap with, labels. Both serve the same purpose: attach metadata to a resource. As a general principle, you can think of the difference between annotations and labels as follows:
- Annotations describe OCI image components, such as manifests, indexes, and descriptors.
- Labels describe Docker resources, such as images, containers, networks, and volumes.
The OCI image specification defines the format of annotations, as well as a set of pre-defined annotation keys. Adhering to the specified standards ensures that metadata about images can be surfaced automatically and consistently, by tools like Docker Scout.
Annotations are not to be confused with attestations:
- Attestations contain information about how an image was built and what it contains. An attestation is attached as a separate manifest on the image index. Attestations are not standardized by the Open Container Initiative.
- Annotations contain arbitrary metadata about an image. Annotations attach to the image config as labels, or on the image index or manifest as properties.
Add annotations
You can add annotations to an image at build-time, or when creating the image manifest or index.
Note
The Docker Engine image store doesn't support loading images with annotations. To build with annotations, make sure to push the image directly to a registry, using the
--push
CLI flag or the registry exporter.
To specify annotations on the command line, use the --annotation
flag for the
docker build
command:
$ docker build --push --annotation ""foo=bar"" .
If you're using
Bake, you can use the annotations
attribute to specify annotations for a given target:
target ""default"" {
output = [""type=registry""]
annotations = [""foo=bar""]
}
For examples on how to add annotations to images built with GitHub Actions, see Add image annotations with GitHub Actions
You can also add annotations to an image created using docker buildx imagetools create
. This command only supports adding annotations to an index
or manifest descriptors, see
CLI reference.
Inspect annotations
To view annotations on an image index, use the docker buildx imagetools inspect
command. This shows you any annotations for the index and descriptors
(references to manifests) that the index contains. The following example shows
an org.opencontainers.image.documentation
annotation on a descriptor, and an
org.opencontainers.image.authors
annotation on the index.
$ docker buildx imagetools inspect <IMAGE> --raw
{
""schemaVersion"": 2,
""mediaType"": ""application/vnd.oci.image.index.v1+json"",
""manifests"": [
{
""mediaType"": ""application/vnd.oci.image.manifest.v1+json"",
""digest"": ""sha256:d20246ef744b1d05a1dd69d0b3fa907db007c07f79fe3e68c17223439be9fefb"",
""size"": 911,
""annotations"": {
""org.opencontainers.image.documentation"": ""https://foo.example/docs"",
},
""platform"": {
""architecture"": ""amd64"",
""os"": ""linux""
}
},
],
""annotations"": {
""org.opencontainers.image.authors"": ""dvdksn""
}
}
To inspect annotations on a manifest, use the docker buildx imagetools inspect
command and specify <IMAGE>@<DIGEST>
, where <DIGEST>
is the digest
of the manifest:
$ docker buildx imagetools inspect <IMAGE>@sha256:d20246ef744b1d05a1dd69d0b3fa907db007c07f79fe3e68c17223439be9fefb --raw
{
""schemaVersion"": 2,
""mediaType"": ""application/vnd.oci.image.manifest.v1+json"",
""config"": {
""mediaType"": ""application/vnd.oci.image.config.v1+json"",
""digest"": ""sha256:4368b6959a78b412efa083c5506c4887e251f1484ccc9f0af5c406d8f76ece1d"",
""size"": 850
},
""layers"": [
{
""mediaType"": ""application/vnd.oci.image.layer.v1.tar+gzip"",
""digest"": ""sha256:2c03dbb20264f09924f9eab176da44e5421e74a78b09531d3c63448a7baa7c59"",
""size"": 3333033
},
{
""mediaType"": ""application/vnd.oci.image.layer.v1.tar+gzip"",
""digest"": ""sha256:4923ad480d60a548e9b334ca492fa547a3ce8879676685b6718b085de5aaf142"",
""size"": 61887305
}
],
""annotations"": {
""index,manifest:org.opencontainers.image.vendor"": ""foocorp"",
""org.opencontainers.image.source"": ""https://git.example/foo.git"",
}
}
Specify annotation level
By default, annotations are added to the image manifest. You can specify which level (OCI image component) to attach the annotation to by prefixing the annotation string with a special type declaration:
$ docker build --annotation ""<TYPE>:<KEY>=<VALUE>"" .
The following types are supported:
manifest
: annotates manifests.index
: annotates the root index.manifest-descriptor
: annotates manifest descriptors in the index.index-descriptor
: annotates the index descriptor in the image layout.
For example, to build an image with the annotation foo=bar
attached to the
image index:
$ docker build --tag <IMAGE> --push --annotation ""index:foo=bar"" .
Note that the build must produce the component that you specify, or else the
build will fail. For example, the following does not work, because the docker
exporter does not produce an index:
$ docker build --output type=docker --annotation ""index:foo=bar"" .
Likewise, the following example also does not work, because buildx creates a
docker
output by default under some circumstances, such as when provenance
attestations are explicitly disabled:
$ docker build --provenance=false --annotation ""index:foo=bar"" .
It is possible to specify types, separated by a comma, to add the annotation to
more than one level. The following example creates an image with the annotation
foo=bar
on both the image index and the image manifest:
$ docker build --tag <IMAGE> --push --annotation ""index,manifest:foo=bar"" .
You can also specify a platform qualifier within square brackets in the type
prefix, to annotate only components matching specific OS and architectures. The
following example adds the foo=bar
annotation only to the linux/amd64
manifest:
$ docker build --tag <IMAGE> --push --annotation ""manifest[linux/amd64]:foo=bar"" .
Related information
Related articles:
Reference information:",,,
d26dbbd66027d55ff2c026a7614c126f18f14b1e5beeb2980f807466632ed82a,"Docker Engine managed plugin system
Docker Engine's plugin system lets you install, start, stop, and remove plugins using Docker Engine.
For information about legacy (non-managed) plugins, refer to Understand legacy Docker Engine plugins.
Note
Docker Engine managed plugins are currently not supported on Windows daemons.
Installing and using a plugin
Plugins are distributed as Docker images and can be hosted on Docker Hub or on a private registry.
To install a plugin, use the docker plugin install
command, which pulls the
plugin from Docker Hub or your private registry, prompts you to grant
permissions or capabilities if necessary, and enables the plugin.
To check the status of installed plugins, use the docker plugin ls
command.
Plugins that start successfully are listed as enabled in the output.
After a plugin is installed, you can use it as an option for another Docker operation, such as creating a volume.
In the following example, you install the sshfs
plugin, verify that it is
enabled, and use it to create a volume.
Note
This example is intended for instructional purposes only. Once the volume is created, your SSH password to the remote host is exposed as plaintext when inspecting the volume. Delete the volume as soon as you are done with the example.
Install the
sshfs
plugin.$ docker plugin install vieux/sshfs Plugin ""vieux/sshfs"" is requesting the following privileges: - network: [host] - capabilities: [CAP_SYS_ADMIN] Do you grant the above permissions? [y/N] y vieux/sshfs
The plugin requests 2 privileges:
- It needs access to the
host
network. - It needs the
CAP_SYS_ADMIN
capability, which allows the plugin to run themount
command.
- It needs access to the
Check that the plugin is enabled in the output of
docker plugin ls
.$ docker plugin ls ID NAME TAG DESCRIPTION ENABLED 69553ca1d789 vieux/sshfs latest the `sshfs` plugin true
Create a volume using the plugin. This example mounts the
/remote
directory on host1.2.3.4
into a volume namedsshvolume
.This volume can now be mounted into containers.
$ docker volume create \ -d vieux/sshfs \ --name sshvolume \ -o sshcmd=user@1.2.3.4:/remote \ -o password=$(cat file_containing_password_for_remote_host) sshvolume
Verify that the volume was created successfully.
$ docker volume ls DRIVER NAME vieux/sshfs sshvolume
Start a container that uses the volume
sshvolume
.$ docker run --rm -v sshvolume:/data busybox ls /data <content of /remote on machine 1.2.3.4>
Remove the volume
sshvolume
$ docker volume rm sshvolume sshvolume
To disable a plugin, use the docker plugin disable
command. To completely
remove it, use the docker plugin remove
command. For other available
commands and options, see the
command line reference.
Developing a plugin
The rootfs directory
The rootfs
directory represents the root filesystem of the plugin. In this
example, it was created from a Dockerfile:
Note
The
/run/docker/plugins
directory is mandatory inside of the plugin's filesystem for Docker to communicate with the plugin.
$ git clone https://github.com/vieux/docker-volume-sshfs
$ cd docker-volume-sshfs
$ docker build -t rootfsimage .
$ id=$(docker create rootfsimage true) # id was cd851ce43a403 when the image was created
$ sudo mkdir -p myplugin/rootfs
$ sudo docker export ""$id"" | sudo tar -x -C myplugin/rootfs
$ docker rm -vf ""$id""
$ docker rmi rootfsimage
The config.json file
The config.json
file describes the plugin. See the
plugins config reference.
Consider the following config.json
file.
{
""description"": ""sshFS plugin for Docker"",
""documentation"": ""https://docs.docker.com/engine/extend/plugins/"",
""entrypoint"": [""/docker-volume-sshfs""],
""network"": {
""type"": ""host""
},
""interface"": {
""types"": [""docker.volumedriver/1.0""],
""socket"": ""sshfs.sock""
},
""linux"": {
""capabilities"": [""CAP_SYS_ADMIN""]
}
}
This plugin is a volume driver. It requires a host
network and the
CAP_SYS_ADMIN
capability. It depends upon the /docker-volume-sshfs
entrypoint and uses the /run/docker/plugins/sshfs.sock
socket to communicate
with Docker Engine. This plugin has no runtime parameters.
Creating the plugin
A new plugin can be created by running
docker plugin create <plugin-name> ./path/to/plugin/data
where the plugin
data contains a plugin configuration file config.json
and a root filesystem
in subdirectory rootfs
.
After that the plugin <plugin-name>
will show up in docker plugin ls
.
Plugins can be pushed to remote registries with
docker plugin push <plugin-name>
.
Debugging plugins
Stdout of a plugin is redirected to dockerd logs. Such entries have a
plugin=<ID>
suffix. Here are a few examples of commands for pluginID
f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62
and their
corresponding log entries in the docker daemon logs.
$ docker plugin install tiborvass/sample-volume-plugin
INFO[0036] Starting... Found 0 volumes on startup plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62
$ docker volume create -d tiborvass/sample-volume-plugin samplevol
INFO[0193] Create Called... Ensuring directory /data/samplevol exists on host... plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62
INFO[0193] open /var/lib/docker/plugin-data/local-persist.json: no such file or directory plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62
INFO[0193] Created volume samplevol with mountpoint /data/samplevol plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62
INFO[0193] Path Called... Returned path /data/samplevol plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62
$ docker run -v samplevol:/tmp busybox sh
INFO[0421] Get Called... Found samplevol plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62
INFO[0421] Mount Called... Mounted samplevol plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62
INFO[0421] Path Called... Returned path /data/samplevol plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62
INFO[0421] Unmount Called... Unmounted samplevol plugin=f52a3df433b9aceee436eaada0752f5797aab1de47e5485f1690a073b860ff62
Using runc to obtain logfiles and shell into the plugin.
Use runc
, the default docker container runtime, for debugging plugins by
collecting plugin logs redirected to a file.
$ sudo runc --root /run/docker/runtime-runc/plugins.moby list
ID PID STATUS BUNDLE CREATED OWNER
93f1e7dbfe11c938782c2993628c895cf28e2274072c4a346a6002446c949b25 15806 running /run/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby-plugins/93f1e7dbfe11c938782c2993628c895cf28e2274072c4a346a6002446c949b25 2018-02-08T21:40:08.621358213Z root
9b4606d84e06b56df84fadf054a21374b247941c94ce405b0a261499d689d9c9 14992 running /run/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby-plugins/9b4606d84e06b56df84fadf054a21374b247941c94ce405b0a261499d689d9c9 2018-02-08T21:35:12.321325872Z root
c5bb4b90941efcaccca999439ed06d6a6affdde7081bb34dc84126b57b3e793d 14984 running /run/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby-plugins/c5bb4b90941efcaccca999439ed06d6a6affdde7081bb34dc84126b57b3e793d 2018-02-08T21:35:12.321288966Z root
$ sudo runc --root /run/docker/runtime-runc/plugins.moby exec 93f1e7dbfe11c938782c2993628c895cf28e2274072c4a346a6002446c949b25 cat /var/log/plugin.log
If the plugin has a built-in shell, then exec into the plugin can be done as follows:
$ sudo runc --root /run/docker/runtime-runc/plugins.moby exec -t 93f1e7dbfe11c938782c2993628c895cf28e2274072c4a346a6002446c949b25 sh
Using curl to debug plugin socket issues.
To verify if the plugin API socket that the docker daemon communicates with
is responsive, use curl. In this example, we will make API calls from the
docker host to volume and network plugins using curl 7.47.0 to ensure that
the plugin is listening on the said socket. For a well functioning plugin,
these basic requests should work. Note that plugin sockets are available on the host under /var/run/docker/plugins/<pluginID>
$ curl -H ""Content-Type: application/json"" -XPOST -d '{}' --unix-socket /var/run/docker/plugins/e8a37ba56fc879c991f7d7921901723c64df6b42b87e6a0b055771ecf8477a6d/plugin.sock http:/VolumeDriver.List
{""Mountpoint"":"""",""Err"":"""",""Volumes"":[{""Name"":""myvol1"",""Mountpoint"":""/data/myvol1""},{""Name"":""myvol2"",""Mountpoint"":""/data/myvol2""}],""Volume"":null}
$ curl -H ""Content-Type: application/json"" -XPOST -d '{}' --unix-socket /var/run/docker/plugins/45e00a7ce6185d6e365904c8bcf62eb724b1fe307e0d4e7ecc9f6c1eb7bcdb70/plugin.sock http:/NetworkDriver.GetCapabilities
{""Scope"":""local""}
When using curl 7.5 and above, the URL should be of the form
http://hostname/APICall
, where hostname
is the valid hostname where the
plugin is installed and APICall
is the call to the plugin API.
For example, http://localhost/VolumeDriver.List",,,
db6c1104fe3c8b3e0ff62c55860a2e25b5631a4dd5df617afb4e935d8ae254b1,"Create a repository
Sign in to Docker Hub.
Select Repositories.
Near the top-right corner, select Create repository.
Select a Namespace.
You can choose to locate it under your own user account, or under any organization where you are an owner or editor.
Specify the Repository Name.
The repository name needs to:
- Be unique
- Have between 2 and 255 characters
- Only contain lowercase letters, numbers, hyphens (
-
), and underscores (_
)
Note
You can't rename a Docker Hub repository once it's created.
Specify the Short description.
The description can be up to 100 characters. It appears in search results.
Select the default visibility.
- Public: The repository appears in Docker Hub search results and can be pulled by everyone.
- Private: The repository doesn't appear in Docker Hub search results and is only accessible to you and collaborators. In addition, if you selected an organization's namespace, then the repository is accessible to those with applicable roles or permissions. For more details, see Roles and permissions.
Note
For organizations creating a new repository, if you're unsure which visibility to choose, then Docker recommends that you select Private.
Select Create.
After the repository is created, the General page appears. You are now able to manage:",,,
cbd577da7deb491d74fd580762d1551c80a2992ff15f88400eae303abdba0c7b,"Part two: Publish
This section describes how to make your extension available and more visible, so users can discover it and install it with a single click.
Release your extension
After you have developed your extension and tested it locally, you are ready to release the extension and make it available for others to install and use (either internally with your team, or more publicly).
Releasing your extension consists of:
- Providing information about your extension: description, screenshots, etc. so users can decide to install your extension
- Validating that the extension is built in the right format and includes the required information
- Making the extension image available on Docker Hub
See Package and release your extension for more details about the release process.
Promote your extension
Once your extension is available on Docker Hub, users who have access to the extension image can install it using the Docker CLI.
Use a share extension link
You can also generate a share URL in order to share your extension within your team, or promote your extension on the internet. The share link lets users view the extension description and screenshots.
Publish your extension in the Marketplace
You can publish your extension in the Extensions Marketplace to make it more discoverable. You must submit your extension if you want to have it published in the Marketplace.
What happens next
New releases
Once you have released your extension, you can push a new release just by pushing a new version of the extension image, with an incremented tag (still using semver
conventions).
Extensions published in the Marketplace benefit from update notifications to all Desktop users that have installed the extension. For more details, see
new releases and updates.
Extension support and user feedback
In addition to providing a description of your extension's features and screenshots, you should also specify additional URLs using extension labels. This direct users to your website for reporting bugs and feedback, and accessing documentation and support.
Already built an extension?
Let us know about your experience using the feedback form.",,,
9ab5d3b77d45c5d3061f9eff5b2535fd563c950ee403cd1eb5f6f0346fa9ce33,"Example prompts for the Docker agent
Table of contents
Availability:
Early Access
Use cases
Here are some examples of the types of questions you can ask the Docker agent:
Ask general Docker questions
You can ask general question about Docker. For example:
@docker what is a Dockerfile?
@docker how do I build a Docker image?
@docker how do I run a Docker container?
@docker what does 'docker buildx imagetools inspect' do?
Get help containerizing your project
You can ask the agent to help you containerize your existing project:
@docker can you help create a compose file for this project?
@docker can you create a Dockerfile for this project?
Opening pull requests
The Docker agent will analyze your project, generate the necessary files, and, if applicable, offer to raise a pull request with the necessary Docker assets.
Automatically opening pull requests against your repositories is only available when the agent generates new Docker assets.
Analyze a project for vulnerabilities
The agent can help you improve your security posture with Docker Scout:
@docker can you help me find vulnerabilities in my project?
@docker does my project contain any insecure dependencies?
The agent will run use Docker Scout to analyze your project's dependencies, and report whether you're vulnerable to any known CVEs.
Limitations
- The agent is currently not able to access specific files in your repository, such as the currently-opened file in your editor, or if you pass a file reference with your message in the chat message.
Feedback
For issues or feedback, visit the GitHub feedback repository.",,,
9eeb8f9e2a51ec5ef492baeb8e4055ae5b47c41e202cadafd4966ffeaa997fa7,"Dashboard
User notifications
Toasts provide a brief notification to the user. They appear temporarily and shouldn't interrupt the user experience. They also don't require user input to disappear.
success
▸ success(msg
): void
Use to display a toast message of type success.
ddClient.desktopUI.toast.success(""message"");
warning
▸ warning(msg
): void
Use to display a toast message of type warning.
ddClient.desktopUI.toast.warning(""message"");
error
▸ error(msg
): void
Use to display a toast message of type error.
ddClient.desktopUI.toast.error(""message"");
For more details about method parameters and the return types available, see Toast API reference.
Deprecated user notifications
These methods are deprecated and will be removed in a future version. Use the methods specified above.
window.ddClient.toastSuccess(""message"");
window.ddClient.toastWarning(""message"");
window.ddClient.toastError(""message"");
Open a file selection dialog
This function opens a file selector dialog that asks the user to select a file or folder.
▸ showOpenDialog(dialogProperties
): Promise
<
OpenDialogResult
>:
The dialogProperties
parameter is a list of flags passed to Electron to customize the dialog's behaviour. For example, you can pass multiSelections
to allow a user to select multiple files. See
Electron's documentation for a full list.
const result = await ddClient.desktopUI.dialog.showOpenDialog({
properties: [""openDirectory""],
});
if (!result.canceled) {
console.log(result.paths);
}
Open a URL
This function opens an external URL with the system default browser.
▸ openExternal(url
): void
ddClient.host.openExternal(""https://docker.com"");
The URL must have the protocol
http
orhttps
.
For more details about method parameters and the return types available, see Desktop host API reference.
Deprecated user notifications
This method is deprecated and will be removed in a future version. Use the methods specified above.
window.ddClient.openExternal(""https://docker.com"");
Navigation to Dashboard routes
From your extension, you can also navigate to other parts of the Docker Desktop Dashboard.",,,
5b3343107d0171919f9ca4b260316192ee093a05c3a3f446026e29cd6fab6fa4,"Docker Scout quickstart
Docker Scout analyzes image contents and generates a detailed report of packages and vulnerabilities that it detects. It can provide you with suggestions for how to remediate issues discovered by image analysis.
This guide takes a vulnerable container image and shows you how to use Docker Scout to identify and fix the vulnerabilities, compare image versions over time, and share the results with your team.
Step 1: Setup
This example project contains a vulnerable Node.js application that you can use to follow along.
Clone its repository:
$ git clone https://github.com/docker/scout-demo-service.git
Move into the directory:
$ cd scout-demo-service
Make sure you're signed in to your Docker account, either by running the
docker login
command or by signing in with Docker Desktop.Build the image and push it to a
<ORG_NAME>/scout-demo:v1
, where<ORG_NAME>
is the Docker Hub namespace you push to.$ docker build --push -t <ORG_NAME>/scout-demo:v1 .
Step 2: Enable Docker Scout
Docker Scout analyzes all local images by default. To analyze images in remote repositories, you need to enable it first. You can do this from Docker Hub, the Docker Scout Dashboard, and CLI. Find out how in the overview guide.
Sign in to your Docker account with the
docker login
command or use the Sign in button in Docker Desktop.Next, enroll your organization with Docker Scout, using the
docker scout enroll
command.$ docker scout enroll <ORG_NAME>
Enable Docker Scout for your image repository with the
docker scout repo enable
command.$ docker scout repo enable --org <ORG_NAME> <ORG_NAME>/scout-demo
Step 3: Analyze image vulnerabilities
After building, use the docker scout
CLI command to see vulnerabilities
detected by Docker Scout.
The example application for this guide uses a vulnerable version of Express. The following command shows all CVEs affecting Express in the image you just built:
$ docker scout cves --only-package express
Docker Scout analyzes the image you built most recently by default, so there's no need to specify the name of the image in this case.
Learn more about the docker scout cves
command in the
CLI reference documentation
.
Step 4: Fix application vulnerabilities
The fix suggested by Docker Scout is to update the underlying vulnerable express version to 4.17.3 or later.
Update the
package.json
file with the new package version.""dependencies"": { - ""express"": ""4.17.1"" + ""express"": ""4.17.3"" }
Rebuild the image with a new tag and push it to your Docker Hub repository:
$ docker build --push -t <ORG_NAME>/scout-demo:v2 .
Now, viewing the latest tag of the image in Docker Desktop, the Docker Scout Dashboard, or CLI, you can see that you have fixed the vulnerability.
$ docker scout cves --only-package express
✓ Provenance obtained from attestation
✓ Image stored for indexing
✓ Indexed 79 packages
✓ No vulnerable package detected
## Overview
│ Analyzed Image
────────────────────┼───────────────────────────────────────────────────
Target │ mobywhale/scout-demo:v2
digest │ ef68417b2866
platform │ linux/arm64
provenance │ https://github.com/docker/scout-demo-service.git
│ 7c3a06793fc8f97961b4a40c73e0f7ed85501857
vulnerabilities │ 0C 0H 0M 0L
size │ 19 MB
packages │ 1
## Packages and Vulnerabilities
No vulnerable packages detected
Step 5: Evaluate policy compliance
While inspecting vulnerabilities based on specific packages can be useful, it isn't the most effective way to improve your supply chain conduct.
Docker Scout also supports policy evaluation, a higher-level concept for detecting and fixing issues in your images. Policies are a set of customizable rules that let organizations track whether images are compliant with their supply chain requirements.
Because policy rules are specific to each organization,
you must specify which organization's policy you're evaluating against.
Use the docker scout config
command to configure your Docker organization.
$ docker scout config organization <ORG_NAME>
✓ Successfully set organization to <ORG_NAME>
Now you can run the quickview
command to get an overview
of the compliance status for the image you just built.
The image is evaluated against the default policy configurations.
$ docker scout quickview
...
Policy status FAILED (2/6 policies met, 2 missing data)
Status │ Policy │ Results
─────────┼──────────────────────────────────────────────┼──────────────────────────────
✓ │ No copyleft licenses │ 0 packages
! │ Default non-root user │
! │ No fixable critical or high vulnerabilities │ 2C 16H 0M 0L
✓ │ No high-profile vulnerabilities │ 0C 0H 0M 0L
? │ No outdated base images │ No data
? │ Supply chain attestations │ No data
Exclamation marks in the status column indicate a violated policy. Question marks indicate that there isn't enough metadata to complete the evaluation. A check mark indicates compliance.
Step 6: Improve compliance
The output of the quickview
command shows that there's room for improvement.
Some of the policies couldn't evaluate successfully (No data
)
because the image lacks provenance and SBOM attestations.
The image also failed the check on a few of the evaluations.
Policy evaluation does more than just check for vulnerabilities.
Take the Default non-root user
policy for example.
This policy helps improve runtime security by ensuring that
images aren't set to run as the root
superuser by default.
To address this policy violation, edit the Dockerfile by adding a USER
instruction, specifying a non-root user:
CMD [""node"",""/app/app.js""]
EXPOSE 3000
+ USER appuser
Additionally, to get a more complete policy evaluation result, your image should have SBOM and provenance attestations attached to it. Docker Scout uses the provenance attestations to determine how the image was built so that it can provide a better evaluation result.
Before you can build an image with attestations,
you must enable the
containerd image store
(or create a custom builder using the docker-container
driver).
The classic image store doesn't support manifest lists,
which is how the provenance attestations are attached to an image.
Open Settings in Docker Desktop. Under the General section, make sure that the Use containerd for pulling and storing images option is checked. Note that changing image stores temporarily hides images and containers of the inactive image store until you switch back.
With the containerd image store enabled, rebuild the image with a new v3
tag.
This time, add the --provenance=true
and --sbom=true
flags.
$ docker build --provenance=true --sbom=true --push -t <ORG_NAME>/scout-demo:v3 .
Step 7: View in Dashboard
After pushing the updated image with attestations, it's time to view the results through a different lens: the Docker Scout Dashboard.
- Open the Docker Scout Dashboard.
- Sign in with your Docker account.
- Select Images in the left-hand navigation.
The images page lists your Scout-enabled repositories. Select the image in the list to open the Image details sidebar. The sidebar shows a compliance overview for the last pushed tag of a repository.
Note
If policy results haven't appeared yet, try refreshing the page. It might take a few minutes before the results appear if this is your first time using the Docker Scout Dashboard.
Inspect the Up-to-Date Base Images policy.
This policy checks whether base images you use are up-to-date.
It currently has a non-compliant status,
because the example image uses an old version alpine
as a base image.
Select the View fix button next to the policy name for details about the violation, and recommendations on how to address it. In this case, the recommended action is to enable Docker Scout's GitHub integration, which helps keep your base images up-to-date automatically.
Tip
You can't enable this integration for the demo app used in this guide. Feel free to push the code to a GitHub repository that you own, and try out the integration there!
Summary
This quickstart guide has scratched the surface on some of the ways Docker Scout can support software supply chain management:
- How to enable Docker Scout for your repositories
- Analyzing images for vulnerabilities
- Policy and compliance
- Fixing vulnerabilities and improving compliance
What's next?
There's lots more to discover, from third-party integrations, to policy customization, and runtime environment monitoring in real-time.
Check out the following sections:",,,
98ed3e56dbcea70d1738c1b0c5f7e8770b5ab3298a74f3db1748fd6274a45063,"Troubleshoot topics for Docker Desktop
Tip
If you do not find a solution in troubleshooting, browse the GitHub repositories or create a new issue:
Topics for all platforms
Make sure certificates are set up correctly
Docker Desktop ignores certificates listed under insecure registries, and
does not send client certificates to them. Commands like docker run
that
attempt to pull from the registry produces error messages on the command line,
like this:
Error response from daemon: Get http://192.168.203.139:5858/v2/: malformed HTTP response ""\x15\x03\x01\x00\x02\x02""
As well as on the registry. For example:
2017/06/20 18:15:30 http: TLS handshake error from 192.168.203.139:52882: tls: client didn't provide a certificate
2017/06/20 18:15:30 http: TLS handshake error from 192.168.203.139:52883: tls: first record does not look like a TLS handshake
Docker Desktop's UI appears green, distorted, or has visual artifacts
Docker Desktop uses hardware-accelerated graphics by default, which may cause problems for some GPUs. In such cases, Docker Desktop will launch successfully, but some screens may appear green, distorted, or have some visual artifacts.
To work around this issue, disable hardware acceleration by creating a ""disableHardwareAcceleration"": true
entry in Docker Desktop's settings-store.json
file (or settings.json
for Docker Desktop versions 4.34 and earlier). You can find this file at:
- Mac:
~/Library/Group Containers/group.com.docker/settings-store.json
- Windows:
C:\Users\[USERNAME]\AppData\Roaming\Docker\settings-store.json
- Linux:
~/.docker/desktop/settings-store.json.
After updating the settings-store.json
file, close and restart Docker Desktop to apply the changes.
Topics for Linux and Mac
Volume mounting requires file sharing for any project directories outside of $HOME
If you are using mounted volumes and get runtime errors indicating an application file is not found, access to a volume mount is denied, or a service cannot start, such as when using Docker Compose, you might need to turn on file sharing.
Volume mounting requires shared drives for projects that live outside of the
/home/<user>
directory. From Settings, select Resources and then File sharing. Share the drive that contains the Dockerfile and volume.
Docker Desktop fails to start on MacOS or Linux platforms
On MacOS and Linux, Docker Desktop creates Unix domain sockets used for inter-process communication.
Docker fails to start if the absolute path length of any of these sockets exceeds the OS limitation which is 104 characters on MacOS and 108 characters on Linux. These sockets are created under the user's home directory. If the user ID length is such that the absolute path of the socket exceeds the OS path length limitation, then Docker Desktop is unable to create the socket and fails to start. The workaround for this is to shorten the user ID which we recommend has a maximum length of 33 characters on MacOS and 55 characters on Linux.
Following are the examples of errors on MacOS which indicate that the startup failure was due to exceeding the above mentioned OS limitation:
[vpnkit-bridge][F] listen unix <HOME>/Library/Containers/com.docker.docker/Data/http-proxy-control.sock: bind: invalid argument
[com.docker.backend][E] listen(vsock:4099) failed: listen unix <HOME>/Library/Containers/com.docker.docker/Data/vms/0/00000002.00001003: bind: invalid argument
Topics for Mac
Incompatible CPU detected
Tip
If you are seeing this error, check you've installed the correct Docker Desktop for your architecture.
Docker Desktop requires a processor (CPU) that supports virtualization and, more specifically, the Apple Hypervisor framework. Docker Desktop is only compatible with Mac systems that have a CPU that supports the Hypervisor framework. Most Macs built in 2010 and later support it,as described in the Apple Hypervisor Framework documentation about supported hardware:
Generally, machines with an Intel VT-x feature set that includes Extended Page Tables (EPT) and Unrestricted Mode are supported.
To check if your Mac supports the Hypervisor framework, run the following command in a terminal window.
$ sysctl kern.hv_support
If your Mac supports the Hypervisor Framework, the command prints
kern.hv_support: 1
.
If not, the command prints kern.hv_support: 0
.
See also, Hypervisor Framework Reference in the Apple documentation, and Docker Desktop Mac system requirements.
VPNKit keeps breaking
In Docker Desktop version 4.19, gVisor replaced VPNKit to enhance the performance of VM networking when using the Virtualization framework on macOS 13 and above.
To continue using VPNKit, add ""networkType"":""vpnkit""
to your settings-store.json
file located at ~/Library/Group Containers/group.com.docker/settings-store.json
.
Topics for Windows
Volumes
Permissions errors on data directories for shared volumes
When sharing files from Windows, Docker Desktop sets permissions on
shared volumes
to a default value of
0777
(read
, write
, execute
permissions for user
and for group
).
The default permissions on shared volumes are not configurable. If you are working with applications that require permissions different from the shared volume defaults at container runtime, you need to either use non-host-mounted volumes or find a way to make the applications work with the default file permissions.
See also, Can I change permissions on shared volumes for container-specific deployment requirements? in the FAQs.
Volume mounting requires shared folders for Linux containers
If you are using mounted volumes and get runtime errors indicating an application file is not found, access is denied to a volume mount, or a service cannot start, such as when using Docker Compose, you might need to turn on shared folders.
With the Hyper-V backend, mounting files from Windows requires shared folders for Linux containers. From Settings, select Shared Folders and share the folder that contains the Dockerfile and volume.
Support for symlinks
Symlinks work within and across containers. To learn more, see How do symlinks work on Windows?.
Avoid unexpected syntax errors, use Unix style line endings for files in containers
Any file destined to run inside a container must use Unix style \n
line
endings. This includes files referenced at the command line for builds and in
RUN commands in Docker files.
Docker containers and docker build
run in a Unix environment, so files in
containers must use Unix style line endings: \n
, not Windows style: \r\n
.
Keep this in mind when authoring files such as shell scripts using Windows
tools, where the default is likely to be Windows style line endings. These
commands ultimately get passed to Unix commands inside a Unix based container
(for example, a shell script passed to /bin/sh
). If Windows style line endings
are used, docker run
fails with syntax errors.
For an example of this issue and the resolution, see this issue on GitHub: Docker RUN fails to execute shell script.
Path conversion on Windows
On Linux, the system takes care of mounting a path to another path. For example, when you run the following command on Linux:
$ docker run --rm -ti -v /home/user/work:/work alpine
It adds a /work
directory to the target container to mirror the specified path.
However, on Windows, you must update the source path. For example, if you are using
the legacy Windows shell (cmd.exe
), you can use the following command:
$ docker run --rm -ti -v C:\Users\user\work:/work alpine
This starts the container and ensures the volume becomes usable. This is possible because Docker Desktop detects the Windows-style path and provides the appropriate conversion to mount the directory.
Docker Desktop also allows you to use Unix-style path to the appropriate format. For example:
$ docker run --rm -ti -v /c/Users/user/work:/work alpine ls /work
Working with Git Bash
Git Bash (or MSYS) provides a Unix-like environment on Windows. These tools apply their own preprocessing on the command line. For example, if you run the following command in Git Bash, it gives an error:
$ docker run --rm -ti -v C:\Users\user\work:/work alpine
docker: Error response from daemon: mkdir C:UsersUserwork: Access is denied.
This is because the \
character has a special meaning in Git Bash. If you are using Git Bash, you must neutralize it using \\
:
$ docker run --rm -ti -v C:\\Users\\user\\work:/work alpine
Also, in scripts, the pwd
command is used to avoid hard-coding file system locations. Its output is a Unix-style path.
$ pwd
/c/Users/user/work
Combined with the $()
syntax, the command below works on Linux, however, it fails on Git Bash.
$ docker run --rm -ti -v $(pwd):/work alpine
docker: Error response from daemon: OCI runtime create failed: invalid mount {Destination:\Program Files\Git\work Type:bind Source:/run/desktop/mnt/host/c/Users/user/work;C Options:[rbind rprivate]}: mount destination \Program Files\Git\work not absolute: unknown.
You can work around this issue by using an extra /
$ docker run --rm -ti -v /$(pwd):/work alpine
Portability of the scripts is not affected as Linux treats multiple /
as a single entry.
Each occurrence of paths on a single line must be neutralized.
$ docker run --rm -ti -v /$(pwd):/work alpine ls /work
ls: C:/Program Files/Git/work: No such file or directory
In this example, The $(pwd)
is not converted because of the preceding '/'. However, the second '/work' is transformed by the
POSIX layer before passing it to Docker Desktop. You can also work around this issue by using an extra /
.
$ docker run --rm -ti -v /$(pwd):/work alpine ls //work
To verify whether the errors are generated from your script, or from another source, you can use an environment variable. For example:
$ MSYS_NO_PATHCONV=1 docker run --rm -ti -v $(pwd):/work alpine ls /work
It only expects the environment variable here. The value doesn't matter.
In some cases, MSYS also transforms colons to semicolon. Similar conversions can also occur
when using ~
because the POSIX layer translates it to a DOS path. MSYS_NO_PATHCONV
also works in this case.
Virtualization
Your machine must have the following features for Docker Desktop to function correctly:
WSL 2 and Windows Home
- Virtual Machine Platform
- Windows Subsystem for Linux
- Virtualization enabled in the BIOS Note that many Windows devices already have virtualization enabled, so this may not apply.
- Hypervisor enabled at Windows startup
Hyper-V
On Windows 10 Pro or Enterprise, you can also use Hyper-V with the following features enabled:
- Hyper-V installed and working
- Virtualization enabled in the BIOS Note that many Windows devices already have virtualization enabled, so this may not apply.
- Hypervisor enabled at Windows startup
Docker Desktop requires Hyper-V as well as the Hyper-V Module for Windows PowerShell to be installed and enabled. The Docker Desktop installer enables it for you.
Docker Desktop also needs two CPU hardware features to use Hyper-V: Virtualization and Second Level Address Translation (SLAT), which is also called Rapid Virtualization Indexing (RVI). On some systems, Virtualization must be enabled in the BIOS. The steps required are vendor-specific, but typically the BIOS option is called Virtualization Technology (VTx)
or something similar. Run the command systeminfo
to check all required Hyper-V features. See
Pre-requisites for Hyper-V on Windows 10 for more details.
To install Hyper-V manually, see Install Hyper-V on Windows 10. A reboot is required after installation. If you install Hyper-V without rebooting, Docker Desktop does not work correctly.
From the start menu, type Turn Windows features on or off and press enter. In the subsequent screen, verify that Hyper-V is enabled.
Virtualization must be turned on
In addition to Hyper-V or WSL 2, virtualization must be turned on. Check the Performance tab on the Task Manager. Alternatively, you can type 'systeminfo' into your terminal. If you see 'Hyper-V Requirements: A hypervisor has been detected. Features required for Hyper-V will not be displayed', then virtualization is enabled.
If you manually uninstall Hyper-V, WSL 2 or turn off virtualization, Docker Desktop cannot start.
To turn on nested virtualization, see Run Docker Desktop for Windows in a VM or VDI environment.
Hypervisor enabled at Windows startup
If you have completed the steps described above and are still experiencing Docker Desktop startup issues, this could be because the Hypervisor is installed, but not launched during Windows startup. Some tools (such as older versions of Virtual Box) and video game installers turn off hypervisor on boot. To turn it back on:
- Open an administrative console prompt.
- Run
bcdedit /set hypervisorlaunchtype auto
. - Restart Windows.
You can also refer to the Microsoft TechNet article on Code flow guard (CFG) settings.
Turn on nested virtualization
If you are using Hyper-V and you get the following error message when running Docker Desktop in a VDI environment:
The Virtual Machine Management Service failed to start the virtual machine 'DockerDesktopVM' because one of the Hyper-V components is not running
Try enabling nested virtualization.
Windows containers and Windows Server
Docker Desktop is not supported on Windows Server. If you have questions about how to run Windows containers on Windows 10, see Switch between Windows and Linux containers.
A full tutorial is available in docker/labs on Getting Started with Windows Containers.
You can install a native Windows binary which allows you to develop and run Windows containers without Docker Desktop. However, if you install Docker this way, you cannot develop or run Linux containers. If you try to run a Linux container on the native Docker daemon, an error occurs:
C:\Program Files\Docker\docker.exe:
image operating system ""linux"" cannot be used on this platform.
See 'C:\Program Files\Docker\docker.exe run --help'.
Docker Desktop Access Denied
error message when starting Docker Desktop
Docker Desktop displays the Docker Desktop - Access Denied error if a Windows user is not part of the docker-users group.
If your admin account is different to your user account, add the docker-users group. Run Computer Management as an administrator and navigate to Local Users and Groups > Groups > docker-users.
Right-click to add the user to the group. Sign out and sign back in for the changes to take effect.",,,
161fbf17d22d472651ff57c04690c3d670bceff1afc5e529e80a05095b2fc1a2,"Configure automated builds from GitHub and BitBucket
Note
Automated builds require a Docker Pro, Team, or Business subscription.
To automate building and testing of your images, you link to your hosted source code service to Docker Hub so that it can access your source code repositories. You can configure this link for user accounts or organizations.
If you are linking a source code provider to create autobuilds for a team, follow the instructions to create a service account for the team before linking the account as described below.
Link to a GitHub user account
Sign in to Docker Hub.
Select the Settings icon in the top-right navigation, then select Repository Settings.
From the Linked accounts tab, select Link provider for the source provider you want to link.
If you want to unlink your current GitHub account and relink to a new GitHub account, make sure to completely sign out of GitHub before linking via Docker Hub.
Review the settings for the Docker Hub Builder OAuth application.
Note
If you are the owner of any GitHub organizations, you might see options to grant Docker Hub access to them from this screen. You can also individually edit an organization's third-party access settings to grant or revoke Docker Hub's access. See Grant access to a GitHub organization to learn more.
Select Authorize docker to save the link.
Grant access to a GitHub organization
If you are the owner of a GitHub organization, you can grant or revoke Docker Hub's access to the organization's repositories. Depending on the GitHub organization's settings, you may need to be an organization owner.
If the organization has not had specific access granted or revoked before, you can often grant access at the same time as you link your user account. In this case, a Grant access button appears next to the organization name in the link accounts screen, as shown below. If this button does not appear, you must manually grant the application's access.
To manually grant Docker Hub access to a GitHub organization:
Link your user account using the instructions above.
From your GitHub Account settings, locate the Organization settings section at the lower left.
Select the organization you want to give Docker Hub access to.
Select Third-party access.
The page displays a list of third party applications and their access status.
Select the pencil icon next to Docker Hub Builder.
Select Grant access next to the organization.
Revoke access to a GitHub organization
To revoke Docker Hub's access to an organization's GitHub repositories:
From your GitHub Account settings, locate the Organization settings section at the lower left.
Select the organization you want to revoke Docker Hub's access to.
From the Organization Profile menu, select Third-party access. The page displays a list of third party applications and their access status.
Select the pencil icon next to Docker Hub Builder.
On the next page, select Deny access.
Unlink a GitHub user account
To revoke Docker Hub's access to your GitHub account, you must unlink it both from Docker Hub, and from your GitHub account.
Select the Settings icon in the top-right navigation, then select Repository Settings.
From the Linked accounts tab, select the plug icon next to the source provider you want to remove.
Go to your GitHub account's Settings page.
Select Applications in the left navigation bar.
Select the
...
menu to the right of the Docker Hub Builder application and select Revoke.
Note
Each repository that is configured as an automated build source contains a webhook that notifies Docker Hub of changes in the repository. This webhook is not automatically removed when you revoke access to a source code provider.
Link to a Bitbucket user account
Sign in to Docker Hub using your Docker ID.
Select the Settings icon in the top-right navigation, then select Repository Settings.
From the Linked accounts tab, select Link provider for the source provider you want to link.
If necessary, sign in to Bitbucket.
On the page that appears, select Grant access.
Unlink a Bitbucket user account
To permanently revoke Docker Hub's access to your Bitbucket account, you must unlink it both from Docker Hub, and revoke authorization in your Bitbucket account.
Sign in to Docker Hub.
Select the Settings icon in the top-right navigation, then select Repository Settings.
From the Linked accounts tab, select the Plug icon next to the source provider you want to remove.
Important
After unlinking the account on Docker Hub, you must also revoke the authorization on the Bitbucket end.
To revoke authorization in your Bitbucket account:
Go to your Bitbucket account and navigate to Bitbucket settings.
On the page that appears, select OAuth.
Select Revoke next to the Docker Hub line.
Note
Each repository that is configured as an automated build source contains a webhook that notifies Docker Hub of changes in the repository. This webhook is not automatically removed when you revoke access to a source code provider.",,,
2e75caf40ac30ea7c7e36089c724712fb7e13cd13203d7ae178757a8c2340058,"Release notes for Docker Home, the Admin Console, billing, security, and subscription features
Table of contents
This page provides details on new features, enhancements, known issues, and bug fixes across Docker Home, the Admin Console, billing, security, and subscription functionalities.
2025-01-30
New
- Installing Docker Desktop via the PKG installer is now generally available.
- Enforcing sign-in via configuration profiles is now generally available.
2024-12-10
New
- New Docker subscriptions are now available. For more information, see Docker subscriptions and features and Announcing Upgraded Docker Plans: Simpler, More Value, Better Development and Productivity.
2024-11-18
New
- Administrators can now:
- Enforce sign-in with configuration profiles (Early Access).
- Enforce sign-in for more than one organization at a time (Early Access).
- Deploy Docker Desktop for Mac in bulk with the PKG installer (Early Access).
- Use Desktop Settings Management via the Docker Admin Console (Early Access).
Bug fixes and enhancements
- Enhance Container Isolation (ECI) has been improved to:
- Permit admins to turn off Docker socket mount restrictions.
- Support wildcard tags when using the
allowedDerivedImages
setting.
2024-11-11
New
- Personal access tokens (PATs) now support expiration dates.
2024-10-15
New
- Beta: You can now create organization access tokens (OATs) to enhance security for organizations and streamline access management for organizations in the Docker Admin Console.
2024-08-29
New
- Deploying Docker Desktop via the MSI installer is now generally available.
- Two new methods to
enforce sign-in (Windows registry key and
.plist
file) are now generally available.
2024-08-24
New
- Administrators can now view organization insights (Early Access).
2024-07-17
New
- You can now centrally access and manage Docker products in Docker Home (Early Access).",,,
55ef25d10bf2f357b54bb8c069aea6a0efa09a933a7c347ddab9c76b9045c178,"Company administration
Table of contents
Subscription:
Business
For:
Administrators
A company provides a single point of visibility across multiple organizations. This view simplifies the management of Docker organizations and settings. Organization owners with a Docker Business subscription can create a company and then manage it through the Docker Admin Console.
The following diagram depicts the setup of a company and how it relates to associated organizations.
Key features
With a company, administrators can:
- View and manage all nested organizations and configure settings centrally
- Carefully control access to the company and company settings
- Have up to ten unique users assigned the company owner role
- Configure SSO and SCIM for all nested organizations
- Enforce SSO for all users in the company
Prerequisites
Before you create a company, verify the following:
- Any organizations you want to add to a company have a Docker Business subscription
- You're an organization owner for your organization and any additional organizations you want to add
Learn how to administer a company in the following sections.",,,
513f45cd7df1f5fe5184399fce485ab4b8a896b279388139634efa4ec8a04f18,"Container security FAQs
How are containers isolated from the host in Docker Desktop?
Docker Desktop runs all containers inside a customized / minimal Linux virtual machine (except for native Windows containers). This adds a strong layer of isolation between containers and the host the machine, even if containers are running rootful.
However note the following:
Containers have access to host files configured for file sharing via Settings -> Resources -> File Sharing (see the next FAQ question below for more info).
By default, containers run as root but with limited capabilities inside the Docker Desktop VM. Containers running with elevated privileges (e.g.,
--privileged
,--pid=host
,--cap-add
, etc.) run as root with elevated privileges inside the Docker Desktop VM which gives them access to Docker Desktop VM internals, including the Docker Engine. Thus, users must be careful which containers they run with such privileges to avoid security breaches by malicious container images.If Enhanced Container Isolation (ECI) mode is enabled, then each container runs within a dedicated Linux User Namespace inside the Docker Desktop VM, which means the container has no privileges within the Docker Desktop VM. Even when using the
--privileged
flag or similar, the container processes will only be privileged within the container's logical boundary, but unprivileged otherwise. In addition, ECI protects uses other advanced techniques to ensure they can't easily breach the Docker Desktop VM and Docker Engine within (see the ECI section for more info). No changes to the containers or user workflows are required as the extra protection is added under the covers.
To which portions of the host filesystem do containers have read and write access?
Containers can only access host files if these are shared via Settings -> Resources -> File Sharing,
and only when such files are bind-mounted into the container (e.g., docker run -v /path/to/host/file:/mnt ...
).
Can containers running as root gain access to admin-owned files or directories on the host?
No; host file sharing (bind mount from the host filesystem) uses a user-space crafted
file server (running in com.docker.backend
as the user running Docker
Desktop), so containers can’t gain any access that the user on the host doesn’t
already have.",,,
7e5a1ae206b15788794751833e41c83578b3c8bb88cb23b41b641eebec8538ec,"Docker Scout metrics exporter
Docker Scout exposes a metrics HTTP endpoint that lets you scrape vulnerability and policy data from Docker Scout, using Prometheus or Datadog. With this you can create your own, self-hosted Docker Scout dashboards for visualizing supply chain metrics.
Metrics
The metrics endpoint exposes the following metrics:
| Metric | Description | Labels | Type |
|---|---|---|---|
scout_stream_vulnerabilities | Vulnerabilities in a stream | streamName , severity | Gauge |
scout_policy_compliant_images | Compliant images for a policy in a stream | id , displayName , streamName | Gauge |
scout_policy_evaluated_images | Total images evaluated against a policy in a stream | id , displayName , streamName | Gauge |
Streams
In Docker Scout, the streams concept is a superset of environments. Streams include all runtime environments that you've defined, as well as the special
latest-indexed
stream. Thelatest-indexed
stream contains the most recently pushed (and analyzed) tag for each repository.Streams is mostly an internal concept in Docker Scout, with the exception of the data exposed through this metrics endpoint.
Creating an access token
To export metrics from your organization, first make sure your organization is enrolled in Docker Scout. Then, create a Personal Access Token (PAT) - a secret token that allows the exporter to authenticate with the Docker Scout API.
The PAT does not require any specific permissions, but it must be created by a user who is an owner of the Docker organization. To create a PAT, follow the steps in Create an access token.
Once you have created the PAT, store it in a secure location. You will need to provide this token to the exporter when scraping metrics.
Prometheus
This section describes how to scrape the metrics endpoint using Prometheus.
Add a job for your organization
In the Prometheus configuration file, add a new job for your organization.
The job should include the following configuration;
replace ORG
with your organization name:
scrape_configs:
- job_name: <ORG>
metrics_path: /v1/exporter/org/<ORG>/metrics
scheme: https
static_configs:
- targets:
- api.scout.docker.com
The address in the targets
field is set to the domain name of the Docker Scout API, api.scout.docker.com
.
Make sure that there's no firewall rule in place preventing the server from communicating with this endpoint.
Add bearer token authentication
To scrape metrics from the Docker Scout Exporter endpoint using Prometheus, you need to configure Prometheus to use the PAT as a bearer token.
The exporter requires the PAT to be passed in the Authorization
header of the request.
Update the Prometheus configuration file to include the authorization
configuration block.
This block defines the PAT as a bearer token stored in a file:
scrape_configs:
- job_name: $ORG
authorization:
type: Bearer
credentials_file: /etc/prometheus/token
The content of the file should be the PAT in plain text:
dckr_pat_...
If you are running Prometheus in a Docker container or Kubernetes pod, mount the file into the container using a volume or secret.
Finally, restart Prometheus to apply the changes.
Prometheus sample project
If you don't have a Prometheus server set up, you can run a sample project using Docker Compose. The sample includes a Prometheus server that scrapes metrics for a Docker organization enrolled in Docker Scout, alongside Grafana with a pre-configured dashboard to visualize the vulnerability and policy metrics.
Clone the starter template for bootstrapping a set of Compose services for scraping and visualizing the Docker Scout metrics endpoint:
$ git clone git@github.com:dockersamples/scout-metrics-exporter.git $ cd scout-metrics-exporter/prometheus
Create a Docker access token and store it in a plain text file at
/prometheus/prometheus/token
under the template directory.token$ echo $DOCKER_PAT > ./prometheus/token
In the Prometheus configuration file at
/prometheus/prometheus/prometheus.yml
, replaceORG
in themetrics_path
property on line 6 with the namespace of your Docker organization.prometheus/prometheus.yml1 2 3 4 5 6 7 8 9 10 11 12 13
global: scrape_interval: 60s scrape_timeout: 40s scrape_configs: - job_name: Docker Scout policy metrics_path: /v1/exporter/org/<ORG>/metrics scheme: https static_configs: - targets: - api.scout.docker.com authorization: type: Bearer credentials_file: /etc/prometheus/token
Start the compose services.
docker compose up -d
This command starts two services: the Prometheus server and Grafana. Prometheus scrapes metrics from the Docker Scout endpoint, and Grafana visualizes the metrics using a pre-configured dashboard.
To stop the demo and clean up any resources created, run:
docker compose down -v
Access to Prometheus
After starting the services, you can access the Prometheus expression browser by visiting http://localhost:9090. The Prometheus server runs in a Docker container and is accessible on port 9090.
After a few seconds, you should see the metrics endpoint as a target in the Prometheus UI at http://localhost:9090/targets.
Viewing the metrics in Grafana
To view the Grafana dashboards, go to
http://localhost:3000/dashboards,
and sign in using the credentials defined in the Docker Compose file (username: admin
, password: grafana
).
The dashboards are pre-configured to visualize the vulnerability and policy metrics scraped by Prometheus.
Datadog
This section describes how to scrape the metrics endpoint using Datadog. Datadog pulls data for monitoring by running a customizable agent that scrapes available endpoints for any exposed metrics. The OpenMetrics and Prometheus checks are included in the agent, so you don’t need to install anything else on your containers or hosts.
This guide assumes you have a Datadog account and a Datadog API Key. Refer to the Datadog documentation to get started.
Configure the Datadog agent
To start collecting the metrics, you will need to edit the agent’s
configuration file for the OpenMetrics check. If you're running the agent as a
container, such file must be mounted at
/etc/datadog-agent/conf.d/openmetrics.d/conf.yaml
.
The following example shows a Datadog configuration that:
- Specifies the OpenMetrics endpoint targeting the
dockerscoutpolicy
Docker organization - A
namespace
that all collected metrics will be prefixed with - The
metrics
you want the agent to scrape (scout_*
) - An
auth_token
section for the Datadog agent to authenticate to the Metrics endpoint, using a Docker PAT as a Bearer token.
instances:
- openmetrics_endpoint: ""https://api.scout.docker.com/v1/exporter/org/dockerscoutpolicy/metrics""
namespace: ""scout-metrics-exporter""
metrics:
- scout_*
auth_token:
reader:
type: file
path: /var/run/secrets/scout-metrics-exporter/token
writer:
type: header
name: Authorization
value: Bearer <TOKEN>
Important
Do not replace the
<TOKEN>
placeholder in the previous configuration example. It must stay as it is. Only make sure the Docker PAT is correctly mounted into the Datadog agent in the specified filesystem path. Save the file asconf.yaml
and restart the agent.
When creating a Datadog agent configuration of your own, make sure to edit the
openmetrics_endpoint
property to target your organization, by replacing
dockerscoutpolicy
with the namespace of your Docker organization.
Datadog sample project
If you don't have a Datadog server set up, you can run a sample project using Docker Compose. The sample includes a Datadog agent, running as a container, that scrapes metrics for a Docker organization enrolled in Docker Scout. This sample project assumes that you have a Datadog account, an API key, and a Datadog site.
Clone the starter template for bootstrapping a Datadog Compose service for scraping the Docker Scout metrics endpoint:
$ git clone git@github.com:dockersamples/scout-metrics-exporter.git $ cd scout-metrics-exporter/datadog
Create a Docker access token and store it in a plain text file at
/datadog/token
under the template directory.token$ echo $DOCKER_PAT > ./token
In the
/datadog/compose.yaml
file, update theDD_API_KEY
andDD_SITE
environment variables with the values for your Datadog deployment.datadog-agent: container_name: datadog-agent image: gcr.io/datadoghq/agent:7 environment: - DD_API_KEY=${DD_API_KEY} # e.g. 1b6b3a42... - DD_SITE=${DD_SITE} # e.g. datadoghq.com - DD_DOGSTATSD_NON_LOCAL_TRAFFIC=true volumes: - /var/run/docker.sock:/var/run/docker.sock:ro - ./conf.yaml:/etc/datadog-agent/conf.d/openmetrics.d/conf.yaml:ro - ./token:/var/run/secrets/scout-metrics-exporter/token:ro
The
volumes
section mounts the Docker socket from the host to the container. This is required to obtain an accurate hostname when running as a container ( more details here).It also mounts the agent's config file and the Docker access token.
Edit the
/datadog/config.yaml
file by replacing the placeholder<ORG>
in theopenmetrics_endpoint
property with the namespace of the Docker organization that you want to collect metrics for.instances: - openmetrics_endpoint: ""https://api.scout.docker.com/v1/exporter/org/<<ORG>>/metrics"" namespace: ""scout-metrics-exporter"" # ...
Start the Compose services.
docker compose up -d
If configured properly, you should see the OpenMetrics check under Running Checks when you run the agent’s status command whose output should look similar to:
openmetrics (4.2.0)
-------------------
Instance ID: openmetrics:scout-prometheus-exporter:6393910f4d92f7c2 [OK]
Configuration Source: file:/etc/datadog-agent/conf.d/openmetrics.d/conf.yaml
Total Runs: 1
Metric Samples: Last Run: 236, Total: 236
Events: Last Run: 0, Total: 0
Service Checks: Last Run: 1, Total: 1
Average Execution Time : 2.537s
Last Execution Date : 2024-05-08 10:41:07 UTC (1715164867000)
Last Successful Execution Date : 2024-05-08 10:41:07 UTC (1715164867000)
For a comprehensive list of options, take a look at this example config file for the generic OpenMetrics check.
Visualizing your data
Once the agent is configured to grab Prometheus metrics, you can use them to build comprehensive Datadog graphs, dashboards, and alerts.
Go into your
Metric summary page
to see the metrics collected from this example. This configuration will collect
all exposed metrics starting with scout_
under the namespace
scout_metrics_exporter
.
The following screenshots show examples of a Datadog dashboard containing graphs about vulnerability and policy compliance for a specific stream.
The reason why the lines in the graphs look flat is due to the own nature of vulnerabilities (they don't change too often) and the short time interval selected in the date picker.
Scrape interval
By default, Prometheus and Datadog scrape metrics at a 15 second interval. Because of the own nature of vulnerability data, the metrics exposed through this API are unlikely to change at a high frequency. For this reason, the metrics endpoint has a 60-minute cache by default, which means a scraping interval of 60 minutes or higher is recommended. If you set the scrape interval to less than 60 minutes, you will see the same data in the metrics for multiple scrapes during that time window.
To change the scrape interval:
- Prometheus: set the
scrape_interval
field in the Prometheus configuration file at the global or job level. - Datadog: set the
min_collection_interval
property in the Datadog agent configuration file, see Datadog documentation.
Revoke an access token
If you suspect that your PAT has been compromised or is no longer needed, you can revoke it at any time. To revoke a PAT, follow the steps in the Create and manage access tokens.
Revoking a PAT immediately invalidates the token, and prevents Prometheus from scraping metrics using that token. You will need to create a new PAT and update the Prometheus configuration to use the new token.",,,
6e36cb0860618a2a19c30817d280726b72689aa60c3df9dfb801d4bd3ec099e3,"Drain a node on the swarm
In earlier steps of the tutorial, all the nodes have been running with Active
availability. The swarm manager can assign tasks to any Active
node, so up to
now all nodes have been available to receive tasks.
Sometimes, such as planned maintenance times, you need to set a node to Drain
availability. Drain
availability prevents a node from receiving new tasks
from the swarm manager. It also means the manager stops tasks running on the
node and launches replica tasks on a node with Active
availability.
Important
Setting a node to
Drain
does not remove standalone containers from that node, such as those created withdocker run
,docker compose up
, or the Docker Engine API. A node's status, includingDrain
, only affects the node's ability to schedule swarm service workloads.
If you haven't already, open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named
manager1
.Verify that all your nodes are actively available.
$ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS 1bcef6utixb0l0ca7gxuivsj0 worker2 Ready Active 38ciaotwjuritcdtn9npbnkuz worker1 Ready Active e216jshn25ckzbvmwlnh5jr3g * manager1 Ready Active Leader
If you aren't still running the
redis
service from the rolling update tutorial, start it now:$ docker service create --replicas 3 --name redis --update-delay 10s redis:7.4.0 c5uo6kdmzpon37mgj9mwglcfw
Run
docker service ps redis
to see how the swarm manager assigned the tasks to different nodes:$ docker service ps redis NAME IMAGE NODE DESIRED STATE CURRENT STATE redis.1.7q92v0nr1hcgts2amcjyqg3pq redis:7.4.0 manager1 Running Running 26 seconds redis.2.7h2l8h3q3wqy5f66hlv9ddmi6 redis:7.4.0 worker1 Running Running 26 seconds redis.3.9bg7cezvedmkgg6c8yzvbhwsd redis:7.4.0 worker2 Running Running 26 seconds
In this case the swarm manager distributed one task to each node. You may see the tasks distributed differently among the nodes in your environment.
Run
docker node update --availability drain <NODE-ID>
to drain a node that had a task assigned to it:$ docker node update --availability drain worker1 worker1
Inspect the node to check its availability:
$ docker node inspect --pretty worker1 ID: 38ciaotwjuritcdtn9npbnkuz Hostname: worker1 Status: State: Ready Availability: Drain ...snip...
The drained node shows
Drain
forAvailability
.Run
docker service ps redis
to see how the swarm manager updated the task assignments for theredis
service:$ docker service ps redis NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR redis.1.7q92v0nr1hcgts2amcjyqg3pq redis:7.4.0 manager1 Running Running 4 minutes redis.2.b4hovzed7id8irg1to42egue8 redis:7.4.0 worker2 Running Running About a minute \_ redis.2.7h2l8h3q3wqy5f66hlv9ddmi6 redis:7.4.0 worker1 Shutdown Shutdown 2 minutes ago redis.3.9bg7cezvedmkgg6c8yzvbhwsd redis:7.4.0 worker2 Running Running 4 minutes
The swarm manager maintains the desired state by ending the task on a node with
Drain
availability and creating a new task on a node withActive
availability.Run
docker node update --availability active <NODE-ID>
to return the drained node to an active state:$ docker node update --availability active worker1 worker1
Inspect the node to see the updated state:
$ docker node inspect --pretty worker1 ID: 38ciaotwjuritcdtn9npbnkuz Hostname: worker1 Status: State: Ready Availability: Active ...snip...
When you set the node back to
Active
availability, it can receive new tasks:- during a service update to scale up
- during a rolling update
- when you set another node to
Drain
availability - when a task fails on another active node
Next steps
Next, you'll learn how to use a Swarm mode routing mesh",,,
499fd1087e7a5bf819279366b37888d646404a1f95ee1fab69494fe11966e19c,"Docker Engine version 28 release notes
Table of contents
This page describes the latest changes, additions, known issues, and fixes for Docker Engine version 28.
For more information about:
- Deprecated and removed features, see Deprecated Engine Features.
- Changes to the Engine API, see Engine API version history.
28.0.1
2025-02-26For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Networking
- Remove dependency on kernel modules
ip_set
,ip_set_hash_net
andnetfilter_xt_set
.- The dependency was introduced in release 28.0.0 but proved too disruptive. The iptables rules using these modules have been replaced. moby/moby#49530
- Allow daemon startup on a host with IPv6 disabled without requiring
--ip6tables=false
. moby/moby#49525 - Fix a bug that was causing containers with
--restart=always
and a published port already in use to restart in a tight loop. moby/moby#49507 - Fix an issue with Swarm ingress, caused by incorrect ordering of iptables rules. moby/moby#49538
- Fix creation of a swarm-scoped network from a
--config-only
network. moby/moby#49521 - Fix
docker network inspect
reporting an IPv6 gateway with CIDR suffix for a newly created network with no specific IPAM config, until a daemon restart. moby/moby#49520 - Improve the error reported when kernel modules
ip_set
,ip_set_hash_net
andnetilter_xt_set
are not available. moby/moby#49524 - Move most of Docker's iptables rules out of the filter-FORWARD chain, so that other applications are free to append rules that must follow Docker's rules. moby/moby#49518
- Update
--help
output and man page lo state which options only apply to the default bridge network. moby/moby#49522
Bug fixes and enhancements
- Fix
docker context create
always returning an error when using the""skip-tls-verify""
option. docker/cli#5850 - Fix shell completion suggesting IDs instead of names for services and nodes. docker/cli#5848
- Fix unintentionally printing exit status to standard error output when
docker exec/run
returns a non-zero status. docker/cli#5854 - Fix regression
protocol ""tcp"" is not supported by the RootlessKit port driver ""slirp4netns""
. moby/moby#49514 - containerd image store: Fix
docker inspect
not being able to show multi-platform images with missing layers for all platforms. moby/moby#49533 - containerd image store: Fix
docker images --tree
reporting wrong content size. moby/moby#49535 - Fix compilation on i386 moby/moby#49526
Packaging updates
- Update
github.com/go-jose/go-jose/v4
to v4.0.5 to address GHSA-c6gw-w398-hv78 / CVE-2025-27144. docker/cli#5867 - Update Buildx to v0.21.1. docker/docker-ce-packaging#1167
- Update Compose to v2.33.1. docker/docker-ce-packaging#1168
API
- containerd image store: Fix
GET /images/json?manifests=1
not fillingManifests
for index-only images moby/moby#49533 - containerd image store: Fix
GET /images/json and /images/<name>/json
Size.Content
field including the size of content that's not available locally moby/moby#49535
28.0.0
2025-02-19For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 28.0.0 milestone
- moby/moby, 28.0.0 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
New
- Add ability to mount an image inside a container via
--mount type=image
. moby/moby#48798- You can also specify
--mount type=image,image-subpath=[subpath],...
option to mount a specific path from the image. docker/cli#5755
- You can also specify
docker images --tree
now shows metadata badges docker/cli#5744docker load
,docker save
, anddocker history
now support a--platform
flag allowing you to choose a specific platform for single-platform operations on multi-platform images. docker/cli#5331- Add
OOMScoreAdj
todocker service create
anddocker stack
. docker/cli#5145 docker buildx prune
now supportsreserved-space
,max-used-space
,min-free-space
andkeep-bytes
filters. moby/moby#48720- Windows: Add support for running containerd as a child process of the daemon, instead of using a system-installed containerd. moby/moby#47955
Networking
- The
docker-proxy
binary has been updated, older versions will not work with the updateddockerd
. moby/moby#48132- Close a window in which the userland proxy (
docker-proxy
) could accept TCP connections, that would then fail afteriptables
NAT rules were set up. - The executable
rootlesskit-docker-proxy
is no longer used, it has been removed from the build and distribution.
- Close a window in which the userland proxy (
- DNS nameservers read from the host's
/etc/resolv.conf
are now always accessed from the host's network namespace. moby/moby#48290- When the host's
/etc/resolv.conf
contains no nameservers and there are no--dns
overrides, Google's DNS servers are no longer used, apart from by the default bridge network and in build containers.
- When the host's
- Container interfaces in bridge and macvlan networks now use randomly generated MAC addresses.
moby/moby#48808
- Gratuitous ARP / Neighbour Advertisement messages will be sent when the interfaces are started so that, when IP addresses are reused, they're associated with the newly generated MAC address.
- IPv6 addresses in the default bridge network are now IPAM-assigned, rather than being derived from the MAC address.
- The deprecated OCI
prestart
hook is now only used by build containers. For other containers, network interfaces are added to the network namespace after task creation is complete, before the container task is started. moby/moby#47406 - Add a new
gw-priority
option todocker run
,docker container create
, anddocker network connect
. This option will be used by the Engine to determine which network provides the default gateway for a container. Ondocker run
, this option is only available through the extended--network
syntax. docker/cli#5664 - Add a new netlabel
com.docker.network.endpoint.ifname
to customize the interface name used when connecting a container to a network. It's supported by all built-in network drivers on Linux. moby/moby#49155- When a container is created with multiple networks specified, there's no guarantee on the order networks will be connected to the container. So, if a custom interface name uses the same prefix as the auto-generated names, for example
eth
, the container might fail to start. - The recommended practice is to use a different prefix, for example
en0
, or a numerical suffix high enough to never collide, for exampleeth100
. - This label can be specified on
docker network connect
via the--driver-opt
flag, for exampledocker network connect --driver-opt=com.docker.network.endpoint.ifname=foobar …
. - Or via the long-form
--network
flag ondocker run
, for exampledocker run --network=name=bridge,driver-opt=com.docker.network.endpoint.ifname=foobar …
- When a container is created with multiple networks specified, there's no guarantee on the order networks will be connected to the container. So, if a custom interface name uses the same prefix as the auto-generated names, for example
- If a custom network driver reports capability
GwAllocChecker
then, before a network is created, it will get aGwAllocCheckerRequest
with the network's options. The custom driver may then reply that no gateway IP address should be allocated. moby/moby#49372
Port publishing in bridge networks
dockerd
now requiresipset
support in the Linux kernel. moby/moby#48596- The
iptables
andip6tables
rules used to implement port publishing and network isolation have been extensively modified. This enables some of the following functional changes, and is a first step in refactoring to enable nativenftables
support in a future release. moby/moby#48815 - If it becomes necessary to downgrade to an earlier version of the daemon, some manual cleanup of the new rules will be necessary. The simplest and surest approach is to reboot the host, or use
iptables -F
andip6tables -F
to flush all existingiptables
rules from thefilter
table before starting the older version of the daemon. When that is not possible, run the following commands as root:iptables -D FORWARD -m set --match-set docker-ext-bridges-v4 dst -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT; ip6tables -D FORWARD -m set --match-set docker-ext-bridges-v6 dst -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
iptables -D FORWARD -m set --match-set docker-ext-bridges-v4 dst -j DOCKER; ip6tables -D FORWARD -m set --match-set docker-ext-bridges-v6 dst -j DOCKER
- If you were previously running with the iptables filter-FORWARD policy set to
ACCEPT
and need to restore access to unpublished ports, also delete per-bridge-network rules from theDOCKER
chains. For example,iptables -D DOCKER ! -i docker0 -o docker0 -j DROP
.
- The
- Fix a security issue that was allowing remote hosts to connect directly to a container on its published ports. moby/moby#49325
- Fix a security issue that was allowing neighbor hosts to connect to ports mapped on a loopback address. moby/moby#49325
- Fix an issue that prevented port publishing to link-local addresses. moby/moby#48570
- UDP ports published by a container are now reliably accessible by containers on other networks, via the host's public IP address. moby/moby#48571
- Docker will now only set the
ip6tables
policy for theFORWARD
chain in thefilter
table toDROP
if it enables IP forwarding on the host itself (sysctlsnet.ipv6.conf.all.forwarding
andnet.ipv6.conf.default.forwarding
). This is now aligned with existing IPv4 behaviour. moby/moby#48594- If IPv6 forwarding is enabled on your host, but you were depending on Docker to set the ip6tables filter-FORWARD policy to
DROP
, you may need to update your host's configuration to make sure it is secure.
- If IPv6 forwarding is enabled on your host, but you were depending on Docker to set the ip6tables filter-FORWARD policy to
- Direct routed access to container ports that are not exposed using
p
/-publish
is now blocked in theDOCKER
iptables chain. moby/moby#48724- If the default iptables filter-FORWARD policy was previously left at
ACCEPT
on your host, and direct routed access to a container's unpublished ports from a remote host is still required, options are:- Publish the ports you need.
- Use the new
gateway_mode_ipv[46]=nat-unprotected
, described below.
- Container ports published to host addresses will continue to be accessible via those host addresses, using NAT or the userland proxy.
- Unpublished container ports continue to be directly accessible from the Docker host via the container's IP address.
- If the default iptables filter-FORWARD policy was previously left at
- Networks created with
gateway_mode_ipv[46]=routed
are now accessible from other bridge networks running on the same Docker host, as well as from outside the host. moby/moby#48596 - Bridge driver options
com.docker.network.bridge.gateway_mode_ipv4
andcom.docker.network.bridge.gateway_mode_ipv6
now accept modenat-unprotected
. moby/moby#48597nat-unprotected
is similar to the defaultnat
mode, but no per port/protocol rules are set up. This means any port on a container can be accessed by direct-routing from a remote host.
- Bridge driver options
com.docker.network.bridge.gateway_mode_ipv4
andcom.docker.network.bridge.gateway_mode_ipv6
now accept modeisolated
, when the network is alsointernal
. moby/moby#49262- An address is normally assigned to the bridge device in an
internal
network. So, processes on the Docker host can access the network, and containers in the network can access host services listening on that bridge address (including services listening on ""any"" host address,0.0.0.0
or::
). - An
internal
bridge network created with gateway modeisolated
does not have an address on the Docker host.
- An address is normally assigned to the bridge device in an
- When a port mapping includes a host IP address or port number that cannot be used because NAT from the host is disabled using
--gateway_mode_ipv[46]
, container creation will no longer fail. The unused fields may be needed if the gateway endpoint changes when networks are connected or disconnected. A message about the unused fields will be logged. moby/moby#48575 - Do not create iptables nat-POSTROUTING masquerade rules for a container's own published ports, when the userland proxy is enabled. moby/moby#48854
IPv6
- Add
docker network create
option--ipv4
. To disable IPv4 address assignment for a network, usedocker network create --ipv4=false [...]
. docker/cli#5599 - Daemon option
--ipv6
(""ipv6"": true
indaemon.json
) can now be used withoutfixed-cidr-v6
. moby/moby#48319 - IPAM now handles subnets bigger than ""/64"". moby/moby#49223
- Duplicate address detection (DAD) is now disabled for addresses assigned to the bridges belonging to bridge networks. moby/moby#48609
- Modifications to
host-gateway
, for compatibility with IPv6-only networks. moby/moby#48807- When special value
host-gateway
is used in an--add-host
option in place of an address, it's replaced by an address on the Docker host to make it possible to refer to the host by name. The address used belongs to the default bridge (normallydocker0
). Until now it's always been an IPv4 address, because all containers on bridge networks had IPv4 addresses. - Now, if IPv6 is enabled on the default bridge network,
/etc/hosts
entries will be created for IPv4 and IPv6 addresses. So, a container that's only connected to IPv6-only networks can access the host by name. - The
--host-gateway-ip
option overrides the address used to replacehost-gateway
. Two of these options are now allowed on the command line, for one IPv4 gateway and one IPv6. - In the
daemon.json
file, to provide two addresses, use""host-gateway-ips""
. For example,""host-gateway-ips"": [""192.0.2.1"", ""2001:db8::1111""]
.
- When special value
Bug fixes and enhancements
- Add IPv6 loopback address as an insecure registry by default. moby/moby#48540
- Add support for Cobra-generated completion scripts for
dockerd
. moby/moby#49339 - Fix DNS queries failing when containers are launched via
systemd
auto-start on boot moby/moby#48812 - Fix Docker Swarm mode ignoring
volume.subpath
docker/cli#5833 - Fix
docker export
continuing the export after the operation is canceled. moby/moby#49265 - Fix
docker export
not releasing the container's writable layer after a failure. moby/moby#48517 - Fix
docker images --tree
unnecessary truncating long image names when multiple names are available docker/cli#5757 - Fix a bug where a container with a name matching another container's ID is not restored on daemon startup. moby/moby#48669
- Fix an issue preventing some IPv6 addresses shown by
docker ps
to be properly bracketed docker/cli#5468 - Fix bug preventing image pulls from being cancelled during
docker run
. docker/cli#5645 - Fix error-handling when running the daemon as a Windows service to prevent unclean exits. moby/moby#48518
- Fix issue causing output of
docker run
to be inconsistent when using--attach stdout
or--attach stderr
versusstdin
.docker run --attach stdin
now exits if the container exits. docker/cli#5662 - Fix rootless Docker setup with
subid
backed by NSS modules. moby/moby#49036 - Generated completion scripts from the CLI now show descriptions next to each command/flag suggestion. docker/cli#5756
- IPv6 addresses shown by
docker ps
in port bindings are now bracketed docker/cli#5363 - Implement the ports validation method for Compose docker/cli#5524
- Improve error-output for invalid flags on the command line. docker/cli#5233
- Improve errors when failing to start a container using anther container's network namespace. moby/moby#49367
- Improve handling of invalid API errors that could result in an empty error message being shown. moby/moby#49373
- Improve output and consistency for unknown (sub)commands and invalid arguments docker/cli#5234
- Improve validation of
exec-opts
in daemon configuration. moby/moby#48979 - Update the handling of the
--gpus=0
flag to be consistent with the NVIDIA Container Runtime. moby/moby#48482 client.ContainerCreate
now normalizesCapAdd
andCapDrop
fields inHostConfig
to their canonical form. moby/moby#48551docker image save
now produces stable timestamps. moby/moby#48611docker inspect
now lets you inspect Swarm configs docker/cli#5573- containerd image store: Add support for
Extracting
layer status indocker pull
. moby/moby#49064 - containerd image store: Fix
commit
,import
, andbuild
not preserving a replaced image as a dangling image. moby/moby#48316 - containerd image store: Make
docker load --platform
return an error when the requested platform isn't loaded. moby/moby#48718 - Fix validation of
--link
option. docker/cli#5739 - Add validation of network-diagnostic-port daemon configuration option. moby/moby#49305
- Unless explicitly configured, an IP address is no longer reserved for a gateway in cases where it is not required. Namely, “internal” bridge networks with option
com.docker.network.bridge.inhibit_ipv4
,ipvlan
ormacvlan
networks with no parent interface, and L3 IPvlan modes. moby/moby#49261 - If a custom network driver reports capability
GwAllocChecker
then, before a network is created, it will get aGwAllocCheckerRequest
with the network's options. The custom driver may then reply that no gateway IP address should be allocated. moby/moby#49372 - Fixed an issue that meant a container could not be attached to an L3 IPvlan at the same time as other network types. moby/moby#49130
- Remove the correct
/etc/hosts
entries when disconnecting a container from a network. moby/moby#48857 - Fix duplicate network disconnect events. moby/moby#48800
- Resolve issues related to changing
fixed-cidr
fordocker0
, and inferring configuration from a user-managed default bridge (--bridge
). moby/moby#48319 - Remove feature flag
windows-dns-proxy
, introduced in release 26.1.0 to control forwarding to external DNS resolvers from Windows containers, to makenslookup
work. It was enabled by default in release 27.0.0. moby/moby#48738 - Remove an
iptables
mangle rule for checksumming SCTP. The rule can be re-enabled by settingDOCKER_IPTABLES_SCTP_CHECKSUM=1
in the daemon's environment. This override will be removed in a future release. moby/moby#48149 - Faster connection to bridge networks, in most cases. moby/moby#49302
Packaging updates
- Update Go runtime to 1.23.6. docker/cli#5795, moby/moby#49393, docker/docker-ce-packaging#1161
- Update
runc
to v1.2.5 (static binaries only). moby/moby#49464 - Update containerd to v1.7.25. moby/moby#49252
- Update BuildKit to v0.20.0. moby/moby#49495
- Update Buildx to v0.21.0. docker/docker-ce-packaging#1166
- Update Compose to v2.32.4. docker/docker-ce-packaging#1143
- The canonical source for the
dockerd(8)
man page has been moved back to themoby/moby
repository itself. moby/moby#48298
Go SDK
- Improve validation of empty object IDs. The client now returns an ""Invalid Parameter"" error when trying to use an empty ID or name. This changes the error returned by some ""Inspect"" functions from a ""Not found"" error to an ""Invalid Parameter"". moby/moby#49381
Client.ImageBuild()
now omits default values from the API request's query string. moby/moby#48651api/types/container
: MergeStats
andStatsResponse
moby/moby#49287client.WithVersion
: Strip v-prefix when setting API version moby/moby#49352client
: AddWithTraceOptions
allowing to specify custom OTe1 trace options. moby/moby#49415client
: AddHijackDialer
interface. moby/moby#49388client
: AddSwarmManagementAPIClient
interface to describe all API client methods related to Swarm-specific objects. moby/moby#49388client
: AddWithTraceOptions
allowing to specify custom OTel trace options. moby/moby#49415client
:ImageHistory
,ImageLoad
andImageSave
now use variadic functional options moby/moby#49466pkg/containerfs
: Move to internal moby/moby#48097pkg/reexec
: Can now be used on platforms other than Linux, Windows, macOS and FreeBSD moby/moby#49118api/types/container
: introduceCommitResponse
type. This is currently an alias forIDResponse
, but may become a distinct type in a future release. moby/moby#49444api/types/container
: introduceExecCreateResponse
type. This is currently an alias forIDResponse
, but may become a distinct type in a future release. moby/moby#49444
API
- Update API version to v1.48 moby/moby#48476
GET /images/{name}/json
response now returns theManifests
field containing information about the sub-manifests contained in the image index. This includes things like platform-specific manifests and build attestations. moby/moby#48264POST /containers/create
now supportsMount
of typeimage
for mounting an image inside a container. moby/moby#48798GET /images/{name}/history
now supports aplatform
parameter (JSON encoded OCI Platform type) that lets you specify a platform to show the history of. moby/moby#48295POST /images/{name}/load
andGET /images/{name}/get
now supports aplatform
parameter (JSON encoded OCI Platform type) that lets you specify a platform to load/save. Not passing this parameter results in loading/saving the full multi-platform image. moby/moby#48295- Improve errors for invalid width/height on container resize and exec resize moby/moby#48679
- The
POST /containers/create
endpoint now includes a warning in the response when setting the container-wideVolumeDriver
option in combination with volumes defined throughMounts
because theVolumeDriver
option has no effect on those volumes. This warning was previously generated by the CLI. moby/moby#48789 - containerd image store:
GET /images/json
andGET /images/{name}/json
responses now includesDescriptor
field, which contains an OCI descriptor of the image target. The new field is only populated if the daemon provides a multi-platform image store. moby/moby#48894 - containerd image store:
GET /containers/{name}/json
now returns anImageManifestDescriptor
field containing the OCI descriptor of the platform-specific image manifest of the image that was used to create the container. moby/moby#48855 - Add debug endpoints (
GET /debug/vars
,GET /debug/pprof/
,GET /debug/pprof/cmdline
,GET /debug/pprof/profile
,GET /debug/pprof/symbol
,GET /debug/pprof/trace
,GET /debug/pprof/{name}
) are now also accessible through the versioned-API paths (/v<API-version>/<endpoint>
). moby/moby#49051 - Fix API returning a
500
status code instead of400
for validation errors. moby/moby#49217 - Fix status codes for archive endpoints
HEAD /containers/{name:.*}/archive
,GET /containers/{name:.*}/archive
,PUT /containers/{name:.*}/archive
returning a500
status instead of a400
status. moby/moby#49219 POST /containers/create
now accepts awritable-cgroups=true
option inHostConfig.SecurityOpt
to mount the container's cgroups writable. This provides a more granular approach thanHostConfig.Privileged
. moby/moby#48828POST /build/prune
renameskeep-bytes
toreserved-space
and now supports additional prune parametersmax-used-space
andmin-free-space
. moby/moby#48720POST /networks/create
now has anEnableIPv4
field. Setting it tofalse
disables IPv4 IPAM for the network. moby/moby#48271GET /networks/{id}
now returns anEnableIPv4
field showing whether the network has IPv4 IPAM enabled. moby/moby#48271- User-defined bridge networks require either IPv4 or IPv6 address assignment to be enabled. IPv4 cannot be disabled for the default bridge network (
docker0
). moby/moby#48323 macvlan
andipvlan
networks can be created with address assignment disabled for IPv4, IPv6, or both address families. moby/moby#48299- IPv4 cannot be disabled for Windows or Swarm networks. moby/moby#48278
- Add a way to specify which network should provide the default gateway for a container.
moby/moby#48936
POST /networks/{id}/connect
andPOST /containers/create
now accept aGwPriority
field inEndpointsConfig
. This value is used to determine which network endpoint provides the default gateway for the container. The endpoint with the highest priority is selected. If multiple endpoints have the same priority, endpoints are sorted lexicographically by their network name, and the one that sorts first is picked. moby/moby#48746GET /containers/json
now returns aGwPriority
field inNetworkSettings
for each network endpoint. TheGwPriority
field is used by the CLI’s newgw-priority
option fordocker run
anddocker network connect
. moby/moby#48746
- Settings for
eth0
in--sysctl
options are no longer automatically migrated to the network endpoint. moby/moby#48746- For example, in the Docker CLI,
docker run --network mynet --sysctl net.ipv4.conf.eth0.log_martians=1 ...
is rejected. Instead, you must usedocker run --network name=mynet,driver-opt=com.docker.network.endpoint.sysctls=net.ipv4.conf.IFNAME.log_martians=1 ...
- For example, in the Docker CLI,
GET /containers/json
now returns anImageManifestDescriptor
field matching the same field in/containers/{name}/json
. This field is only populated if the daemon provides a multi-platform image store. moby/moby#49407
Removed
- The Fluent logger option
fluentd-async-connect
has been deprecated in v20.10 and is now removed. moby/moby#46114 - The
--time
option ondocker stop
anddocker restart
is deprecated and renamed to--timeout
. docker/cli#5485 - Go-SDK:
pkg/ioutils
: RemoveNewReaderErrWrapper
as it was never used. moby/moby#49258 - Go-SDK:
pkg/ioutils
: Remove deprecatedBytesPipe
,NewBytesPipe
,ErrClosed
,WriteCounter
,NewWriteCounter
,NewReaderErrWrapper
,NopFlusher
. moby/moby#49245 - Go-SDK:
pkg/ioutils
: Remove deprecatedNopWriter
andNopWriteCloser
. moby/moby#49256 - Go-SDK:
pkg/sysinfo
: Remove deprecated NumCPU. moby/moby#49242 - Go-SDK: Remove
pkg/broadcaster
, as it was only used internally moby/moby#49172 - Go-SDK: Remove deprecated
cli.Errors
type docker/cli#5549 - Remove
pkg/ioutils.ReadCloserWrapper
, as it was only used in tests. moby/moby#49237 - Remove deprecated
api-cors-header
config parameter and thedockerd
--api-cors-header
option moby/moby#48209 - Remove deprecated
APIEndpoint.Version
field,APIVersion
type, andAPIVersion1
andAPIVersion2
consts. moby/moby#49004 - Remove deprecated
api-cors-header
config parameter and the Docker daemon's--api-cors-header
option. docker/cli#5437 - Remove deprecated
pkg/directory
package moby/moby#48779 - Remove deprecated
pkg/dmsg.Dmesg()
moby/moby#48109 - Remove deprecated image/spec package, which was moved to a separate module (
github.com/moby/docker-image-spec
) moby/moby#48460 - Remove migration code and errors for the deprecated
logentries
logging driver. moby/moby#48891 - Remove support for deprecated external graph-driver plugins. moby/moby#48072
api/types
: Remove deprecatedcontainer.ContainerNode
andContainerJSONBase.Node
field. moby/moby#48107api/types
: Remove deprecated aliases:ImagesPruneReport
,VolumesPruneReport
,NetworkCreateRequest
,NetworkCreate
,NetworkListOptions
,NetworkCreateResponse
,NetworkInspectOptions
,NetworkConnect
,NetworkDisconnect
,EndpointResource
,NetworkResource
,NetworksPruneReport
,ExecConfig
,ExecStartCheck
,ContainerExecInspect
,ContainersPruneReport
,ContainerPathStat
,CopyToContainerOptions
,ContainerStats
,ImageSearchOptions
,ImageImportSource
,ImageLoadResponse
,ContainerNode
. moby/moby#48107libnetwork/iptables
: Remove deprecatedIPV
,Iptables
,IP6Tables
andPassthrough()
. moby/moby#49121pkg/archive
: Remove deprecatedCanonicalTarNameForPath
,NewTempArchive
,TempArchive
moby/moby#48708pkg/fileutils
: Remove deprecatedGetTotalUsedFds
moby/moby#49210pkg/ioutils
: RemoveOnEOFReader
, which was only used internally moby/moby#49170pkg/longpath
: Remove deprecatedPrefix
constant. moby/moby#48779pkg/stringid
: Remove deprecatedIsShortID
andValidateID
functions moby/moby#48705runconfig/opts
: Remove deprecatedConvertKVStringsToMap
moby/moby#48102runconfig
: Remove deprecatedContainerConfigWrapper
,SetDefaultNetModeIfBlank
,DefaultDaemonNetworkMode
,IsPreDefinedNetwork
moby/moby#48102container
: Remove deprecatedErrNameReserved
,ErrNameNotReserved
. moby/moby#48728- Remove
Daemon.ContainerInspectCurrent()
method and changeDaemon.ContainerInspect()
signature to accept abackend.ContainerInspectOptions
struct moby/moby#48672 - Remove deprecated
Daemon.Exists()
andDaemon.IsPaused()
methods. moby/moby#48723
Deprecations
- API: The
BridgeNfIptables
andBridgeNfIp6tables
fields in theGET /info
response are now always befalse
and will be omitted in API v1.49. The netfilter module is now loaded on-demand, and no longer during daemon startup, making these fields obsolete. moby/moby#49114 - API: The
error
andprogress
fields in streaming responses for endpoints that return a JSON progress response, such asPOST /images/create
,POST /images/{name}/push
, andPOST /build
are deprecated. moby/moby#49447- Users should use the information in the
errorDetail
andprogressDetail
fields instead. - These fields were marked deprecated in API v1.4 (docker v0.6.0) and API v1.8 (docker v0.7.1) respectively, but still returned.
- These fields will be left empty or will be omitted in a future API version.
- Users should use the information in the
- Deprecate
Daemon.Register()
. This function is unused and will be removed in the next release. moby/moby#48702 - Deprecate
client.ImageInspectWithRaw
function in favor of the newclient.ImageInspect
. moby/moby#48264 - Deprecate
daemon/config.Config.ValidatePlatformConfig()
. This method was used as helper forconfig.Validate
, which should be used instead. moby/moby#48985 - Deprecate
pkg/reexec
. This package is deprecated and moved to a separate module. Usegithub.com/moby/sys/reexec
instead. moby/moby#49129 - Deprecate configuration for pushing non-distributable artifacts docker/cli#5724
- Deprecate the
--allow-nondistributable-artifacts
daemon flag and correspondingallow-nondistributable-artifacts
field indaemon.json
. Setting either option will no longer take an effect, but a deprecation warning log is added. moby/moby#49065 - Deprecate the
RegistryConfig.AllowNondistributableArtifactsCIDRs
andRegistryConfig.AllowNondistributableArtifactsHostnames
fields in theGET /info
API response. For API version v1.48 and older, the fields are still included in the response, but alwaysnull
. In API version v1.49 and later, the field will be omitted entirely. moby/moby#49065 - Go-SDK: Deprecate
registry.ServiceOptions.AllowNondistributableArtifacts
field. moby/moby#49065 - Go-SDK: The
BridgeNfIptables
,BridgeNfIp6tables
fields inapi/types/system.Info
andBridgeNFCallIPTablesDisabled
,BridgeNFCallIP6TablesDisabled
fields inpkg/sysinfo.SysInfo
are deprecated and will be removed in the next release. moby/moby#49114 - Go-SDK:
client
: DeprecateCommonAPIClient
interface in favor of theAPIClient
interface. TheCommonAPIClient
will be changed to an alias forAPIClient
in the next release, and removed in the release after. moby/moby#49388 - Go-SDK:
client
: DeprecateErrorConnectionFailed
helper. This function was only used internally, and will be removed in the next release. moby/moby#49389 - Go-SDK:
pkg/ioutils
: DeprecateNewAtomicFileWriter
,AtomicWriteFile
,AtomicWriteSet
,NewAtomicWriteSet
in favor ofpkg/atomicwriter
equivalents. moby/moby#49171 - Go-SDK:
pkg/sysinfo
: DeprecateNumCPU
. This utility has the same behavior asruntime.NumCPU
. moby/moby#49241 - Go-SDK:
pkg/system
: DeprecateMkdirAll
. This function provided custom handling for Windows GUID volume paths. Handling for such paths is now supported by Go standard library in go1.22 and newer, and this function is now an alias foros.MkdirAll
, which should be used instead. This alias will be removed in the next release. moby/moby#49162 - Go-SDK: Deprecate
pkg/parsers.ParseKeyValueOpt
. moby/moby#49177 - Go-SDK: Deprecate
pkg/parsers.ParseUintListMaximum
,pkg/parsers.ParseUintList
. These utilities were only used internally and will be removed in the next release. moby/moby#49222 - Go-SDK: Deprecate
api/type.IDResponse
in favor ofcontainer.CommitResponse
andcontainer.ExecCreateResponse
, which are currently an alias, but may become distinct types in a future release. This type will be removed in the next release. moby/moby#49446 - Go-SDK: Deprecate
api/types/container.ContainerUpdateOKBody
in favor ofUpdateResponse
. This type will be removed in the next release. moby/moby#49442 - Go-SDK: Deprecate
api/types/container.ContainerTopOKBody
in favor ofTopResponse
. This type will be removed in the next release. moby/moby#49442 - Go-SDK:
pkg/jsonmessage
: Fix deprecation ofProgressMessage
,ErrorMessage
, which were deprecated in Docker v0.6.0 and v0.7.1 respectively. moby/moby#49447 - Move
GraphDriverData
fromapi/types
toapi/types/storage
. The old type is deprecated and will be removed in the next release. moby/moby#48108 - Move
RequestPrivilegeFunc
fromapi/types
toapi/types/registry
. The old type is deprecated and will be removed in the next release. moby/moby#48119 - Move from
api/types
toapi/types/container
-NetworkSettings
,NetworkSettingsBase
,DefaultNetworkSettings
,SummaryNetworkSettings
,Health
,HealthcheckResult
,NoHealthcheck
,Starting
,Healthy
, andUnhealthy
constants,MountPoint
,Port
,ContainerState
,Container
,ContainerJSONBase
,ContainerJSON
,ContainerNode
. The old types are deprecated and will be removed in the next release. moby/moby#48108 - Move from
api/types
toapi/types/image
-ImageInspect
,RootFS
. The old types are deprecated and will be removed in the next release. moby/moby#48108 ContainerdCommit.Expected
,RuncCommit.Expected
, andInitCommit.Expected
fields in theGET /info
endpoint are deprecated and will be omitted in API v1.49. moby/moby#48478api/types/registry
: DeprecateServiceConfig.AllowNondistributableArtifactsCIDRs
andServiceConfig.AllowNondistributableArtifactsHostnames
fields. These fields will be removed in the next release. moby/moby#49065api/types/system/Commit.Expected
field is deprecated and should no longer be used. moby/moby#48478daemon/graphdriver
: DeprecateGetDriver()
moby/moby#48079libnetwork/iptables
: DeprecatePassthrough
. This function was only used internally, and will be removed in the next release. moby/moby#49115pkg/directory.Size()
function is deprecated, and will be removed in the next release. moby/moby#48057registry
: DeprecateAPIEndpoint.TrimHostName
; hostname is now trimmed unconditionally for remote names. This field will be removed in the next release. moby/moby#49005allow-nondistributable-artifacts
field indaemon.json
. Setting either option will no longer take effect, but a deprecation warning log is added to raise awareness about the deprecation. This warning is planned to become an error in the next release. moby/moby#49065",,,
c968ffa7dd83f4e5fa538d75f8dff883debec5064ce46106411a562b5211cb08,"Docker Build Cloud setup
Before you can start using Docker Build Cloud, you must add the builder to your local environment.
Prerequisites
To get started with Docker Build Cloud, you need to:
- Download and install Docker Desktop version 4.26.0 or later.
- Sign up for a Docker Build Cloud subscription in the Docker Build Cloud Dashboard.
Use Docker Build Cloud without Docker Desktop
To use Docker Build Cloud without Docker Desktop, you must download and install
a version of Buildx with support for Docker Build Cloud (the cloud
driver).
You can find compatible Buildx binaries on the releases page of
this repository.
If you plan on building with Docker Build Cloud using the docker compose build
command, you also need a version of Docker Compose that supports Docker
Build Cloud. You can find compatible Docker Compose binaries on the releases
page of
this repository.
Steps
You can add a cloud builder using the CLI, with the docker buildx create
command, or using the Docker Desktop settings GUI.
Sign in to your Docker account.
$ docker login
Add the cloud builder endpoint.
$ docker buildx create --driver cloud <ORG>/<BUILDER_NAME>
Replace
ORG
with the Docker Hub namespace of your Docker organization.
This creates a builder named cloud-ORG-BUILDER_NAME
.
Sign in to your Docker account using the Sign in button in Docker Desktop.
Open the Docker Desktop settings and navigate to the Builders tab.
Under Available builders, select Connect to builder.
The builder has native support for the linux/amd64
and linux/arm64
architectures. This gives you a high-performance build cluster for building
multi-platform images natively.
Firewall configuration
To use Docker Build Cloud behind a firewall, ensure that your firewall allows traffic to the following addresses:
What's next
- See Building with Docker Build Cloud for examples on how to use Docker Build Cloud.
- See Use Docker Build Cloud in CI for examples on how to use Docker Build Cloud with CI systems.",,,
b036f10afce96525f339f19b2791e653cc8a1dc5adf4936692f482d7d5d940a3,"Scale your subscription
Note
Owners of legacy Docker subscription plans must upgrade their subscription to a new Docker subscription plan in order to scale their subscription.
Legacy Docker plans apply to Docker subscribers who last purchased or renewed their subscription before December 10, 2024. These subscribers will keep their current plan and pricing until their next renewal date that falls on or after December 10, 2024. To see purchase or renewal history, view your billing history. For more details about legacy after December 10, 2024. For more details about legacy subscriptions, see Announcing Upgraded Docker Plans.
Docker subscriptions let you scale your consumption as your needs evolve. Except for legacy Docker subscription plans, all paid Docker subscriptions come with access to Docker Hub, Docker Build Cloud, and Testcontainers Cloud with a base amount of consumption. See Docker subscriptions and features to learn how much base consumption comes with each subscription. You can scale your consumption at any time during your subscription period. All purchased consumption expires at the end of your subscription term.
You can scale consumption for the following:
- Docker Build Cloud build minutes
- Docker Testcontainers Cloud runtime minutes
To better understand your needs, you can view your consumption at any time. For more details, see View Docker product usage.
Add Docker Build Cloud build minutes
You can pre-purchase Docker Build Cloud build minutes in the Docker Build Cloud Dashboard:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- On the plans and usage page, select View build minutes. This will launch the Docker Build Cloud settings page.
- Select Add minutes.
- Select your additional minute amount, then Continue to payment.
- Enter your payment details and billing address.
- Review your order and select Pay.
Your additional minutes will now display on the Build minutes page.
Add Docker Testcontainers Cloud runtime minutes
You can pre-purchase Testcontainers Cloud runtime minutes by contacting sales. In addition to pre-purchase, you are able to use as many minutes as you need on-demand. The usage will be billed at the end of each monthly billing cycle. On-demand usage is billed at a higher rate than pre-purchased capacity. To avoid on-demand charges, pre-purchase additional minutes.",,,
fb56c08b0c81ec809672224f1b55be1d8159ce4c76d496a27ce6b546fb1e52cc,"Software artifacts on Docker Hub
You can use Docker Hub to store any kind of software artifact, not just container images. A software artifact is any item produced during the software development process that contributes to the creation, maintenance, or understanding of the software. Docker Hub supports OCI artifacts by leveraging the config property on the image manifest.
What are OCI artifacts?
OCI artifacts are any arbitrary files related to a software application. Some examples include:
- Helm charts
- Software Bill of Materials (SBOM)
- Digital signatures
- Provenance data
- Attestations
- Vulnerability reports
Docker Hub supporting OCI artifacts means you can use one repository for storing and distributing container images as well as other assets.
A common use case for OCI artifacts is Helm charts. Helm charts is a packaging format that defines a Kubernetes deployment for an application. Since Kubernetes is a popular runtime for containers, it makes sense to host application images and deployment templates all in one place.
Using OCI artifacts with Docker Hub
You manage OCI artifacts on Docker Hub in a similar way you would container images.
Pushing and pulling OCI artifacts to and from a registry is done using a registry client. ORAS CLI is a command-line tool that provides the capability of managing OCI artifacts in a registry. If you use Helm charts, the Helm CLI provides built-in functionality for pushing and pulling charts to and from a registry.
Registry clients invoke HTTP requests to the Docker Hub registry API. The registry API conforms to a standard protocol defined in the OCI distribution specification.
Examples
This section shows some examples on using OCI artifacts with Docker Hub.
Push a Helm chart
The following procedure shows how to push a Helm chart as an OCI artifact to Docker Hub.
Prerequisites:
- Helm version 3.0.0 or later
Steps:
Create a new Helm chart
$ helm create demo
This command generates a boilerplate template chart.
Package the Helm chart into a tarball.
$ helm package demo Successfully packaged chart and saved it to: /Users/hubuser/demo-0.1.0.tgz
Sign in to Docker Hub with Helm, using your Docker credentials.
$ helm registry login registry-1.docker.io -u hubuser
Push the chart to a Docker Hub repository.
$ helm push demo-0.1.0.tgz oci://registry-1.docker.io/docker
This uploads the Helm chart tarball to a
demo
repository in thedocker
namespace.Go to the repository page on Docker Hub. The Tags section of the page shows the Helm chart tag.
Select the tag name to go to the page for that tag.
The page lists a few useful commands for working with Helm charts.
Push a volume
The following procedure shows how to push container volume as an OCI artifact to Docker Hub.
Prerequisites:
- ORAS CLI version 0.15 or later
Steps:
Create a dummy file to use as volume content.
$ touch myvolume.txt
Sign in to Docker Hub using the ORAS CLI.
$ oras login -u hubuser registry-1.docker.io
Push the file to Docker Hub.
$ oras push registry-1.docker.io/docker/demo:0.0.1 \ --artifact-type=application/vnd.docker.volume.v1+tar.gz \ myvolume.txt:text/plain
This uploads the volume to a
demo
repository in thedocker
namespace. The--artifact-type
flag specifies a special media type that makes Docker Hub recognize the artifact as a container volume.Go to the repository page on Docker Hub. The Tags section on that page shows the volume tag.
Push a generic artifact file
The following procedure shows how to push a generic OCI artifact to Docker Hub.
Prerequisites:
- ORAS CLI version 0.15 or later
Steps:
Create your artifact file.
$ touch myartifact.txt
Sign in to Docker Hub using the ORAS CLI.
$ oras login -u hubuser registry-1.docker.io
Push the file to Docker Hub.
$ oras push registry-1.docker.io/docker/demo:0.0.1 myartifact.txt:text/plain
Go to the repository page on Docker Hub. The Tags section on that page shows the artifact tag.",,,
aca277a2fd7b98e588f55ab96edf77b99e8c2aa7d9a7ea4198a8a0a2a161a895,"GitHub Actions build summary
Docker's GitHub Actions for building and pushing images generate a job summary for your build that outlines the execution and materials used:
- A summary showing the Dockerfile used, the build duration, and cache utilization
- Inputs for the build, such as build arguments, tags, labels, and build contexts
- For builds with Bake, the full bake definition for the build
Job summaries for Docker builds appear automatically if you use the following versions of the Build and push Docker images or Docker Buildx Bake GitHub Actions:
docker/build-push-action@v6
docker/bake-action@v6
To view the job summary, open the details page for the job in GitHub after the job has finished. The summary is available for both failed and successful builds. In the case of a failed build, the summary also displays the error message that caused the build to fail:
Import build records to Docker Desktop
The job summary includes a link for downloading a build record archive for the
run. The build record archive is a ZIP file containing the details about a build
(or builds, if you use docker/bake-action
to build multiple targets). You can
import this build record archive into Docker Desktop, which gives you a
powerful, graphical interface for further analyzing the build's performance via
the
Docker Desktop Builds view.
To import the build record archive into Docker Desktop:
Download and install Docker Desktop.
Download the build record archive from the job summary in GitHub Actions.
Open the Builds view in Docker Desktop.
Select the Import build button, and then browse for the
.zip
archive job summary that you downloaded. Alternatively, you can drag-and-drop the build record archive ZIP file onto the Docker Desktop window after opening the import build dialog.Select Import to add the build records.
After a few seconds, the builds from the GitHub Actions run appear under the Completed builds tab in the Builds view. To inspect a build and see a detailed view of all the inputs, results, build steps, and cache utilization, select the item in the list.
Disable job summary
To disable job summaries, set the DOCKER_BUILD_SUMMARY
environment variable
in the YAML configuration for your build step:
- name: Build
uses: docker/docker-build-push-action@v6
env:
DOCKER_BUILD_SUMMARY: false
with:
tags: ${{ steps.meta.outputs.tags }}
labels: ${{ steps.meta.outputs.labels }}
Disable build record upload
To disable the upload of the build record archive to GitHub, set the
DOCKER_BUILD_RECORD_UPLOAD
environment variable in the YAML configuration for
your build step:
- name: Build
uses: docker/docker-build-push-action@v6
env:
DOCKER_BUILD_RECORD_UPLOAD: false
with:
tags: ${{ steps.meta.outputs.tags }}
labels: ${{ steps.meta.outputs.labels }}
With this configuration, the build summary is still generated, but does not contain a link to download the build record archive.
Limitations
Build summaries are currently not supported for:
- Builds using Docker Build Cloud. Support for Docker Build Cloud is planned for a future release.
- Repositories hosted on GitHub Enterprise Servers. Summaries can only be viewed for repositories hosted on GitHub.com.",,,
6f21430006ff385f95741b8cfe3e3b0c9ea714aff0c0f9d7e2709d7f07b6115c,"windowsfilter storage driver
The windowsfilter storage driver is the default storage driver for Docker Engine on Windows. The windowsfilter driver uses Windows-native file system layers to for storing Docker layers and volume data on disk. The windowsfilter storage driver only works on file systems formatted with NTFS.
Configure the windowsfilter storage driver
For most use case, no configuring the windowsfilter storage driver is not necessary.
The default storage limit for Docker Engine on Windows is 127GB. To use a
different storage size, set the size
option for the windowsfilter storage
driver. See
windowsfilter options.
Data is stored on the Docker host in image
and windowsfilter
subdirectories
within C:\ProgramData\docker
by default. You can change the storage location
by configuring the data-root
option in the
Daemon configuration file:
{
""data-root"": ""d:\\docker""
}
You must restart the daemon for the configuration change to take effect.
Additional information
For more information about how container storage works on Windows, refer to Microsoft's Containers on Windows documentation.",,,
9e0b1f04da50e1def521ddb365c0b0e9628eda02e0eb8af559235d092a3102f8,"Optimize cache usage in builds
When building with Docker, a layer is reused from the build cache if the instruction and the files it depends on hasn't changed since it was previously built. Reusing layers from the cache speeds up the build process because Docker doesn't have to rebuild the layer again.
Here are a few techniques you can use to optimize build caching and speed up the build process:
- Order your layers: Putting the commands in your Dockerfile into a logical order can help you avoid unnecessary cache invalidation.
- Keep the context small: The context is the set of files and directories that are sent to the builder to process a build instruction. Keeping the context as small as possible reduces the amount of data that needs to be sent to the builder, and reduces the likelihood of cache invalidation.
- Use bind mounts: Bind mounts let you mount a file or directory from the host machine into the build container. Using bind mounts can help you avoid unnecessary layers in the image, which can slow down the build process.
- Use cache mounts: Cache mounts let you specify a persistent package cache to be used during builds. The persistent cache helps speed up build steps, especially steps that involve installing packages using a package manager. Having a persistent cache for packages means that even if you rebuild a layer, you only download new or changed packages.
- Use an external cache: An external cache lets you store build cache at a remote location. The external cache image can be shared between multiple builds, and across different environments.
Order your layers
Putting the commands in your Dockerfile into a logical order is a great place to start. Because a change causes a rebuild for steps that follow, try to make expensive steps appear near the beginning of the Dockerfile. Steps that change often should appear near the end of the Dockerfile, to avoid triggering rebuilds of layers that haven't changed.
Consider the following example. A Dockerfile snippet that runs a JavaScript build from the source files in the current directory:
# syntax=docker/dockerfile:1
FROM node
WORKDIR /app
COPY . . # Copy over all files in the current directory
RUN npm install # Install dependencies
RUN npm build # Run build
This Dockerfile is rather inefficient. Updating any file causes a reinstall of all dependencies every time you build the Docker image even if the dependencies didn't change since last time.
Instead, the COPY
command can be split in two. First, copy over the package
management files (in this case, package.json
and yarn.lock
). Then, install
the dependencies. Finally, copy over the project source code, which is subject
to frequent change.
# syntax=docker/dockerfile:1
FROM node
WORKDIR /app
COPY package.json yarn.lock . # Copy package management files
RUN npm install # Install dependencies
COPY . . # Copy over project files
RUN npm build # Run build
By installing dependencies in earlier layers of the Dockerfile, there is no need to rebuild those layers when a project file has changed.
Keep the context small
The easiest way to make sure your context doesn't include unnecessary files is
to create a .dockerignore
file in the root of your build context. The
.dockerignore
file works similarly to .gitignore
files, and lets you
exclude files and directories from the build context.
Here's an example .dockerignore
file that excludes the node_modules
directory, all files and directories that start with tmp
:
node_modules
tmp*
Ignore-rules specified in the .dockerignore
file apply to the entire build
context, including subdirectories. This means it's a rather coarse-grained
mechanism, but it's a good way to exclude files and directories that you know
you don't need in the build context, such as temporary files, log files, and
build artifacts.
Use bind mounts
You might be familiar with bind mounts for when you run containers with docker run
or Docker Compose. Bind mounts let you mount a file or directory from the
host machine into a container.
# bind mount using the -v flag
docker run -v $(pwd):/path/in/container image-name
# bind mount using the --mount flag
docker run --mount=type=bind,src=.,dst=/path/in/container image-name
To use bind mounts in a build, you can use the --mount
flag with the RUN
instruction in your Dockerfile:
FROM golang:latest
WORKDIR /app
RUN --mount=type=bind,target=. go build -o /app/hello
In this example, the current directory is mounted into the build container
before the go build
command gets executed. The source code is available in
the build container for the duration of that RUN
instruction. When the
instruction is done executing, the mounted files are not persisted in the final
image, or in the build cache. Only the output of the go build
command
remains.
The COPY
and ADD
instructions in a Dockerfile lets you copy files from the
build context into the build container. Using bind mounts is beneficial for
build cache optimization because you're not adding unnecessary layers to the
cache. If you have build context that's on the larger side, and it's only used
to generate an artifact, you're better off using bind mounts to temporarily
mount the source code required to generate the artifact into the build. If you
use COPY
to add the files to the build container, BuildKit will include all
of those files in the cache, even if the files aren't used in the final image.
There are a few things to be aware of when using bind mounts in a build:
Bind mounts are read-only by default. If you need to write to the mounted directory, you need to specify the
rw
option. However, even with therw
option, the changes are not persisted in the final image or the build cache. The file writes are sustained for the duration of theRUN
instruction, and are discarded after the instruction is done.Mounted files are not persisted in the final image. Only the output of the
RUN
instruction is persisted in the final image. If you need to include files from the build context in the final image, you need to use theCOPY
orADD
instructions.If the target directory is not empty, the contents of the target directory are hidden by the mounted files. The original contents are restored after the
RUN
instruction is done.For example, given a build context with only a
Dockerfile
in it:. └── Dockerfile
And a Dockerfile that mounts the current directory into the build container:
FROM alpine:latest WORKDIR /work RUN touch foo.txt RUN --mount=type=bind,target=. ls RUN ls
The first
ls
command with the bind mount shows the contents of the mounted directory. The secondls
lists the contents of the original build context.Build log#8 [stage-0 3/5] RUN touch foo.txt #8 DONE 0.1s #9 [stage-0 4/5] RUN --mount=target=. ls -1 #9 0.040 Dockerfile #9 DONE 0.0s #10 [stage-0 5/5] RUN ls -1 #10 0.046 foo.txt #10 DONE 0.1s
Use cache mounts
Regular cache layers in Docker correspond to an exact match of the instruction and the files it depends on. If the instruction and the files it depends on have changed since the layer was built, the layer is invalidated, and the build process has to rebuild the layer.
Cache mounts are a way to specify a persistent cache location to be used during builds. The cache is cumulative across builds, so you can read and write to the cache multiple times. This persistent caching means that even if you need to rebuild a layer, you only download new or changed packages. Any unchanged packages are reused from the cache mount.
To use cache mounts in a build, you can use the --mount
flag with the RUN
instruction in your Dockerfile:
FROM node:latest
WORKDIR /app
RUN --mount=type=cache,target=/root/.npm npm install
In this example, the npm install
command uses a cache mount for the
/root/.npm
directory, the default location for the npm cache. The cache mount
is persisted across builds, so even if you end up rebuilding the layer, you
only download new or changed packages. Any changes to the cache are persisted
across builds, and the cache is shared between multiple builds.
How you specify cache mounts depends on the build tool you're using. If you're unsure how to specify cache mounts, refer to the documentation for the build tool you're using. Here are a few examples:
RUN --mount=type=cache,target=/go/pkg/mod \
go build -o /app/hello
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
--mount=type=cache,target=/var/lib/apt,sharing=locked \
apt update && apt-get --no-install-recommends install -y gcc
RUN --mount=type=cache,target=/root/.cache/pip \
pip install -r requirements.txt
RUN --mount=type=cache,target=/root/.gem \
bundle install
RUN --mount=type=cache,target=/app/target/ \
--mount=type=cache,target=/usr/local/cargo/git/db \
--mount=type=cache,target=/usr/local/cargo/registry/ \
cargo build
RUN --mount=type=cache,target=/root/.nuget/packages \
dotnet restore
RUN --mount=type=cache,target=/tmp/cache \
composer install
It's important that you read the documentation for the build tool you're using
to make sure you're using the correct cache mount options. Package managers
have different requirements for how they use the cache, and using the wrong
options can lead to unexpected behavior. For example, Apt needs exclusive
access to its data, so the caches use the option sharing=locked
to ensure
parallel builds using the same cache mount wait for each other and not access
the same cache files at the same time.
Use an external cache
The default cache storage for builds is internal to the builder (BuildKit instance) you're using. Each builder uses its own cache storage. When you switch between different builders, the cache is not shared between them. Using an external cache lets you define a remote location for pushing and pulling cache data.
External caches are especially useful for CI/CD pipelines, where the builders are often ephemeral, and build minutes are precious. Reusing the cache between builds can drastically speed up the build process and reduce cost. You can even make use of the same cache in your local development environment.
To use an external cache, you specify the --cache-to
and --cache-from
options with the docker buildx build
command.
--cache-to
exports the build cache to the specified location.--cache-from
specifies remote caches for the build to use.
The following example shows how to set up a GitHub Actions workflow using
docker/build-push-action
, and push the build cache layers to an OCI registry
image:
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build and push
uses: docker/build-push-action@v6
with:
push: true
tags: user/app:latest
cache-from: type=registry,ref=user/app:buildcache
cache-to: type=registry,ref=user/app:buildcache,mode=max
This setup tells BuildKit to look for cache in the user/app:buildcache
image.
And when the build is done, the new build cache is pushed to the same image,
overwriting the old cache.
This cache can be used locally as well. To pull the cache in a local build,
you can use the --cache-from
option with the docker buildx build
command:
$ docker buildx build --cache-from type=registry,ref=user/app:buildcache .
Summary
Optimizing cache usage in builds can significantly speed up the build process. Keeping the build context small, using bind mounts, cache mounts, and external caches are all techniques you can use to make the most of the build cache and speed up the build process.
For more information about the concepts discussed in this guide, see:",,,
34f2488922aea438738cda8de54ac04e8678e76fe25be24c089312fb955d8528,"Troubleshooting the Docker daemon
This page describes how to troubleshoot and debug the daemon if you run into issues.
You can turn on debugging on the daemon to learn about the runtime activity of
the daemon and to aid in troubleshooting. If the daemon is unresponsive, you can
also
force a full stack trace of all
threads to be added to the daemon log by sending the SIGUSR
signal to the
Docker daemon.
Daemon
Unable to connect to the Docker daemon
Cannot connect to the Docker daemon. Is 'docker daemon' running on this host?
This error may indicate:
- The Docker daemon isn't running on your system. Start the daemon and try running the command again.
- Your Docker client is attempting to connect to a Docker daemon on a different host, and that host is unreachable.
Check whether Docker is running
The operating-system independent way to check whether Docker is running is to
ask Docker, using the docker info
command.
You can also use operating system utilities, such as
sudo systemctl is-active docker
or sudo status docker
or
sudo service docker status
, or checking the service status using Windows
utilities.
Finally, you can check in the process list for the dockerd
process, using
commands like ps
or top
.
Check which host your client is connecting to
To see which host your client is connecting to, check the value of the
DOCKER_HOST
variable in your environment.
$ env | grep DOCKER_HOST
If this command returns a value, the Docker client is set to connect to a Docker daemon running on that host. If it's unset, the Docker client is set to connect to the Docker daemon running on the local host. If it's set in error, use the following command to unset it:
$ unset DOCKER_HOST
You may need to edit your environment in files such as ~/.bashrc
or
~/.profile
to prevent the DOCKER_HOST
variable from being set erroneously.
If DOCKER_HOST
is set as intended, verify that the Docker daemon is running on
the remote host and that a firewall or network outage isn't preventing you from
connecting.
Troubleshoot conflicts between the daemon.json
and startup scripts
If you use a daemon.json
file and also pass options to the dockerd
command
manually or using start-up scripts, and these options conflict, Docker fails to
start with an error such as:
unable to configure the Docker daemon with file /etc/docker/daemon.json:
the following directives are specified both as a flag and in the configuration
file: hosts: (from flag: [unix:///var/run/docker.sock], from file: [tcp://127.0.0.1:2376])
If you see an error similar to this one and you are starting the daemon manually
with flags, you may need to adjust your flags or the daemon.json
to remove the
conflict.
Note
If you see this specific error message about
hosts
, continue to the next section for a workaround.
If you are starting Docker using your operating system's init scripts, you may need to override the defaults in these scripts in ways that are specific to the operating system.
Configure the daemon host with systemd
One notable example of a configuration conflict that's difficult to
troubleshoot is when you want to specify a different daemon address from the
default. Docker listens on a socket by default. On Debian and Ubuntu systems
using systemd
, this means that a host flag -H
is always used when starting
dockerd
. If you specify a hosts
entry in the daemon.json
, this causes a
configuration conflict and results in the Docker daemon failing to start.
To work around this problem, create a new file
/etc/systemd/system/docker.service.d/docker.conf
with the following contents,
to remove the -H
argument that's used when starting the daemon by default.
[Service]
ExecStart=
ExecStart=/usr/bin/dockerd
There are other times when you might need to configure systemd
with Docker,
such as
configuring a HTTP or HTTPS proxy.
Note
If you override this option without specifying a
hosts
entry in thedaemon.json
or a-H
flag when starting Docker manually, Docker fails to start.
Run sudo systemctl daemon-reload
before attempting to start Docker. If Docker
starts successfully, it's now listening on the IP address specified in the
hosts
key of the daemon.json
instead of a socket.
Important
Setting
hosts
in thedaemon.json
isn't supported on Docker Desktop for Windows or Docker Desktop for Mac.
Out of memory issues
If your containers attempt to use more memory than the system has available, you may experience an Out of Memory (OOM) exception, and a container, or the Docker daemon, might be stopped by the kernel OOM killer. To prevent this from happening, ensure that your application runs on hosts with adequate memory and see Understand the risks of running out of memory.
Kernel compatibility
Docker can't run correctly if your kernel is older than version 3.10, or if it's
missing kernel modules. To check kernel compatibility, you can download and run
the
check-config.sh
script.
$ curl https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh > check-config.sh
$ bash ./check-config.sh
The script only works on Linux.
Kernel cgroup swap limit capabilities
On Ubuntu or Debian hosts, you may see messages similar to the following when working with an image.
WARNING: Your kernel does not support swap limit capabilities. Limitation discarded.
If you don't need these capabilities, you can ignore the warning.
You can turn on these capabilities on Ubuntu or Debian by following these instructions. Memory and swap accounting incur an overhead of about 1% of the total available memory and a 10% overall performance degradation, even when Docker isn't running.
Log into the Ubuntu or Debian host as a user with
sudo
privileges.Edit the
/etc/default/grub
file. Add or edit theGRUB_CMDLINE_LINUX
line to add the following two key-value pairs:GRUB_CMDLINE_LINUX=""cgroup_enable=memory swapaccount=1""
Save and close the file.
Update the GRUB boot loader.
$ sudo update-grub
An error occurs if your GRUB configuration file has incorrect syntax. In this case, repeat steps 2 and 3.
The changes take effect when you reboot the system.
Networking
IP forwarding problems
If you manually configure your network using systemd-network
with systemd
version 219 or later, Docker containers may not be able to access your network.
Beginning with systemd version 220, the forwarding setting for a given network
(net.ipv4.conf.<interface>.forwarding
) defaults to off. This setting prevents
IP forwarding. It also conflicts with Docker's behavior of enabling the
net.ipv4.conf.all.forwarding
setting within containers.
To work around this on RHEL, CentOS, or Fedora, edit the <interface>.network
file in /usr/lib/systemd/network/
on your Docker host, for example,
/usr/lib/systemd/network/80-container-host0.network
.
Add the following block within the [Network]
section.
[Network]
...
IPForward=kernel
# OR
IPForward=true
This configuration allows IP forwarding from the container as expected.
DNS resolver issues
DNS resolver found in resolv.conf and containers can't use it
Linux desktop environments often have a network manager program running, that
uses dnsmasq
to cache DNS requests by adding them to /etc/resolv.conf
. The
dnsmasq
instance runs on a loopback address such as 127.0.0.1
or
127.0.1.1
. It speeds up DNS look-ups and provides DHCP services. Such a
configuration doesn't work within a Docker container. The Docker container uses
its own network namespace, and resolves loopback addresses such as 127.0.0.1
to itself, and it's unlikely to be running a DNS server on its own loopback
address.
If Docker detects that no DNS server referenced in /etc/resolv.conf
is a fully
functional DNS server, the following warning occurs:
WARNING: Local (127.0.0.1) DNS resolver found in resolv.conf and containers
can't use it. Using default external servers : [8.8.8.8 8.8.4.4]
If you see this warning, first check to see if you use dnsmasq
:
$ ps aux | grep dnsmasq
If your container needs to resolve hosts which are internal to your network, the public nameservers aren't adequate. You have two choices:
Specify DNS servers for Docker to use.
Turn off
dnsmasq
.Turning off
dnsmasq
adds the IP addresses of actual DNS nameservers to/etc/resolv.conf
, and you lose the benefits ofdnsmasq
.
You only need to use one of these methods.
Specify DNS servers for Docker
The default location of the configuration file is /etc/docker/daemon.json
. You
can change the location of the configuration file using the --config-file
daemon flag. The following instruction assumes that the location of the
configuration file is /etc/docker/daemon.json
.
Create or edit the Docker daemon configuration file, which defaults to
/etc/docker/daemon.json
file, which controls the Docker daemon configuration.$ sudo nano /etc/docker/daemon.json
Add a
dns
key with one or more DNS server IP addresses as values.{ ""dns"": [""8.8.8.8"", ""8.8.4.4""] }
If the file has existing contents, you only need to add or edit the
dns
line. If your internal DNS server can't resolve public IP addresses, include at least one DNS server that can. Doing so allows you to connect to Docker Hub, and your containers to resolve internet domain names.Save and close the file.
Restart the Docker daemon.
$ sudo service docker restart
Verify that Docker can resolve external IP addresses by trying to pull an image:
$ docker pull hello-world
If necessary, verify that Docker containers can resolve an internal hostname by pinging it.
$ docker run --rm -it alpine ping -c4 <my_internal_host> PING google.com (192.168.1.2): 56 data bytes 64 bytes from 192.168.1.2: seq=0 ttl=41 time=7.597 ms 64 bytes from 192.168.1.2: seq=1 ttl=41 time=7.635 ms 64 bytes from 192.168.1.2: seq=2 ttl=41 time=7.660 ms 64 bytes from 192.168.1.2: seq=3 ttl=41 time=7.677 ms
Turn off dnsmasq
If you prefer not to change the Docker daemon's configuration to use a specific
IP address, follow these instructions to turn off dnsmasq
in NetworkManager.
Edit the
/etc/NetworkManager/NetworkManager.conf
file.Comment out the
dns=dnsmasq
line by adding a#
character to the beginning of the line.# dns=dnsmasq
Save and close the file.
Restart both NetworkManager and Docker. As an alternative, you can reboot your system.
$ sudo systemctl restart network-manager $ sudo systemctl restart docker
To turn off dnsmasq
on RHEL, CentOS, or Fedora:
Turn off the
dnsmasq
service:$ sudo systemctl stop dnsmasq $ sudo systemctl disable dnsmasq
Configure the DNS servers manually using the Red Hat documentation.
Docker networks disappearing
If a Docker network, such as the docker0
bridge or a custom network, randomly
disappears or otherwise appears to be working incorrectly, it could be because
another service is interfering with or modifying Docker interfaces. Tools that
manage networking interfaces on the host are known to sometimes also
inappropriately modify Docker interfaces.
Refer to the following sections for instructions on how to configure your network manager to set Docker interfaces as un-managed, depending on the network management tools that exist on the host:
- If
netscript
is installed, consider uninstalling it - Configure the network manager to treat Docker interfaces as un-managed
- If you're using Netplan, you may need to apply a custom Netplan configuration
Uninstall netscript
If netscript
is installed on your system, you can likely fix this issue by
uninstalling it. For example, on a Debian-based system:
$ sudo apt-get remove netscript-2.4
Un-manage Docker interfaces
In some cases, the network manager will attempt to manage Docker interfaces by default. You can try to explicitly flag Docker networks as un-managed by editing your system's network configuration settings.
If you're using NetworkManager
, edit your system network configuration under
/etc/network/interfaces
Create a file at
/etc/network/interfaces.d/20-docker0
with the following contents:iface docker0 inet manual
Note that this example configuration only ""un-manages"" the default
docker0
bridge, not custom networks.Restart
NetworkManager
for the configuration change to take effect.$ systemctl restart NetworkManager
Verify that the
docker0
interface has theunmanaged
state.$ nmcli device
If you're running Docker on a system using systemd-networkd
as a networking
daemon, configure the Docker interfaces as un-managed by creating configuration
files under /etc/systemd/network
:
Create
/etc/systemd/network/docker.network
with the following contents:# Ensure that the Docker interfaces are un-managed [Match] Name=docker0 br-* veth* [Link] Unmanaged=yes
Reload the configuration.
$ sudo systemctl restart systemd-networkd
Restart the Docker daemon.
$ sudo systemctl restart docker
Verify that the Docker interfaces have the
unmanaged
state.$ networkctl
Prevent Netplan from overriding network configuration
On systems that use
Netplan through
cloud-init
, you may
need to apply a custom configuration to prevent netplan
from overriding the
network manager configuration:
Follow the steps in Un-manage Docker interfaces for creating the network manager configuration.
Create a
netplan
configuration file under/etc/netplan/50-cloud-init.yml
.The following example configuration file is a starting point. Adjust it to match the interfaces you want to un-manage. Incorrect configuration can lead to network connectivity issues.
/etc/netplan/50-cloud-init.ymlnetwork: ethernets: all: dhcp4: true dhcp6: true match: # edit this filter to match whatever makes sense for your system name: en* renderer: networkd version: 2
Apply the new Netplan configuration.
$ sudo netplan apply
Restart the Docker daemon:
$ sudo systemctl restart docker
Verify that the Docker interfaces have the
unmanaged
state.$ networkctl
Volumes
Unable to remove filesystem
Error: Unable to remove filesystem
Some container-based utilities, such
as
Google cAdvisor, mount Docker system
directories, such as /var/lib/docker/
, into a container. For instance, the
documentation for cadvisor
instructs you to run the cadvisor
container as
follows:
$ sudo docker run \
--volume=/:/rootfs:ro \
--volume=/var/run:/var/run:rw \
--volume=/sys:/sys:ro \
--volume=/var/lib/docker/:/var/lib/docker:ro \
--publish=8080:8080 \
--detach=true \
--name=cadvisor \
google/cadvisor:latest
When you bind-mount /var/lib/docker/
, this effectively mounts all resources of
all other running containers as filesystems within the container which mounts
/var/lib/docker/
. When you attempt to remove any of these containers, the
removal attempt may fail with an error like the following:
Error: Unable to remove filesystem for
74bef250361c7817bee19349c93139621b272bc8f654ae112dd4eb9652af9515:
remove /var/lib/docker/containers/74bef250361c7817bee19349c93139621b272bc8f654ae112dd4eb9652af9515/shm:
Device or resource busy
The problem occurs if the container which bind-mounts /var/lib/docker/
uses statfs
or fstatfs
on filesystem handles within /var/lib/docker/
and does not close them.
Typically, we would advise against bind-mounting /var/lib/docker
in this way.
However, cAdvisor
requires this bind-mount for core functionality.
If you are unsure which process is causing the path mentioned in the error to
be busy and preventing it from being removed, you can use the lsof
command
to find its process. For instance, for the error above:
$ sudo lsof /var/lib/docker/containers/74bef250361c7817bee19349c93139621b272bc8f654ae112dd4eb9652af9515/shm
To work around this problem, stop the container which bind-mounts
/var/lib/docker
and try again to remove the other container.",,,
c1e976412be0d5b2feb4d3988542ad6b0c2e5dddbdf41a0283ae3eb759bc117f,"Explore the Builds view in Docker Desktop
The Builds view is a simple interface that lets you inspect your build history and manage builders using Docker Desktop.
Opening the Builds view in Docker Desktop displays a list of completed builds. By default, the list is sorted by date, showing the most recent builds at the top. You can switch to Active builds to view any ongoing builds.
If you're connected to a cloud builder through Docker Build Cloud, the Builds view also lists any active or completed cloud builds by other team members connected to the same cloud builder.
Show build list
Select the Builds view in the Docker Desktop Dashboard to open the build list.
The build list shows your completed and ongoing builds. The Build history tab shows completed historical builds, and from here you can inspect the build logs, dependencies, traces, and more. The Active builds tab shows builds that are currently running.
The list shows builds for your active, running builders. It doesn't list builds for inactive builders: builders that you've removed from your system, or builders that have been stopped.
Builder settings
The top-right corner shows the name of your currently selected builder, and the Builder settings button lets you manage builders in the Docker Desktop settings.
Import builds
The Import builds button lets you import build records for builds by other
people, or builds in a CI environment. When you've imported a build record, it
gives you full access to the logs, traces, and other data for that build,
directly in Docker Desktop. The
build summary
for the docker/build-push-action
and docker/bake-action
GitHub Actions
includes a link to download the build records, for inspecting CI jobs with
Docker Desktop.
Inspect builds
To inspect a build, select the build that you want to view in the list. The inspection view contains a number of tabs.
The Info tab displays details about the build.
If you're inspecting a multi-platform build, the drop-down menu in the top-right of this tab lets you filter the information down to a specific platform:
The Source details section shows information about the frontend frontend and, if available, the source code repository used for the build.
Build timing
The Build timing section of the Info tab contains charts showing a breakdown of the build execution from various angles.
- Real time refers to the wall-clock time that it took to complete the build.
- Accumulated time shows the total CPU time for all steps.
- Cache usage shows the extent to which build operations were cached.
- Parallel execution shows how much of the build execution time was spent running steps in parallel.
The chart colors and legend keys describe the different build operations. Build operations are defined as follows:
| Build operation | Description |
|---|---|
| Local file transfers | Time spent transferring local files from the client to the builder. |
| File operations | Any operations that involve creating and copying files in the build. For example, the COPY , WORKDIR , ADD instructions in a Dockerfile frontend all incur file operations. |
| Image pulls | Time spent pulling images. |
| Executions | Container executions, for example commands defined as RUN instructions in a Dockerfile frontend. |
| HTTP | Remote artifact downloads using ADD . |
| Git | Same as HTTP but for Git URLs. |
| Result exports | Time spent exporting the build results. |
| SBOM | Time spent generating the SBOM attestation. |
| Idle | Idle time for build workers, which can happen if you have configured a max parallelism limit. |
Build dependencies
The Dependencies section shows images and remote resources used during the build. Resources listed here include:
- Container images used during the build
- Git repositories included using the
ADD
Dockerfile instruction - Remote HTTPS resources included using the
ADD
Dockerfile instruction
Arguments, secrets, and other parameters
The Configuration section of the Info tab shows parameters passed to the build:
- Build arguments, including the resolved value
- Secrets, including their IDs (but not their values)
- SSH sockets
- Labels
- Additional contexts
Outputs and artifacts
The Build results section shows a summary of the generated build artifacts, including image manifest details, attestations, and build traces.
Attestations are metadata records attached to a container image. The metadata describes something about the image, for example how it was built or what packages it contains. For more information about attestations, see Build attestations.
Build traces capture information about the build execution steps in Buildx and BuildKit. The traces are available in two formats: OTLP and Jaeger. You can download build traces from Docker Desktop by opening the actions menu and selecting the format you want to download.
Inspect build traces with Jaeger
Using a Jaeger client, you can import and inspect build traces from Docker Desktop. The following steps show you how to export a trace from Docker Desktop and view it in Jaeger:
Start Jaeger UI:
$ docker run -d --name jaeger -p ""16686:16686"" jaegertracing/all-in-one
Open the Builds view in Docker Desktop, and select a completed build.
Navigate to the Build results section, open the actions menu and select Download as Jaeger format.
Go to http://localhost:16686 in your browser to open Jaeger UI.
Select the Upload tab and open the Jaeger build trace you just exported.
Now you can analyze the build trace using the Jaeger UI:
Dockerfile source and errors
When inspecting a successful completed build or an ongoing active build, the Source tab shows the frontend used to create the build.
If the build failed, an Error tab displays instead of the Source tab. The error message is inlined in the Dockerfile source, indicating where the failure happened and why.
Build logs
The Logs tab displays the build logs. For active builds, the logs are updated in real-time.
You can toggle between a List view and a Plain-text view of a build log.
The List view presents all build steps in a collapsible format, with a timeline for navigating the log along a time axis.
The Plain-text view displays the log as plain text.
The Copy button lets you copy the plain-text version of the log to your clipboard.
Build history
The History tab displays statistics data about completed builds.
The time series chart illustrates trends in duration, build steps, and cache usage for related builds, helping you identify patterns and shifts in build operations over time. For instance, significant spikes in build duration or a high number of cache misses could signal opportunities for optimizing the Dockerfile.
You can navigate to and inspect a related build by selecting it in the chart, or using the Past builds list below the chart.
Manage builders
The Builder settings view in the Docker Desktop settings lets you:
- Inspect the state and configuration of active builders
- Start and stop a builder
- Delete build history
- Add or remove builders (or connect and disconnect, in the case of cloud builders)
For more information about managing builders, see Change settings",,,
d7e7d58ad4f87cdf702097e84c2a8584e66010d6ceb3f5743cf33f6cd536e3e9,"Install Docker Engine from binaries
Important
This page contains information on how to install Docker using binaries. These instructions are mostly suitable for testing purposes. We do not recommend installing Docker using binaries in production environments as they don't have automatic security updates. The Linux binaries described on this page are statically linked, which means that vulnerabilities in build-time dependencies are not automatically patched by security updates of your Linux distribution.
Updating binaries is also slightly more involved when compared to Docker packages installed using a package manager or through Docker Desktop, as it requires (manually) updating the installed version whenever there is a new release of Docker.
Also, static binaries may not include all functionalities provided by the dynamic packages.
On Windows and Mac, we recommend that you install Docker Desktop instead. For Linux, we recommend that you follow the instructions specific for your distribution.
If you want to try Docker or use it in a testing environment, but you're not on a supported platform, you can try installing from static binaries. If possible, you should use packages built for your operating system, and use your operating system's package management system to manage Docker installation and upgrades.
Static binaries for the Docker daemon binary are only available for Linux (as
dockerd
) and Windows (as dockerd.exe
).
Static binaries for the Docker client are available for Linux, Windows, and macOS (as docker
).
This topic discusses binary installation for Linux, Windows, and macOS:
- Install daemon and client binaries on Linux
- Install client binaries on macOS
- Install server and client binaries on Windows
Install daemon and client binaries on Linux
Prerequisites
Before attempting to install Docker from binaries, be sure your host machine meets the prerequisites:
- A 64-bit installation
- Version 3.10 or higher of the Linux kernel. The latest version of the kernel available for your platform is recommended.
iptables
version 1.4 or highergit
version 1.7 or higher- A
ps
executable, usually provided byprocps
or a similar package. - XZ Utils 4.9 or higher
- A
properly mounted
cgroupfs
hierarchy; a single, all-encompassingcgroup
mount point is not sufficient. See Github issues #2683, #3485, #4568).
Secure your environment as much as possible
OS considerations
Enable SELinux or AppArmor if possible.
It is recommended to use AppArmor or SELinux if your Linux distribution supports either of the two. This helps improve security and blocks certain types of exploits. Review the documentation for your Linux distribution for instructions for enabling and configuring AppArmor or SELinux.
Security warning
If either of the security mechanisms is enabled, do not disable it as a work-around to make Docker or its containers run. Instead, configure it correctly to fix any problems.
Docker daemon considerations
Enable
seccomp
security profiles if possible. See Enablingseccomp
for Docker.Enable user namespaces if possible. See the Daemon user namespace options.
Install static binaries
Download the static binary archive. Go to https://download.docker.com/linux/static/stable/, choose your hardware platform, and download the
.tgz
file relating to the version of Docker Engine you want to install.Extract the archive using the
tar
utility. Thedockerd
anddocker
binaries are extracted.$ tar xzvf /path/to/<FILE>.tar.gz
Optional: Move the binaries to a directory on your executable path, such as
/usr/bin/
. If you skip this step, you must provide the path to the executable when you invokedocker
ordockerd
commands.$ sudo cp docker/* /usr/bin/
Start the Docker daemon:
$ sudo dockerd &
If you need to start the daemon with additional options, modify the above command accordingly or create and edit the file
/etc/docker/daemon.json
to add the custom configuration options.Verify that Docker is installed correctly by running the
hello-world
image.$ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Install client binaries on macOS
Note
The following instructions are mostly suitable for testing purposes. The macOS binary includes the Docker client only. It does not include the
dockerd
daemon which is required to run containers. Therefore, we recommend that you install Docker Desktop instead.
The binaries for Mac also do not contain:
- A runtime environment. You must set up a functional engine either in a Virtual Machine, or on a remote Linux machine.
- Docker components such as
buildx
anddocker compose
.
To install client binaries, perform the following steps:
Download the static binary archive. Go to https://download.docker.com/mac/static/stable/ and select
x86_64
(for Mac on Intel chip) oraarch64
(for Mac on Apple silicon), and then download the.tgz
file relating to the version of Docker Engine you want to install.Extract the archive using the
tar
utility. Thedocker
binary is extracted.$ tar xzvf /path/to/<FILE>.tar.gz
Clear the extended attributes to allow it run.
$ sudo xattr -rc docker
Now, when you run the following command, you can see the Docker CLI usage instructions:
$ docker/docker
Optional: Move the binary to a directory on your executable path, such as
/usr/local/bin/
. If you skip this step, you must provide the path to the executable when you invokedocker
ordockerd
commands.$ sudo cp docker/docker /usr/local/bin/
Verify that Docker is installed correctly by running the
hello-world
image. The value of<hostname>
is a hostname or IP address running the Docker daemon and accessible to the client.$ sudo docker -H <hostname> run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a message and exits.
Install server and client binaries on Windows
Note
The following section describes how to install the Docker daemon on Windows Server which allows you to run Windows containers only. When you install the Docker daemon on Windows Server, the daemon doesn't contain Docker components such as
buildx
andcompose
. If you're running Windows 10 or 11, we recommend that you install Docker Desktop instead.
Binary packages on Windows include both dockerd.exe
and docker.exe
. On Windows,
these binaries only provide the ability to run native Windows containers (not
Linux containers).
To install server and client binaries, perform the following steps:
Download the static binary archive. Go to https://download.docker.com/win/static/stable/x86_64 and select the latest version from the list.
Run the following PowerShell commands to install and extract the archive to your program files:
PS C:\> Expand-Archive /path/to/<FILE>.zip -DestinationPath $Env:ProgramFiles
Register the service and start the Docker Engine:
PS C:\> &$Env:ProgramFiles\Docker\dockerd --register-service PS C:\> Start-Service docker
Verify that Docker is installed correctly by running the
hello-world
image.PS C:\> &$Env:ProgramFiles\Docker\docker run hello-world:nanoserver
This command downloads a test image and runs it in a container. When the container runs, it prints a message and exits.
Upgrade static binaries
To upgrade your manual installation of Docker Engine, first stop any
dockerd
or dockerd.exe
processes running locally, then follow the
regular installation steps to install the new version on top of the existing
version.
Next steps
- Continue to Post-installation steps for Linux.",,,
22c1350e0b4fe60689118802fefad36f4991ff8933f11f8467c01a65ca71ac91,"Get support
Note
Docker Desktop offers support for developers with a Pro, Team, or Business subscription.
How do I get Docker Desktop support?
If you have a paid Docker subscription, you can contact the Support team.
All Docker users can seek support through the following resources, where Docker or the community respond on a best effort basis.
- Docker Desktop for Windows GitHub repo
- Docker Desktop for Mac GitHub repo
- Docker Desktop for Linux GitHub repo
- Docker Community Forums
- Docker Community Slack
What support can I get?
Account management related issues
- Automated builds
- Basic product 'how to' questions
- Billing or subscription issues
- Configuration issues
- Desktop installation issues
- Installation crashes
- Failure to launch Docker Desktop on first run
- Desktop update issues
- Sign-in issues in both the command line interface and Docker Hub user interface
- Push or pull issues, including rate limiting
- Usage issues
- Crash closing software
- Docker Desktop not behaving as expected
For Windows users, you can also request support on:
- Turning on virtualization in BIOS
- Turning on Windows features
- Running inside certain VM or VDI environments (Docker Business customers only)
What is not supported?
Docker Desktop excludes support for the following types of issues:
- Use on or in conjunction with hardware or software other than that specified in the applicable documentation
- Running on unsupported operating systems, including beta/preview versions of operating systems
- Running containers of a different architecture using emulation
- Support for the Docker engine, Docker CLI, or other bundled Linux components
- Support for Kubernetes
- Features labeled as experimental
- System/Server administration activities
- Supporting Desktop as a production runtime
- Scale deployment/multi-machine installation of Desktop
- Routine product maintenance (data backup, cleaning disk space and configuring log rotation)
- Third-party applications not provided by Docker
- Altered or modified Docker software
- Defects in the Docker software due to hardware malfunction, abuse, or improper use
- Any version of the Docker software other than the latest version
- Reimbursing and expenses spent for third-party services not provided by Docker
- Docker support excludes training, customization, and integration
- Running multiple instances of Docker Desktop on a single machine
Note
Support for running Docker Desktop in a VM or VDI environment is only available to Docker Business customers.
What versions are supported?
For Docker Business customers, Docker offers support for versions up to six months older than the latest version, although any fixes will be on the latest version.
For Pro and Team customers, Docker only offers support for the latest version of Docker Desktop. If you are running an older version, Docker may ask you to update before investigating your support request.
How many machines can I get support for Docker Desktop on?
As a Pro user you can get support for Docker Desktop on a single machine. As a Team, you can get support for Docker Desktop for the number of machines equal to the number of seats as part of your plan.
What OS’s are supported?
Docker Desktop is available for Mac, Linux, and Windows. The supported version information can be found on the following pages:
How is personal diagnostic data handled in Docker Desktop?
When uploading diagnostics to help Docker with investigating issues, the uploaded diagnostics bundle may contain personal data such as usernames and IP addresses. The diagnostics bundles are only accessible to Docker, Inc. employees who are directly involved in diagnosing Docker Desktop issues.
By default, Docker, Inc. will delete uploaded diagnostics bundles after 30 days. You may also request the removal of a diagnostics bundle by either specifying the diagnostics ID or via your GitHub ID (if the diagnostics ID is mentioned in a GitHub issue). Docker, Inc. will only use the data in the diagnostics bundle to investigate specific user issues but may derive high-level (non personal) metrics such as the rate of issues from it.
For more information, see Docker Data Processing Agreement.",,,
765eabe53164dc06435366c5565040519ea4836f596ab7bc95627fd40fee02c8,"Docker daemon configuration overview
This page shows you how to customize the Docker daemon, dockerd
.
Note
This page is for users who've installed Docker Engine manually. If you're using Docker Desktop, refer to the settings page.
Configure the Docker daemon
There are two ways to configure the Docker daemon:
- Use a JSON configuration file. This is the preferred option, since it keeps all configurations in a single place.
- Use flags when starting
dockerd
.
You can use both of these options together as long as you don't specify the same option both as a flag and in the JSON file. If that happens, the Docker daemon won't start and prints an error message.
Configuration file
The following table shows the location where the Docker daemon expects to find the configuration file by default, depending on your system and how you're running the daemon.
| OS and configuration | File location |
|---|---|
| Linux, regular setup | /etc/docker/daemon.json |
| Linux, rootless mode | ~/.config/docker/daemon.json |
| Windows | C:\ProgramData\docker\config\daemon.json |
For rootless mode, the daemon respects the XDG_CONFIG_HOME
variable. If set,
the expected file location is $XDG_CONFIG_HOME/docker/daemon.json
.
You can also explicitly specify the location of the configuration file on
startup, using the dockerd --config-file
flag.
Learn about the available configuration options in the dockerd reference docs
Configuration using flags
You can also start the Docker daemon manually and configure it using flags. This can be useful for troubleshooting problems.
Here's an example of how to manually start the Docker daemon, using the same configurations as shown in the previous JSON configuration:
$ dockerd --debug \
--tls=true \
--tlscert=/var/docker/server.pem \
--tlskey=/var/docker/serverkey.pem \
--host tcp://192.168.59.3:2376
Learn about the available configuration options in the dockerd reference docs, or by running:
$ dockerd --help
Daemon data directory
The Docker daemon persists all data in a single directory. This tracks everything related to Docker, including containers, images, volumes, service definition, and secrets.
By default this directory is:
/var/lib/docker
on Linux.C:\ProgramData\docker
on Windows.
You can configure the Docker daemon to use a different directory, using the
data-root
configuration option. For example:
{
""data-root"": ""/mnt/docker-data""
}
Since the state of a Docker daemon is kept on this directory, make sure you use a dedicated directory for each daemon. If two daemons share the same directory, for example, an NFS share, you are going to experience errors that are difficult to troubleshoot.
Next steps
Many specific configuration options are discussed throughout the Docker documentation. Some places to go next include:",,,
16bc58e7cfd55588511acf542172eec99391d4a49033c589ff2a3178f60ce6c1,"tmpfs mounts
Volumes and bind mounts let you share files between the host machine and container so that you can persist data even after the container is stopped.
If you're running Docker on Linux, you have a third option: tmpfs mounts. When you create a container with a tmpfs mount, the container can create files outside the container's writable layer.
As opposed to volumes and bind mounts, a tmpfs mount is temporary, and only persisted in the host memory. When the container stops, the tmpfs mount is removed, and files written there won't be persisted.
tmpfs mounts are best used for cases when you do not want the data to persist either on the host machine or within the container. This may be for security reasons or to protect the performance of the container when your application needs to write a large volume of non-persistent state data.
Important
tmpfs mounts in Docker map directly to tmpfs in the Linux kernel. As such, the temporary data may be written to a swap file, and thereby persisted to the filesystem.
Mounting over existing data
If you create a tmpfs mount into a directory in the container in which files or
directories exist, the pre-existing files are obscured by the mount. This is
similar to if you were to save files into /mnt
on a Linux host, and then
mounted a USB drive into /mnt
. The contents of /mnt
would be obscured by
the contents of the USB drive until the USB drive was unmounted.
With containers, there's no straightforward way of removing a mount to reveal the obscured files again. Your best option is to recreate the container without the mount.
Limitations of tmpfs mounts
- Unlike volumes and bind mounts, you can't share tmpfs mounts between containers.
- This functionality is only available if you're running Docker on Linux.
- Setting permissions on tmpfs may cause them to reset after container restart. In some cases setting the uid/gid can serve as a workaround.
Syntax
To mount a tmpfs with the docker run
command, you can use either the
--mount
or --tmpfs
flag.
$ docker run --mount type=tmpfs,dst=<mount-path>
$ docker run --tmpfs <mount-path>
In general, --mount
is preferred. The main difference is that the --mount
flag is more explicit. On the other hand, --tmpfs
is less verbose and gives
you more flexibility as it lets you set more mount options.
The --tmpfs
flag cannot be used with swarm services. You must use --mount
.
Options for --tmpfs
The --tmpfs
flag consists of two fields, separated by a colon character
(:
).
$ docker run --tmpfs <mount-path>[:opts]
The first field is the container path to mount into a tmpfs. The second field
is optional and lets you set mount options. Valid mount options for --tmpfs
include:
| Option | Description |
|---|---|
ro | Creates a read-only tmpfs mount. |
rw | Creates a read-write tmpfs mount (default behavior). |
nosuid | Prevents setuid and setgid bits from being honored during execution. |
suid | Allows setuid and setgid bits to be honored during execution (default behavior). |
nodev | Device files can be created but are not functional (access results in an error). |
dev | Device files can be created and are fully functional. |
exec | Allows the execution of executable binaries in the mounted file system. |
noexec | Does not allow the execution of executable binaries in the mounted file system. |
sync | All I/O to the file system is done synchronously. |
async | All I/O to the file system is done asynchronously (default behavior). |
dirsync | Directory updates within the file system are done synchronously. |
atime | Updates file access time each time the file is accessed. |
noatime | Does not update file access times when the file is accessed. |
diratime | Updates directory access times each time the directory is accessed. |
nodiratime | Does not update directory access times when the directory is accessed. |
size | Specifies the size of the tmpfs mount, for example, size=64m . |
mode | Specifies the file mode (permissions) for the tmpfs mount (for example, mode=1777 ). |
uid | Specifies the user ID for the owner of the tmpfs mount (for example, uid=1000 ). |
gid | Specifies the group ID for the owner of the tmpfs mount (for example, gid=1000 ). |
nr_inodes | Specifies the maximum number of inodes for the tmpfs mount (for example, nr_inodes=400k ). |
nr_blocks | Specifies the maximum number of blocks for the tmpfs mount (for example, nr_blocks=1024 ). |
$ docker run --tmpfs /data:noexec,size=1024,mode=1777
Not all tmpfs mount features available in the Linux mount command are supported
with the --tmpfs
flag. If you require advanced tmpfs options or features, you
may need to use a privileged container or configure the mount outside of
Docker.
Caution
Running containers with
--privileged
grants elevated permissions and can expose the host system to security risks. Use this option only when absolutely necessary and in trusted environments.
$ docker run --privileged -it debian sh
/# mount -t tmpfs -o <options> tmpfs /data
Options for --mount
The --mount
flag consists of multiple key-value pairs, separated by commas
and each consisting of a <key>=<value>
tuple. The order of the keys isn't
significant.
$ docker run --mount type=tmpfs,dst=<mount-path>[,<key>=<value>...]
Valid options for --mount type=tmpfs
include:
| Option | Description |
|---|---|
destination , dst , target | Size of the tmpfs mount in bytes. If unset, the default maximum size of a tmpfs volume is 50% of the host's total RAM. |
tmpfs-size | Size of the tmpfs mount in bytes. If unset, the default maximum size of a tmpfs volume is 50% of the host's total RAM. |
tmpfs-mode | File mode of the tmpfs in octal. For instance, 700 or 0770 . Defaults to 1777 or world-writable. |
$ docker run --mount type=tmpfs,dst=/app,tmpfs-size=21474836480,tmpfs-mode=1770
Use a tmpfs mount in a container
To use a tmpfs
mount in a container, use the --tmpfs
flag, or use the
--mount
flag with type=tmpfs
and destination
options. There is no
source
for tmpfs
mounts. The following example creates a tmpfs
mount at
/app
in a Nginx container. The first example uses the --mount
flag and the
second uses the --tmpfs
flag.
$ docker run -d \
-it \
--name tmptest \
--mount type=tmpfs,destination=/app \
nginx:latest
Verify that the mount is a tmpfs
mount by looking in the Mounts
section of
the docker inspect
output:
$ docker inspect tmptest --format '{{ json .Mounts }}'
[{""Type"":""tmpfs"",""Source"":"""",""Destination"":""/app"",""Mode"":"""",""RW"":true,""Propagation"":""""}]
$ docker run -d \
-it \
--name tmptest \
--tmpfs /app \
nginx:latest
Verify that the mount is a tmpfs
mount by looking in the Mounts
section of
the docker inspect
output:
$ docker inspect tmptest --format '{{ json .Mounts }}'
{""/app"":""""}
Stop and remove the container:
$ docker stop tmptest
$ docker rm tmptest
Next steps
- Learn about volumes
- Learn about bind mounts
- Learn about storage drivers",,,
c50e040523c028050e3a646630c9907bbf8a94940125b59d5e48bbfc1e3389d5,"Extension UI API
The extensions UI runs in a sandboxed environment and doesn't have access to any electron or nodejs APIs.
The extension UI API provides a way for the frontend to perform different actions and communicate with the Docker Desktop dashboard or the underlying system.
JavaScript API libraries, with Typescript support, are available in order to get all the API definitions in to your extension code.
- @docker/extension-api-client gives access to the extension API entrypoint
DockerDesktopCLient
. - @docker/extension-api-client-types can be added as a dev dependency in order to get types auto-completion in your IDE.
import { createDockerDesktopClient } from '@docker/extension-api-client';
export function App() {
// obtain Docker Desktop client
const ddClient = createDockerDesktopClient();
// use ddClient to perform extension actions
}
The ddClient
object gives access to various APIs:
Find the Extensions API reference here.",,,
a744dca245d3252d1c66ed8d1af0df3746a83504fa7f14ddc4824ecd4adb1d47,"Completion
You can generate a shell completion script for the Docker CLI using the docker completion
command. The completion script gives you word completion for
commands, flags, and Docker objects (such as container and volume names) when
you hit <Tab>
as you type into your terminal.
You can generate completion scripts for the following shells:
Bash
To get Docker CLI completion with Bash, you first need to install the
bash-completion
package which contains a number of Bash functions for shell
completion.
# Install using APT:
sudo apt install bash-completion
# Install using Homebrew (Bash version 4 or later):
brew install bash-completion@2
# Homebrew install for older versions of Bash:
brew install bash-completion
# With pacman:
sudo pacman -S bash-completion
After installing bash-completion
, source the script in your shell
configuration file (in this example, .bashrc
):
# On Linux:
cat <<EOT >> ~/.bashrc
if [ -f /etc/bash_completion ]; then
. /etc/bash_completion
fi
EOT
# On macOS / with Homebrew:
cat <<EOT >> ~/.bash_profile
[[ -r ""$(brew --prefix)/etc/profile.d/bash_completion.sh"" ]] && . ""$(brew --prefix)/etc/profile.d/bash_completion.sh""
EOT
And reload your shell configuration:
$ source ~/.bashrc
Now you can generate the Bash completion script using the docker completion
command:
$ mkdir -p ~/.local/share/bash-completion/completions
$ docker completion bash > ~/.local/share/bash-completion/completions/docker
Zsh
The Zsh
completion system
takes care of things as long as the completion can be sourced using FPATH
.
If you use Oh My Zsh, you can install completions without modifying ~/.zshrc
by storing the completion script in the ~/.oh-my-zsh/completions
directory.
$ mkdir -p ~/.oh-my-zsh/completions
$ docker completion zsh > ~/.oh-my-zsh/completions/_docker
If you're not using Oh My Zsh, store the completion script in a directory of
your choice and add the directory to FPATH
in your .zshrc
.
$ mkdir -p ~/.docker/completions
$ docker completion zsh > ~/.docker/completions/_docker
$ cat <<""EOT"" >> ~/.zshrc
FPATH=""$HOME/.docker/completions:$FPATH""
autoload -Uz compinit
compinit
EOT
Fish
fish shell supports a
completion system natively.
To activate completion for Docker commands, copy or symlink the completion script to your fish shell completions/
directory:
$ mkdir -p ~/.config/fish/completions
$ docker completion fish > ~/.config/fish/completions/docker.fish",,,
7d8966e7bc4551781d0ae3997c38347cf38c4cc0cff3729c905953b9553d078d,"Continuous integration with Docker
Continuous Integration (CI) is the part of the development process where you're looking to get your code changes merged with the main branch of the project. At this point, development teams run tests and builds to vet that the code changes don't cause any unwanted or unexpected behaviors.
There are several uses for Docker at this stage of development, even if you don't end up packaging your application as a container image.
Docker as a build environment
Containers are reproducible, isolated environments that yield predictable results. Building and testing your application in a Docker container makes it easier to prevent unexpected behaviors from occurring. Using a Dockerfile, you define the exact requirements for the build environment, including programming runtimes, operating system, binaries, and more.
Using Docker to manage your build environment also eases maintenance. For example, updating to a new version of a programming runtime can be as simple as changing a tag or digest in a Dockerfile. No need to SSH into a pet VM to manually reinstall a newer version and update the related configuration files.
Additionally, just as you expect third-party open source packages to be secure, the same should go for your build environment. You can scan and index a builder image, just like you would for any other containerized application.
The following links provide instructions for how you can get started using Docker for building your applications in CI:
Docker in Docker
You can also use a Dockerized build environment to build container images using Docker. That is, your build environment runs inside a container which itself is equipped to run Docker builds. This method is referred to as ""Docker in Docker"".
Docker provides an official Docker image that you can use for this purpose.
What's next
Docker maintains a set of official GitHub Actions that you can use to build, annotate, and push container images on the GitHub Actions platform. See Introduction to GitHub Actions to learn more and get started.",,,
bf9bfaa879279b987e144e8722b5fe2499c87cd52215e2c6e83f98d9bddd6414,"Docker Desktop
Docker Desktop is a one-click-install application for your Mac, Linux, or Windows environment that lets you build, share, and run containerized applications and microservices.
It provides a straightforward GUI (Graphical User Interface) that lets you manage your containers, applications, and images directly from your machine.
Docker Desktop reduces the time spent on complex setups so you can focus on writing code. It takes care of port mappings, file system concerns, and other default settings, and is regularly updated with bug fixes and security updates.
- Docker Engine
- Docker CLI client
- Docker Scout (additional subscription may apply)
- Docker Build
- Docker Extensions
- Docker Compose
- Docker Content Trust
- Kubernetes
- Credential Helper
- Ability to containerize and share any application on any cloud platform, in multiple languages and frameworks.
- Quick installation and setup of a complete Docker development environment.
- Includes the latest version of Kubernetes.
- On Windows, the ability to toggle between Linux and Windows containers to build applications.
- Fast and reliable performance with native Windows Hyper-V virtualization.
- Ability to work natively on Linux through WSL 2 on Windows machines.
- Volume mounting for code and data, including file change notifications and easy access to running containers on the localhost network.
Docker Desktop works with your choice of development tools and languages and gives you access to a vast library of certified images and templates in Docker Hub. This allows development teams to extend their environment to rapidly auto-build, continuously integrate, and collaborate using a secure repository.",,,
e389f3c539429ba1062f037d0a8b8781dcd3856b2e0cbde6d72ce67ad6f2be24,"Content trust in Docker
When transferring data among networked systems, trust is a central concern. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the integrity and the publisher of all the data a system operates on. You use Docker Engine to push and pull images (data) to a public or private registry. Content trust gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.
About Docker Content Trust (DCT)
Docker Content Trust (DCT) provides the ability to use digital signatures for data sent to and received from remote Docker registries. These signatures allow client-side or runtime verification of the integrity and publisher of specific image tags.
Through DCT, image publishers can sign their images and image consumers can ensure that the images they pull are signed. Publishers could be individuals or organizations manually signing their content or automated software supply chains signing content as part of their release process.
Image tags and DCT
An individual image record has the following identifier:
[REGISTRY_HOST[:REGISTRY_PORT]/]REPOSITORY[:TAG]
A particular image REPOSITORY
can have multiple tags. For example, latest
and
3.1.2
are both tags on the mongo
image. An image publisher can build an image
and tag combination many times changing the image with each build.
DCT is associated with the TAG
portion of an image. Each image repository has
a set of keys that image publishers use to sign an image tag. Image publishers
have discretion on which tags they sign.
An image repository can contain an image with one tag that is signed and another
tag that is not. For example, consider
the Mongo image
repository. The latest
tag could be unsigned while the 3.1.6
tag could be signed. It is the
responsibility of the image publisher to decide if an image tag is signed or
not. In this representation, some image tags are signed, others are not:
Publishers can choose to sign a specific tag or not. As a result, the content of
an unsigned tag and that of a signed tag with the same name may not match. For
example, a publisher can push a tagged image someimage:latest
and sign it.
Later, the same publisher can push an unsigned someimage:latest
image. This second
push replaces the last unsigned tag latest
but does not affect the signed latest
version.
The ability to choose which tags they can sign, allows publishers to iterate over
the unsigned version of an image before officially signing it.
Image consumers can enable DCT to ensure that images they use were signed. If a consumer enables DCT, they can only pull, run, or build with trusted images. Enabling DCT is a bit like applying a ""filter"" to your registry. Consumers ""see"" only signed image tags and the less desirable, unsigned image tags are ""invisible"" to them.
To the consumer who has not enabled DCT, nothing about how they work with Docker images changes. Every image is visible regardless of whether it is signed or not.
Docker Content Trust Keys
Trust for an image tag is managed through the use of signing keys. A key set is created when an operation using DCT is first invoked. A key set consists of the following classes of keys:
- An offline key that is the root of DCT for an image tag
- Repository or tagging keys that sign tags
- Server-managed keys such as the timestamp key, which provides freshness security guarantees for your repository
The following image depicts the various signing keys and their relationships:
Warning
The root key once lost is not recoverable. If you lose any other key, send an email to Docker Hub Support. This loss also requires manual intervention from every consumer that used a signed tag from this repository prior to the loss.
You should back up the root key somewhere safe. Given that it is only required to create new repositories, it is a good idea to store it offline in hardware. For details on securing, and backing up your keys, make sure you read how to manage keys for DCT.
Signing images with Docker Content Trust
Within the Docker CLI we can sign and push a container image with the
$ docker trust
command syntax. This is built on top of the Notary feature
set. For more information, see the
Notary GitHub repository.
A prerequisite for signing an image is a Docker Registry with a Notary server attached (Such as the Docker Hub ). Instructions for standing up a self-hosted environment can be found here.
To sign a Docker Image you will need a delegation key pair. These keys
can be generated locally using $ docker trust key generate
or generated
by a certificate authority.
First we will add the delegation private key to the local Docker trust
repository. (By default this is stored in ~/.docker/trust/
). If you are
generating delegation keys with $ docker trust key generate
, the private key
is automatically added to the local trust store. If you are importing a separate
key, you will need to use the
$ docker trust key load
command.
$ docker trust key generate jeff
Generating key for jeff...
Enter passphrase for new jeff key with ID 9deed25:
Repeat passphrase for new jeff key with ID 9deed25:
Successfully generated and loaded private key. Corresponding public key available: /home/ubuntu/Documents/mytrustdir/jeff.pub
Or if you have an existing key:
$ docker trust key load key.pem --name jeff
Loading key from ""key.pem""...
Enter passphrase for new jeff key with ID 8ae710e:
Repeat passphrase for new jeff key with ID 8ae710e:
Successfully imported key from key.pem
Next we will need to add the delegation public key to the Notary server; this is specific to a particular image repository in Notary known as a Global Unique Name (GUN). If this is the first time you are adding a delegation to that repository, this command will also initiate the repository, using a local Notary canonical root key. To understand more about initiating a repository, and the role of delegations, head to delegations for content trust.
$ docker trust signer add --key cert.pem jeff registry.example.com/admin/demo
Adding signer ""jeff"" to registry.example.com/admin/demo...
Enter passphrase for new repository key with ID 10b5e94:
Finally, we will use the delegation private key to sign a particular tag and push it up to the registry.
$ docker trust sign registry.example.com/admin/demo:1
Signing and pushing trust data for local image registry.example.com/admin/demo:1, may overwrite remote trust data
The push refers to repository [registry.example.com/admin/demo]
7bff100f35cb: Pushed
1: digest: sha256:3d2e482b82608d153a374df3357c0291589a61cc194ec4a9ca2381073a17f58e size: 528
Signing and pushing trust metadata
Enter passphrase for signer key with ID 8ae710e:
Successfully signed registry.example.com/admin/demo:1
Alternatively, once the keys have been imported an image can be pushed with the
$ docker push
command, by exporting the DCT environmental variable.
$ export DOCKER_CONTENT_TRUST=1
$ docker push registry.example.com/admin/demo:1
The push refers to repository [registry.example.com/admin/demo:1]
7bff100f35cb: Pushed
1: digest: sha256:3d2e482b82608d153a374df3357c0291589a61cc194ec4a9ca2381073a17f58e size: 528
Signing and pushing trust metadata
Enter passphrase for signer key with ID 8ae710e:
Successfully signed registry.example.com/admin/demo:1
Remote trust data for a tag or a repository can be viewed by the
$ docker trust inspect
command:
$ docker trust inspect --pretty registry.example.com/admin/demo:1
Signatures for registry.example.com/admin/demo:1
SIGNED TAG DIGEST SIGNERS
1 3d2e482b82608d153a374df3357c0291589a61cc194ec4a9ca2381073a17f58e jeff
List of signers and their keys for registry.example.com/admin/demo:1
SIGNER KEYS
jeff 8ae710e3ba82
Administrative keys for registry.example.com/admin/demo:1
Repository Key: 10b5e94c916a0977471cc08fa56c1a5679819b2005ba6a257aa78ce76d3a1e27
Root Key: 84ca6e4416416d78c4597e754f38517bea95ab427e5f95871f90d460573071fc
Remote Trust data for a tag can be removed by the $ docker trust revoke
command:
$ docker trust revoke registry.example.com/admin/demo:1
Enter passphrase for signer key with ID 8ae710e:
Successfully deleted signature for registry.example.com/admin/demo:1
Client enforcement with Docker Content Trust
Content trust is disabled by default in the Docker Client. To enable
it, set the DOCKER_CONTENT_TRUST
environment variable to 1
. This prevents
users from working with tagged images unless they contain a signature.
When DCT is enabled in the Docker client, docker
CLI commands that operate on
tagged images must either have content signatures or explicit content hashes.
The commands that operate with DCT are:
push
build
create
pull
run
For example, with DCT enabled a docker pull someimage:latest
only
succeeds if someimage:latest
is signed. However, an operation with an explicit
content hash always succeeds as long as the hash exists:
$ docker pull registry.example.com/user/image:1
Error: remote trust data does not exist for registry.example.com/user/image: registry.example.com does not have trust data for registry.example.com/user/image
$ docker pull registry.example.com/user/image@sha256:d149ab53f8718e987c3a3024bb8aa0e2caadf6c0328f1d9d850b2a2a67f2819a
sha256:ee7491c9c31db1ffb7673d91e9fac5d6354a89d0e97408567e09df069a1687c1: Pulling from user/image
ff3a5c916c92: Pull complete
a59a168caba3: Pull complete
Digest: sha256:ee7491c9c31db1ffb7673d91e9fac5d6354a89d0e97408567e09df069a1687c1
Status: Downloaded newer image for registry.example.com/user/image@sha256:ee7491c9c31db1ffb7673d91e9fac5d6354a89d0e97408567e09df069a1687c1",,,
3d5e7585c2f862a23c8aa0f55501a7ac3ab4e390c590c8aededf3bd5c434ff00,"Docker Engine 17.10 release notes
Table of contents
17.10.0-ce
2017-10-17
Important
docker service scale
anddocker service rollback
use non-detached mode as default, use--detach
to keep the old behaviour.
Builder
- Reset uid/gid to 0 in uploaded build context to share build cache with other clients docker/cli#513
- Add support for
ADD
urls without any sub path moby/moby#34217
Client
- Move output of
docker stack rm
to stdout docker/cli#491 - Use natural sort for secrets and configs in cli docker/cli#307
- Use non-detached mode as default for
docker service
commands docker/cli#525 - Set APIVersion on the client, even when Ping fails docker/cli#546
- Fix loader error with different build syntax in
docker stack deploy
docker/cli#544
- Change the default output format for
docker container stats
to showCONTAINER ID
andNAME
docker/cli#565
- Add
--no-trunc
flag todocker container stats
docker/cli#565 - Add experimental
docker trust
:view
,revoke
,sign
subcommands docker/cli#472
- Various doc and shell completion fixes docker/cli#610 docker/cli#611 docker/cli#618 docker/cli#580 docker/cli#598 docker/cli#603
Networking
- Enabling ILB/ELB on windows using per-node, per-network LB endpoint moby/moby#34674
- Overlay fix for transient IP reuse docker/libnetwork#1935
- Serializing bitseq alloc docker/libnetwork#1788
- Disable hostname lookup on chain exists check docker/libnetwork#1974
Runtime
- LCOW: Add UVM debuggability by grabbing logs before tear-down moby/moby#34846
- LCOW: Prepare work for bind mounts moby/moby#34258
- LCOW: Support for docker cp, ADD/COPY on build moby/moby#34252
- LCOW: VHDX boot to readonly moby/moby#34754
- Volume: evaluate symlinks before relabeling mount source moby/moby#34792
- Fixing ‘docker cp’ to allow new target file name in a host symlinked directory moby/moby#31993
- Add support for Windows version filtering on pull moby/moby#35090
Swarm mode
- Produce an error if
docker swarm init --force-new-cluster
is executed on worker nodes moby/moby#34881
- Add support for
.Node.Hostname
templating in swarm services moby/moby#34686
- Increase gRPC request timeout to 20 seconds for sending snapshots docker/swarmkit#2391
- Do not filter nodes if logdriver is set to
none
docker/swarmkit#2396
- Adding ipam options to ipam driver requests docker/swarmkit#2324",,,
63352193104c42b7fde2835172d9a210c4df9df9e3fab22b45add4cdc19d42c1,"Distribute your dev environment
Important
Dev Environments is no longer under active development.
While the current functionality remains available, it may take us longer to respond to support requests.
The compose-dev.yaml
config file makes distributing your dev environment easy so everyone can access the same code and any dependencies.
Distribute your dev environment
When you are ready to share your environment, simply copy the link to the Github repo where your project is stored, and share the link with your team members.
You can also create a link that automatically starts your dev environment when opened. This can then be placed on a GitHub README or pasted into a Slack channel, for example.
To create the link simply join the following link with the link to your dev environment's GitHub repository:
https://open.docker.com/dashboard/dev-envs?url=
The following example opens a Compose sample, a Go server with an Nginx proxy and a MariaDB/MySQL database, in Docker Desktop.
Open a dev environment that has been distributed to you
To open a dev environment that has been shared with you, select the Create button in the top right-hand corner, select source Existing Git repo, and then paste the URL.",,,
db081240b24c2c981be25f571e0fc9d4ab01b4edc73a268e9ea88169802e2199,Home / Manuals / BillingBilling and paymentsUse the resources in this section to manage your billing and payment settings for your Docker subscription plans.Add or update a payment methodLearn how to add or update a payment method for your personal account or organization.Update the billing informationDiscover how to update the billing information for your personal account or organization.View billing historyLearn how to view billing history and download past invoices.Billing FAQsFind the answers you need and explore common questions.Register a tax certificateLearn how to register a tax exemption certificate.3D Secure authenticationDiscover how Docker billing supports 3DS and how to troubleshoot potential issues.,,,
079204a9e5270d5a36d17bc4734d96048aec8859109464a851ef4c7be004c19a,"Docker Engine 17.04 release notes
Table of contents
17.04.0-ce
2017-04-05
Builder
Client
- Output of docker CLI --help is now wrapped to the terminal width #28751
- Suppress image digest in docker ps #30848
- Hide command options that are related to Windows #30788
- Fix
docker plugin install
prompt to accept ""enter"" for the ""N"" default #30769
- Add
truncate
function for Go templates #30484
- Support expanded syntax of ports in
stack deploy
#30476 - Support expanded syntax of mounts in
stack deploy
#30597 #31795
- Add
--add-host
for docker build #30383 - Add
.CreatedAt
placeholder fordocker network ls --format
#29900
- Update order of
--secret-rm
and--secret-add
#29802
- Add
--filter enabled=true
fordocker plugin ls
#28627 - Add
--format
todocker service ls
#28199 - Add
publish
andexpose
filter fordocker ps --filter
#27557
- Support multiple service IDs on
docker service ps
#25234
- Allow swarm join with
--availability=drain
#24993
- Docker inspect now shows ""docker-default"" when AppArmor is enabled and no other profile was defined #27083
Logging
- Implement optional ring buffer for container logs #28762
- Add
--log-opt awslogs-create-group=<true|false>
for awslogs (CloudWatch) to support creation of log groups as needed #29504
- Fix segfault when using the gcplogs logging driver with a ""static"" binary #29478
Networking
- Check parameter
--ip
,--ip6
and--link-local-ip
indocker network connect
#30807
- Added support for
dns-search
#30117 - Added --verbose option for docker network inspect to show task details from all swarm nodes #31710
- Clear stale datapath encryption states when joining the cluster docker/libnetwork#1354
- Ensure iptables initialization only happens once docker/libnetwork#1676
- Fix bad order of iptables filter rules docker/libnetwork#961
- Add anonymous container alias to service record on attachable network docker/libnetwork#1651
- Support for
com.docker.network.container_iface_prefix
driver label docker/libnetwork#1667 - Improve network list performance by omitting network details that are not used #30673
Runtime
- Handle paused container when restoring without live-restore set #31704
- Do not allow sub second in healthcheck options in Dockerfile #31177
- Support name and id prefix in
secret update
#30856 - Use binary frame for websocket attach endpoint #30460
- Fix linux mount calls not applying propagation type changes #30416
- Fix ExecIds leak on failed
exec -i
#30340 - Prune named but untagged images if
danglingOnly=true
#30330
- Add daemon flag to set
no_new_priv
as default for unprivileged containers #29984 - Add daemon option
--default-shm-size
#29692 - Support registry mirror config reload #29650
- Ignore the daemon log config when building images #29552
- Move secret name or ID prefix resolving from client to daemon #29218
- Allow adding rules to
cgroup devices.allow
on container create/run #22563
- Fix
cpu.cfs_quota_us
being reset when runningsystemd daemon-reload
#31736
Swarm Mode
- Topology-aware scheduling #30725
- Automatic service rollback on failure #31108
- Worker and manager on the same node are now connected through a UNIX socket docker/swarmkit#1828, docker/swarmkit#1850, docker/swarmkit#1851
- Improve raft transport package docker/swarmkit#1748
- No automatic manager shutdown on demotion/removal docker/swarmkit#1829
- Use TransferLeadership to make leader demotion safer docker/swarmkit#1939
- Decrease default monitoring period docker/swarmkit#1967
- Add Service logs formatting #31672
- Fix service logs API to be able to specify stream #31313
- Add
--stop-signal
forservice create
andservice update
#30754 - Add
--read-only
forservice create
andservice update
#30162 - Renew the context after communicating with the registry #31586
- (experimental) Add
--tail
and--since
options todocker service logs
#31500 - (experimental) Add
--no-task-ids
and--no-trunc
options todocker service logs
#31672
Windows
- Block pulling Windows images on non-Windows daemons #29001",,,
ec781471da7a46bb167cf00005ef4c1b9a8d178eeda27a027ba2a891c69cc0ce,"Provision users
Once you've configured your SSO connection, the next step is to provision users. This process ensures that users can access your organization. This guide provides an overview of user provisioning and supported provisioning methods.
What is provisioning?
Provisioning helps manage users by automating tasks like creating, updating, and deactivating users based on data from your identity provider (IdP). There are three methods for user provisioning, with benefits for different organization needs:
| Provisioning method | Description | Default setting in Docker | Recommended for |
|---|---|---|---|
| Just-in-Time (JIT) | Automatically create and provisions user accounts when they first sign in via SSO | Enabled by default | Best for organizations who need minimal setup, who have smaller teams, or low-security environments |
| System for Cross-domain Identity Management (SCIM) | Continuously syncs user data between your IdP and Docker, ensuring user attributes remain updated without requiring manual updates | Disabled by default | Best for larger organizations or environments with frequent changes in user information or roles |
| Group mapping | Maps user groups from your IdP to specific roles and permissions within Docker, enabling fine-tuned access control based on group membership | Disabled by default | Best for organizations that require strict access control and for managing users based on their roles and permissions |
Default provisioning setup
By default, Docker enables JIT provisioning when you configure an SSO connection. With JIT enabled, user accounts are automatically created the first time a user signs in using your SSO flow.
JIT provisioning may not provide the level of control or security some organizations need. In such cases, SCIM or group mapping can be configured to give administrators more control over user access and attributes.
SSO attributes
When a user signs in through SSO, Docker obtains several attributes from your IdP to manage the user's identity and permissions. These attributes include:
- Email address: The unique identifier for the user
- Full name: The user's complete name
- Groups: Optional. Used for group-based access control
- Docker Org: Optional. Specifies the organization the user belongs to
- Docker Team: Optional. Defines the team the user belongs to within the organization
- Docker Role: Optional. Determines the user's permission within Docker
If your organization uses SAML for SSO, Docker retrieves these attributes from the SAML assertion message. Keep in mind that different IdPs may use different names for these attributes. The following reference table outlines possible SAML attributes used by Docker:
| SSO Attribute | SAML Assertion Message Attributes |
|---|---|
| Email address | ""http://schemas.xmlsoap.org/ws/2005/05/identity/claims/nameidentifier"" , ""http://schemas.xmlsoap.org/ws/2005/05/identity/claims/upn"" , ""http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress"" , email |
| Full name | ""http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name"" , name , ""http://schemas.xmlsoap.org/ws/2005/05/identity/claims/givenname"" , ""http://schemas.xmlsoap.org/ws/2005/05/identity/claims/surname"" |
| Groups (optional) | ""http://schemas.xmlsoap.org/claims/Group"" , ""http://schemas.microsoft.com/ws/2008/06/identity/claims/groups"" , Groups , groups |
| Docker Org (optional) | dockerOrg |
| Docker Team (optional) | dockerTeam |
| Docker Role (optional) | dockerRole |
What's next?
Review the provisioning method guides for steps on configuring provisioning methods:",,,
adbb92e441f25faa59b13c2d69622cdf0054bb7801978953bc84d1a22f9c879e,"Activity logs
Activity logs display a chronological list of activities that occur at organization and repository levels. It provides a report to owners on all their member activities.
With activity logs, owners can view and track:
- What changes were made
- The date when a change was made
- Who initiated the change
For example, activity logs display activities such as the date when a repository was created or deleted, the member who created the repository, the name of the repository, and when there was a change to the privacy settings.
Owners can also see the activity logs for their repository if the repository is part of the organization subscribed to a Docker Business or Team plan.
Manage activity logs
View the activity logs
To view the activity logs:
- Sign in to Docker Hub.
- Select Organizations, your organization, and then Activity.
Note
Docker retains the activity data for a period of three months.
Customize the activity logs
By default, all activities that occur are displayed on the Activity tab. Use the calendar option to select a date range and customize your results. After you have selected a date range, the activity logs of all the activities that occurred during that period are displayed.
Note
Activities created by the Docker Support team as part of resolving customer issues appear in the activity logs as dockersupport.
Select the All Activities drop-down to view activities that are specific to an organization, repository, or billing. In Docker Hub, if you select the Activities tab from the Repository view, you can only filter repository-level activities.
After choosing Organization, Repository, or Billing, you can further refine the results using the All Actions drop-down.
View the activity logs
To view the activity logs:
- Sign in to the Admin Console.
- Select your organization in the left navigation drop-down menu, and then select Activity logs.
Note
Docker retains the activity data for a period of three months.
Customize the activity logs
By default, all activities that occur are displayed on the Activity tab. Use the calendar option to select a date range and customize your results. After you have selected a date range, the activity logs of all the activities that occurred during that period are displayed.
Note
Activities created by the Docker Support team as part of resolving customer issues appear in the activity logs as dockersupport.
Select the All Activities drop-down to view activities that are specific to an organization, repository, or billing. In Docker Hub, if you select the Activities tab from the Repository view, you can only filter repository-level activities.
After choosing Organization, Repository, or Billing, you can further refine the results using the All Actions drop-down.
Event definitions
Refer to the following section for a list of events and their descriptions:
Organization events
| Event | Description |
|---|---|
| Team Created | Activities related to the creation of a team |
| Team Updated | Activities related to the modification of a team |
| Team Deleted | Activities related to the deletion of a team |
| Team Member Added | Details of the member added to your team |
| Team Member Removed | Details of the member removed from your team |
| Team Member Invited | Details of the member invited to your team |
| Organization Member Added | Details of the member added to your organization |
| Organization Member Removed | Details about the member removed from your organization |
| Member Role Changed | Details about the role changed for a member in your organization |
| Organization Created | Activities related to the creation of a new organization |
| Organization Settings Updated | Details related to the organization setting that was updated |
| Registry Access Management enabled | Activities related to enabling Registry Access Management |
| Registry Access Management disabled | Activities related to disabling Registry Access Management |
| Registry Access Management registry added | Activities related to the addition of a registry |
| Registry Access Management registry removed | Activities related to the removal of a registry |
| Registry Access Management registry updated | Details related to the registry that was updated |
| Single Sign-On domain added | Details of the single sign-on domain added to your organization |
| Single Sign-On domain removed | Details of the single sign-on domain removed from your organization |
| Single Sign-On domain verified | Details of the single sign-on domain verified for your organization |
| Access token created | Access token created in organization |
| Access token updated | Access token updated in organization |
| Access token deleted | Access token deleted in organization |
| Policy created | Details of adding a settings policy |
| Policy updated | Details of updating a settings policy |
| Policy deleted | Details of deleting a settings policy |
| Policy transferred | Details of transferring a settings policy to another owner |
Repository events
Note
Event descriptions that include a user action can refer to a Docker username, personal access token (PAT) or organization access token (OAT). For example, if a user pushes a tag to a repository, the event would include the description:
<user-access-token>
pushed the tag to the repository.
| Event | Description |
|---|---|
| Repository Created | Activities related to the creation of a new repository |
| Repository Deleted | Activities related to the deletion of a repository |
| Repository Updated | Activities related to updating the description, full description, or status of a repository |
| Privacy Changed | Details related to the privacy policies that were updated |
| Tag Pushed | Activities related to the tags pushed |
| Tag Deleted | Activities related to the tags deleted |
| Categories Updated | Activities related to setting or updating categories of a repository |
Billing events
| Event | Description |
|---|---|
| Plan Upgraded | Occurs when your organization’s billing plan is upgraded to a higher tier plan. |
| Plan Downgraded | Occurs when your organization’s billing plan is downgraded to a lower tier plan. |
| Seat Added | Occurs when a seat is added to your organization’s billing plan. |
| Seat Removed | Occurs when a seat is removed from your organization’s billing plan. |
| Billing Cycle Changed | Occurs when there is a change in the recurring interval that your organization is charged. |
| Plan Downgrade Canceled | Occurs when a scheduled plan downgrade for your organization is canceled. |
| Seat Removal Canceled | Occurs when a scheduled seat removal for an organization’s billing plan is canceled. |
| Plan Upgrade Requested | Occurs when a user in your organization requests a plan upgrade. |
| Plan Downgrade Requested | Occurs when a user in your organization requests a plan downgrade. |
| Seat Addition Requested | Occurs when a user in your organization requests an increase in the number of seats. |
| Seat Removal Requested | Occurs when a user in your organization requests a decrease in the number of seats. |
| Billing Cycle Change Requested | Occurs when a user in your organization requests a change in the billing cycle. |
| Plan Downgrade Cancellation Requested | Occurs when a user in your organization requests a cancellation of a scheduled plan downgrade. |
| Seat Removal Cancellation Requested | Occurs when a user in your organization requests a cancellation of a scheduled seat removal. |",,,
c7afd31e7cd72c58ce5676db8c76b5495c431ec7990c3ef83c61678a3d624170,"Build variables
In Docker Build, build arguments (ARG
) and environment variables (ENV
)
both serve as a means to pass information into the build process.
You can use them to parameterize the build, allowing for more flexible and configurable builds.
Warning
Build arguments and environment variables are inappropriate for passing secrets to your build, because they're exposed in the final image. Instead, use secret mounts or SSH mounts, which expose secrets to your builds securely.
See Build secrets for more information.
Similarities and differences
Build arguments and environment variables are similar.
They're both declared in the Dockerfile and can be set using flags for the docker build
command.
Both can be used to parameterize the build.
But they each serve a distinct purpose.
Build arguments
Build arguments are variables for the Dockerfile itself. Use them to parameterize values of Dockerfile instructions. For example, you might use a build argument to specify the version of a dependency to install.
Build arguments have no effect on the build unless it's used in an instruction. They're not accessible or present in containers instantiated from the image unless explicitly passed through from the Dockerfile into the image filesystem or configuration. They may persist in the image metadata, as provenance attestations and in the image history, which is why they're not suitable for holding secrets.
They make Dockerfiles more flexible, and easier to maintain.
For an example on how you can use build arguments,
see
ARG
usage example.
Environment variables
Environment variables are passed through to the build execution environment, and persist in containers instantiated from the image.
Environment variables are primarily used to:
- Configure the execution environment for builds
- Set default environment variables for containers
Environment variables, if set, can directly influence the execution of your build, and the behavior or configuration of the application.
You can't override or set an environment variable at build-time. Values for environment variables must be declared in the Dockerfile. You can combine environment variables and build arguments to allow environment variables to be configured at build-time.
For an example on how to use environment variables for configuring builds,
see
ENV
usage example.
ARG
usage example
Build arguments are commonly used to specify versions of components, such as image variants or package versions, used in a build.
Specifying versions as build arguments lets you build with different versions without having to manually update the Dockerfile. It also makes it easier to maintain the Dockerfile, since it lets you declare versions at the top of the file.
Build arguments can also be a way to reuse a value in multiple places.
For example, if you use multiple flavors of alpine
in your build,
you can ensure you're using the same version of alpine
everywhere:
golang:1.22-alpine${ALPINE_VERSION}
python:3.12-alpine${ALPINE_VERSION}
nginx:1-alpine${ALPINE_VERSION}
The following example defines the version of node
and alpine
using build arguments.
# syntax=docker/dockerfile:1
ARG NODE_VERSION=""20""
ARG ALPINE_VERSION=""3.21""
FROM node:${NODE_VERSION}-alpine${ALPINE_VERSION} AS base
WORKDIR /src
FROM base AS build
COPY package*.json ./
RUN npm ci
RUN npm run build
FROM base AS production
COPY package*.json ./
RUN npm ci --omit=dev && npm cache clean --force
COPY --from=build /src/dist/ .
CMD [""node"", ""app.js""]
In this case, the build arguments have default values.
Specifying their values when you invoke a build is optional.
To override the defaults, you would use the --build-arg
CLI flag:
$ docker build --build-arg NODE_VERSION=current .
For more information on how to use build arguments, refer to:
ENV
usage example
Declaring an environment variable with ENV
makes the variable
available to all subsequent instructions in the build stage.
The following example shows an example setting NODE_ENV
to production
before installing JavaScript dependencies with npm
.
Setting the variable makes npm
omits packages needed only for local development.
# syntax=docker/dockerfile:1
FROM node:20
WORKDIR /app
COPY package*.json ./
ENV NODE_ENV=production
RUN npm ci && npm cache clean --force
COPY . .
CMD [""node"", ""app.js""]
Environment variables aren't configurable at build-time by default.
If you want to change the value of an ENV
at build-time,
you can combine environment variables and build arguments:
# syntax=docker/dockerfile:1
FROM node:20
ARG NODE_ENV=production
ENV NODE_ENV=$NODE_ENV
WORKDIR /app
COPY package*.json ./
RUN npm ci && npm cache clean --force
COPY . .
CMD [""node"", ""app.js""]
With this Dockerfile, you can use --build-arg
to override the default value of NODE_ENV
:
$ docker build --build-arg NODE_ENV=development .
Note that, because the environment variables you set persist in containers, using them can lead to unintended side-effects for the application's runtime.
For more information on how to use environment variables in builds, refer to:
Scoping
Build arguments declared in the global scope of a Dockerfile aren't automatically inherited into the build stages. They're only accessible in the global scope.
# syntax=docker/dockerfile:1
# The following build argument is declared in the global scope:
ARG NAME=""joe""
FROM alpine
# The following instruction doesn't have access to the $NAME build argument
# because the argument was defined in the global scope, not for this stage.
RUN echo ""hello ${NAME}!""
The echo
command in this example evaluates to hello !
because the value of the NAME
build argument is out of scope.
To inherit global build arguments into a stage, you must consume them:
# syntax=docker/dockerfile:1
# Declare the build argument in the global scope
ARG NAME=""joe""
FROM alpine
# Consume the build argument in the build stage
ARG NAME
RUN echo $NAME
Once a build argument is declared or consumed in a stage, it's automatically inherited by child stages.
# syntax=docker/dockerfile:1
FROM alpine AS base
# Declare the build argument in the build stage
ARG NAME=""joe""
# Create a new stage based on ""base""
FROM base AS build
# The NAME build argument is available here
# since it's declared in a parent stage
RUN echo ""hello $NAME!""
The following diagram further exemplifies how build argument and environment variable inheritance works for multi-stage builds.
Pre-defined build arguments
This section describes pre-defined build arguments available to all builds by default.
Multi-platform build arguments
Multi-platform build arguments describe the build and target platforms for the build.
The build platform is the operating system, architecture, and platform variant of the host system where the builder (the BuildKit daemon) is running.
BUILDPLATFORM
BUILDOS
BUILDARCH
BUILDVARIANT
The target platform arguments hold the same values for the target platforms for the build,
specified using the --platform
flag for the docker build
command.
TARGETPLATFORM
TARGETOS
TARGETARCH
TARGETVARIANT
These arguments are useful for doing cross-compilation in multi-platform builds. They're available in the global scope of the Dockerfile, but they aren't automatically inherited by build stages. To use them inside stage, you must declare them:
# syntax=docker/dockerfile:1
# Pre-defined build arguments are available in the global scope
FROM --platform=$BUILDPLATFORM golang
# To inherit them to a stage, declare them with ARG
ARG TARGETOS
RUN GOOS=$TARGETOS go build -o ./exe .
For more information about multi-platform build arguments, refer to Multi-platform arguments
Proxy arguments
Proxy build arguments let you specify proxies to use for your build.
You don't need to declare or reference these arguments in the Dockerfile.
Specifying a proxy with --build-arg
is enough to make your build use the proxy.
Proxy arguments are automatically excluded from the build cache
and the output of docker history
by default.
If you do reference the arguments in your Dockerfile,
the proxy configuration ends up in the build cache.
The builder respects the following proxy build arguments. The variables are case insensitive.
HTTP_PROXY
HTTPS_PROXY
FTP_PROXY
NO_PROXY
ALL_PROXY
To configure a proxy for your build:
$ docker build --build-arg HTTP_PROXY=https://my-proxy.example.com .
For more information about proxy build arguments, refer to Proxy arguments.
Build tool configuration variables
The following environment variables enable, disable, or change the behavior of Buildx and BuildKit.
Note that these variables aren't used to configure the build container;
they aren't available inside the build and they have no relation to the ENV
instruction.
They're used to configure the Buildx client, or the BuildKit daemon.
| Variable | Type | Description |
|---|---|---|
| BUILDKIT_COLORS | String | Configure text color for the terminal output. |
| BUILDKIT_HOST | String | Specify host to use for remote builders. |
| BUILDKIT_PROGRESS | String | Configure type of progress output. |
| BUILDKIT_TTY_LOG_LINES | String | Number of log lines (for active steps in TTY mode). |
| BUILDX_BAKE_GIT_AUTH_HEADER | String | HTTP authentication scheme for remote Bake files. |
| BUILDX_BAKE_GIT_AUTH_TOKEN | String | HTTP authentication token for remote Bake files. |
| BUILDX_BAKE_GIT_SSH | String | SSH authentication for remote Bake files. |
| BUILDX_BUILDER | String | Specify the builder instance to use. |
| BUILDX_CONFIG | String | Specify location for configuration, state, and logs. |
| BUILDX_CPU_PROFILE | String | Generate a pprof CPU profile at the specified location. |
| BUILDX_EXPERIMENTAL | Boolean | Turn on experimental features. |
| BUILDX_GIT_CHECK_DIRTY | Boolean | Enable dirty Git checkout detection. |
| BUILDX_GIT_INFO | Boolean | Remove Git information in provenance attestations. |
| BUILDX_GIT_LABELS | String | Boolean | Add Git provenance labels to images. |
| BUILDX_MEM_PROFILE | String | Generate a pprof memory profile at the specified location. |
| BUILDX_METADATA_PROVENANCE | String | Boolean | Customize provenance informations included in the metadata file. |
| BUILDX_METADATA_WARNINGS | String | Include build warnings in the metadata file. |
| BUILDX_NO_DEFAULT_ATTESTATIONS | Boolean | Turn off default provenance attestations. |
| BUILDX_NO_DEFAULT_LOAD | Boolean | Turn off loading images to image store by default. |
| EXPERIMENTAL_BUILDKIT_SOURCE_POLICY | String | Specify a BuildKit source policy file. |
BuildKit also supports a few additional configuration parameters. Refer to BuildKit built-in build args.
You can express Boolean values for environment variables in different ways.
For example, true
, 1
, and T
all evaluate to true.
Evaluation is done using the strconv.ParseBool
function in the Go standard library.
See the
reference documentation for details.
BUILDKIT_COLORS
Changes the colors of the terminal output. Set BUILDKIT_COLORS
to a CSV string
in the following format:
$ export BUILDKIT_COLORS=""run=123,20,245:error=yellow:cancel=blue:warning=white""
Color values can be any valid RGB hex code, or one of the BuildKit predefined colors.
Setting NO_COLOR
to anything turns off colorized output, as recommended by
no-color.org.
BUILDKIT_HOST
You use the BUILDKIT_HOST
to specify the address of a BuildKit daemon to use
as a remote builder. This is the same as specifying the address as a positional
argument to docker buildx create
.
Usage:
$ export BUILDKIT_HOST=tcp://localhost:1234
$ docker buildx create --name=remote --driver=remote
If you specify both the BUILDKIT_HOST
environment variable and a positional
argument, the argument takes priority.
BUILDKIT_PROGRESS
Sets the type of the BuildKit progress output. Valid values are:
auto
(default)plain
tty
quiet
rawjson
Usage:
$ export BUILDKIT_PROGRESS=plain
BUILDKIT_TTY_LOG_LINES
You can change how many log lines are visible for active steps in TTY mode by
setting BUILDKIT_TTY_LOG_LINES
to a number (default to 6
).
$ export BUILDKIT_TTY_LOG_LINES=8
EXPERIMENTAL_BUILDKIT_SOURCE_POLICY
Lets you specify a BuildKit source policy file for creating reproducible builds with pinned dependencies.
$ export EXPERIMENTAL_BUILDKIT_SOURCE_POLICY=./policy.json
Example:
{
""rules"": [
{
""action"": ""CONVERT"",
""selector"": {
""identifier"": ""docker-image://docker.io/library/alpine:latest""
},
""updates"": {
""identifier"": ""docker-image://docker.io/library/alpine:latest@sha256:4edbd2beb5f78b1014028f4fbb99f3237d9561100b6881aabbf5acce2c4f9454""
}
},
{
""action"": ""CONVERT"",
""selector"": {
""identifier"": ""https://raw.githubusercontent.com/moby/buildkit/v0.10.1/README.md""
},
""updates"": {
""attrs"": {""http.checksum"": ""sha256:6e4b94fc270e708e1068be28bd3551dc6917a4fc5a61293d51bb36e6b75c4b53""}
}
},
{
""action"": ""DENY"",
""selector"": {
""identifier"": ""docker-image://docker.io/library/golang*""
}
}
]
}
BUILDX_BAKE_GIT_AUTH_HEADER
Sets the HTTP authentication scheme when using a remote Bake definition in a private Git repository.
This is equivalent to the
GIT_AUTH_HEADER
secret,
but facilitates the pre-flight authentication in Bake when loading the remote Bake file.
Supported values are bearer
(default) and basic
.
Usage:
$ export BUILDX_BAKE_GIT_AUTH_HEADER=basic
BUILDX_BAKE_GIT_AUTH_TOKEN
Sets the HTTP authentication token when using a remote Bake definition in a private Git repository.
This is equivalent to the
GIT_AUTH_TOKEN
secret,
but facilitates the pre-flight authentication in Bake when loading the remote Bake file.
Usage:
$ export BUILDX_BAKE_GIT_AUTH_TOKEN=$(cat git-token.txt)
BUILDX_BAKE_GIT_SSH
Lets you specify a list of SSH agent socket filepaths to forward to Bake for authenticating to a Git server when using a remote Bake definition in a private repository. This is similar to SSH mounts for builds, but facilitates the pre-flight authentication in Bake when resolving the build definition.
Setting this environment is typically not necessary, because Bake will use the SSH_AUTH_SOCK
agent socket by default.
You only need to specify this variable if you want to use a socket with a different filepath.
This variable can take multiple paths using a comma-separated string.
Usage:
$ export BUILDX_BAKE_GIT_SSH=/run/foo/listener.sock,~/.creds/ssh.sock
BUILDX_BUILDER
Overrides the configured builder instance. Same as the docker buildx --builder
CLI flag.
Usage:
$ export BUILDX_BUILDER=my-builder
BUILDX_CONFIG
You can use BUILDX_CONFIG
to specify the directory to use for build
configuration, state, and logs. The lookup order for this directory is as
follows:
$BUILDX_CONFIG
$DOCKER_CONFIG/buildx
~/.docker/buildx
(default)
Usage:
$ export BUILDX_CONFIG=/usr/local/etc
BUILDX_CPU_PROFILE
If specified, Buildx generates a pprof
CPU profile at the specified location.
Note
This property is only useful for when developing Buildx. The profiling data is not relevant for analyzing a build's performance.
Usage:
$ export BUILDX_CPU_PROFILE=buildx_cpu.prof
BUILDX_EXPERIMENTAL
Enables experimental build features.
Usage:
$ export BUILDX_EXPERIMENTAL=1
BUILDX_GIT_CHECK_DIRTY
When set to true, checks for dirty state in source control information for provenance attestations.
Usage:
$ export BUILDX_GIT_CHECK_DIRTY=1
BUILDX_GIT_INFO
When set to false, removes source control information from provenance attestations.
Usage:
$ export BUILDX_GIT_INFO=0
BUILDX_GIT_LABELS
Adds provenance labels, based on Git information, to images that you build. The labels are:
com.docker.image.source.entrypoint
: Location of the Dockerfile relative to the project rootorg.opencontainers.image.revision
: Git commit revisionorg.opencontainers.image.source
: SSH or HTTPS address of the repository
Example:
""Labels"": {
""com.docker.image.source.entrypoint"": ""Dockerfile"",
""org.opencontainers.image.revision"": ""5734329c6af43c2ae295010778cd308866b95d9b"",
""org.opencontainers.image.source"": ""git@github.com:foo/bar.git""
}
Usage:
- Set
BUILDX_GIT_LABELS=1
to include theentrypoint
andrevision
labels. - Set
BUILDX_GIT_LABELS=full
to include all labels.
If the repository is in a dirty state, the revision
gets a -dirty
suffix.
BUILDX_MEM_PROFILE
If specified, Buildx generates a pprof
memory profile at the specified
location.
Note
This property is only useful for when developing Buildx. The profiling data is not relevant for analyzing a build's performance.
Usage:
$ export BUILDX_MEM_PROFILE=buildx_mem.prof
BUILDX_METADATA_PROVENANCE
By default, Buildx includes minimal provenance information in the metadata file
through
--metadata-file
flag.
This environment variable allows you to customize the provenance information
included in the metadata file:
min
sets minimal provenance (default).max
sets full provenance.disabled
,false
or0
does not set any provenance.
BUILDX_METADATA_WARNINGS
By default, Buildx does not include build warnings in the metadata file through
--metadata-file
flag.
You can set this environment variable to 1
or true
to include them.
BUILDX_NO_DEFAULT_ATTESTATIONS
By default, BuildKit v0.11 and later adds
provenance attestations to images you
build. Set BUILDX_NO_DEFAULT_ATTESTATIONS=1
to disable the default provenance
attestations.
Usage:
$ export BUILDX_NO_DEFAULT_ATTESTATIONS=1
BUILDX_NO_DEFAULT_LOAD
When you build an image using the docker
driver, the image is automatically
loaded to the image store when the build finishes. Set BUILDX_NO_DEFAULT_LOAD
to disable automatic loading of images to the local container store.
Usage:
$ export BUILDX_NO_DEFAULT_LOAD=1",,,
7c22bf5ed45e8378e6530feb076c7633d06258db23f334cf3ae7c137282364a7,"Synchronized file shares
Synchronized file shares is an alternative file sharing mechanism that provides fast and flexible host-to-VM file sharing, enhancing bind mount performance through the use of synchronized filesystem caches.
Who is it for?
Synchronized file shares is ideal for developers who:
- Have large repositories or monorepos with 100 000 files or more totaling hundreds of megabytes or even gigabytes.
- Are using virtual filesystems, such as VirtioFS, gRPC FUSE, and osxfs, which are no longer scaling well with their codebases.
- Regularly encounter performance limitations.
- Don't want to worry about file ownership or spend time resolving conflicting file-ownership information when modifying multiple containers.
How does Synchronized file shares work?
A Synchronized file share behaves just like a virtual file share, but takes advantage of a high-performance, low-latency code synchronization engine to create a synchronized cache of the host files on an ext4 filesystem within the Docker Desktop VM. If you make filesystem changes on the host or in the VM’s containers, it propagates via bidirectional synchronization.
After creating a file share instance, any container using a bind mount that points to a location on the host filesystem matching the specified synchronized file share location, or a subdirectory within it, utilizes the Synchronized File Shares feature. Bind mounts that don't satisfy this condition are passed to the normal virtual filesystem bind-mounting mechanism, for example VirtioFS or gRPC-FUSE.
Note
Synchronized file shares is not used by Kubernetes'
hostPath
volumes in Docker Desktop.
Important
Synchronized file shares isn't available on WSL or when using Windows containers.
Create a file share instance
To create a file share instance:
- Sign in to Docker Desktop.
- In Settings, navigate to the File sharing tab within the Resources section.
- In the Synchronized File Shares section, select the Create share icon.
- Select a host folder to share. The synchronized file share should initialize and be usable.
File shares take a few seconds to initialize as files are copied into the Docker Desktop VM. During this time, the status indicator displays Preparing. There is also a status icon in the footer of the Docker Desktop Dashboard that keeps you updated.
When the status indicator displays Watching for filesystem changes, your files are available to the VM through all the standard bind mount mechanisms, whether that's -v
in the command line or specified in your compose.yml
file.
Note
When you create a new service, setting the bind mount option consistency to
:consistent
bypasses Synchronized file shares.
Tip
Compose can now automatically create file shares for bind mounts. Ensure you're signed in to Docker with a paid subscription and have enabled both Access experimental features and Manage Synchronized file shares with Compose in Docker Desktop's settings.
Explore your file share instance
The Synchronized file shares section displays all your file share instances and provides useful information about each instance including:
- The origin of the file share content
- A status update
- How much space each file share is using
- The number of filesystem entry counts
- The number of symbolic links
- Which container(s) is using the file share instance
Selecting a file share instance expands the dropdown and exposes this information.
Use .syncignore
You can use a .syncignore
file at the root of each file share, to exclude local files from your file share instance. It supports the same syntax as .dockerignore
files and excludes, and/or re-includes, paths from synchronization. .syncignore
files are ignored at any location other than the root of the file share.
Some example of things you might want to add to your .syncignore
file are:
- Large dependency directories, for example
node_modules
andcomposer
directories (unless you rely on accessing them via a bind mount) .git
directories (again, unless you need them)
In general, use your .syncignore
file to exclude items that aren't critical to your workflow, especially those that would be slow to sync or use significant storage.
Known issues
Changes made to
.syncignore
don't lead to immediate deletions unless the file share is recreated. In other words, files that are newly ignored due to modifications in the.syncignore
file remain in their current location, but are no longer updated during synchronization.File share instances are currently limited to approximately 2 million files per share. For best performance, if you have a file share instance of this size, try to decompose it into multiple shares corresponding to individual bind mount locations.
Case conflicts, due to Linux being case-sensitive and macOS/Windows only being case-preserving, display as File exists problems in the GUI. These can be ignored. However, if they persist, you can report the issue.
Synchronized file shares proactively reports temporary issues, which can result in occasional Conflict and Problem indicators appearing in the GUI during synchronization. These can be ignored. However, if they persist, you can report the issue.
If you switch from WSL2 to Hyper-V on Windows, Docker Desktop needs to be fully restarted.
POSIX-style Windows paths are not supported. Avoid setting the
COMPOSE_CONVERT_WINDOWS_PATHS
environment variable in Docker Compose.If you don't have the correct permissions to create symbolic links and your container attempts to create symbolic links in your file share instance, an unable to create symbolic link error message displays. For Windows users, see Microsoft's Create symbolic links documentation for best practices and location of the Create symbolic links security policy setting. For Mac and Linux users, check that you have write permissions on the folder.
Feedback and support
To give feedback or report bugs, visit:",,,
edf25198bf3a4c99156ab3eeafd2c30b3870917ebe2cc1fc2b81facab35ca217,"Troubleshoot single sign-on
While configuring or using single sign-on (SSO), you may encounter issues that can stem from your identity provider (IdP) or Docker configuration. The following sections describe some common SSO errors and possible solutions.
Check for errors
If you experience issues with SSO, check both the Docker Admin Console and your identity provider (IdP) for errors first.
Check Docker error logs
- Sign in to the Admin Console and select your organization.
- Select SSO and SCIM.
- In the SSO connections table, select the Action menu and then View error logs.
- For more details on specific errors, select View error details next to an error message.
- Note any errors you see on this page for further troubleshooting.
Check for errors in your IdP
- Review your IdP’s logs or audit trails for any failed authentication or provisioning attempts.
- Confirm that your IdP’s SSO settings match the values provided in Docker.
- If applicable, confirm that you have configured user provisioning correctly and that it is enabled in your IdP.
- If applicable, verify that your IdP correctly maps Docker's required user attributes.
- Try provisioning a test user from your IdP and verify if they appear in Docker.
For further troubleshooting, check your IdP’s documentation. You can also contact their support team for guidance on error messages.
Groups are not formatted correctly
Error message
When this issue occurs, the following error message is common:
Some of the groups assigned to the user are not formatted as '<organization name>:<team name>'. Directory groups will be ignored and user will be provisioned into the default organization and team.
Possible causes
- Incorrect group name formatting in your identity provider (IdP): Docker requires groups to follow the format
<organization>:<team>
. If the groups assigned to a user do not follow this format, they will be ignored. - Non-matching groups between IdP and Docker organization: If a group in your IdP does not have a corresponding team in Docker, it will not be recognized, and the user will be placed in the default organization and team.
Affected environments
- Docker single sign-on setup using IdPs such as Okta or Azure AD
- Organizations using group-based role assignments in Docker
Steps to replicate
To replicate this issue:
- Attempt to sign in to Docker using SSO.
- The user is assigned groups in the IdP but does not get placed in the expected Docker Team.
- Review Docker logs or IdP logs to find the error message.
Solutions
Update group names in your IdP:
- Go to your IdP's group management section.
- Check the groups assigned to the affected user.
- Ensure each group follows the required format:
<organization>:<team>
- Update any incorrectly formatted groups to match this pattern.
- Save changes and retry signing in with SSO.
User is not assigned to the organization
Error message
When this issue occurs, the following error message is common:
User '$username' is not assigned to this SSO organization. Contact your administrator. TraceID: XXXXXXXXXXXXX
Possible causes
- User is not assigned to the organization: If Just-in-Time (JIT) provisioning is disabled, the user may not be assigned to your organization.
- User is not invited to the organization: If JIT is disabled and you do not want to enable it, the user must be manually invited.
- SCIM provisioning is misconfigured: If you use SCIM for user provisioning, it may not be correctly syncing users from your IdP.
Solutions
Enable JIT provisioning
JIT is enabled by default when you enable SSO. If you have JIT disabled and need to re-enable it:
- Sign in to the Admin Console and select your organization.
- Select SSO and SCIM.
- In the SSO connections table, select the Action menu and then Enable JIT provisioning.
- Select Enable to confirm.
Manually invite users
When JIT is disabled, users are not automatically added to your organization when they authenticate through SSO. To manually invite users, see Invite members
Configure SCIM provisioning
If you have SCIM enabled, troubleshoot your SCIM connection using the following steps:
- Sign in to the Admin Console and select your organization.
- Select SSO and SCIM.
- In the SSO connections table, select the Action menu and then View error logs. For more details on specific errors, select View error details next to an error message. Note any errors you see on this page.
- Navigate back to the SSO and SCIM page of the Admin Console and verify your SCIM configuration:
- Ensure that the SCIM Base URL and API Token in your IdP match those provided in the Docker Admin Console.
- Verify that SCIM is enabled in both Docker and your IdP.
- Ensure that the attributes being synced from your IdP match Docker's supported attributes for SCIM.
- Test user provisioning by trying to provision a test user through your IdP and verify if they appear in Docker.
IdP-initiated sign in is not enabled for connection
Error message
When this issue occurs, the following error message is common:
IdP-Initiated sign in is not enabled for connection '$ssoConnection'.
Possible causes
Docker does not support an IdP-initiated SAML flow. This error occurs when a user attempts to authenticate from your IdP, such as using the Docker SSO app tile on the sign in page.
Solutions
Authenticate from Docker apps
The user must initiate authentication from Docker applications (Hub, Desktop, etc). The user needs to enter their email address in a Docker app and they will get redirected to the configured SSO IdP for their domain.
Hide the Docker SSO app
You can hide the Docker SSO app from users in your IdP. This prevents users from attempting to start authentication from the IdP dashboard. You must hide and configure this in your IdP.
Not enough seats in organization
Error message
When this issue occurs, the following error message is common:
Not enough seats in organization '$orgName'. Add more seats or contact your administrator.
Possible causes
This error occurs when the organization has no available seats for the user when provisioning via Just-in-Time (JIT) provisioning or SCIM.
Solutions
Add more seats to the organization
Purchase additional Docker Business subscription seats. For details, see Manage subscription seats.
Remove users or pending invitations
Review your organization members and pending invitations. Remove inactive users or pending invitations to free up seats. For more details, see Manage organization members.
Domain is not verified for SSO connection
Error message
When this issue occurs, the following error message is common:
Domain '$emailDomain' is not verified for your SSO connection. Contact your company administrator. TraceID: XXXXXXXXXXXXXX
Possible causes
This error occurs if the IdP authenticated a user through SSO and the User Principal Name (UPN) returned to Docker doesn’t match any of the verified domains associated to the SSO connection configured in Docker.
Solutions
Verify UPN attribute mapping
Ensure that the IdP SSO connection is returning the correct UPN value in the assertion attributes.
Add and verify all domains
Add and verify all domains and subdomains used as UPN by your IdP and associate them with your Docker SSO connection. For details, see Configure single sign-on.
Unable to find session
Error message
When this issue occurs, the following error message is common:
We couldn't find your session. You may have pressed the back button, refreshed the page, opened too many sign-in dialogs, or there is some issue with cookies. Try signing in again. If the issue persists, contact your administrator.
Possible causes
The following causes may create this issue:
- The user pressed the back or refresh button during authentication.
- The authentication flow lost track of the initial request, preventing completion.
Solutions
Do not disrupt the authentication flow
Do not press the back or refresh button during sign-in.
Restart authentication
Close the browser tab and restart the authentication flow from the Docker application (Desktop, Hub, etc).
Name ID is not an email address
Error message
When this issue occurs, the following error message is common:
The name ID sent by the identity provider is not an email address. Contact your company administrator.
Possible causes
The following causes may create this issue:
- The IdP sends a Name ID (UPN) that does not comply with the email format required by Docker.
- Docker SSO requires the Name ID to be the primary email address of the user.
Solutions
In your IdP, ensure the Name ID attribute format is correct:
- Verify that the Name ID attribute format in your IdP is set to
EmailAddress
. - Adjust your IdP settings to return the correct Name ID format.",,,
bb62cd83ca15168789e93b3f7abc2691be725d55236fff7f07762b411b151f13,"Cache management with GitHub Actions
This page contains examples on using the cache storage backends with GitHub Actions.
Note
See Cache storage backends for more details about cache storage backends.
Inline cache
In most cases you want to use the
inline cache exporter.
However, note that the inline
cache exporter only supports min
cache mode.
To use max
cache mode, push the image and the cache separately using the
registry cache exporter with the cache-to
option, as shown in the
registry cache example.
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build and push
uses: docker/build-push-action@v6
with:
push: true
tags: user/app:latest
cache-from: type=registry,ref=user/app:latest
cache-to: type=inline
Registry cache
You can import/export cache from a cache manifest or (special) image configuration on the registry with the registry cache exporter.
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build and push
uses: docker/build-push-action@v6
with:
push: true
tags: user/app:latest
cache-from: type=registry,ref=user/app:buildcache
cache-to: type=registry,ref=user/app:buildcache,mode=max
GitHub cache
Cache backend API
The
GitHub Actions cache exporter
backend uses the
GitHub Cache API
to fetch and upload cache blobs. That's why you should only use this cache
backend in a GitHub Action workflow, as the url
($ACTIONS_CACHE_URL
) and
token
($ACTIONS_RUNTIME_TOKEN
) attributes only get populated in a workflow
context.
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build and push
uses: docker/build-push-action@v6
with:
push: true
tags: user/app:latest
cache-from: type=gha
cache-to: type=gha,mode=max
Cache mounts
BuildKit doesn't preserve cache mounts in the GitHub Actions cache by default.
If you wish to put your cache mounts into GitHub Actions cache and reuse it
between builds, you can use a workaround provided by
reproducible-containers/buildkit-cache-dance
.
This GitHub Action creates temporary containers to extract and inject the cache mount data with your Docker build steps.
The following example shows how to use this workaround with a Go project.
Example Dockerfile in build/package/Dockerfile
FROM golang:1.21.1-alpine as base-build
WORKDIR /build
RUN go env -w GOMODCACHE=/root/.cache/go-build
COPY go.mod go.sum ./
RUN --mount=type=cache,target=/root/.cache/go-build go mod download
COPY ./src ./
RUN --mount=type=cache,target=/root/.cache/go-build go build -o /bin/app /build/src
...
Example CI action
name: ci
on:
push:
jobs:
build:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up QEMU
uses: docker/setup-qemu-action@v3
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Docker meta
id: meta
uses: docker/metadata-action@v5
with:
images: user/app
tags: |
type=ref,event=branch
type=ref,event=pr
type=semver,pattern={{version}}
type=semver,pattern={{major}}.{{minor}}
- name: Go Build Cache for Docker
uses: actions/cache@v4
with:
path: go-build-cache
key: ${{ runner.os }}-go-build-cache-${{ hashFiles('**/go.sum') }}
- name: Inject go-build-cache
uses: reproducible-containers/buildkit-cache-dance@4b2444fec0c0fb9dbf175a96c094720a692ef810 # v2.1.4
with:
cache-source: go-build-cache
- name: Build and push
uses: docker/build-push-action@v6
with:
cache-from: type=gha
cache-to: type=gha,mode=max
file: build/package/Dockerfile
push: ${{ github.event_name != 'pull_request' }}
tags: ${{ steps.meta.outputs.tags }}
labels: ${{ steps.meta.outputs.labels }}
platforms: linux/amd64,linux/arm64
For more information about this workaround, refer to the GitHub repository.
Local cache
Warning
At the moment, old cache entries aren't deleted, so the cache size keeps growing. The following example uses the
Move cache
step as a workaround (seemoby/buildkit#1896
for more info).
You can also leverage GitHub cache using the actions/cache and local cache exporter with this action:
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Cache Docker layers
uses: actions/cache@v4
with:
path: ${{ runner.temp }}/.buildx-cache
key: ${{ runner.os }}-buildx-${{ github.sha }}
restore-keys: |
${{ runner.os }}-buildx-
- name: Build and push
uses: docker/build-push-action@v6
with:
push: true
tags: user/app:latest
cache-from: type=local,src=${{ runner.temp }}/.buildx-cache
cache-to: type=local,dest=${{ runner.temp }}/.buildx-cache-new,mode=max
- # Temp fix
# https://github.com/docker/build-push-action/issues/252
# https://github.com/moby/buildkit/issues/1896
name: Move cache
run: |
rm -rf ${{ runner.temp }}/.buildx-cache
mv ${{ runner.temp }}/.buildx-cache-new ${{ runner.temp }}/.buildx-cache",,,
fe47337bd819da69e458cc9894e836cfca9720234260683e476ea58e42e0c5bf,"Explore Docker Desktop
When you open Docker Desktop, the Docker Desktop Dashboard displays.
The Containers view provides a runtime view of all your containers and applications. It allows you to interact with containers and applications, and manage the lifecycle of your applications directly from your machine. This view also provides an intuitive interface to perform common actions to inspect, interact with, and manage your Docker objects including containers and Docker Compose-based applications. For more information, see Explore running containers and applications.
The Images view displays a list of your Docker images and allows you to run an image as a container, pull the latest version of an image from Docker Hub, and inspect images. It also displays a summary of image vulnerabilities. In addition, the Images view contains clean-up options to remove unwanted images from the disk to reclaim space. If you are logged in, you can also see the images you and your organization have shared on Docker Hub. For more information, see Explore your images.
The Volumes view displays a list of volumes and allows you to easily create and delete volumes and see which ones are being used. For more information, see Explore volumes.
The Builds view lets you inspect your build history and manage builders. By default, it displays a list of all your ongoing and completed builds. Explore builds.
In addition, the Docker Desktop Dashboard lets you:
Navigate to the Settings menu to configure your Docker Desktop settings. Select the Settings icon in the Dashboard header.
Access the Troubleshoot menu to debug and perform restart operations. Select the Troubleshoot icon in the Dashboard header.
Be notified of new releases, installation progress updates, and more in the Notifications center. Select the bell icon in the bottom-right corner of the Docker Desktop Dashboard to access the notification center.
Access the Learning center from the Dashboard header. It helps you get started with quick in-app walkthroughs and provides other resources for learning about Docker.
For a more detailed guide about getting started, see Get started.
Get to the Docker Scout dashboard.
Check the status of Docker services.
Access Docker Hub to search, browse, pull, run, or view details of images.
Quick search
From the Docker Desktop Dashboard you can use Quick Search, which is located in the Dashboard header, to search for:
Any container or Compose application on your local system. You can see an overview of associated environment variables or perform quick actions, such as start, stop, or delete.
Public Docker Hub images, local images, and images from remote repositories (private repositories from organizations you're a part of in Hub). Depending on the type of image you select, you can either pull the image by tag, view documentation, go to Docker Hub for more details, or run a new container using the image.
Extensions. From here, you can learn more about the extension and install it with a single click. Or, if you already have an extension installed, you can open it straight from the search results.
Any volume. From here you can view the associated container.
Docs. Find help from Docker's official documentation straight from Docker Desktop.
The Docker menu
Docker Desktop also provides an easy-access tray icon that appears in the taskbar and is referred to as the Docker menu .
To display the Docker menu, select the icon. It displays the following options:
- Dashboard. This takes you to the Docker Desktop Dashboard.
- Sign in/Sign up
- Settings
- Check for updates
- Troubleshoot
- Give feedback
- Switch to Windows containers (if you're on Windows)
- About Docker Desktop. Contains information on the versions you are running, and links to the Subscription Service Agreement for example.
- Docker Hub
- Documentation
- Extensions
- Kubernetes
- Restart
- Quit Docker Desktop",,,
97fee3fd6e65e70e6f9c8a5687a2dfee8d09d580775a02f58389be89f4da0220,"Rootless mode
Rootless mode allows running the Docker daemon and containers as a non-root user to mitigate potential vulnerabilities in the daemon and the container runtime.
Rootless mode does not require root privileges even during the installation of the Docker daemon, as long as the prerequisites are met.
How it works
Rootless mode executes the Docker daemon and containers inside a user namespace.
This is very similar to
userns-remap
mode, except that
with userns-remap
mode, the daemon itself is running with root privileges,
whereas in rootless mode, both the daemon and the container are running without
root privileges.
Rootless mode does not use binaries with SETUID
bits or file capabilities,
except newuidmap
and newgidmap
, which are needed to allow multiple
UIDs/GIDs to be used in the user namespace.
Prerequisites
You must install
newuidmap
andnewgidmap
on the host. These commands are provided by theuidmap
package on most distributions./etc/subuid
and/etc/subgid
should contain at least 65,536 subordinate UIDs/GIDs for the user. In the following example, the usertestuser
has 65,536 subordinate UIDs/GIDs (231072-296607).
$ id -u
1001
$ whoami
testuser
$ grep ^$(whoami): /etc/subuid
testuser:231072:65536
$ grep ^$(whoami): /etc/subgid
testuser:231072:65536
Distribution-specific hint
Tip
We recommend that you use the Ubuntu kernel.
Install
dbus-user-session
package if not installed. Runsudo apt-get install -y dbus-user-session
and relogin.Install
uidmap
package if not installed. Runsudo apt-get install -y uidmap
.If running in a terminal where the user was not directly logged into, you will need to install
systemd-container
withsudo apt-get install -y systemd-container
, then switch to TheUser with the commandsudo machinectl shell TheUser@
.overlay2
storage driver is enabled by default ( Ubuntu-specific kernel patch).Ubuntu 24.04 and later enables restricted unprivileged user namespaces by default, which prevents unprivileged processes in creating user namespaces unless an AppArmor profile is configured to allow programs to use unprivileged user namespaces.
If you install
docker-ce-rootless-extras
using the deb package (apt-get install docker-ce-rootless-extras
), then the AppArmor profile forrootlesskit
is already bundled with theapparmor
deb package. With this installation method, you don't need to add any manual the AppArmor configuration. If you install the rootless extras using the installation script, however, you must add an AppArmor profile forrootlesskit
manually:Create and install the currently logged-in user's AppArmor profile:
$ filename=$(echo $HOME/bin/rootlesskit | sed -e s@^/@@ -e s@/@.@g) $ cat <<EOF > ~/${filename} abi <abi/4.0>, include <tunables/global> ""$HOME/bin/rootlesskit"" flags=(unconfined) { userns, include if exists <local/${filename}> } EOF $ sudo mv ~/${filename} /etc/apparmor.d/${filename}
Restart AppArmor.
$ systemctl restart apparmor.service
Install
dbus-user-session
package if not installed. Runsudo apt-get install -y dbus-user-session
and relogin.For Debian 11, installing
fuse-overlayfs
is recommended. Runsudo apt-get install -y fuse-overlayfs
. This step is not required on Debian 12.Rootless docker requires version of
slirp4netns
greater thanv0.4.0
(whenvpnkit
is not installed). Check you have this with$ slirp4netns --version
If you do not have this download and install with
sudo apt-get install -y slirp4netns
or download the latest release.
Installing
fuse-overlayfs
is recommended. Runsudo pacman -S fuse-overlayfs
.Add
kernel.unprivileged_userns_clone=1
to/etc/sysctl.conf
(or/etc/sysctl.d
) and runsudo sysctl --system
For openSUSE 15 and SLES 15, Installing
fuse-overlayfs
is recommended. Runsudo zypper install -y fuse-overlayfs
. This step is not required on openSUSE Tumbleweed.sudo modprobe ip_tables iptable_mangle iptable_nat iptable_filter
is required. This might be required on other distributions as well depending on the configuration.Known to work on openSUSE 15 and SLES 15.
For RHEL 8 and similar distributions, installing
fuse-overlayfs
is recommended. Runsudo dnf install -y fuse-overlayfs
. This step is not required on RHEL 9 and similar distributions.You might need
sudo dnf install -y iptables
.
Known limitations
- Only the following storage drivers are supported:
overlay2
(only if running with kernel 5.11 or later, or Ubuntu-flavored kernel)fuse-overlayfs
(only if running with kernel 4.18 or later, andfuse-overlayfs
is installed)btrfs
(only if running with kernel 4.18 or later, or~/.local/share/docker
is mounted withuser_subvol_rm_allowed
mount option)vfs
- Cgroup is supported only when running with cgroup v2 and systemd. See Limiting resources.
- Following features are not supported:
- AppArmor
- Checkpoint
- Overlay network
- Exposing SCTP ports
- To use the
ping
command, see Routing ping packets. - To expose privileged TCP/UDP ports (< 1024), see Exposing privileged ports.
IPAddress
shown indocker inspect
is namespaced inside RootlessKit's network namespace. This means the IP address is not reachable from the host withoutnsenter
-ing into the network namespace.- Host network (
docker run --net=host
) is also namespaced inside RootlessKit. - NFS mounts as the docker ""data-root"" is not supported. This limitation is not specific to rootless mode.
Install
Note
If the system-wide Docker daemon is already running, consider disabling it:
$ sudo systemctl disable --now docker.service docker.socket $ sudo rm /var/run/docker.sock
Should you choose not to shut down the
docker
service and socket, you will need to use the--force
parameter in the next section. There are no known issues, but until you shutdown and disable you're still running rootful Docker.
If you installed Docker 20.10 or later with
RPM/DEB packages, you should have dockerd-rootless-setuptool.sh
in /usr/bin
.
Run dockerd-rootless-setuptool.sh install
as a non-root user to set up the daemon:
$ dockerd-rootless-setuptool.sh install
[INFO] Creating /home/testuser/.config/systemd/user/docker.service
...
[INFO] Installed docker.service successfully.
[INFO] To control docker.service, run: `systemctl --user (start|stop|restart) docker.service`
[INFO] To run docker.service on system startup, run: `sudo loginctl enable-linger testuser`
[INFO] Make sure the following environment variables are set (or add them to ~/.bashrc):
export PATH=/usr/bin:$PATH
export DOCKER_HOST=unix:///run/user/1000/docker.sock
If dockerd-rootless-setuptool.sh
is not present, you may need to install the docker-ce-rootless-extras
package manually, e.g.,
$ sudo apt-get install -y docker-ce-rootless-extras
If you do not have permission to run package managers like apt-get
and dnf
,
consider using the installation script available at
https://get.docker.com/rootless.
Since static packages are not available for s390x
, hence it is not supported for s390x
.
$ curl -fsSL https://get.docker.com/rootless | sh
...
[INFO] Creating /home/testuser/.config/systemd/user/docker.service
...
[INFO] Installed docker.service successfully.
[INFO] To control docker.service, run: `systemctl --user (start|stop|restart) docker.service`
[INFO] To run docker.service on system startup, run: `sudo loginctl enable-linger testuser`
[INFO] Make sure the following environment variables are set (or add them to ~/.bashrc):
export PATH=/home/testuser/bin:$PATH
export DOCKER_HOST=unix:///run/user/1000/docker.sock
The binaries will be installed at ~/bin
.
See Troubleshooting if you faced an error.
Uninstall
To remove the systemd service of the Docker daemon, run dockerd-rootless-setuptool.sh uninstall
:
$ dockerd-rootless-setuptool.sh uninstall
+ systemctl --user stop docker.service
+ systemctl --user disable docker.service
Removed /home/testuser/.config/systemd/user/default.target.wants/docker.service.
[INFO] Uninstalled docker.service
[INFO] This uninstallation tool does NOT remove Docker binaries and data.
[INFO] To remove data, run: `/usr/bin/rootlesskit rm -rf /home/testuser/.local/share/docker`
Unset environment variables PATH and DOCKER_HOST if you have added them to ~/.bashrc
.
To remove the data directory, run rootlesskit rm -rf ~/.local/share/docker
.
To remove the binaries, remove docker-ce-rootless-extras
package if you installed Docker with package managers.
If you installed Docker with
https://get.docker.com/rootless (
Install without packages),
remove the binary files under ~/bin
:
$ cd ~/bin
$ rm -f containerd containerd-shim containerd-shim-runc-v2 ctr docker docker-init docker-proxy dockerd dockerd-rootless-setuptool.sh dockerd-rootless.sh rootlesskit rootlesskit-docker-proxy runc vpnkit
Usage
Daemon
The systemd unit file is installed as ~/.config/systemd/user/docker.service
.
Use systemctl --user
to manage the lifecycle of the daemon:
$ systemctl --user start docker
To launch the daemon on system startup, enable the systemd service and lingering:
$ systemctl --user enable docker
$ sudo loginctl enable-linger $(whoami)
Starting Rootless Docker as a systemd-wide service (/etc/systemd/system/docker.service
)
is not supported, even with the User=
directive.
To run the daemon directly without systemd, you need to run dockerd-rootless.sh
instead of dockerd
.
The following environment variables must be set:
$HOME
: the home directory$XDG_RUNTIME_DIR
: an ephemeral directory that is only accessible by the expected user, e,g,~/.docker/run
. The directory should be removed on every host shutdown. The directory can be on tmpfs, however, should not be under/tmp
. Locating this directory under/tmp
might be vulnerable to TOCTOU attack.
Remarks about directory paths:
- The socket path is set to
$XDG_RUNTIME_DIR/docker.sock
by default.$XDG_RUNTIME_DIR
is typically set to/run/user/$UID
. - The data dir is set to
~/.local/share/docker
by default. The data dir should not be on NFS. - The daemon config dir is set to
~/.config/docker
by default. This directory is different from~/.docker
that is used by the client.
Client
You need to specify either the socket path or the CLI context explicitly.
To specify the socket path using $DOCKER_HOST
:
$ export DOCKER_HOST=unix://$XDG_RUNTIME_DIR/docker.sock
$ docker run -d -p 8080:80 nginx
To specify the CLI context using docker context
:
$ docker context use rootless
rootless
Current context is now ""rootless""
$ docker run -d -p 8080:80 nginx
Best practices
Rootless Docker in Docker
To run Rootless Docker inside ""rootful"" Docker, use the docker:<version>-dind-rootless
image instead of docker:<version>-dind
.
$ docker run -d --name dind-rootless --privileged docker:25.0-dind-rootless
The docker:<version>-dind-rootless
image runs as a non-root user (UID 1000).
However, --privileged
is required for disabling seccomp, AppArmor, and mount
masks.
Expose Docker API socket through TCP
To expose the Docker API socket through TCP, you need to launch dockerd-rootless.sh
with DOCKERD_ROOTLESS_ROOTLESSKIT_FLAGS=""-p 0.0.0.0:2376:2376/tcp""
.
$ DOCKERD_ROOTLESS_ROOTLESSKIT_FLAGS=""-p 0.0.0.0:2376:2376/tcp"" \
dockerd-rootless.sh \
-H tcp://0.0.0.0:2376 \
--tlsverify --tlscacert=ca.pem --tlscert=cert.pem --tlskey=key.pem
Expose Docker API socket through SSH
To expose the Docker API socket through SSH, you need to make sure $DOCKER_HOST
is set on the remote host.
$ ssh -l <REMOTEUSER> <REMOTEHOST> 'echo $DOCKER_HOST'
unix:///run/user/1001/docker.sock
$ docker -H ssh://<REMOTEUSER>@<REMOTEHOST> run ...
Routing ping packets
On some distributions, ping
does not work by default.
Add net.ipv4.ping_group_range = 0 2147483647
to /etc/sysctl.conf
(or
/etc/sysctl.d
) and run sudo sysctl --system
to allow using ping
.
Exposing privileged ports
To expose privileged ports (< 1024), set CAP_NET_BIND_SERVICE
on rootlesskit
binary and restart the daemon.
$ sudo setcap cap_net_bind_service=ep $(which rootlesskit)
$ systemctl --user restart docker
Or add net.ipv4.ip_unprivileged_port_start=0
to /etc/sysctl.conf
(or
/etc/sysctl.d
) and run sudo sysctl --system
.
Limiting resources
Limiting resources with cgroup-related docker run
flags such as --cpus
, --memory
, --pids-limit
is supported only when running with cgroup v2 and systemd.
See
Changing cgroup version to enable cgroup v2.
If docker info
shows none
as Cgroup Driver
, the conditions are not satisfied.
When these conditions are not satisfied, rootless mode ignores the cgroup-related docker run
flags.
See
Limiting resources without cgroup for workarounds.
If docker info
shows systemd
as Cgroup Driver
, the conditions are satisfied.
However, typically, only memory
and pids
controllers are delegated to non-root users by default.
$ cat /sys/fs/cgroup/user.slice/user-$(id -u).slice/user@$(id -u).service/cgroup.controllers
memory pids
To allow delegation of all controllers, you need to change the systemd configuration as follows:
# mkdir -p /etc/systemd/system/user@.service.d
# cat > /etc/systemd/system/user@.service.d/delegate.conf << EOF
[Service]
Delegate=cpu cpuset io memory pids
EOF
# systemctl daemon-reload
Note
Delegating
cpuset
requires systemd 244 or later.
Limiting resources without cgroup
Even when cgroup is not available, you can still use the traditional ulimit
and
cpulimit
,
though they work in process-granularity rather than in container-granularity,
and can be arbitrarily disabled by the container process.
For example:
To limit CPU usage to 0.5 cores (similar to
docker run --cpus 0.5
):docker run <IMAGE> cpulimit --limit=50 --include-children <COMMAND>
To limit max VSZ to 64MiB (similar to
docker run --memory 64m
):docker run <IMAGE> sh -c ""ulimit -v 65536; <COMMAND>""
To limit max number of processes to 100 per namespaced UID 2000 (similar to
docker run --pids-limit=100
):docker run --user 2000 --ulimit nproc=100 <IMAGE> <COMMAND>
Troubleshooting
Unable to install with systemd when systemd is present on the system
$ dockerd-rootless-setuptool.sh install
[INFO] systemd not detected, dockerd-rootless.sh needs to be started manually:
...
rootlesskit
cannot detect systemd properly if you switch to your user via sudo su
. For users which cannot be logged-in, you must use the machinectl
command which is part of the systemd-container
package. After installing systemd-container
switch to myuser
with the following command:
$ sudo machinectl shell myuser@
Where myuser@
is your desired username and @ signifies this machine.
Errors when starting the Docker daemon
[rootlesskit:parent] error: failed to start the child: fork/exec /proc/self/exe: operation not permitted
This error occurs mostly when the value of /proc/sys/kernel/unprivileged_userns_clone
is set to 0:
$ cat /proc/sys/kernel/unprivileged_userns_clone
0
To fix this issue, add kernel.unprivileged_userns_clone=1
to
/etc/sysctl.conf
(or /etc/sysctl.d
) and run sudo sysctl --system
.
[rootlesskit:parent] error: failed to start the child: fork/exec /proc/self/exe: no space left on device
This error occurs mostly when the value of /proc/sys/user/max_user_namespaces
is too small:
$ cat /proc/sys/user/max_user_namespaces
0
To fix this issue, add user.max_user_namespaces=28633
to
/etc/sysctl.conf
(or /etc/sysctl.d
) and run sudo sysctl --system
.
[rootlesskit:parent] error: failed to setup UID/GID map: failed to compute uid/gid map: No subuid ranges found for user 1001 (""testuser"")
This error occurs when /etc/subuid
and /etc/subgid
are not configured. See
Prerequisites.
could not get XDG_RUNTIME_DIR
This error occurs when $XDG_RUNTIME_DIR
is not set.
On a non-systemd host, you need to create a directory and then set the path:
$ export XDG_RUNTIME_DIR=$HOME/.docker/xrd
$ rm -rf $XDG_RUNTIME_DIR
$ mkdir -p $XDG_RUNTIME_DIR
$ dockerd-rootless.sh
Note
You must remove the directory every time you log out.
On a systemd host, log into the host using pam_systemd
(see below).
The value is automatically set to /run/user/$UID
and cleaned up on every logout.
systemctl --user
fails with ""Failed to connect to bus: No such file or directory""
This error occurs mostly when you switch from the root user to a non-root user with sudo
:
# sudo -iu testuser
$ systemctl --user start docker
Failed to connect to bus: No such file or directory
Instead of sudo -iu <USERNAME>
, you need to log in using pam_systemd
. For example:
- Log in through the graphic console
ssh <USERNAME>@localhost
machinectl shell <USERNAME>@
The daemon does not start up automatically
You need sudo loginctl enable-linger $(whoami)
to enable the daemon to start
up automatically. See
Usage.
iptables failed: iptables -t nat -N DOCKER: Fatal: can't open lock file /run/xtables.lock: Permission denied
This error may happen with an older version of Docker when SELinux is enabled on the host.
The issue has been fixed in Docker 20.10.8.
A known workaround for older version of Docker is to run the following commands to disable SELinux for iptables
:
$ sudo dnf install -y policycoreutils-python-utils && sudo semanage permissive -a iptables_t
docker pull
errors
docker: failed to register layer: Error processing tar file(exit status 1): lchown <FILE>: invalid argument
This error occurs when the number of available entries in /etc/subuid
or
/etc/subgid
is not sufficient. The number of entries required vary across
images. However, 65,536 entries are sufficient for most images. See
Prerequisites.
docker: failed to register layer: ApplyLayer exit status 1 stdout: stderr: lchown <FILE>: operation not permitted
This error occurs mostly when ~/.local/share/docker
is located on NFS.
A workaround is to specify non-NFS data-root
directory in ~/.config/docker/daemon.json
as follows:
{""data-root"":""/somewhere-out-of-nfs""}
docker run
errors
docker: Error response from daemon: OCI runtime create failed: ...: read unix @->/run/systemd/private: read: connection reset by peer: unknown.
This error occurs on cgroup v2 hosts mostly when the dbus daemon is not running for the user.
$ systemctl --user is-active dbus
inactive
$ docker run hello-world
docker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:385: applying cgroup configuration for process caused: error while starting unit ""docker
-931c15729b5a968ce803784d04c7421f791d87e5ca1891f34387bb9f694c488e.scope"" with properties [{Name:Description Value:""libcontainer container 931c15729b5a968ce803784d04c7421f791d87e5ca1891f34387bb9f694c488e""} {Name:Slice Value:""use
r.slice""} {Name:PIDs Value:@au [4529]} {Name:Delegate Value:true} {Name:MemoryAccounting Value:true} {Name:CPUAccounting Value:true} {Name:IOAccounting Value:true} {Name:TasksAccounting Value:true} {Name:DefaultDependencies Val
ue:false}]: read unix @->/run/systemd/private: read: connection reset by peer: unknown.
To fix the issue, run sudo apt-get install -y dbus-user-session
or sudo dnf install -y dbus-daemon
, and then relogin.
If the error still occurs, try running systemctl --user enable --now dbus
(without sudo).
--cpus
, --memory
, and --pids-limit
are ignored
This is an expected behavior on cgroup v1 mode. To use these flags, the host needs to be configured for enabling cgroup v2. For more information, see Limiting resources.
Networking errors
This section provides troubleshooting tips for networking in rootless mode.
Networking in rootless mode is supported via network and port drivers in RootlessKit. Network performance and characteristics depend on the combination of network and port driver you use. If you're experiencing unexpected behavior or performance related to networking, review the following table which shows the configurations supported by RootlessKit, and how they compare:
| Network driver | Port driver | Net throughput | Port throughput | Source IP propagation | No SUID | Note |
|---|---|---|---|---|---|---|
slirp4netns | builtin | Slow | Fast ✅ | ❌ | ✅ | Default in a typical setup |
vpnkit | builtin | Slow | Fast ✅ | ❌ | ✅ | Default when slirp4netns isn't installed |
slirp4netns | slirp4netns | Slow | Slow | ✅ | ✅ | |
pasta | implicit | Slow | Fast ✅ | ✅ | ✅ | Experimental; Needs pasta version 2023_12_04 or later |
lxc-user-nic | builtin | Fast ✅ | Fast ✅ | ❌ | ❌ | Experimental |
bypass4netns | bypass4netns | Fast ✅ | Fast ✅ | ✅ | ✅ | Note: Not integrated to RootlessKit as it needs a custom seccomp profile |
For information about troubleshooting specific networking issues, see:
docker run -p
fails withcannot expose privileged port
- Ping doesn't work
IPAddress
shown indocker inspect
is unreachable--net=host
doesn't listen ports on the host network namespace- Network is slow
docker run -p
does not propagate source IP addresses
docker run -p
fails with cannot expose privileged port
docker run -p
fails with this error when a privileged port (< 1024) is specified as the host port.
$ docker run -p 80:80 nginx:alpine
docker: Error response from daemon: driver failed programming external connectivity on endpoint focused_swanson (9e2e139a9d8fc92b37c36edfa6214a6e986fa2028c0cc359812f685173fa6df7): Error starting userland proxy: error while calling PortManager.AddPort(): cannot expose privileged port 80, you might need to add ""net.ipv4.ip_unprivileged_port_start=0"" (currently 1024) to /etc/sysctl.conf, or set CAP_NET_BIND_SERVICE on rootlesskit binary, or choose a larger port number (>= 1024): listen tcp 0.0.0.0:80: bind: permission denied.
When you experience this error, consider using an unprivileged port instead. For example, 8080 instead of 80.
$ docker run -p 8080:80 nginx:alpine
To allow exposing privileged ports, see Exposing privileged ports.
Ping doesn't work
Ping does not work when /proc/sys/net/ipv4/ping_group_range
is set to 1 0
:
$ cat /proc/sys/net/ipv4/ping_group_range
1 0
For details, see Routing ping packets.
IPAddress
shown in docker inspect
is unreachable
This is an expected behavior, as the daemon is namespaced inside RootlessKit's
network namespace. Use docker run -p
instead.
--net=host
doesn't listen ports on the host network namespace
This is an expected behavior, as the daemon is namespaced inside RootlessKit's
network namespace. Use docker run -p
instead.
Network is slow
Docker with rootless mode uses slirp4netns as the default network stack if slirp4netns v0.4.0 or later is installed. If slirp4netns is not installed, Docker falls back to VPNKit. Installing slirp4netns may improve the network throughput.
For more information about network drivers for RootlessKit, see RootlessKit documentation.
Also, changing MTU value may improve the throughput.
The MTU value can be specified by creating ~/.config/systemd/user/docker.service.d/override.conf
with the following content:
[Service]
Environment=""DOCKERD_ROOTLESS_ROOTLESSKIT_MTU=<INTEGER>""
And then restart the daemon:
$ systemctl --user daemon-reload
$ systemctl --user restart docker
docker run -p
does not propagate source IP addresses
This is because Docker in rootless mode uses RootlessKit's builtin
port
driver by default, which doesn't support source IP propagation. To enable
source IP propagation, you can:
- Use the
slirp4netns
RootlessKit port driver - Use the
pasta
RootlessKit network driver, with theimplicit
port driver
The pasta
network driver is experimental, but provides improved throughput
performance compared to the slirp4netns
port driver. The pasta
driver
requires Docker Engine version 25.0 or later.
To change the RootlessKit networking configuration:
Create a file at
~/.config/systemd/user/docker.service.d/override.conf
.Add the following contents, depending on which configuration you would like to use:
slirp4netns
[Service] Environment=""DOCKERD_ROOTLESS_ROOTLESSKIT_NET=slirp4netns"" Environment=""DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=slirp4netns""
pasta
network driver withimplicit
port driver[Service] Environment=""DOCKERD_ROOTLESS_ROOTLESSKIT_NET=pasta"" Environment=""DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=implicit""
Restart the daemon:
$ systemctl --user daemon-reload $ systemctl --user restart docker
For more information about networking options for RootlessKit, see:
Tips for debugging
Entering into dockerd
namespaces
The dockerd-rootless.sh
script executes dockerd
in its own user, mount, and network namespaces.
For debugging, you can enter the namespaces by running
nsenter -U --preserve-credentials -n -m -t $(cat $XDG_RUNTIME_DIR/docker.pid)
.",,,
67634d79a025b9b4464d67ab7ade43eceab1f9ed9896108eeeb305f148eabc76,"Docker network driver plugins
This document describes Docker Engine network driver plugins generally available in Docker Engine. To view information on plugins managed by Docker Engine, refer to Docker Engine plugin system.
Docker Engine network plugins enable Engine deployments to be extended to support a wide range of networking technologies, such as VXLAN, IPVLAN, MACVLAN or something completely different. Network driver plugins are supported via the LibNetwork project. Each plugin is implemented as a ""remote driver"" for LibNetwork, which shares plugin infrastructure with Engine. Effectively, network driver plugins are activated in the same way as other plugins, and use the same kind of protocol.
Network plugins and Swarm mode
Legacy plugins do not work in Swarm mode. However, plugins written using the v2 plugin system do work in Swarm mode, as long as they are installed on each Swarm worker node.
Use network driver plugins
The means of installing and running a network driver plugin depend on the particular plugin. So, be sure to install your plugin according to the instructions obtained from the plugin developer.
Once running however, network driver plugins are used just like the built-in network drivers: by being mentioned as a driver in network-oriented Docker commands. For example,
$ docker network create --driver weave mynet
Some network driver plugins are listed in plugins
The mynet
network is now owned by weave
, so subsequent commands
referring to that network will be sent to the plugin,
$ docker run --network=mynet busybox top
Find network plugins
Network plugins are written by third parties, and are published by those third parties, either on Docker Hub or on the third party's site.
Write a network plugin
Network plugins implement the Docker plugin API and the network plugin protocol
Network plugin protocol
The network driver protocol, in addition to the plugin activation call, is documented as part of libnetwork: https://github.com/moby/moby/blob/master/libnetwork/docs/remote.md.
Related Information
To interact with the Docker maintainers and other interested users, see the IRC channel #docker-network
.
- Docker networks feature overview
- The LibNetwork project",,,
753fe70853b8d550e8596babda2c271b28ebbfd7435ca0c9c687b9045c1f837a,"Administer and maintain a swarm of Docker Engines
When you run a swarm of Docker Engines, manager nodes are the key components for managing the swarm and storing the swarm state. It is important to understand some key features of manager nodes to properly deploy and maintain the swarm.
Refer to How nodes work for a brief overview of Docker Swarm mode and the difference between manager and worker nodes.
Operate manager nodes in a swarm
Swarm manager nodes use the Raft Consensus Algorithm to manage the swarm state. You only need to understand some general concepts of Raft in order to manage a swarm.
There is no limit on the number of manager nodes. The decision about how many manager nodes to implement is a trade-off between performance and fault-tolerance. Adding manager nodes to a swarm makes the swarm more fault-tolerant. However, additional manager nodes reduce write performance because more nodes must acknowledge proposals to update the swarm state. This means more network round-trip traffic.
Raft requires a majority of managers, also called the quorum, to agree on proposed updates to the swarm, such as node additions or removals. Membership operations are subject to the same constraints as state replication.
Maintain the quorum of managers
If the swarm loses the quorum of managers, the swarm cannot perform management tasks. If your swarm has multiple managers, always have more than two. To maintain quorum, a majority of managers must be available. An odd number of managers is recommended, because the next even number does not make the quorum easier to keep. For instance, whether you have 3 or 4 managers, you can still only lose 1 manager and maintain the quorum. If you have 5 or 6 managers, you can still only lose two.
Even if a swarm loses the quorum of managers, swarm tasks on existing worker nodes continue to run. However, swarm nodes cannot be added, updated, or removed, and new or existing tasks cannot be started, stopped, moved, or updated.
See Recovering from losing the quorum for troubleshooting steps if you do lose the quorum of managers.
Configure the manager to advertise on a static IP address
When initiating a swarm, you must specify the --advertise-addr
flag to
advertise your address to other manager nodes in the swarm. For more
information, see
Run Docker Engine in swarm mode. Because manager nodes are
meant to be a stable component of the infrastructure, you should use a fixed
IP address for the advertise address to prevent the swarm from becoming
unstable on machine reboot.
If the whole swarm restarts and every manager node subsequently gets a new IP address, there is no way for any node to contact an existing manager. Therefore the swarm is hung while nodes try to contact one another at their old IP addresses.
Dynamic IP addresses are OK for worker nodes.
Add manager nodes for fault tolerance
You should maintain an odd number of managers in the swarm to support manager node failures. Having an odd number of managers ensures that during a network partition, there is a higher chance that the quorum remains available to process requests if the network is partitioned into two sets. Keeping the quorum is not guaranteed if you encounter more than two network partitions.
| Swarm Size | Majority | Fault Tolerance |
|---|---|---|
| 1 | 1 | 0 |
| 2 | 2 | 0 |
| 3 | 2 | 1 |
| 4 | 3 | 1 |
| 5 | 3 | 2 |
| 6 | 4 | 2 |
| 7 | 4 | 3 |
| 8 | 5 | 3 |
| 9 | 5 | 4 |
For example, in a swarm with 5 nodes, if you lose 3 nodes, you don't have a quorum. Therefore you can't add or remove nodes until you recover one of the unavailable manager nodes or recover the swarm with disaster recovery commands. See Recover from disaster.
While it is possible to scale a swarm down to a single manager node, it is
impossible to demote the last manager node. This ensures you maintain access to
the swarm and that the swarm can still process requests. Scaling down to a
single manager is an unsafe operation and is not recommended. If
the last node leaves the swarm unexpectedly during the demote operation, the
swarm becomes unavailable until you reboot the node or restart with
--force-new-cluster
.
You manage swarm membership with the docker swarm
and docker node
subsystems. Refer to
Add nodes to a swarm for more information
on how to add worker nodes and promote a worker node to be a manager.
Distribute manager nodes
In addition to maintaining an odd number of manager nodes, pay attention to datacenter topology when placing managers. For optimal fault-tolerance, distribute manager nodes across a minimum of 3 availability-zones to support failures of an entire set of machines or common maintenance scenarios. If you suffer a failure in any of those zones, the swarm should maintain the quorum of manager nodes available to process requests and rebalance workloads.
| Swarm manager nodes | Repartition (on 3 Availability zones) |
|---|---|
| 3 | 1-1-1 |
| 5 | 2-2-1 |
| 7 | 3-2-2 |
| 9 | 3-3-3 |
Run manager-only nodes
By default manager nodes also act as a worker nodes. This means the scheduler can assign tasks to a manager node. For small and non-critical swarms assigning tasks to managers is relatively low-risk as long as you schedule services using resource constraints for cpu and memory.
However, because manager nodes use the Raft consensus algorithm to replicate data in a consistent way, they are sensitive to resource starvation. You should isolate managers in your swarm from processes that might block swarm operations like swarm heartbeat or leader elections.
To avoid interference with manager node operation, you can drain manager nodes to make them unavailable as worker nodes:
$ docker node update --availability drain <NODE>
When you drain a node, the scheduler reassigns any tasks running on the node to other available worker nodes in the swarm. It also prevents the scheduler from assigning tasks to the node.
Add worker nodes for load balancing
Add nodes to the swarm to balance your swarm's load. Replicated service tasks are distributed across the swarm as evenly as possible over time, as long as the worker nodes are matched to the requirements of the services. When limiting a service to run on only specific types of nodes, such as nodes with a specific number of CPUs or amount of memory, remember that worker nodes that do not meet these requirements cannot run these tasks.
Monitor swarm health
You can monitor the health of manager nodes by querying the docker nodes
API
in JSON format through the /nodes
HTTP endpoint. Refer to the
nodes API documentation
for more information.
From the command line, run docker node inspect <id-node>
to query the nodes.
For instance, to query the reachability of the node as a manager:
$ docker node inspect manager1 --format ""{{ .ManagerStatus.Reachability }}""
reachable
To query the status of the node as a worker that accept tasks:
$ docker node inspect manager1 --format ""{{ .Status.State }}""
ready
From those commands, we can see that manager1
is both at the status
reachable
as a manager and ready
as a worker.
An unreachable
health status means that this particular manager node is unreachable
from other manager nodes. In this case you need to take action to restore the unreachable
manager:
- Restart the daemon and see if the manager comes back as reachable.
- Reboot the machine.
- If neither restarting nor rebooting works, you should add another manager node or promote a worker to be a manager node. You also need to cleanly remove the failed node entry from the manager set with
docker node demote <NODE>
anddocker node rm <id-node>
.
Alternatively you can also get an overview of the swarm health from a manager
node with docker node ls
:
$ docker node ls
ID HOSTNAME MEMBERSHIP STATUS AVAILABILITY MANAGER STATUS
1mhtdwhvsgr3c26xxbnzdc3yp node05 Accepted Ready Active
516pacagkqp2xc3fk9t1dhjor node02 Accepted Ready Active Reachable
9ifojw8of78kkusuc4a6c23fx * node01 Accepted Ready Active Leader
ax11wdpwrrb6db3mfjydscgk7 node04 Accepted Ready Active
bb1nrq2cswhtbg4mrsqnlx1ck node03 Accepted Ready Active Reachable
di9wxgz8dtuh9d2hn089ecqkf node06 Accepted Ready Active
Troubleshoot a manager node
You should never restart a manager node by copying the raft
directory from another node. The data directory is unique to a node ID. A node can only use a node ID once to join the swarm. The node ID space should be globally unique.
To cleanly re-join a manager node to a cluster:
- Demote the node to a worker using
docker node demote <NODE>
. - Remove the node from the swarm using
docker node rm <NODE>
. - Re-join the node to the swarm with a fresh state using
docker swarm join
.
For more information on joining a manager node to a swarm, refer to Join nodes to a swarm.
Forcibly remove a node
In most cases, you should shut down a node before removing it from a swarm with
the docker node rm
command. If a node becomes unreachable, unresponsive, or
compromised you can forcefully remove the node without shutting it down by
passing the --force
flag. For instance, if node9
becomes compromised:
$ docker node rm node9
Error response from daemon: rpc error: code = 9 desc = node node9 is not down and can't be removed
$ docker node rm --force node9
Node node9 removed from swarm
Before you forcefully remove a manager node, you must first demote it to the worker role. Make sure that you always have an odd number of manager nodes if you demote or remove a manager.
Back up the swarm
Docker manager nodes store the swarm state and manager logs in the
/var/lib/docker/swarm/
directory. This data includes the keys used to encrypt
the Raft logs. Without these keys, you cannot restore the swarm.
You can back up the swarm using any manager. Use the following procedure.
If the swarm has auto-lock enabled, you need the unlock key to restore the swarm from backup. Retrieve the unlock key if necessary and store it in a safe location. If you are unsure, read Lock your swarm to protect its encryption key.
Stop Docker on the manager before backing up the data, so that no data is being changed during the backup. It is possible to take a backup while the manager is running (a ""hot"" backup), but this is not recommended and your results are less predictable when restoring. While the manager is down, other nodes continue generating swarm data that is not part of this backup.
Note
Be sure to maintain the quorum of swarm managers. During the time that a manager is shut down, your swarm is more vulnerable to losing the quorum if further nodes are lost. The number of managers you run is a trade-off. If you regularly take down managers to do backups, consider running a five manager swarm, so that you can lose an additional manager while the backup is running, without disrupting your services.
Back up the entire
/var/lib/docker/swarm
directory.Restart the manager.
To restore, see Restore from a backup.
Recover from disaster
Restore from a backup
After backing up the swarm as described in Back up the swarm, use the following procedure to restore the data to a new swarm.
Shut down Docker on the target host machine for the restored swarm.
Remove the contents of the
/var/lib/docker/swarm
directory on the new swarm.Restore the
/var/lib/docker/swarm
directory with the contents of the backup.Note
The new node uses the same encryption key for on-disk storage as the old one. It is not possible to change the on-disk storage encryption keys at this time.
In the case of a swarm with auto-lock enabled, the unlock key is also the same as on the old swarm, and the unlock key is needed to restore the swarm.
Start Docker on the new node. Unlock the swarm if necessary. Re-initialize the swarm using the following command, so that this node does not attempt to connect to nodes that were part of the old swarm, and presumably no longer exist.
$ docker swarm init --force-new-cluster
Verify that the state of the swarm is as expected. This may include application-specific tests or simply checking the output of
docker service ls
to be sure that all expected services are present.If you use auto-lock, rotate the unlock key.
Add manager and worker nodes to bring your new swarm up to operating capacity.
Reinstate your previous backup regimen on the new swarm.
Recover from losing the quorum
Swarm is resilient to failures and can recover from any number of temporary node failures (machine reboots or crash with restart) or other transient errors. However, a swarm cannot automatically recover if it loses a quorum. Tasks on existing worker nodes continue to run, but administrative tasks are not possible, including scaling or updating services and joining or removing nodes from the swarm. The best way to recover is to bring the missing manager nodes back online. If that is not possible, continue reading for some options for recovering your swarm.
In a swarm of N
managers, a quorum (a majority) of manager nodes must always
be available. For example, in a swarm with five managers, a minimum of three must be
operational and in communication with each other. In other words, the swarm can
tolerate up to (N-1)/2
permanent failures beyond which requests involving
swarm management cannot be processed. These types of failures include data
corruption or hardware failures.
If you lose the quorum of managers, you cannot administer the swarm. If you have lost the quorum and you attempt to perform any management operation on the swarm, an error occurs:
Error response from daemon: rpc error: code = 4 desc = context deadline exceeded
The best way to recover from losing the quorum is to bring the failed nodes back
online. If you can't do that, the only way to recover from this state is to use
the --force-new-cluster
action from a manager node. This removes all managers
except the manager the command was run from. The quorum is achieved because
there is now only one manager. Promote nodes to be managers until you have the
desired number of managers.
From the node to recover, run:
$ docker swarm init --force-new-cluster --advertise-addr node01:2377
When you run the docker swarm init
command with the --force-new-cluster
flag, the Docker Engine where you run the command becomes the manager node of a
single-node swarm which is capable of managing and running services. The manager
has all the previous information about services and tasks, worker nodes are
still part of the swarm, and services are still running. You need to add or
re-add manager nodes to achieve your previous task distribution and ensure that
you have enough managers to maintain high availability and prevent losing the
quorum.
Force the swarm to rebalance
Generally, you do not need to force the swarm to rebalance its tasks. When you add a new node to a swarm, or a node reconnects to the swarm after a period of unavailability, the swarm does not automatically give a workload to the idle node. This is a design decision. If the swarm periodically shifted tasks to different nodes for the sake of balance, the clients using those tasks would be disrupted. The goal is to avoid disrupting running services for the sake of balance across the swarm. When new tasks start, or when a node with running tasks becomes unavailable, those tasks are given to less busy nodes. The goal is eventual balance, with minimal disruption to the end user.
You can use the --force
or -f
flag with the docker service update
command
to force the service to redistribute its tasks across the available worker nodes.
This causes the service tasks to restart. Client applications may be disrupted.
If you have configured it, your service uses a
rolling update.
If you use an earlier version and you want to achieve an even balance of load
across workers and don't mind disrupting running tasks, you can force your swarm
to re-balance by temporarily scaling the service upward. Use
docker service inspect --pretty <servicename>
to see the configured scale
of a service. When you use docker service scale
, the nodes with the lowest
number of tasks are targeted to receive the new workloads. There may be multiple
under-loaded nodes in your swarm. You may need to scale the service up by modest
increments a few times to achieve the balance you want across all the nodes.
When the load is balanced to your satisfaction, you can scale the service back
down to the original scale. You can use docker service ps
to assess the current
balance of your service across nodes.
See also
docker service scale
and
docker service ps
.",,,
537cff888d16075e072ccbff5395356935deccf7953b9639538bf663ef387402,"Subscription
A Docker subscription includes licensing for commercial use of Docker products including Docker Desktop, Docker Hub, Docker Build Cloud, Docker Scout, and Testcontainers Cloud.
Use the resources here to decide what subscription you need, or manage an existing subscription.",,,
dd5209a8b842e5744e0e4f55bcac0588ffeca16f05a8402548e359548d6bd4db,"Cache storage backends
To ensure fast builds, BuildKit automatically caches the build result in its own internal cache. Additionally, BuildKit also supports exporting build cache to an external location, making it possible to import in future builds.
An external cache becomes almost essential in CI/CD build environments. Such environments usually have little-to-no persistence between runs, but it's still important to keep the runtime of image builds as low as possible.
The default docker
driver supports the inline
, local
, registry
, and
gha
cache backends, but only if you have enabled the
containerd image store.
Other cache backends require you to select a different
driver.
Warning
If you use secrets or credentials inside your build process, ensure you manipulate them using the dedicated
--secret
option. Manually managing secrets usingCOPY
orARG
could result in leaked credentials.
Backends
Buildx supports the following cache storage backends:
inline
: embeds the build cache into the image.The inline cache gets pushed to the same location as the main output result. This only works with the
image
exporter.registry
: embeds the build cache into a separate image, and pushes to a dedicated location separate from the main output.local
: writes the build cache to a local directory on the filesystem.gha
: uploads the build cache to GitHub Actions cache (beta).s3
: uploads the build cache to an AWS S3 bucket (unreleased).azblob
: uploads the build cache to Azure Blob Storage (unreleased).
Command syntax
To use any of the cache backends, you first need to specify it on build with the
--cache-to
option
to export the cache to your storage backend of choice. Then, use the
--cache-from
option
to import the cache from the storage backend into the current build. Unlike the
local BuildKit cache (which is always enabled), all of the cache storage
backends must be explicitly exported to, and explicitly imported from.
Example buildx
command using the registry
backend, using import and export
cache:
$ docker buildx build --push -t <registry>/<image> \
--cache-to type=registry,ref=<registry>/<cache-image>[,parameters...] \
--cache-from type=registry,ref=<registry>/<cache-image>[,parameters...] .
Warning
As a general rule, each cache writes to some location. No location can be written to twice, without overwriting the previously cached data. If you want to maintain multiple scoped caches (for example, a cache per Git branch), then ensure that you use different locations for exported cache.
Multiple caches
BuildKit currently only supports a single cache exporter. But you can import from as many remote caches as you like. For example, a common pattern is to use the cache of both the current branch and the main branch. The following example shows importing cache from multiple locations using the registry cache backend:
$ docker buildx build --push -t <registry>/<image> \
--cache-to type=registry,ref=<registry>/<cache-image>:<branch> \
--cache-from type=registry,ref=<registry>/<cache-image>:<branch> \
--cache-from type=registry,ref=<registry>/<cache-image>:main .
Configuration options
This section describes some configuration options available when generating cache exports. The options described here are common for at least two or more backend types. Additionally, the different backend types support specific parameters as well. See the detailed page about each backend type for more information about which configuration parameters apply.
The common parameters described here are:
Cache mode
When generating a cache output, the --cache-to
argument accepts a mode
option for defining which layers to include in the exported cache. This is
supported by all cache backends except for the inline
cache.
Mode can be set to either of two options: mode=min
or mode=max
. For example,
to build the cache with mode=max
with the registry backend:
$ docker buildx build --push -t <registry>/<image> \
--cache-to type=registry,ref=<registry>/<cache-image>,mode=max \
--cache-from type=registry,ref=<registry>/<cache-image> .
This option is only set when exporting a cache, using --cache-to
. When
importing a cache (--cache-from
) the relevant parameters are automatically
detected.
In min
cache mode (the default), only layers that are exported into the
resulting image are cached, while in max
cache mode, all layers are cached,
even those of intermediate steps.
While min
cache is typically smaller (which speeds up import/export times, and
reduces storage costs), max
cache is more likely to get more cache hits.
Depending on the complexity and location of your build, you should experiment
with both parameters to find the results that work best for you.
Cache compression
The cache compression options are the same as the
exporter compression options. This is
supported by the local
and registry
cache backends.
For example, to compress the registry
cache with zstd
compression:
$ docker buildx build --push -t <registry>/<image> \
--cache-to type=registry,ref=<registry>/<cache-image>,compression=zstd \
--cache-from type=registry,ref=<registry>/<cache-image> .
OCI media types
The cache OCI options are the same as the
exporter OCI options. These are
supported by the local
and registry
cache backends.
For example, to export OCI media type cache, use the oci-mediatypes
property:
$ docker buildx build --push -t <registry>/<image> \
--cache-to type=registry,ref=<registry>/<cache-image>,oci-mediatypes=true \
--cache-from type=registry,ref=<registry>/<cache-image> .
This property is only meaningful with the --cache-to
flag. When fetching
cache, BuildKit will auto-detect the correct media types to use.
By default, the OCI media type generates an image index for the cache image.
Some OCI registries, such as Amazon ECR, don't support the image index media
type: application/vnd.oci.image.index.v1+json
. If you export cache images to
ECR, or any other registry that doesn't support image indices, set the
image-manifest
parameter to true
to generate a single image manifest
instead of an image index for the cache image:
$ docker buildx build --push -t <registry>/<image> \
--cache-to type=registry,ref=<registry>/<cache-image>,oci-mediatypes=true,image-manifest=true \
--cache-from type=registry,ref=<registry>/<cache-image> .",,,
5856b98b52590725655e7c3d872c50687c39883c15bde9b04fc61d22d6c1ec98,"Give feedback
Table of contents
There are many ways you can provide feedback on Docker Compose.
In-product feedback
If you have obtained Docker Compose through Docker Desktop, you can use the docker feedback
command to submit feedback directly from the command line.
Report bugs or problems on GitHub
To report bugs or problems, visit Docker Compose on GitHub
Feedback via Community Slack channels
You can also provide feedback through the #docker-compose Docker Community Slack channel.",,,
2aba812a36bc0e163a04856b8763061256cbf0e84da91c3ef7535c3584a8553c,"Install Docker Desktop on Fedora
Docker Desktop terms
Commercial use of Docker Desktop in larger enterprises (more than 250 employees OR more than $10 million USD in annual revenue) requires a paid subscription.
This page contains information on how to install, launch and upgrade Docker Desktop on a Fedora distribution.
Prerequisites
To install Docker Desktop successfully, you must:
- Meet the general system requirements.
- Have a 64-bit version of Fedora 40 or Fedora 41.
Additionally, for a GNOME desktop environment you must install AppIndicator and KStatusNotifierItem GNOME extensions.
For non-GNOME desktop environments, gnome-terminal
must be installed:
$ sudo dnf install gnome-terminal
Install Docker Desktop
To install Docker Desktop on Fedora:
Set up Docker's package repository.
Download the latest RPM package. For checksums, see the Release notes.
Install the package with dnf as follows:
$ sudo dnf install ./docker-desktop-x86_64.rpm
By default, Docker Desktop is installed at
/opt/docker-desktop
.
There are a few post-install configuration steps done through the post-install script contained in the RPM package.
The post-install script:
- Sets the capability on the Docker Desktop binary to map privileged ports and set resource limits.
- Adds a DNS name for Kubernetes to
/etc/hosts
. - Creates a symlink from
/usr/local/bin/com.docker.cli
to/usr/bin/docker
. This is because the classic Docker CLI is installed at/usr/bin/docker
. The Docker Desktop installer also installs a Docker CLI binary that includes cloud-integration capabilities and is essentially a wrapper for the Compose CLI, at/usr/local/bin/com.docker.cli
. The symlink ensures that the wrapper can access the classic Docker CLI.
Launch Docker Desktop
To start Docker Desktop for Linux:
Navigate to the Docker Desktop application in your Gnome/KDE Desktop.
Select Docker Desktop to start Docker.
The Docker Subscription Service Agreement displays.
Select Accept to continue. Docker Desktop starts after you accept the terms.
Note that Docker Desktop won't run if you do not agree to the terms. You can choose to accept the terms at a later date by opening Docker Desktop.
For more information, see Docker Desktop Subscription Service Agreement. It is recommended that you also read the FAQs.
Alternatively, open a terminal and run:
$ systemctl --user start docker-desktop
When Docker Desktop starts, it creates a dedicated context that the Docker CLI can use as a target and sets it as the current context in use. This is to avoid a clash with a local Docker Engine that may be running on the Linux host and using the default context. On shutdown, Docker Desktop resets the current context to the previous one.
The Docker Desktop installer updates Docker Compose and the Docker CLI binaries
on the host. It installs Docker Compose V2 and gives users the choice to
link it as docker-compose from the Settings panel. Docker Desktop installs
the new Docker CLI binary that includes cloud-integration capabilities in /usr/local/bin/com.docker.cli
and creates a symlink to the classic Docker CLI at /usr/local/bin
.
After you’ve successfully installed Docker Desktop, you can check the versions of these binaries by running the following commands:
$ docker compose version
Docker Compose version v2.29.1
$ docker --version
Docker version 27.1.1, build 6312585
$ docker version
Client:
Version: 23.0.5
API version: 1.42
Go version: go1.21.12
<...>
To enable Docker Desktop to start on sign in, from the Docker menu, select Settings > General > Start Docker Desktop when you sign in to your computer.
Alternatively, open a terminal and run:
$ systemctl --user enable docker-desktop
To stop Docker Desktop, select the Docker menu icon to open the Docker menu and select Quit Docker Desktop.
Alternatively, open a terminal and run:
$ systemctl --user stop docker-desktop
Upgrade Docker Desktop
Once a new version for Docker Desktop is released, the Docker UI shows a notification. You need to first remove the previous version and then download the new package each time you want to upgrade Docker Desktop. Run:
$ sudo dnf remove docker-desktop
$ sudo dnf install ./docker-desktop-x86_64.rpm
Next steps
- Explore Docker's subscriptions to see what Docker can offer you.
- Take a look at the Docker workshop to learn how to build an image and run it as a containerized application.
- Explore Docker Desktop and all its features.
- Troubleshooting describes common problems, workarounds, how to run and submit diagnostics, and submit issues.
- FAQs provide answers to frequently asked questions.
- Release notes lists component updates, new features, and improvements associated with Docker Desktop releases.
- Back up and restore data provides instructions on backing up and restoring data related to Docker.",,,
e63c112d30e75eaca2ccae223a83f37be74b3cea4d26fcdb5261509690171a2e,"Group mapping
Group mapping lets you sync user groups from your identity provider (IdP) with teams in your Docker organization. This automates team membership management, keeping your Docker teams up to date based on changes in your IdP. You can use group mapping once you have configured single sign-on (SSO).
Tip
Group mapping is ideal for adding users to multiple organizations or multiple teams within one organization. If you don't need to set up multi-organization or multi-team assignment, you can use SCIM user-level attributes.
How group mapping works
With group mapping enabled, when a user authenticates through SSO, your IdP shares key attributes with Docker, such as the user's email address, name, and groups. Docker uses these attributes to create or update the user's profile, as well as to manage their team and organization assignments. With group mapping, users’ team memberships in Docker automatically reflect changes made in your IdP groups.
It's important to note that Docker uses the user's email address as a unique identifier. Each Docker account must always have a unique email address.
Use group mapping
To assign users to Docker teams through your IdP, you must create groups in your IdP following the naming pattern: organization:team
. For example, if your organization is called ""moby"" and you want to manage the ""developers"" team, the group name in your IdP should be moby:developers
. In this example, any user added to this group in your IdP is automatically assigned to the ""developers"" team in Docker.
You can also use this format to assign users to multiple organizations. For example, to add a user to the ""backend"" team in the ""moby"" organization and the ""desktop"" team in the ""whale"" organization, the group names would be moby:backend
and whale:desktop
.
Tip
Match the group names in your IdP with your Docker teams. When groups are synced, Docker creates a team if it doesn’t already exist.
The following lists the supported group mapping attributes:
| Attribute | Description |
|---|---|
| id | Unique ID of the group in UUID format. This attribute is read-only. |
| displayName | Name of the group following the group mapping format: organization:team . |
| members | A list of users that are members of this group. |
| members(x).value | Unique ID of the user that is a member of this group. Members are referenced by ID. |
The general steps to use group mapping are:
- In your IdP, create groups with the
organization:team
format. - Add users to the group.
- Add the Docker application that you created in your IdP to the group.
- Add attributes in the IdP.
- Push groups to Docker.
The exact configuration may vary depending on your IdP. You can use group mapping with SSO, or with SSO and SCIM enabled.
Use group mapping with SSO
The following steps describe how to set up and use group mapping with SSO connections that use the SAML authentication method. Note that group mapping with SSO isn't supported with the Azure AD (OIDC) authentication method. Additionally, SCIM isn't required for these configurations.
The user interface for your IdP may differ slightly from the following steps. You can refer to the Okta documentation to verify.
To set up group mapping:
- Sign in to Okta and open your application.
- Navigate to the SAML Settings page for your application.
- In the Group Attribute Statements (optional) section, configure like the following:
- Name:
groups
- Name format:
Unspecified
- Filter:
Starts with
+organization:
whereorganization
is the name of your organization The filter option will filter out the groups that aren't affiliated with your Docker organization.
- Name:
- Create your groups by selecting Directory, then Groups.
- Add your groups using the format
organization:team
that matches the names of your organization(s) and team(s) in Docker. - Assign users to the group(s) that you create.
The next time you sync your groups with Docker, your users will map to the Docker groups you defined.
The user interface for your IdP may differ slightly from the following steps. You can refer to the Entra ID documentation to verify.
To set up group mapping:
- Sign in to Entra ID and open your application.
- Select Manage, then Single sign-on.
- Select Add a group claim.
- In the Group Claims section, select Groups assigned to the application with the source attribute Cloud-only group display names (Preview).
- Select Advanced options, then the Filter groups option.
- Configure the attribute like the following:
- Attribute to match:
Display name
- Match with:
Contains
- String:
:
- Attribute to match:
- Select Save.
- Select Groups, All groups, then New group to create your group(s).
- Assign users to the group(s) that you create.
The next time you sync your groups with Docker, your users will map to the Docker groups you defined.
Use group mapping with SCIM
The following steps describe how to set up and use group mapping with SCIM. Before you begin, make sure you set up SCIM first.
The user interface for your IdP may differ slightly from the following steps. You can refer to the Okta documentation to verify.
To set up your groups:
- Sign in to Okta and open your application.
- Select Applications, then Provisioning, and Integration.
- Select Edit to enable groups on your connection, then select Push groups.
- Select Save. Saving this configuration will add the Push Groups tab to your application.
- Create your groups by navigating to Directory and selecting Groups.
- Add your groups using the format
organization:team
that matches the names of your organization(s) and team(s) in Docker. - Assign users to the group(s) that you create.
- Return to the Integration page, then select the Push Groups tab to open the view where you can control and manage how groups are provisioned.
- Select Push Groups, then Find groups by rule.
- Configure the groups by rule like the following:
- Enter a rule name, for example
Sync groups with Docker Hub
- Match group by name, for example starts with
docker:
or contains:
for multi-organization - If you enable Immediately push groups by rule, sync will happen as soon as there's a change to the group or group assignments. Enable this if you don't want to manually push groups.
- Enter a rule name, for example
Find your new rule under By rule in the Pushed Groups column. The groups that match that rule are listed in the groups table on the right-hand side.
To push the groups from this table:
- Select Group in Okta.
- Select the Push Status drop-down.
- Select Push Now.
The user interface for your IdP may differ slightly from the following steps. You can refer to the Entra ID documentation to verify.
Complete the following before configuring group mapping:
- Sign in to Entra ID and go to your application.
- In your application, select Provisioning, then Mappings.
- Select Provision Microsoft Entra ID Groups.
- Select Show advanced options, then Edit attribute list.
- Update the
externalId
type toreference
, then select the Multi-Value checkbox and choose the referenced object attributeurn:ietf:params:scim:schemas:core:2.0:Group
. - Select Save, then Yes to confirm.
- Go to Provisioning.
- Toggle Provision Status to On, then select Save.
Next, set up group mapping:
- Go to the application overview page.
- Under Provision user accounts, select Get started.
- Select Add user/group.
- Create your group(s) using the
organization:team
format. - Assign the group to the provisioning group.
- Select Start provisioning to start the sync.
To verify, select Monitor, then Provisioning logs to see that your groups were provisioned successfully. In your Docker organization, you can check that the groups were correctly provisioned and the members were added to the appropriate teams.
Once complete, a user who signs in to Docker through SSO is automatically added to the organizations and teams mapped in the IdP.
Tip
Enable SCIM to take advantage of automatic user provisioning and de-provisioning. If you don't enable SCIM users are only automatically provisioned. You have to de-provision them manually.
More resources
The following videos demonstrate how to use group mapping with your IdP with SCIM enabled:",,,
198f1df57222109277e7208f586c26db2c3724315ae956c6cd794cde8cc734a2,"Filter commands
You can use the --filter
flag to scope your commands. When filtering, the
commands only include entries that match the pattern you specify.
Using filters
The --filter
flag expects a key-value pair separated by an operator.
$ docker COMMAND --filter ""KEY=VALUE""
The key represents the field that you want to filter on.
The value is the pattern that the specified field must match.
The operator can be either equals (=
) or not equals (!=
).
For example, the command docker images --filter reference=alpine
filters the
output of the docker images
command to only print alpine
images.
$ docker images
REPOSITORY TAG IMAGE ID CREATED SIZE
ubuntu 20.04 33a5cc25d22c 36 minutes ago 101MB
ubuntu 18.04 152dc042452c 36 minutes ago 88.1MB
alpine 3.16 a8cbb8c69ee7 40 minutes ago 8.67MB
alpine latest 7144f7bab3d4 40 minutes ago 11.7MB
busybox uclibc 3e516f71d880 48 minutes ago 2.4MB
busybox glibc 7338d0c72c65 48 minutes ago 6.09MB
$ docker images --filter reference=alpine
REPOSITORY TAG IMAGE ID CREATED SIZE
alpine 3.16 a8cbb8c69ee7 40 minutes ago 8.67MB
alpine latest 7144f7bab3d4 40 minutes ago 11.7MB
The available fields (reference
in this case) depend on the command you run.
Some filters expect an exact match. Others handle partial matches. Some filters
let you use regular expressions.
Refer to the CLI reference description for each command to learn about the supported filtering capabilities for each command.
Combining filters
You can combine multiple filters by passing multiple --filter
flags. The
following example shows how to print all images that match alpine:latest
or
busybox
- a logical OR
.
$ docker images
REPOSITORY TAG IMAGE ID CREATED SIZE
ubuntu 20.04 33a5cc25d22c 2 hours ago 101MB
ubuntu 18.04 152dc042452c 2 hours ago 88.1MB
alpine 3.16 a8cbb8c69ee7 2 hours ago 8.67MB
alpine latest 7144f7bab3d4 2 hours ago 11.7MB
busybox uclibc 3e516f71d880 2 hours ago 2.4MB
busybox glibc 7338d0c72c65 2 hours ago 6.09MB
$ docker images --filter reference=alpine:latest --filter=reference=busybox
REPOSITORY TAG IMAGE ID CREATED SIZE
alpine latest 7144f7bab3d4 2 hours ago 11.7MB
busybox uclibc 3e516f71d880 2 hours ago 2.4MB
busybox glibc 7338d0c72c65 2 hours ago 6.09MB
Multiple negated filters
Some commands support negated filters on
labels.
Negated filters only consider results that don't match the specified patterns.
The following command prunes all containers that aren't labeled foo
.
$ docker container prune --filter ""label!=foo""
There's a catch in combining multiple negated label filters. Multiple negated
filters create a single negative constraint - a logical AND
. The following
command prunes all containers except those labeled both foo
and bar
.
Containers labeled either foo
or bar
, but not both, will be pruned.
$ docker container prune --filter ""label!=foo"" --filter ""label!=bar""
Reference
For more information about filtering commands, refer to the CLI reference
description for commands that support the --filter
flag:
docker config ls
docker container prune
docker image prune
docker image ls
docker network ls
docker network prune
docker node ls
docker node ps
docker plugin ls
docker container ls
docker search
docker secret ls
docker service ls
docker service ps
docker stack ps
docker system prune
docker volume ls
docker volume prune",,,
439481ddd9689ac3d82787559990bf0019455df9e758f6d9aa1ba441160eff86,"Launch a dev environment
Important
Dev Environments is no longer under active development.
While the current functionality remains available, it may take us longer to respond to support requests.
You can launch a dev environment from a:
- Git repository
- Branch or tag of a Git repository
- Sub-folder of a Git repository
- Local folder
This does not conflict with any of the local files or local tooling set up on your host.
Tip
Install the Dev Environments browser extension for Chrome or Firefox, to launch a dev environment faster.
Prerequisites
To get started with Dev Environments, you must also install the following tools and extension on your machine:
- Git. Make sure add Git to your PATH if you're a Windows user.
- Visual Studio Code
- Visual Studio Code Remote Containers Extension
After Git is installed, restart Docker Desktop. Select Quit Docker Desktop, and then start it again.
Launch a dev environment from a Git repository
Note
When cloning a Git repository using SSH, ensure you've added your SSH key to the ssh-agent. To do this, open a terminal and run
ssh-add <path to your private ssh key>
.
Important
If you have enabled the WSL 2 integration in Docker Desktop for Windows, make sure you have an SSH agent running in your WSL 2 distribution.
If your WSL 2 distribution doesn't have an ssh-agent
running, you can append this script at the end of your profile file (that is: ~/.profile, ~/.zshrc, ...).
SSH_ENV=""$HOME/.ssh/agent-environment""
function start_agent {
echo ""Initializing new SSH agent...""
/usr/bin/ssh-agent | sed 's/^echo/#echo/' > ""${SSH_ENV}""
echo succeeded
chmod 600 ""${SSH_ENV}""
. ""${SSH_ENV}"" > /dev/null
}
# Source SSH settings, if applicable
if [ -f ""${SSH_ENV}"" ]; then
. ""${SSH_ENV}"" > /dev/null
ps -ef | grep ${SSH_AGENT_PID} | grep ssh-agent$ > /dev/null || {
start_agent;
}
else
start_agent;
fi
To launch a dev environment:
- From the Dev Environments tab in Docker Dashboard, select Create. The Create a Dev Environment dialog displays.
- Select Get Started.
- Optional: Provide a name for you dev environment.
- Select Existing Git repo as the source and then paste your Git repository link into the field provided.
- Choose your IDE. You can choose either:
- Visual Studio Code. The Git repository is cloned into a Volume and attaches to your containers. This allows you to develop directly inside of them using Visual Studio Code.
- Other. The Git repository is cloned into your chosen local directory and attaches to your containers as a bind mount. This shares the directory from your computer to the container, and allows you to develop using any local editor or IDE.
- Select Continue.
To launch the application, run the command make run
in your terminal. This opens an http server on port 8080. Open
http://localhost:8080 in your browser to see the running application.
Launch from a specific branch or tag
You can launch a dev environment from a specific branch, for example a branch corresponding to a Pull Request, or a tag by adding @mybranch
or @tag
as a suffix to your Git URL:
https://github.com/dockersamples/single-dev-env@mybranch
or
git@github.com:dockersamples/single-dev-env.git@mybranch
Docker then clones the repository with your specified branch or tag.
Launch from a subfolder of a Git repository
Note
Currently, Dev Environments is not able to detect the main language of the subdirectory. You need to define your own base image or services in a
compose-dev.yaml
file located in your subdirectory. For more information on how to configure, see the React application with a Spring backend and a MySQL database sample or the Go server with an Nginx proxy and a Postgres database sample.
- From Dev Environments in Docker Dashboard, select Create. The Create a Dev Environment dialog displays.
- Select Get Started.
- Optional: Provide a name for you dev environment.
- Select Existing Git repo as the source and then paste the link of your Git repo subfolder into the field provided.
- Choose your IDE. You can choose either:
- Visual Studio Code. The Git repository is cloned into a Volume and attaches to your containers. This allows you to develop directly inside of them using Visual Studio Code.
- Other. The Git repository is cloned into your chosen local directory and attaches to your containers as a bind mount. This shares the directory from your computer to the container, and allows you to develop using any local editor or IDE.
- Select Continue.
To launch the application, run the command make run
in your terminal. This opens an http server on port 8080. Open
http://localhost:8080 in your browser to see the running application.
Launch from a local folder
From Dev Environments in Docker Dashboard, select Create. The Create a Dev Environment dialog displays.
Select Get Started.
Optional: Provide a name for your dev environment.
Choose Local directory as the source.
Select Select to open the root directory of the code that you would like to work on.
A directory from your computer is bind mounted to the container, so any changes you make locally is reflected in the dev environment. You can use an editor or IDE of your choice.
Note
When using a local folder for a dev environment, file changes are synchronized between your environment container and your local files. This can affect the performance inside the container, depending on the number of files in your local folder and the operations performed in the container.
What's next?
Learn how to:",,,
c3f6b425ed67b43c17372d79b5da57655c755df0ecc02302befd61b0a1a4f220,"Use Intune
Learn how to deploy Docker Desktop for Windows and Mac using Intune, Microsoft's cloud-based device management tool.
Sign in to your Intune admin center.
Add a new app. Select Apps, then Windows, then Add.
For the app type, select Windows app (Win32)
Select the
intunewin
package.Complete any relevant details such as the description, publisher, or app version and then select Next.
Optional: On the Program tab, you can update the Install command field to suit your needs. The field is pre-populated with
msiexec /i ""DockerDesktop.msi"" /qn
. See the Common installation scenarios for examples on the changes you can make.Tip
It's recommended you configure the Intune deployment to schedule a reboot of the machine on successful installs.
This is because the Docker Desktop installer installs Windows features depending on your engine selection and also updates the membership of the
docker-users
local group.You may also want to set Intune to determine behaviour based on return codes and watch for a return code of
3010
.Complete the rest of the tabs and then review and create the app.
First, upload the package:
- Sign in to your Intune admin center.
- Add a new app. Select Apps, then macOSs, then Add.
- Select Line-of-business app and then Select.
- Upload the
Docker.pkg
file and fill in the required details.
Next, assign the app:
- Once the app is added, navigate to Assignments in Intune.
- Select Add group and choose the user or device groups you want to assign the app to.
- Select Save.
Additional resources
- Explore the FAQs.
- Learn how to Enforce sign-in for your users.",,,
f0b5fa9e77e90f38dbb8d98cb219c0d6297ea113f24e7e7fdcc047002aa6f628,"OpenTelemetry support
Both Buildx and BuildKit support OpenTelemetry.
To capture the trace to
Jaeger,
set JAEGER_TRACE
environment variable to the collection address using a
driver-opt
.
First create a Jaeger container:
$ docker run -d --name jaeger -p ""6831:6831/udp"" -p ""16686:16686"" --restart unless-stopped jaegertracing/all-in-one
Then
create a docker-container
builder
that will use the Jaeger instance via the JAEGER_TRACE
environment variable:
$ docker buildx create --use \
--name mybuilder \
--driver docker-container \
--driver-opt ""network=host"" \
--driver-opt ""env.JAEGER_TRACE=localhost:6831""
Boot and
inspect mybuilder
:
$ docker buildx inspect --bootstrap
Buildx commands should be traced at http://127.0.0.1:16686/
:",,,
edc55eabbd8f37439770dd03ebf9e7f969e704d08e21f3018b1e77b251a64a2a,"Docker Engine security
There are four major areas to consider when reviewing Docker security:
- The intrinsic security of the kernel and its support for namespaces and cgroups
- The attack surface of the Docker daemon itself
- Loopholes in the container configuration profile, either by default, or when customized by users.
- The ""hardening"" security features of the kernel and how they interact with containers.
Kernel namespaces
Docker containers are very similar to LXC containers, and they have
similar security features. When you start a container with
docker run
, behind the scenes Docker creates a set of namespaces and control
groups for the container.
Namespaces provide the first and most straightforward form of isolation. Processes running within a container cannot see, and even less affect, processes running in another container, or in the host system.
Each container also gets its own network stack, meaning that a container doesn't get privileged access to the sockets or interfaces of another container. Of course, if the host system is setup accordingly, containers can interact with each other through their respective network interfaces — just like they can interact with external hosts. When you specify public ports for your containers or use links then IP traffic is allowed between containers. They can ping each other, send/receive UDP packets, and establish TCP connections, but that can be restricted if necessary. From a network architecture point of view, all containers on a given Docker host are sitting on bridge interfaces. This means that they are just like physical machines connected through a common Ethernet switch; no more, no less.
How mature is the code providing kernel namespaces and private networking? Kernel namespaces were introduced between kernel version 2.6.15 and 2.6.26. This means that since July 2008 (date of the 2.6.26 release ), namespace code has been exercised and scrutinized on a large number of production systems. And there is more: the design and inspiration for the namespaces code are even older. Namespaces are actually an effort to reimplement the features of OpenVZ in such a way that they could be merged within the mainstream kernel. And OpenVZ was initially released in 2005, so both the design and the implementation are pretty mature.
Control groups
Control Groups are another key component of Linux containers. They implement resource accounting and limiting. They provide many useful metrics, but they also help ensure that each container gets its fair share of memory, CPU, disk I/O; and, more importantly, that a single container cannot bring the system down by exhausting one of those resources.
So while they do not play a role in preventing one container from accessing or affecting the data and processes of another container, they are essential to fend off some denial-of-service attacks. They are particularly important on multi-tenant platforms, like public and private PaaS, to guarantee a consistent uptime (and performance) even when some applications start to misbehave.
Control Groups have been around for a while as well: the code was started in 2006, and initially merged in kernel 2.6.24.
Docker daemon attack surface
Running containers (and applications) with Docker implies running the
Docker daemon. This daemon requires root
privileges unless you opt-in
to
Rootless mode, and you should therefore be aware of
some important details.
First of all, only trusted users should be allowed to control your
Docker daemon. This is a direct consequence of some powerful Docker
features. Specifically, Docker allows you to share a directory between
the Docker host and a guest container; and it allows you to do so
without limiting the access rights of the container. This means that you
can start a container where the /host
directory is the /
directory
on your host; and the container can alter your host filesystem
without any restriction. This is similar to how virtualization systems
allow filesystem resource sharing. Nothing prevents you from sharing your
root filesystem (or even your root block device) with a virtual machine.
This has a strong security implication: for example, if you instrument Docker from a web server to provision containers through an API, you should be even more careful than usual with parameter checking, to make sure that a malicious user cannot pass crafted parameters causing Docker to create arbitrary containers.
For this reason, the REST API endpoint (used by the Docker CLI to communicate with the Docker daemon) changed in Docker 0.5.2, and now uses a Unix socket instead of a TCP socket bound on 127.0.0.1 (the latter being prone to cross-site request forgery attacks if you happen to run Docker directly on your local machine, outside of a VM). You can then use traditional Unix permission checks to limit access to the control socket.
You can also expose the REST API over HTTP if you explicitly decide to do so. However, if you do that, be aware of the above mentioned security implications. Note that even if you have a firewall to limit accesses to the REST API endpoint from other hosts in the network, the endpoint can be still accessible from containers, and it can easily result in the privilege escalation. Therefore it is mandatory to secure API endpoints with HTTPS and certificates. Exposing the daemon API over HTTP without TLS is not permitted, and such a configuration causes the daemon to fail early on startup, see Unauthenticated TCP connections. It is also recommended to ensure that it is reachable only from a trusted network or VPN.
You can also use DOCKER_HOST=ssh://USER@HOST
or ssh -L /path/to/docker.sock:/var/run/docker.sock
instead if you prefer SSH over TLS.
The daemon is also potentially vulnerable to other inputs, such as image
loading from either disk with docker load
, or from the network with
docker pull
. As of Docker 1.3.2, images are now extracted in a chrooted
subprocess on Linux/Unix platforms, being the first-step in a wider effort
toward privilege separation. As of Docker 1.10.0, all images are stored and
accessed by the cryptographic checksums of their contents, limiting the
possibility of an attacker causing a collision with an existing image.
Finally, if you run Docker on a server, it is recommended to run exclusively Docker on the server, and move all other services within containers controlled by Docker. Of course, it is fine to keep your favorite admin tools (probably at least an SSH server), as well as existing monitoring/supervision processes, such as NRPE and collectd.
Linux kernel capabilities
By default, Docker starts containers with a restricted set of capabilities. What does that mean?
Capabilities turn the binary ""root/non-root"" dichotomy into a
fine-grained access control system. Processes (like web servers) that
just need to bind on a port below 1024 do not need to run as root: they
can just be granted the net_bind_service
capability instead. And there
are many other capabilities, for almost all the specific areas where root
privileges are usually needed. This means a lot for container security.
Typical servers run several processes as root
, including the SSH daemon,
cron
daemon, logging daemons, kernel modules, network configuration tools,
and more. A container is different, because almost all of those tasks are
handled by the infrastructure around the container:
- SSH access are typically managed by a single server running on the Docker host
cron
, when necessary, should run as a user process, dedicated and tailored for the app that needs its scheduling service, rather than as a platform-wide facility- Log management is also typically handed to Docker, or to third-party services like Loggly or Splunk
- Hardware management is irrelevant, meaning that you never need to
run
udevd
or equivalent daemons within containers - Network management happens outside of the containers, enforcing
separation of concerns as much as possible, meaning that a container
should never need to perform
ifconfig
,route
, or ip commands (except when a container is specifically engineered to behave like a router or firewall, of course)
This means that in most cases, containers do not need ""real"" root privileges at all* And therefore, containers can run with a reduced capability set; meaning that ""root"" within a container has much less privileges than the real ""root"". For instance, it is possible to:
- Deny all ""mount"" operations
- Deny access to raw sockets (to prevent packet spoofing)
- Deny access to some filesystem operations, like creating new device nodes, changing the owner of files, or altering attributes (including the immutable flag)
- Deny module loading
This means that even if an intruder manages to escalate to root within a container, it is much harder to do serious damage, or to escalate to the host.
This doesn't affect regular web apps, but reduces the vectors of attack by malicious users considerably. By default Docker drops all capabilities except those needed, an allowlist instead of a denylist approach. You can see a full list of available capabilities in Linux manpages.
One primary risk with running Docker containers is that the default set of capabilities and mounts given to a container may provide incomplete isolation, either independently, or when used in combination with kernel vulnerabilities.
Docker supports the addition and removal of capabilities, allowing use of a non-default profile. This may make Docker more secure through capability removal, or less secure through the addition of capabilities. The best practice for users would be to remove all capabilities except those explicitly required for their processes.
Docker Content Trust signature verification
Docker Engine can be configured to only run signed images. The Docker Content
Trust signature verification feature is built directly into the dockerd
binary.
This is configured in the Dockerd configuration file.
To enable this feature, trustpinning can be configured in daemon.json
, whereby
only repositories signed with a user-specified root key can be pulled and run.
This feature provides more insight to administrators than previously available with the CLI for enforcing and performing image signature verification.
For more information on configuring Docker Content Trust Signature Verification, go to Content trust in Docker.
Other kernel security features
Capabilities are just one of the many security features provided by modern Linux kernels. It is also possible to leverage existing, well-known systems like TOMOYO, AppArmor, SELinux, GRSEC, etc. with Docker.
While Docker currently only enables capabilities, it doesn't interfere with the other systems. This means that there are many different ways to harden a Docker host. Here are a few examples.
- You can run a kernel with GRSEC and PAX. This adds many safety checks, both at compile-time and run-time; it also defeats many exploits, thanks to techniques like address randomization. It doesn't require Docker-specific configuration, since those security features apply system-wide, independent of containers.
- If your distribution comes with security model templates for Docker containers, you can use them out of the box. For instance, we ship a template that works with AppArmor and Red Hat comes with SELinux policies for Docker. These templates provide an extra safety net (even though it overlaps greatly with capabilities).
- You can define your own policies using your favorite access control mechanism.
Just as you can use third-party tools to augment Docker containers, including special network topologies or shared filesystems, tools exist to harden Docker containers without the need to modify Docker itself.
As of Docker 1.10 User Namespaces are supported directly by the docker daemon. This feature allows for the root user in a container to be mapped to a non uid-0 user outside the container, which can help to mitigate the risks of container breakout. This facility is available but not enabled by default.
Refer to the daemon command in the command line reference for more information on this feature. Additional information on the implementation of User Namespaces in Docker can be found in this blog post.
Conclusions
Docker containers are, by default, quite secure; especially if you run your processes as non-privileged users inside the container.
You can add an extra layer of safety by enabling AppArmor, SELinux, GRSEC, or another appropriate hardening system.
If you think of ways to make docker more secure, we welcome feature requests, pull requests, or comments on the Docker community forums.",,,
4e4dc6d8dd9e19faf2de4968614b78a43aa3b0cd4fda3854611f7f5e5ad70f9d,"Install Docker Engine on RHEL
To get started with Docker Engine on RHEL, make sure you meet the prerequisites, and then follow the installation steps.
Prerequisites
OS requirements
To install Docker Engine, you need a maintained version of one of the following RHEL versions:
- RHEL 8
- RHEL 9
Uninstall old versions
Before you can install Docker Engine, you need to uninstall any conflicting packages.
Your Linux distribution may provide unofficial Docker packages, which may conflict with the official packages provided by Docker. You must uninstall these packages before you install the official version of Docker Engine.
$ sudo dnf remove docker \
docker-client \
docker-client-latest \
docker-common \
docker-latest \
docker-latest-logrotate \
docker-logrotate \
docker-engine \
podman \
runc
dnf
might report that you have none of these packages installed.
Images, containers, volumes, and networks stored in /var/lib/docker/
aren't
automatically removed when you uninstall Docker.
Installation methods
You can install Docker Engine in different ways, depending on your needs:
You can set up Docker's repositories and install from them, for ease of installation and upgrade tasks. This is the recommended approach.
You can download the RPM package, install it manually, and manage upgrades completely manually. This is useful in situations such as installing Docker on air-gapped systems with no access to the internet.
In testing and development environments, you can use automated convenience scripts to install Docker.
Install using the rpm repository
Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository.
Set up the repository
Install the dnf-plugins-core
package (which provides the commands to manage
your DNF repositories) and set up the repository.
$ sudo dnf -y install dnf-plugins-core
$ sudo dnf config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo
Install Docker Engine
Install the Docker packages.
To install the latest version, run:
$ sudo dnf install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
If prompted to accept the GPG key, verify that the fingerprint matches
060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35
, and if so, accept it.This command installs Docker, but it doesn't start Docker. It also creates a
docker
group, however, it doesn't add any users to the group by default.To install a specific version, start by listing the available versions in the repository:
$ dnf list docker-ce --showduplicates | sort -r docker-ce.x86_64 3:28.0.1-1.el9 docker-ce-stable docker-ce.x86_64 3:28.0.0-1.el9 docker-ce-stable <...>
The list returned depends on which repositories are enabled, and is specific to your version of RHEL (indicated by the
.el9
suffix in this example).Install a specific version by its fully qualified package name, which is the package name (
docker-ce
) plus the version string (2nd column), separated by a hyphen (-
). For example,docker-ce-3:28.0.1-1.el9
.Replace
<VERSION_STRING>
with the desired version and then run the following command to install:$ sudo dnf install docker-ce-<VERSION_STRING> docker-ce-cli-<VERSION_STRING> containerd.io docker-buildx-plugin docker-compose-plugin
This command installs Docker, but it doesn't start Docker. It also creates a
docker
group, however, it doesn't add any users to the group by default.Start Docker Engine.
$ sudo systemctl enable --now docker
This configures the Docker systemd service to start automatically when you boot your system. If you don't want Docker to start automatically, use
sudo systemctl start docker
instead.Verify that the installation is successful by running the
hello-world
image:$ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Upgrade Docker Engine
To upgrade Docker Engine, follow the installation instructions, choosing the new version you want to install.
Install from a package
If you can't use Docker's rpm
repository to install Docker Engine, you can
download the .rpm
file for your release and install it manually. You need to
download a new file each time you want to upgrade Docker Engine.
Select your RHEL version in the list.
Select the applicable architecture (
x86_64
,aarch64
, ors390x
), and then go tostable/Packages/
.Download the following
rpm
files for the Docker Engine, CLI, containerd, and Docker Compose packages:containerd.io-<version>.<arch>.rpm
docker-ce-<version>.<arch>.rpm
docker-ce-cli-<version>.<arch>.rpm
docker-buildx-plugin-<version>.<arch>.rpm
docker-compose-plugin-<version>.<arch>.rpm
Install Docker Engine, changing the following path to the path where you downloaded the packages.
$ sudo dnf install ./containerd.io-<version>.<arch>.rpm \ ./docker-ce-<version>.<arch>.rpm \ ./docker-ce-cli-<version>.<arch>.rpm \ ./docker-buildx-plugin-<version>.<arch>.rpm \ ./docker-compose-plugin-<version>.<arch>.rpm
Docker is installed but not started. The
docker
group is created, but no users are added to the group.Start Docker Engine.
$ sudo systemctl enable --now docker
This configures the Docker systemd service to start automatically when you boot your system. If you don't want Docker to start automatically, use
sudo systemctl start docker
instead.Verify that the installation is successful by running the
hello-world
image:$ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Upgrade Docker Engine
To upgrade Docker Engine, download the newer package files and repeat the
installation procedure, using dnf upgrade
instead of dnf install
, and point to the new files.
Install using the convenience script
Docker provides a convenience script at
https://get.docker.com/ to install Docker into
development environments non-interactively. The convenience script isn't
recommended for production environments, but it's useful for creating a
provisioning script tailored to your needs. Also refer to the
install using the repository steps to learn
about installation steps to install using the package repository. The source code
for the script is open source, and you can find it in the
docker-install
repository on GitHub.
Always examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script:
- The script requires
root
orsudo
privileges to run. - The script attempts to detect your Linux distribution and version and configure your package management system for you.
- The script doesn't allow you to customize most installation parameters.
- The script installs dependencies and recommendations without asking for confirmation. This may install a large number of packages, depending on the current configuration of your host machine.
- By default, the script installs the latest stable release of Docker, containerd, and runc. When using this script to provision a machine, this may result in unexpected major version upgrades of Docker. Always test upgrades in a test environment before deploying to your production systems.
- The script isn't designed to upgrade an existing Docker installation. When using the script to update an existing installation, dependencies may not be updated to the expected version, resulting in outdated versions.
Tip
Preview script steps before running. You can run the script with the
--dry-run
option to learn what steps the script will run when invoked:$ curl -fsSL https://get.docker.com -o get-docker.sh $ sudo sh ./get-docker.sh --dry-run
This example downloads the script from https://get.docker.com/ and runs it to install the latest stable release of Docker on Linux:
$ curl -fsSL https://get.docker.com -o get-docker.sh
$ sudo sh get-docker.sh
Executing docker install script, commit: 7cae5f8b0decc17d6571f9f52eb840fbc13b2737
<...>
You have now successfully installed and started Docker Engine. The docker
service starts automatically on Debian based distributions. On RPM
based
distributions, such as CentOS, Fedora, RHEL or SLES, you need to start it
manually using the appropriate systemctl
or service
command. As the message
indicates, non-root users can't run Docker commands by default.
Use Docker as a non-privileged user, or install in rootless mode?
The installation script requires
root
orsudo
privileges to install and use Docker. If you want to grant non-root users access to Docker, refer to the post-installation steps for Linux. You can also install Docker withoutroot
privileges, or configured to run in rootless mode. For instructions on running Docker in rootless mode, refer to run the Docker daemon as a non-root user (rootless mode).
Install pre-releases
Docker also provides a convenience script at
https://test.docker.com/ to install pre-releases of
Docker on Linux. This script is equal to the script at get.docker.com
, but
configures your package manager to use the test channel of the Docker package
repository. The test channel includes both stable and pre-releases (beta
versions, release-candidates) of Docker. Use this script to get early access to
new releases, and to evaluate them in a testing environment before they're
released as stable.
To install the latest version of Docker on Linux from the test channel, run:
$ curl -fsSL https://test.docker.com -o test-docker.sh
$ sudo sh test-docker.sh
Upgrade Docker after using the convenience script
If you installed Docker using the convenience script, you should upgrade Docker using your package manager directly. There's no advantage to re-running the convenience script. Re-running it can cause issues if it attempts to re-install repositories which already exist on the host machine.
Uninstall Docker Engine
Uninstall the Docker Engine, CLI, containerd, and Docker Compose packages:
$ sudo dnf remove docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras
Images, containers, volumes, or custom configuration files on your host aren't automatically removed. To delete all images, containers, and volumes:
$ sudo rm -rf /var/lib/docker $ sudo rm -rf /var/lib/containerd
You have to delete any edited configuration files manually.
Next steps
- Continue to Post-installation steps for Linux.",,,
5f6f2333136e2618f3222f8b37bb6a85b05169f9507429731673a405616ce9f9,"Reproducible builds with GitHub Actions
Table of contents
SOURCE_DATE_EPOCH
is a
standardized environment variable
for instructing build tools to produce a reproducible output.
Setting the environment variable for a build makes the timestamps in the
image index, config, and file metadata reflect the specified Unix time.
To set the environment variable in GitHub Actions,
use the built-in env
property on the build step.
Unix epoch timestamps
The following example sets the SOURCE_DATE_EPOCH
variable to 0, Unix epoch.
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build
uses: docker/build-push-action@v6
with:
tags: user/app:latest
env:
SOURCE_DATE_EPOCH: 0
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build
uses: docker/bake-action@v6
env:
SOURCE_DATE_EPOCH: 0
Git commit timestamps
The following example sets SOURCE_DATE_EPOCH
to the Git commit timestamp.
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Get Git commit timestamps
run: echo ""TIMESTAMP=$(git log -1 --pretty=%ct)"" >> $GITHUB_ENV
- name: Build
uses: docker/build-push-action@v6
with:
tags: user/app:latest
env:
SOURCE_DATE_EPOCH: ${{ env.TIMESTAMP }}
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Get Git commit timestamps
run: echo ""TIMESTAMP=$(git log -1 --pretty=%ct)"" >> $GITHUB_ENV
- name: Build
uses: docker/bake-action@v6
env:
SOURCE_DATE_EPOCH: ${{ env.TIMESTAMP }}
Additional information
For more information about the SOURCE_DATE_EPOCH
support in BuildKit,
see
BuildKit documentation.",,,
61e89af895644b5e054f14f304fa32544d8d571ba8c41d9b40daf4b13385a361,"Explore the Volumes view in Docker Desktop
The Volumes view in Docker Desktop Dashboard lets you create, delete, and perform other actions on your volumes. You can also see which volumes are being used as well as inspect the files and folders in your volumes.
View your volumes
You can view the following information about your volumes:
- Name: The name of the volume.
- Status: Whether the volume is in-use by a container or not.
- Created: How long ago the volume was created.
- Size: The size of the volume.
- Scheduled exports: Whether a scheduled export is active or not.
By default, the Volumes view displays a list of all the volumes.
You can filter and sort volumes as well as modify which columns are displayed by doing the following:
- Filter volumes by name: Use the Search field.
- Filter volumes by status: To the right of the search bar, filter volumes by In use or Unused.
- Sort volumes: Select a column name to sort the volumes.
- Customize columns: To the right of the search bar, choose what volume information to display.
Create a volume
You use the following steps to create an empty volume. Alternatively, if you start a container with a volume that doesn't yet exist, Docker creates the volume for you.
To create a volume:
- In the Volumes view, select the Create button.
- In the New Volume modal, specify a volume name, and then select Create.
To use the volume with a container, see Use volumes.
Inspect a volume
To explore the details of a specific volume, select a volume from the list. This opens the detailed view.
The Container in-use tab displays the name of the container using the volume, the image name, the port number used by the container, and the target. A target is a path inside a container that gives access to the files in the volume.
The Stored data tab displays the files and folders in the volume and the file size. To save a file or a folder, right-click on the file or folder to display the options menu, select Save as..., and then specify a location to download the file.
To delete a file or a folder from the volume, right-click on the file or folder to display the options menu, select Delete, and then select Delete again to confirm.
The Exports tab lets you export the volume.
Clone a volume
Cloning a volume creates a new volume with a copy of all of the data from the cloned volume. When cloning a volume used by one or more running containers, the containers are temporarily stopped while Docker clones the data, and then restarted when the cloning process is completed.
To clone a volume:
- Sign in to Docker Desktop. You must be signed in to clone a volume.
- In the Volumes view, select the Clone icon in the Actions column for the volume you want to clone.
- In the Clone a volume modal, specify a Volume name, and then select Clone.
Delete one or more volumes
Deleting a volume deletes the volume and all its data. When a container is using a volume, you can't delete the volume, even if the container is stopped. You must first stop and remove any containers using the volume before you can delete the volume.
To delete a volume:
- In the Volumes view, select Delete icon in the Actions column for the volume you want to delete.
- In the Delete volume? modal, select Delete forever.
To delete multiple volumes:
- In the Volumes view, select the checkbox next to all the volumes you want to delete.
- Select Delete.
- In the Delete volumes? modal, select Delete forever.
Empty a volume
Emptying a volume deletes all a volume's data, but doesn't delete the volume. When emptying a volume used by one or more running containers, the containers are temporarily stopped while Docker empties the data, and then restarted when the emptying process is completed.
To empty a volume:
- Sign in to Docker Desktop. You must be signed in to empty a volume.
- In the Volumes view, select the volume you want to empty.
- Next to Import, select the More volume actions icon, and then select Empty volume.
- In the Empty a volume? modal, select Empty.
Export a volume
You can export the content of a volume to a local file, a local image, and to an image in Docker Hub, or to a supported cloud provider. When exporting content from a volume used by one or more running containers, the containers are temporarily stopped while Docker exports the content, and then restarted when the export process is completed.
You can either export a volume now or schedule a recurring export.
Export a volume now
Sign in to Docker Desktop. You must be signed in to export a volume.
In the Volumes view, select the volume you want to export.
Select the Exports tab.
Select Quick export.
Select whether to export the volume to Local or Hub storage or External cloud storage, then specify the following additional details depending on your selection.
- Local file: Specify a file name and select a folder.
- Local image: Select a local image to export the content to. Any existing data in the image will be replaced by the exported content.
- New image: Specify a name for the new image.
- Registry: Specify a Docker Hub repository.
You must have a Docker Business subscription to export to an external cloud provider.
Select your cloud provider and then specify the URL to upload to the storage. Refer to the following documentation for your cloud provider to learn how to obtain a URL.
- Amazon Web Services: Create a presigned URL for Amazon S3 using an AWS SDK
- Microsoft Azure: Generate a SAS token and URL
- Google Cloud: Create a signed URL to upload an object
Select Save.
Schedule a volume export
Sign in to Docker Desktop. You must be signed in and have a paid Docker subscription to schedule a volume export.
In the Volumes view, select the volume you want to export.
Select the Exports tab.
Select Schedule export.
In Recurrence, select how often the export occurs, and then specify the following additional details based on your selection.
- Daily: Specify the time that the backup occurs each day.
- Weekly: Specify one or more days, and the time that the backup occurs each week.
- Monthly: Specify which day of the month and the time that the backup occurs each month.
Select whether to export the volume to Local or Hub storage or External cloud storage, then specify the following additional details depending on your selection.
- Local file: Specify a file name and select a folder.
- Local image: Select a local image to export the content to. Any existing data in the image will be replaced by the exported content.
- New image: Specify a name for the new image.
- Registry: Specify a Docker Hub repository.
You must have a Docker Business subscription to export to an external cloud provider.
Select your cloud provider and then specify the URL to upload to the storage. Refer to the following documentation for your cloud provider to learn how to obtain a URL.
- Amazon Web Services: Create a presigned URL for Amazon S3 using an AWS SDK
- Microsoft Azure: Generate a SAS token and URL
- Google Cloud: Create a signed URL to upload an object
Select Save.
Import a volume
You can import a local file, a local image, or an image from Docker Hub. Any existing data in the volume is replaced by the imported content. When importing content to a volume used by one or more running containers, the containers are temporarily stopped while Docker imports the content, and then restarted when the import process is completed.
To import a volume:
Sign in to Docker Desktop. You must be signed in to import a volume.
Optionally, create a new volume to import the content into.
Select the volume you want to import content in to.
Select Import.
Select where the content is coming from and then specify the following additional details depending on your selection:
- Local file: Select the file that contains the content.
- Local image: Select the local image that contains the content.
- Registry: Specify the image from Docker Hub that contains the content.
Select Import.",,,
58cb7aa6f04d4964c372297c0d9cb56101c1dff17c094186675a6255dc18682b,"Use Jamf Pro
Table of contents
For:
Administrators
Learn how to deploy Docker Desktop for Mac using Jamf Pro.
First, upload the package:
- From the Jamf pro console, Navigate to Computers > Management Settings > Computer Management > Packages.
- Select New to add a new package.
- Upload the
Docker.pkg
file.
Next, create a policy for deployment:
- Navigate to Computers > Policies.
- Select Newto create a new policy.
- Enter a name for the policy, for example ""Deploy Docker Desktop"".
- Under the Packages tab, add the Docker package you uploaded.
- Configure the scope to target the devices or device groups you want to install Docker on.
- Save the policy and deploy.
For more information, see Jamf Pro's official documentation.
Additional resources
- Learn how to Enforce sign-in for your users.",,,
7898700de6533989e88c0ca677a70f1aa0f1e5dff774f63f1257997d695f9c0f,"Integrate Docker Scout with Jenkins
You can add the following stage and steps definition to a Jenkinsfile
to run
Docker Scout as part of a Jenkins pipeline. The pipeline needs a DOCKER_HUB
credential containing the username and password for authenticating to Docker
Hub. It also needs an environment variable defined for the image and tag.
pipeline {
agent {
// Agent details
}
environment {
DOCKER_HUB = credentials('jenkins-docker-hub-credentials')
IMAGE_TAG = 'myorg/scout-demo-service:latest'
}
stages {
stage('Analyze image') {
steps {
// Install Docker Scout
sh 'curl -sSfL https://raw.githubusercontent.com/docker/scout-cli/main/install.sh | sh -s -- -b /usr/local/bin'
// Log into Docker Hub
sh 'echo $DOCKER_HUB_PSW | docker login -u $DOCKER_HUB_USR --password-stdin'
// Analyze and fail on critical or high vulnerabilities
sh 'docker-scout cves $IMAGE_TAG --exit-code --only-severity critical,high'
}
}
}
}
This installs Docker Scout, logs into Docker Hub, and then runs Docker Scout to generate a CVE report for an image and tag. It only shows critical or high-severity vulnerabilities.
Note
If you're seeing a
permission denied
error related to the image cache, try setting theDOCKER_SCOUT_CACHE_DIR
environment variable to a writable directory. Or alternatively, disable local caching entirely withDOCKER_SCOUT_NO_CACHE=true
.",,,
2c783fd0446ad5266da5e04b03f2ceccbfbf75008611e4d8372123040854caa2,"Insights
Insights helps administrators visualize and understand how Docker is used within their organizations. With Insights, administrators can ensure their teams are fully equipped to utilize Docker to its fullest potential, leading to improved productivity and efficiency across the organization.
Key benefits include:
- Uniform working environment. Establish and maintain standardized configurations across teams.
- Best practices. Promote and enforce usage guidelines to ensure optimal performance.
- Increased visibility. Monitor and drive adoption of organizational configurations and policies.
- Optimized license use. Ensure that developers have access to advanced features provided by a Docker subscription.
Prerequisites
- Docker Business subscription
- Administrators must enforce sign-in for users
- Insights enabled by your Customer Success Manager
View Insights for organization users
To access Insights, you must contact your Customer Success Manager to have the feature enabled. Once the feature is enabled, access Insights using the following steps:
- Go to the Admin Console and sign in to an account that is an organization owner.
- Select your company on the Choose profile page.
- Select Insights.
- On the Insights page, select the period of time for the data.
Note
Insights data is not real-time and is updated daily. At the top-right of the Insights page, view the Last updated date to understand when the data was last updated.
You can view data in the following charts:
Docker Desktop users
Track active Docker Desktop users in your domain, differentiated by license status. This chart helps you understand the engagement levels within your organization, providing insights into how many users are actively using Docker Desktop. Note that users who opt out of analytics aren't included in the active counts.
The chart contains the following data.
| Data | Description |
|---|---|
| Active user | The number of users that have actively used Docker Desktop and either signed in with a Docker account that has a license in your organization or signed in to a Docker account with an email address from a domain associated with your organization. Users who don’t sign in to an account associated with your organization are not represented in the data. To ensure users sign in with an account associated with your organization, you can enforce sign-in. |
| Total organization members | The number of users that have used Docker Desktop, regardless of their Insights activity. |
| Users opted out of analytics | The number of users that are a member of your organization that have opted out of sending analytics. When users opt out of sending analytics, you won't see any of their data in Insights. To ensure that the data includes all users, you can use Settings Management to set analyticsEnabled for all your users. |
| Active users (graph) | The view over time for total active users. |
Builds
Monitor development efficiency and the time your team invests in builds with this chart. It provides a clear view of the build activity, helping you identify patterns, optimize build times, and enhance overall development productivity.
The chart contains the following data.
| Data | Description |
|---|---|
| Average build per user | The average number of builds per active user. A build includes any time a user runs one of the following commands:
|
| Average build time | The average build time per build. |
| Build success rate | The percentage of builds that were successful out of the total number of builds. A successful build includes any build that exits normally. |
| Total builds (graph) | The total number of builds separated into successful builds and failed builds. A successful build includes any build that exits normally. A failed build includes any build that exits abnormally. |
Containers
View the total and average number of containers run by users with this chart. It lets you gauge container usage across your organization, helping you understand usage trends and manage resources effectively.
The chart contains the following data.
| Data | Description |
|---|---|
| Total containers run | The total number of containers run by active users. Containers run include those run using the Docker Desktop graphical user interface, docker run , or docker compose . |
| Average number of containers run | The average number of containers run per active user. |
| Containers run by active users (graph) | The number of containers run over time by active users. |
Docker Desktop usage
Explore Docker Desktop usage patterns with this chart to optimize your team's workflows and ensure compatibility. It provides valuable insights into how Docker Desktop is being utilized, enabling you to streamline processes and improve efficiency.
The chart contains the following data.
| Data | Description |
|---|---|
| Most used version | The most used version of Docker Desktop by users in your organization. |
| Most used OS | The most used operating system by users. |
| Versions by active users (graph) | The number of active users using each version of Docker Desktop. To learn more about each version and release dates, see the Docker Desktop release notes. |
| Interface by active users (graph) | The number of active users grouped into the type of interface they used to interact with Docker Desktop. A CLI user is any active user who has run a docker command. A GUI user is any active user who has interacted with the Docker Desktop graphical user interface. |
Docker Hub images
Analyze image distribution activity with this chart and view the most utilized Docker Hub images within your domain. This information helps you manage image usage, ensuring that the most critical resources are readily available and efficiently used.
Note
Data for images is only for Docker Hub. Data for third-party registries and mirrors aren't included.
The chart contains the following data.
| Data | Description |
|---|---|
| Total pulled images | The total number of images pulled by users from Docker Hub. |
| Total pushed images | The total number of images pushed by users to Docker Hub. |
| Top 10 pulled images | A list of the top 10 images pulled by users from Docker Hub and the number of times each image has been pulled. |
Extensions
Monitor extension installation activity with this chart. It provides visibility into the Docker Desktop extensions your team are using, letting you track adoption and identify popular tools that enhance productivity.
The chart contains the following data.
| Data | Description |
|---|---|
| Percentage of org with extensions installed | The percentage of users in your organization with at least one Docker Desktop extension installed. |
| Top 5 extensions installed in the organization | A list of the top 5 Docker Desktop extensions installed by users in your organization and the number of users who have installed each extension. |
Troubleshoot Insights
If you’re experiencing issues with data in Insights, consider the following solutions to resolve common problems.
Update users to the latest version of Docker Desktop.
Data is not shown for users using versions 4.16 or lower of Docker Desktop. In addition, older versions may not provide all data. Ensure all users have installed the latest version of Docker Desktop.
Enable Send usage statistics in Docker Desktop for all your users.
If users have opted out of sending usage statistics for Docker Desktop, then their usage data will not be a part of Insights. To manage the setting at scale for all your users, you can use Settings Management and enable the
analyticsEnabled
setting.Ensure that users are using Docker Desktop and aren't using the standalone version of Docker Engine.
Only Docker Desktop can provide data for Insights. If a user installs and uses Docker Engine outside of Docker Desktop, Docker Engine won't provide data for that user.
Ensure that users are signing in to an account associated with your organization.
Users who don’t sign in to an account associated with your organization are not represented in the data. To ensure users sign in with an account associated with your organization, you can enforce sign-in.",,,
db9b76393a2fe2917d0127754efec13c638496e8fab0d1855f455b2d29d61dcc,"Build garbage collection
While
docker builder prune
or
docker buildx prune
commands run at once, Garbage Collection (GC) runs periodically and follows an
ordered list of prune policies. The BuildKit daemon clears the build cache when
the cache size becomes too big, or when the cache age expires.
For most users, the default GC behavior is sufficient and doesn't require any intervention. Advanced users, particularly those working with large-scale builds, self-managed builders, or constrained storage environments, might benefit from customizing these settings to better align with their workflow needs. The following sections explain how GC works and provide guidance on tailoring its behavior through custom configuration.
Garbage collection policies
GC policies define a set of rules that determine how the build cache is managed and cleaned up. These policies include criteria for when to remove cache entries, such as the age of the cache, the amount of space being used, and the type of cache records to prune.
Each GC policy is evaluated in sequence, starting with the most specific criteria, and proceeds to broader rules if previous policies do not free up enough cache. This lets BuildKit prioritize cache entries, preserving the most valuable cache while ensuring the system maintains performance and availability.
For example, say you have the following GC policies:
- Find ""stale"" cache records that haven't been used in the past 48 hours, and delete records until there's maximum 5GB of ""stale"" cache left.
- If the build cache size exceeds 10GB, delete records until the total cache size is no more than 10GB.
The first rule is more specific, prioritizing stale cache records and setting a lower limit for a less valuable type of cache. The second rule imposes a higher hard limit that applies to any type of cache records. With these policies, if you have 11GB worth of build cache, where:
- 7GB of which is ""stale"" cache
- 4GB is other, more valuable cache
A GC sweep would delete 5GB of stale cache as part of the 1st policy, with a remainder of 6GB, meaning the 2nd policy does not need to clear any more cache.
The default GC policies are (approximately):
- Remove cache that can be easily regenerated, such as build contexts from local directories or remote Git repositories, and cache mounts, if hasn't been used for more than 48 hours.
- Remove cache that hasn't been used in a build for more than 60 days.
- Remove unshared cache that exceeds the build cache size limit. Unshared cache records refers to layer blobs that are not used by other resources (typically, as image layers).
- Remove any build cache that exceeds the build cache size limit.
The precise algorithm and the means of configuring the policies differ slightly depending on what kind of builder you're using. Refer to Configuration for more details.
Configuration
Note
If you're satisfied with the default garbage collection behavior and don't need to fine-tune its settings, you can skip this section. Default configurations work well for most use cases and require no additional setup.
Depending on the type of build driver you use, you will use different configuration files to change the builder's GC settings:
- If you use the default builder for Docker Engine (the
docker
driver), use the Docker daemon configuration file. - If you use a custom builder, use a BuildKit configuration file.
Docker daemon configuration file
If you're using the default
docker
driver,
GC is configured in the
daemon.json
configuration file,
or if you use Docker Desktop, in
Settings > Docker Engine.
The following snippet shows the default builder configuration for the docker
driver for Docker Desktop users:
{
""builder"": {
""gc"": {
""defaultKeepStorage"": ""20GB"",
""enabled"": true
}
}
}
The defaultKeepStorage
option configures the size limit of the build cache,
which influences the GC policies. The default policies for the docker
driver
work as follows:
- Remove ephemeral, unused build cache older than 48 hours if it exceeds 13.8%
of
defaultKeepStorage
, or at minimum 512MB. - Remove unused build cache older than 60 days.
- Remove unshared build cache that exceeds the
defaultKeepStorage
limit. - Remove any build cache that exceeds the
defaultKeepStorage
limit.
Given the Docker Desktop default value for defaultKeepStorage
of 20GB, the
default GC policies resolve to:
{
""builder"": {
""gc"": {
""enabled"": true,
""policy"": [
{
""keepStorage"": ""2.764GB"",
""filter"": [
""unused-for=48h"",
""type==source.local,type==exec.cachemount,type==source.git.checkout""
]
},
{ ""keepStorage"": ""20GB"", ""filter"": [""unused-for=1440h""] },
{ ""keepStorage"": ""20GB"" },
{ ""keepStorage"": ""20GB"", ""all"": true }
]
}
}
}
The easiest way to tweak the build cache configuration for the docker
driver
is to adjust the defaultKeepStorage
option:
- Increase the limit if you feel like you think the GC is too aggressive.
- Decrease the limit if you need to preserve space.
If you need even more control, you can define your own GC policies directly. The following example defines a more conservative GC configuration with the following policies:
- Remove unused cache entries older than 1440 hours, or 60 days, if build cache exceeds 50GB.
- Remove unshared cache entries if build cache exceeds 50GB.
- Remove any cache entries if build cache exceeds 100GB.
{
""builder"": {
""gc"": {
""enabled"": true,
""defaultKeepStorage"": ""50GB"",
""policy"": [
{ ""keepStorage"": ""0"", ""filter"": [""unused-for=1440h""] },
{ ""keepStorage"": ""0"" },
{ ""keepStorage"": ""100GB"", ""all"": true }
]
}
}
}
Policies 1 and 2 here set keepStorage
to 0
, which means they'll fall back
to the default limit of 50GB as defined by defaultKeepStorage
.
BuildKit configuration file
For build drivers other than docker
, GC is configured using a
buildkitd.toml
configuration file. This
file uses the following high-level configuration options that you can use to
tweak the thresholds for how much disk space BuildKit should use for cache:
| Option | Description | Default value |
|---|---|---|
reservedSpace | The minimum amount of disk space BuildKit is allowed to allocate for cache. Usage below this threshold will not be reclaimed during garbage collection. | 10% of total disk space or 10GB (whichever is lower) |
maxUsedSpace | The maximum amount of disk space that BuildKit is allowed to use. Usage above this threshold will be reclaimed during garbage collection. | 60% of total disk space or 100GB (whichever is lower) |
minFreeSpace | The amount of disk space that must be kept free. | 20GB |
You can set these options either as number of bytes, a unit string (for
example, 512MB
), or as a percentage of the total disk size. Changing these
options influences the default GC policies used by the BuildKit worker. With
the default thresholds, the GC policies resolve as follows:
# Global defaults
[worker.oci]
gc = true
reservedSpace = ""10GB""
maxUsedSpace = ""100GB""
minFreeSpace = ""20%""
# Policy 1
[[worker.oci.gcpolicy]]
filters = [ ""type==source.local"", ""type==exec.cachemount"", ""type==source.git.checkout"" ]
keepDuration = ""48h""
maxUsedSpace = ""512MB""
# Policy 2
[[worker.oci.gcpolicy]]
keepDuration = ""1440h"" # 60 days
reservedSpace = ""10GB""
maxUsedSpace = ""100GB""
# Policy 3
[[worker.oci.gcpolicy]]
reservedSpace = ""10GB""
maxUsedSpace = ""100GB""
# Policy 4
[[worker.oci.gcpolicy]]
all = true
reservedSpace = ""10GB""
maxUsedSpace = ""100GB""
In practical terms, this means:
- Policy 1: If the build cache exceeds 512MB, BuildKit removes cache records for local build contexts, remote Git contexts, and cache mounts that haven’t been used in the last 48 hours.
- Policy 2: If disk usage exceeds 100GB, unshared build cache older than 60 days is removed, ensuring at least 10GB of disk space is reserved for cache.
- Policy 3: If disk usage exceeds 100GB, any unshared cache is removed, ensuring at least 10GB of disk space is reserved for cache.
- Policy 4: If disk usage exceeds 100GB, all cache—including shared and internal records—is removed, ensuring at least 10GB of disk space is reserved for cache.
reservedSpace
has the highest priority in defining the lower limit for build
cache size. If maxUsedSpace
or minFreeSpace
would define a lower value, the
minimum cache size would never be brought below reservedSpace
.
If both reservedSpace
and maxUsedSpace
are set, a GC sweep results in a
cache size between those thresholds. For example, if reservedSpace
is set to
10GB, and maxUsedSpace
is set to 20GB, the resulting amount of cache after a
GC run is less than 20GB, but at least 10GB.
You can also define completely custom GC policies. Custom policies also let you define filters, which lets you pinpoint the types of cache entries that a given policy is allowed to prune.
Custom GC policies in BuildKit
Custom GC policies let you fine-tune how BuildKit manages its cache, and gives you full control over cache retention based on criteria such as cache type, duration, or disk space thresholds. If you need full control over the cache thresholds and how cache records should be prioritized, defining custom GC policies is the way to go.
To define a custom GC policy, use the [[worker.oci.gcpolicy]]
configuration
block in buildkitd.toml
. Each policy define the thresholds that will be used
for that policy. The global values for reservedSpace
, maxUsedSpace
, and
minFreeSpace
do not apply if you use custom policies.
Here’s an example configuration:
# Custom GC Policy 1: Remove unused local contexts older than 24 hours
[[worker.oci.gcpolicy]]
filters = [""type==source.local""]
keepDuration = ""24h""
reservedSpace = ""5GB""
maxUsedSpace = ""50GB""
# Custom GC Policy 2: Remove remote Git contexts older than 30 days
[[worker.oci.gcpolicy]]
filters = [""type==source.git.checkout""]
keepDuration = ""720h""
reservedSpace = ""5GB""
maxUsedSpace = ""30GB""
# Custom GC Policy 3: Aggressively clean all cache if disk usage exceeds 90GB
[[worker.oci.gcpolicy]]
all = true
reservedSpace = ""5GB""
maxUsedSpace = ""90GB""
In addition to the reservedSpace
, maxUsedSpace
, and minFreeSpace
threshold,
when defining a GC policy you have two additional configuration options:
all
: By default, BuildKit will exclude some cache records from being pruned during GC. Setting this option totrue
will allow any cache records to be pruned.filters
: Filters let you specify specific types of cache records that a GC policy is allowed to prune.",,,
acfc6044b8082f336a0e68126a2a584017b729e24d448644976c599aa0fad63b,"Networking in Compose
Important
Docker's documentation refers to and describes Compose V2 functionality.
Effective July 2023, Compose V1 stopped receiving updates and is no longer in new Docker Desktop releases. Compose V2 has replaced it and is now integrated into all current Docker Desktop versions. For more information, see Migrate to Compose V2.
By default Compose sets up a single network for your app. Each container for a service joins the default network and is both reachable by other containers on that network, and discoverable by the service's name.
Note
Your app's network is given a name based on the ""project name"", which is based on the name of the directory it lives in. You can override the project name with either the
--project-name
flag or theCOMPOSE_PROJECT_NAME
environment variable.
For example, suppose your app is in a directory called myapp
, and your compose.yaml
looks like this:
services:
web:
build: .
ports:
- ""8000:8000""
db:
image: postgres
ports:
- ""8001:5432""
When you run docker compose up
, the following happens:
- A network called
myapp_default
is created. - A container is created using
web
's configuration. It joins the networkmyapp_default
under the nameweb
. - A container is created using
db
's configuration. It joins the networkmyapp_default
under the namedb
.
Each container can now look up the service name web
or db
and
get back the appropriate container's IP address. For example, web
's
application code could connect to the URL postgres://db:5432
and start
using the Postgres database.
It is important to note the distinction between HOST_PORT
and CONTAINER_PORT
.
In the above example, for db
, the HOST_PORT
is 8001
and the container port is
5432
(postgres default). Networked service-to-service
communication uses the CONTAINER_PORT
. When HOST_PORT
is defined,
the service is accessible outside the swarm as well.
Within the web
container, your connection string to db
would look like
postgres://db:5432
, and from the host machine, the connection string would
look like postgres://{DOCKER_IP}:8001
for example postgres://localhost:8001
if your container is running locally.
Update containers on the network
If you make a configuration change to a service and run docker compose up
to update it, the old container is removed and the new one joins the network under a different IP address but the same name. Running containers can look up that name and connect to the new address, but the old address stops working.
If any containers have connections open to the old container, they are closed. It is a container's responsibility to detect this condition, look up the name again and reconnect.
Tip
Reference containers by name, not IP, whenever possible. Otherwise you’ll need to constantly update the IP address you use.
Link containers
Links allow you to define extra aliases by which a service is reachable from another service. They are not required to enable services to communicate. By default, any service can reach any other service at that service's name. In the following example, db
is reachable from web
at the hostnames db
and database
:
services:
web:
build: .
links:
- ""db:database""
db:
image: postgres
See the links reference for more information.
Multi-host networking
When deploying a Compose application on a Docker Engine with
Swarm mode enabled,
you can make use of the built-in overlay
driver to enable multi-host communication.
Overlay networks are always created as attachable
. You can optionally set the
attachable
property to false
.
Consult the Swarm mode section, to see how to set up a Swarm cluster, and the Getting started with multi-host networking to learn about multi-host overlay networks.
Specify custom networks
Instead of just using the default app network, you can specify your own networks with the top-level networks
key. This lets you create more complex topologies and specify
custom network drivers and options. You can also use it to connect services to externally-created networks which aren't managed by Compose.
Each service can specify what networks to connect to with the service-level networks
key, which is a list of names referencing entries under the top-level networks
key.
The following example shows a Compose file which defines two custom networks. The proxy
service is isolated from the db
service, because they do not share a network in common. Only app
can talk to both.
services:
proxy:
build: ./proxy
networks:
- frontend
app:
build: ./app
networks:
- frontend
- backend
db:
image: postgres
networks:
- backend
networks:
frontend:
# Specify driver options
driver: bridge
driver_opts:
com.docker.network.bridge.host_binding_ipv4: ""127.0.0.1""
backend:
# Use a custom driver
driver: custom-driver
Networks can be configured with static IP addresses by setting the ipv4_address and/or ipv6_address for each attached network.
Networks can also be given a custom name:
services:
# ...
networks:
frontend:
name: custom_frontend
driver: custom-driver-1
Configure the default network
Instead of, or as well as, specifying your own networks, you can also change the settings of the app-wide default network by defining an entry under networks
named default
:
services:
web:
build: .
ports:
- ""8000:8000""
db:
image: postgres
networks:
default:
# Use a custom driver
driver: custom-driver-1
Use a pre-existing network
If you want your containers to join a pre-existing network, use the
external
option
services:
# ...
networks:
network1:
name: my-pre-existing-network
external: true
Instead of attempting to create a network called [projectname]_default
, Compose looks for a network called my-pre-existing-network
and connects your app's containers to it.
Further reference information
For full details of the network configuration options available, see the following references:",,,
90f7ad4c8dd5bbcacb657fe3a0ef0a8c06d9386411d0c1311945232d2a2e7abc,"Marketplace extensions
There are two types of extensions available in the Extensions Marketplace:
- Docker-reviewed extensions
- Self-published extensions
Docker-reviewed extensions are manually reviewed by the Docker Extensions team to ensure an extra level of trust and quality. They appear as Reviewed in the Marketplace.
Self-published extensions are autonomously published by extension developers and go through an automated validation process. They appear as Not reviewed in the Marketplace.
Install an extension
Note
For some extensions, a separate account needs to be created before use.
To install an extension:
- Open Docker Desktop.
- From the Docker Desktop Dashboard, select the Extensions tab. The Extensions Marketplace opens on the Browse tab.
- Browse the available extensions. You can sort the list of extensions by Recently added, Most installed, or alphabetically. Alternatively, use the Content or Categories drop-down menu to search for extensions by whether they have been reviewed or not, or by category.
- Choose an extension and select Install.
From here, you can select Open to access the extension or install additional extensions. The extension also appears in the left-hand menu and in the Manage tab.
Update an extension
You can update any extension outside of Docker Desktop releases. To update an extension to the latest version, navigate to the Docker Desktop Dashboard and select the Manage tab.
The Manage tab displays with all your installed extensions. If an extension has a new version available, it displays an Update button.
Uninstall an extension
You can uninstall an extension at any time.
Note
Any data used by the extension that's stored in a volume must be manually deleted.
- Navigate to the Docker Desktop Dashboard and select the Manage tab. This displays a list of extensions you've installed.
- Select the ellipsis to the right of extension you want to uninstall.
- Select Uninstall.",,,
5e68515fb60ec1d0303e1a52c81cb701d5ea91b10b2327edc961257303deab1a,"CLI reference
The Extensions CLI is an extension development tool that is used to manage Docker extensions. Actions include install, list, remove, and validate extensions.
docker extension enable
turns on Docker extensions.docker extension dev
commands for extension development.docker extension disable
turns off Docker extensions.docker extension init
creates a new Docker extension.docker extension install
installs a Docker extension with the specified image.docker extension ls
list installed Docker extensions.docker extension rm
removes a Docker extension.docker extension update
removes and re-installs a Docker extension.docker extension validate
validates the extension metadata file against the JSON schema.",,,
186d3504c846ae194724e05bd5128b597d4e3993d17f9490dfbbe8bbbdb4a01f,"Bridge network driver
In terms of networking, a bridge network is a Link Layer device which forwards traffic between network segments. A bridge can be a hardware device or a software device running within a host machine's kernel.
In terms of Docker, a bridge network uses a software bridge which lets containers connected to the same bridge network communicate, while providing isolation from containers that aren't connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks can't communicate directly with each other.
Bridge networks apply to containers running on the same Docker daemon host. For communication among containers running on different Docker daemon hosts, you can either manage routing at the OS level, or you can use an overlay network.
When you start Docker, a
default bridge network (also
called bridge
) is created automatically, and newly-started containers connect
to it unless otherwise specified. You can also create user-defined custom bridge
networks. User-defined bridge networks are superior to the default bridge
network.
Differences between user-defined bridges and the default bridge
User-defined bridges provide automatic DNS resolution between containers.
Containers on the default bridge network can only access each other by IP addresses, unless you use the
--link
option, which is considered legacy. On a user-defined bridge network, containers can resolve each other by name or alias.Imagine an application with a web front-end and a database back-end. If you call your containers
web
anddb
, the web container can connect to the db container atdb
, no matter which Docker host the application stack is running on.If you run the same application stack on the default bridge network, you need to manually create links between the containers (using the legacy
--link
flag). These links need to be created in both directions, so you can see this gets complex with more than two containers which need to communicate. Alternatively, you can manipulate the/etc/hosts
files within the containers, but this creates problems that are difficult to debug.User-defined bridges provide better isolation.
All containers without a
--network
specified, are attached to the default bridge network. This can be a risk, as unrelated stacks/services/containers are then able to communicate.Using a user-defined network provides a scoped network in which only containers attached to that network are able to communicate.
Containers can be attached and detached from user-defined networks on the fly.
During a container's lifetime, you can connect or disconnect it from user-defined networks on the fly. To remove a container from the default bridge network, you need to stop the container and recreate it with different network options.
Each user-defined network creates a configurable bridge.
If your containers use the default bridge network, you can configure it, but all the containers use the same settings, such as MTU and
iptables
rules. In addition, configuring the default bridge network happens outside of Docker itself, and requires a restart of Docker.User-defined bridge networks are created and configured using
docker network create
. If different groups of applications have different network requirements, you can configure each user-defined bridge separately, as you create it.Linked containers on the default bridge network share environment variables.
Originally, the only way to share environment variables between two containers was to link them using the
--link
flag. This type of variable sharing isn't possible with user-defined networks. However, there are superior ways to share environment variables. A few ideas:Multiple containers can mount a file or directory containing the shared information, using a Docker volume.
Multiple containers can be started together using
docker-compose
and the compose file can define the shared variables.You can use swarm services instead of standalone containers, and take advantage of shared secrets and configs.
Containers connected to the same user-defined bridge network effectively expose all ports
to each other. For a port to be accessible to containers or non-Docker hosts on
different networks, that port must be published using the -p
or --publish
flag.
Options
The following table describes the driver-specific options that you can pass to
--opt
when creating a custom network using the bridge
driver.
| Option | Default | Description |
|---|---|---|
com.docker.network.bridge.name | Interface name to use when creating the Linux bridge. | |
com.docker.network.bridge.enable_ip_masquerade | true | Enable IP masquerading. |
com.docker.network.bridge.gateway_mode_ipv4 com.docker.network.bridge.gateway_mode_ipv6 | nat | Control external connectivity. See Packet filtering and firewalls. |
com.docker.network.bridge.enable_icc | true | Enable or Disable inter-container connectivity. |
com.docker.network.bridge.host_binding_ipv4 | all IPv4 and IPv6 addresses | Default IP when binding container ports. |
com.docker.network.driver.mtu | 0 (no limit) | Set the containers network Maximum Transmission Unit (MTU). |
com.docker.network.container_iface_prefix | eth | Set a custom prefix for container interfaces. |
com.docker.network.bridge.inhibit_ipv4 | false | Prevent Docker from assigning an IP address to the bridge. |
Some of these options are also available as flags to the dockerd
CLI, and you
can use them to configure the default docker0
bridge when starting the Docker
daemon. The following table shows which options have equivalent flags in the
dockerd
CLI.
| Option | Flag |
|---|---|
com.docker.network.bridge.name | - |
com.docker.network.bridge.enable_ip_masquerade | --ip-masq |
com.docker.network.bridge.enable_icc | --icc |
com.docker.network.bridge.host_binding_ipv4 | --ip |
com.docker.network.driver.mtu | --mtu |
com.docker.network.container_iface_prefix | - |
The Docker daemon supports a --bridge
flag, which you can use to define
your own docker0
bridge. Use this option if you want to run multiple daemon
instances on the same host. For details, see
Run multiple daemons.
Default host binding address
When no host address is given in port publishing options like -p 80
or -p 8080:80
, the default is to make the container's port 80 available on all
host addresses, IPv4 and IPv6.
The bridge network driver option com.docker.network.bridge.host_binding_ipv4
can be used to modify the default address for published ports.
Despite the option's name, it is possible to specify an IPv6 address.
When the default binding address is an address assigned to a specific interface, the container's port will only be accessible via that address.
Setting the default binding address to ::
means published ports will only be
available on the host's IPv6 addresses. However, setting it to 0.0.0.0
means it
will be available on the host's IPv4 and IPv6 addresses.
To restrict a published port to IPv4 only, the address must be included in the
container's publishing options. For example, -p 0.0.0.0:8080:80
.
Manage a user-defined bridge
Use the docker network create
command to create a user-defined bridge
network.
$ docker network create my-net
You can specify the subnet, the IP address range, the gateway, and other
options. See the
docker network create
reference or the output of docker network create --help
for details.
Use the docker network rm
command to remove a user-defined bridge
network. If containers are currently connected to the network,
disconnect them
first.
$ docker network rm my-net
What's really happening?
When you create or remove a user-defined bridge or connect or disconnect a container from a user-defined bridge, Docker uses tools specific to the operating system to manage the underlying network infrastructure (such as adding or removing bridge devices or configuring
iptables
rules on Linux). These details should be considered implementation details. Let Docker manage your user-defined networks for you.
Connect a container to a user-defined bridge
When you create a new container, you can specify one or more --network
flags.
This example connects an Nginx container to the my-net
network. It also
publishes port 80 in the container to port 8080 on the Docker host, so external
clients can access that port. Any other container connected to the my-net
network has access to all ports on the my-nginx
container, and vice versa.
$ docker create --name my-nginx \
--network my-net \
--publish 8080:80 \
nginx:latest
To connect a running container to an existing user-defined bridge, use the
docker network connect
command. The following command connects an already-running
my-nginx
container to an already-existing my-net
network:
$ docker network connect my-net my-nginx
Disconnect a container from a user-defined bridge
To disconnect a running container from a user-defined bridge, use the
docker network disconnect
command. The following command disconnects
the my-nginx
container from the my-net
network.
$ docker network disconnect my-net my-nginx
Use IPv6 in a user-defined bridge network
When you create your network, you can specify the --ipv6
flag to enable IPv6.
$ docker network create --ipv6 --subnet 2001:db8:1234::/64 my-net
If you do not provide a --subnet
option, a Unique Local Address (ULA) prefix
will be chosen automatically.
IPv6-only bridge networks
To skip IPv4 address configuration on the bridge and in its containers, create
the network with option --ipv4=false
, and enable IPv6 using --ipv6
.
$ docker network create --ipv6 --ipv4=false v6net
IPv4 address configuration cannot be disabled in the default bridge network.
Use the default bridge network
The default bridge
network is considered a legacy detail of Docker and is not
recommended for production use. Configuring it is a manual operation, and it has
technical shortcomings.
Connect a container to the default bridge network
If you do not specify a network using the --network
flag, and you do specify a
network driver, your container is connected to the default bridge
network by
default. Containers connected to the default bridge
network can communicate,
but only by IP address, unless they're linked using the
legacy --link
flag.
Configure the default bridge network
To configure the default bridge
network, you specify options in daemon.json
.
Here is an example daemon.json
with several options specified. Only specify
the settings you need to customize.
{
""bip"": ""192.168.1.1/24"",
""fixed-cidr"": ""192.168.1.0/25"",
""mtu"": 1500,
""default-gateway"": ""192.168.1.254"",
""dns"": [""10.20.1.2"",""10.20.1.3""]
}
In this example:
- The bridge's address is ""192.168.1.1/24"" (from
bip
). - The bridge network's subnet is ""192.168.1.0/24"" (from
bip
). - Container addresses will be allocated from ""192.168.1.0/25"" (from
fixed-cidr
).
Use IPv6 with the default bridge network
IPv6 can be enabled for the default bridge using the following options in
daemon.json
, or their command line equivalents.
These three options only affect the default bridge, they are not used by user-defined networks. The addresses in below are examples from the IPv6 documentation range.
- Option
ipv6
is required. - Option
bip6
is optional, it specifies the address of the default bridge, which will be used as the default gateway by containers. It also specifies the subnet for the bridge network. - Option
fixed-cidr-v6
is optional, it specifies the address range Docker may automatically allocate to containers.- The prefix should normally be
/64
or shorter. - For experimentation on a local network, it is better to use a Unique Local
Address (ULA) prefix (matching
fd00::/8
) than a Link Local prefix (matchingfe80::/10
).
- The prefix should normally be
- Option
default-gateway-v6
is optional. If unspecified, the default is the first address in thefixed-cidr-v6
subnet.
{
""ipv6"": true,
""bip6"": ""2001:db8::1111/64"",
""fixed-cidr-v6"": ""2001:db8::/64"",
""default-gateway-v6"": ""2001:db8:abcd::89""
}
If no bip6
is specified, fixed-cidr-v6
defines the subnet for the bridge
network. If no bip6
or fixed-cidr-v6
is specified, a ULA prefix will be
chosen.
Restart Docker for changes to take effect.
Connection limit for bridge networks
Due to limitations set by the Linux kernel, bridge networks become unstable and inter-container communications may break when 1000 containers or more connect to a single network.
For more information about this limitation, see moby/moby#44973.
Skip Bridge IP address configuration
The bridge is normally assigned the network's --gateway
address, which is
used as the default route from the bridge network to other networks.
The com.docker.network.bridge.inhibit_ipv4
option lets you create a network
without the IPv4 gateway address being assigned to the bridge. This is useful
if you want to configure the gateway IP address for the bridge manually. For
instance if you add a physical interface to your bridge, and need it to have
the gateway address.
With this configuration, north-south traffic (to and from the bridge network) won't work unless you've manually configured the gateway address on the bridge, or a device attached to it.
This option can only be used with user-defined bridge networks.
Next steps
- Go through the standalone networking tutorial
- Learn about networking from the container's point of view
- Learn about overlay networks
- Learn about Macvlan networks",,,
34f499aa71e1ac63fa03ff8ef3afd9a43016c0f84cb413e0e1765469ee6423f3,"Docker Hub
Docker Hub simplifies development with the world's largest container registry for storing, managing, and sharing Docker images. By integrating seamlessly with your tools, it enhances productivity and ensures reliable deployment, distribution, and access to containerized applications. It also provides developers with pre-built images and assets to speed up development workflows.
Key features of Docker Hub:
- Unlimited public repositories
- Private repositories
- Webhooks to automate workflows
- GitHub and Bitbucket integrations
- Concurrent and automated builds
- Trusted content featuring high-quality, secure images
In addition to the graphical interface, you can interact with Docker Hub using the Docker Hub API or experimental Docker Hub CLI tool.",,,
2c729ced2e547c82ec2836f862a5a7bfef8c2d22122f8a63bc2e10c1bcc176bc,"View container logs
The docker logs
command shows information logged by a running container. The
docker service logs
command shows information logged by all containers
participating in a service. The information that's logged and the format of the
log depends almost entirely on the container's endpoint command.
By default, docker logs
or docker service logs
shows the command's output
just as it would appear if you ran the command interactively in a terminal. Unix
and Linux commands typically open three I/O streams when they run, called
STDIN
, STDOUT
, and STDERR
. STDIN
is the command's input stream, which
may include input from the keyboard or input from another command. STDOUT
is
usually a command's normal output, and STDERR
is typically used to output
error messages. By default, docker logs
shows the command's STDOUT
and
STDERR
. To read more about I/O and Linux, see the
Linux Documentation Project article on I/O redirection.
In some cases, docker logs
may not show useful information unless you take
additional steps.
- If you use a
logging driver which sends logs to a file, an
external host, a database, or another logging back-end, and have
""dual logging""
disabled,
docker logs
may not show useful information. - If your image runs a non-interactive process such as a web server or a
database, that application may send its output to log files instead of
STDOUT
andSTDERR
.
In the first case, your logs are processed in other ways and you may choose not
to use docker logs
. In the second case, the official nginx
image shows one
workaround, and the official Apache httpd
image shows another.
The official nginx
image creates a symbolic link from /var/log/nginx/access.log
to /dev/stdout
, and creates another symbolic link
from /var/log/nginx/error.log
to /dev/stderr
, overwriting the log files and
causing logs to be sent to the relevant special device instead. See the
Dockerfile.
The official httpd
driver changes the httpd
application's configuration to
write its normal output directly to /proc/self/fd/1
(which is STDOUT
) and
its errors to /proc/self/fd/2
(which is STDERR
). See the
Dockerfile.
Next steps
- Configure logging drivers.
- Write a Dockerfile.",,,
b5b6e8e5b4a03616b57efaa4fbe367119b15038d5c18dcc704b9cc426d7627ca,"Plugin Config Version 1 of Plugin V2
This document outlines the format of the V0 plugin configuration.
Plugin configs describe the various constituents of a Docker engine plugin. Plugin configs can be serialized to JSON format with the following media types:
| Config Type | Media Type |
|---|---|
| config | application/vnd.docker.plugin.v1+json |
Config Field Descriptions
Config provides the base accessible fields for working with V0 plugin format in the registry.
description
stringDescription of the plugin
documentation
stringLink to the documentation about the plugin
interface
PluginInterfaceInterface implemented by the plugins, struct consisting of the following fields:
types
string arrayTypes indicate what interface(s) the plugin currently implements.
Supported types:
docker.volumedriver/1.0
docker.networkdriver/1.0
docker.ipamdriver/1.0
docker.authz/1.0
docker.logdriver/1.0
docker.metricscollector/1.0
socket
stringSocket is the name of the socket the engine should use to communicate with the plugins. the socket will be created in
/run/docker/plugins
.
entrypoint
string arrayEntrypoint of the plugin, see
ENTRYPOINT
workdir
stringWorking directory of the plugin, see
WORKDIR
network
PluginNetworkNetwork of the plugin, struct consisting of the following fields:
type
stringNetwork type.
Supported types:
bridge
host
none
mounts
PluginMount arrayMount of the plugin, struct consisting of the following fields. See
MOUNTS
.name
stringName of the mount.
description
stringDescription of the mount.
source
stringSource of the mount.
destination
stringDestination of the mount.
type
stringMount type.
options
string arrayOptions of the mount.
ipchost
BooleanAccess to host ipc namespace.
pidhost
BooleanAccess to host PID namespace.
propagatedMount
stringPath to be mounted as rshared, so that mounts under that path are visible to Docker. This is useful for volume plugins. This path will be bind-mounted outside of the plugin rootfs so it's contents are preserved on upgrade.
env
PluginEnv arrayEnvironment variables of the plugin, struct consisting of the following fields:
name
stringName of the environment variable.
description
stringDescription of the environment variable.
value
stringValue of the environment variable.
args
PluginArgsArguments of the plugin, struct consisting of the following fields:
name
stringName of the arguments.
description
stringDescription of the arguments.
value
string arrayValues of the arguments.
linux
PluginLinuxcapabilities
string arrayCapabilities of the plugin (Linux only), see list
here
allowAllDevices
BooleanIf
/dev
is bind mounted from the host, and allowAllDevices is set to true, the plugin will haverwm
access to all devices on the host.devices
PluginDevice arrayDevice of the plugin, (Linux only), struct consisting of the following fields. See
DEVICES
.name
stringName of the device.
description
stringDescription of the device.
path
stringPath of the device.
Example Config
The following example shows the 'tiborvass/sample-volume-plugin' plugin config.
{
""Args"": {
""Description"": """",
""Name"": """",
""Settable"": null,
""Value"": null
},
""Description"": ""A sample volume plugin for Docker"",
""Documentation"": ""https://docs.docker.com/engine/extend/plugins/"",
""Entrypoint"": [
""/usr/bin/sample-volume-plugin"",
""/data""
],
""Env"": [
{
""Description"": """",
""Name"": ""DEBUG"",
""Settable"": [
""value""
],
""Value"": ""0""
}
],
""Interface"": {
""Socket"": ""plugin.sock"",
""Types"": [
""docker.volumedriver/1.0""
]
},
""Linux"": {
""Capabilities"": null,
""AllowAllDevices"": false,
""Devices"": null
},
""Mounts"": null,
""Network"": {
""Type"": """"
},
""PropagatedMount"": ""/data"",
""User"": {},
""Workdir"": """"
}",,,
75a16bb9f801d3ea122dbfd254831361eb88980cf24aa4d856ec91fd4acfb20d,"Install Docker Engine
This section describes how to install Docker Engine on Linux, also known as Docker CE. Docker Engine is also available for Windows, macOS, and Linux, through Docker Desktop. For instructions on how to install Docker Desktop, see: Overview of Docker Desktop.
Supported platforms
| Platform | x86_64 / amd64 | arm64 / aarch64 | arm (32-bit) | ppc64le | s390x |
|---|---|---|---|---|---|
| CentOS | ✅ | ✅ | ✅ | ||
| Debian | ✅ | ✅ | ✅ | ✅ | |
| Fedora | ✅ | ✅ | ✅ | ||
| Raspberry Pi OS (32-bit) | ✅ | ||||
| RHEL | ✅ | ✅ | ✅ | ||
| SLES | ✅ | ||||
| Ubuntu | ✅ | ✅ | ✅ | ✅ | ✅ |
| Binaries | ✅ | ✅ | ✅ |
Other Linux distributions
Note
While the following instructions may work, Docker doesn't test or verify installation on distribution derivatives.
- If you use Debian derivatives such as ""BunsenLabs Linux"", ""Kali Linux"" or ""LMDE"" (Debian-based Mint) should follow the installation instructions for Debian, substitute the version of your distribution for the corresponding Debian release. Refer to the documentation of your distribution to find which Debian release corresponds with your derivative version.
- Likewise, if you use Ubuntu derivatives such as ""Kubuntu"", ""Lubuntu"" or ""Xubuntu"" you should follow the installation instructions for Ubuntu, substituting the version of your distribution for the corresponding Ubuntu release. Refer to the documentation of your distribution to find which Ubuntu release corresponds with your derivative version.
- Some Linux distributions provide a package of Docker Engine through their package repositories. These packages are built and maintained by the Linux distribution's package maintainers and may have differences in configuration or are built from modified source code. Docker isn't involved in releasing these packages and you should report any bugs or issues involving these packages to your Linux distribution's issue tracker.
Docker provides binaries for manual installation of Docker Engine. These binaries are statically linked and you can use them on any Linux distribution.
Release channels
Docker Engine has two types of update channels, stable and test:
- The stable channel gives you the latest versions released for general availability.
- The test channel gives you pre-release versions that are ready for testing before general availability.
Use the test channel with caution. Pre-release versions include experimental and early-access features that are subject to breaking changes.
Support
Docker Engine is an open source project, supported by the Moby project maintainers and community members. Docker doesn't provide support for Docker Engine. Docker provides support for Docker products, including Docker Desktop, which uses Docker Engine as one of its components.
For information about the open source project, refer to the Moby project website.
Upgrade path
Patch releases are always backward compatible with its major and minor version.
Licensing
Docker Engine is licensed under the Apache License, Version 2.0. See LICENSE for the full license text.
Reporting security issues
If you discover a security issue, we request that you bring it to our attention immediately.
DO NOT file a public issue. Instead, submit your report privately to security@docker.com.
Security reports are greatly appreciated, and Docker will publicly thank you for it.
Get started
After setting up Docker, you can learn the basics with Getting started with Docker.",,,
6988aa9c26baa053fb4c87356362a2b2aaddc86b55974449056727c7c3ba03fb,"Host network driver
If you use the host
network mode for a container, that container's network
stack isn't isolated from the Docker host (the container shares the host's
networking namespace), and the container doesn't get its own IP-address allocated.
For instance, if you run a container which binds to port 80 and you use host
networking, the container's application is available on port 80 on the host's IP
address.
Note
Given that the container does not have its own IP-address when using
host
mode networking, port-mapping doesn't take effect, and the-p
,--publish
,-P
, and--publish-all
option are ignored, producing a warning instead:WARNING: Published ports are discarded when using host network mode
Host mode networking can be useful for the following use cases:
- To optimize performance
- In situations where a container needs to handle a large range of ports
This is because it doesn't require network address translation (NAT), and no ""userland-proxy"" is created for each port.
The host networking driver is supported on Docker Engine (Linux only) and Docker Desktop version 4.34 and later.
You can also use a host
network for a swarm service, by passing --network host
to the docker service create
command. In this case, control traffic (traffic
related to managing the swarm and the service) is still sent across an overlay
network, but the individual swarm service containers send data using the Docker
daemon's host network and ports. This creates some extra limitations. For instance,
if a service container binds to port 80, only one service container can run on a
given swarm node.
Docker Desktop
Host networking is supported on Docker Desktop version 4.34 and later. To enable this feature:
- Sign in to your Docker account in Docker Desktop.
- Navigate to Settings.
- Under the Resources tab, select Network.
- Check the Enable host networking option.
- Select Apply and restart.
This feature works in both directions. This means you can access a server that is running in a container from your host and you can access servers running on your host from any container that is started with host networking enabled. TCP as well as UDP are supported as communication protocols.
Examples
The following command starts netcat in a container that listens on port 8000
:
$ docker run --rm -it --net=host nicolaka/netshoot nc -lkv 0.0.0.0 8000
Port 8000
will then be available on the host and you can connect to it with the following
command from another terminal:
$ nc localhost 8000
What you type in here will then appear on the terminal where the container is running.
To access a service running on the host from the container, you can start a container with host networking enabled with this command:
$ docker run --rm -it --net=host nicolaka/netshoot
If you then want to access a service on your host from the container (in this
example a web server running on port 80
), you can do it like this:
$ nc localhost 80
Limitations
- Processes inside the container cannot bind to the IP addresses of the host because the container has no direct access to the interfaces of the host.
- The host network feature of Docker Desktop works on layer 4. This means that unlike with Docker on Linux, network protocols that operate below TCP or UDP are not supported.
- This feature doesn't work with Enhanced Container Isolation enabled, since isolating your containers from the host and allowing them access to the host network contradict each other.
- Only Linux containers are supported. Host networking does not work with Windows containers.
Next steps
- Go through the host networking tutorial
- Learn about networking from the container's point of view
- Learn about bridge networks
- Learn about overlay networks
- Learn about Macvlan networks",,,
871aef613a105254b19b3501eb6c38b54e6827a3adb1f6574f566815f6815ac0,"Docker driver
The Buildx Docker driver is the default driver. It uses the BuildKit server components built directly into the Docker Engine. The Docker driver requires no configuration.
Unlike the other drivers, builders using the Docker driver can't be manually created. They're only created automatically from the Docker context.
Images built with the Docker driver are automatically loaded to the local image store.
Synopsis
# The Docker driver is used by buildx by default
docker buildx build .
It's not possible to configure which BuildKit version to use, or to pass any additional BuildKit parameters to a builder using the Docker driver. The BuildKit version and parameters are preset by the Docker Engine internally.
If you need additional configuration and flexibility, consider using the Docker container driver.
Further reading
For more information on the Docker driver, see the buildx reference.",,,
349621f4dbfcd053c70c95f995ac98f62deec97ea4b06b0a16b19aa2ca35d60a,"Design guidelines for Docker extensions
At Docker, we aim to build tools that integrate into a user's existing workflows rather than requiring them to adopt new ones. We strongly recommend that you follow these guidelines when creating extensions. We review and approve your Marketplace publication based on these requirements.
Here is a simple checklist to go through when creating your extension:
- Is it easy to get started?
- Is it easy to use?
- Is it easy to get help when needed?
Create a consistent experience with Docker Desktop
Use the Docker Material UI Theme and the Docker Extensions Styleguide to ensure that your extension feels like it is part of Docker Desktop to create a seamless experience for users.
Ensure the extension has both a light and dark theme. Using the components and styles as per the Docker style guide ensures that your extension meets the level AA accessibility standard..
Ensure that your extension icon is visible both in light and dark mode.
Ensure that the navigational behavior is consistent with the rest of Docker Desktop. Add a header to set the context for the extension.
Avoid embedding terminal windows. The advantage we have with Docker Desktop over the CLI is that we have the opportunity to provide rich information to users. Make use of this interface as much as possible.
Build features natively
In order not to disrupt the flow of users, avoid scenarios where the user has to navigate outside Docker Desktop, to the CLI or a webpage for example, in order to carry out certain functionalities. Instead, build features that are native to Docker Desktop.
Break down complicated user flows
If a flow is too complicated or the concept is abstract, break down the flow into multiple steps with one simple call-to-action in each step. This helps when onboarding novice users to your extension
Where there are multiple call-to-actions, ensure you use the primary (filled button style) and secondary buttons (outline button style) to convey the importance of each action.
Onboarding new users
When creating your extension, ensure that first time users of the extension and your product can understand its value-add and adopt it easily. Ensure you include contextual help within the extension.
Ensure that all necessary information is added to the extensions Marketplace as well as the extensions detail page. This should include:
- Screenshots of the extension. Note that the recommended size for screenshots is 2400x1600 pixels.
- A detailed description that covers what the purpose of the extension is, who would find it useful and how it works.
- Link to necessary resources such as documentation.
If your extension has particularly complex functionality, add a demo or video to the start page. This helps onboard a first time user quickly.
What's next?
- Explore our design principles.
- Take a look at our UI styling guidelines.
- Learn how to publish your extension.",,,
37b09ffcb19951be9cf97a87a413c52689b6006e1872fdb71e82b640a35b0a3f,"Docker Engine 25.0 release notes
This page describes the latest changes, additions, known issues, and fixes for Docker Engine version 25.0.
For more information about:
- Deprecated and removed features, see Deprecated Engine Features.
- Changes to the Engine API, see Engine API version history.
25.0.5
2024-03-19For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Security
This release contains a security fix for CVE-2024-29018, a potential data exfiltration from 'internal' networks via authoritative DNS servers.
Bug fixes and enhancements
CVE-2024-29018: Do not forward requests to external DNS servers for a container that is only connected to an 'internal' network. Previously, requests were forwarded if the host's DNS server was running on a loopback address, like systemd's 127.0.0.53. moby/moby#47589
plugin: fix mounting /etc/hosts when running in UserNS. moby/moby#47588
rootless: fix
open /etc/docker/plugins: permission denied
. moby/moby#47587Fix multiple parallel
docker build
runs leaking disk space. moby/moby#47527
25.0.4
2024-03-07For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- Restore DNS names for containers in the default ""nat"" network on Windows. moby/moby#47490
- Fix
docker start
failing when used with--checkpoint
moby/moby#47466 - Don't enforce new validation rules for existing swarm networks moby/moby#47482
- Restore IP connectivity between the host and containers on an internal bridge network. moby/moby#47481
- Fix a regression introduced in v25.0 that prevented the classic builder from adding tar archive with
xattrs
created on a non-Linux OS moby/moby#47483 - containerd image store: Fix image pull not emitting
Pulling fs layer status
moby/moby#47484 - API: To preserve backwards compatibility, make read-only mounts non-recursive by default when using older clients (API versions < v1.44). moby/moby#47393
- API:
GET /images/{id}/json
omits theCreated
field (previously it was0001-01-01T00:00:00Z
) if theCreated
field was missing from the image config. moby/moby#47451 - API: Populate a missing
Created
field inGET /images/{id}/json
with0001-01-01T00:00:00Z
for API versions <= 1.43. moby/moby#47387 - API: Fix a regression that caused API socket connection failures to report an API version negotiation failure instead. moby/moby#47470
- API: Preserve supplied endpoint configuration in a container-create API request, when a container-wide MAC address is specified, but
NetworkMode
name or id is not the same as the name or id used inNetworkSettings.Networks
. moby/moby#47510
Packaging updates
- Upgrade Go runtime to 1.21.8. moby/moby#47503
- Upgrade RootlessKit to v2.0.2. moby/moby#47508
- Upgrade Compose to v2.24.7. docker/docker-ce-packaging#998
- Upgrade Buildx to v0.13.0. docker/docker-ce-packaging#997
25.0.3
2024-02-06For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
containerd image store: Fix a bug where
docker image history
would fail if a manifest wasn't found in the content store. moby/moby#47348Ensure that a generated MAC address is not restored when a container is restarted, but a configured MAC address is preserved. moby/moby#47304
Note
- Containers created with Docker Engine version 25.0.0 may have duplicate MAC addresses. They must be re-created.
- Containers with user-defined MAC addresses created with Docker Engine versions 25.0.0 or 25.0.1 receive new MAC addresses when started using Docker Engine version 25.0.2. They must also be re-created.
Fix
docker save <image>@<digest>
producing an OCI archive with index without manifests. moby/moby#47294Fix a bug preventing bridge networks from being created with an MTU higher than 1500 on RHEL and CentOS 7. moby/moby#47308, moby/moby#47311
Fix a bug where containers are unable to communicate over an
internal
network. moby/moby#47303Fix a bug where the value of the
ipv6
daemon option was ignored. moby/moby#47310Fix a bug where trying to install a pulling using a digest revision would cause a panic. moby/moby#47323
Fix a potential race condition in the managed containerd supervisor. moby/moby#47313
Fix an issue with the
journald
log driver preventing container logs from being followed correctly with systemd version 255. moby/moby#47243seccomp: Update the builtin seccomp profile to include syscalls added in kernel v5.17 - v6.7 to align the profile with the profile used by containerd. moby/moby#47341
Windows: Fix cache not being used when building images based on Windows versions older than the host's version. moby/moby#47307, moby/moby#47337
Packaging updates
- Removed support for Ubuntu Lunar (23.04). docker/ce-packaging#986
25.0.2
2024-01-31For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Security
This release contains security fixes for the following CVEs affecting Docker Engine and its components.
| CVE | Component | Fix version | Severity |
|---|---|---|---|
| CVE-2024-21626 | runc | 1.1.12 | High, CVSS 8.6 |
| CVE-2024-23651 | BuildKit | 1.12.5 | High, CVSS 8.7 |
| CVE-2024-23652 | BuildKit | 1.12.5 | High, CVSS 8.7 |
| CVE-2024-23653 | BuildKit | 1.12.5 | High, CVSS 7.7 |
| CVE-2024-23650 | BuildKit | 1.12.5 | Medium, CVSS 5.5 |
| CVE-2024-24557 | Docker Engine | 25.0.2 | Medium, CVSS 6.9 |
The potential impacts of the above vulnerabilities include:
- Unauthorized access to the host filesystem
- Compromising the integrity of the build cache
- In the case of CVE-2024-21626, a scenario that could lead to full container escape
For more information about the security issues addressed in this release, refer to the blog post. For details about each vulnerability, see the relevant security advisory:
Packaging updates
- Upgrade containerd to v1.6.28.
- Upgrade containerd to v1.7.13 (static binaries only). moby/moby#47280
- Upgrade runc to v1.1.12. moby/moby#47269
- Upgrade Compose to v2.24.5. docker/docker-ce-packaging#985
- Upgrade BuildKit to v0.12.5. moby/moby#47273
25.0.1
2024-01-23For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- API: Fix incorrect HTTP status code for containers with an invalid network configuration created before upgrading to Docker Engine v25.0. moby/moby#47159
- Ensure that a MAC address based on a container's IP address is re-generated when the container is stopped and restarted, in case the generated IP/MAC addresses have been reused. moby/moby#47171
- Fix
host-gateway-ip
not working during build when not set through configuration. moby/moby#47192 - Fix a bug that prevented a container from being renamed twice. moby/moby#47196
- Fix an issue causing containers to have their short ID added to their network alias when inspecting them. moby/moby#47182
- Fix an issue in detecting whether a remote build context is a Git repository. moby/moby#47136
- Fix an issue with layers order in OCI manifests. moby/moby#47150
- Fix volume mount error when passing an
addr
orip
mount option. moby/moby#47185 - Improve error message related to extended attributes that can't be set due to improperly namespaced attribute names. moby/moby#47178
- Swarm: Fixed
start_interval
not being passed to the container config. moby/moby#47163
Packaging updates
- Upgrade Compose to
2.24.2
. docker/docker-ce-packaging#981
25.0.0
2024-01-19For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Note
In earlier versions of Docker Engine, recursive mounts (submounts) would always be mounted as writable, even when specifying a read-only mount. This behavior has changed in v25.0.0, for hosts running on kernel version 5.12 or later. Now, read-only bind mounts are recursively read-only by default.
To get the same behavior as earlier releases, you can specify the
bind-recursive
option for the--mount
flag.$ docker run --mount type=bind,src=SRC,dst=DST,readonly,bind-recursive=writable IMAGE
This option isn't supported with the
-v
or--volume
flag. For more information, see Recursive mounts.
New
The daemon now uses systemd's default
LimitNOFILE
. In earlier versions of Docker Engine, this limit was set toinfinity
. This would cause issues with recent versions of systemd, where the hard limit was increased, causing programs that adjusted their behaviors based on ulimits to consume a high amount of memory. moby/moby#45534The new setting makes containers behave the same way as programs running on the host, but may cause programs that make incorrect assumptions based on the soft limit to misbehave. To get the previous behavior, you can set
LimitNOFILE=1048576
.This change currently only affects build containers created with
docker build
when using BuildKit with thedocker
driver. Future versions of containerd will also use this limit, which will cause this behavior to affect all containers, not only build containers.If you're experiencing issues with the higher ulimit in systemd v240 or later, consider adding a system
drop-in
oroverride
file to configure the ulimit settings for your setup. The Flatcar Container Linux documentation has a great article covering this topic in detail.Add OpenTelemetry tracing. moby/moby#45652, moby/moby#45579
Add support for CDI devices under Linux. moby/moby#45134, docker/cli#4510, moby/moby#46004
Add an additional interval to be used by healthchecks during the container start period. moby/moby#40894, docker/cli#4405, moby/moby#45965
Add a
--log-format
flag todockerd
to control the logging format: text (default) or JSON. moby/moby#45737Add support for recursive read-only mounts. moby/moby#45278, moby/moby#46037
Add support for filtering images based on timestamp with
docker image ls --filter=until=<timestamp>
. moby/moby#46577
Bug fixes and enhancements
- API: Fix error message for invalid policies at
ValidateRestartPolicy
. moby/moby#46352 - API: Update
/info
endpoint to use singleflight. moby/moby#45847 - Add an error message for when specifying a Dockerfile filename with
-f
, and also usingstdin
. docker/cli#4346 - Add support for
mac-address
andlink-local-ip
fields in--network
long format. docker/cli#4419 - Add support for specifying multiple
--network
flags withdocker container create
anddocker run
. moby/moby#45906 - Automatically enable IPv6 on a network when an IPv6 subnet is specified. moby/moby#46455
- Add support for overlay networks over IPv6 transport. moby/moby#46790
- Configuration reloading is now more robust: if there's an error during the configuration reload process, no configuration changes are applied. moby/moby#43980
- Live restore: Containers with auto remove (
docker run --rm
) are no longer forcibly removed on engine restart. moby/moby#46857 - Live restore: containers that are live-restored will now be given another health-check start period when the daemon restarts. moby/moby#47051
- Container health status is flushed to disk less frequently, reducing wear on flash storage. moby/moby#47044
- Ensure network names are unique. moby/moby#46251
- Ensure that overlay2 layer metadata is correct. moby/moby#46471
- Fix
Downloading
progress message on image pull. moby/moby#46515 - Fix
NetworkConnect
andContainerCreate
with improved data validation, and return all validation errors at once. moby/moby#46183 - Fix
com.docker.network.host_ipv4
option when IPv6 and ip6tables are enabled. moby/moby#46446 - Fix daemon's
cleanupContainer
if containerd is stopped. moby/moby#46213 - Fix returning incorrect HTTP status codes for libnetwork errors. moby/moby#46146
- Fix various issues with images/json API filters and image list. moby/moby#46034
- CIFS volumes now resolves FQDN correctly. moby/moby#46863
- Improve validation of the
userland-proxy-path
daemon configuration option. Validation now happens during daemon startup, instead of producing an error when starting a container with port-mapping. moby/moby#47000 - Set the MAC address of container's interface when network mode is a short network ID. moby/moby#46406
- Sort unconsumed build arguments before display in build output. moby/moby#45917
- The
docker image save
tarball output is now OCI compliant. moby/moby#44598 - The daemon no longer appends
ACCEPT
rules to the end of theINPUT
iptables chain for encrypted overlay networks. Depending on firewall configuration, a rule may be needed to permit incoming encrypted overlay network traffic. moby/moby#45280 - Unpacking layers with extended attributes onto an incompatible filesystem will now fail instead of silently discarding extended attributes. moby/moby#45464
- Update daemon MTU option to BridgeConfig and display warning on Windows. moby/moby#45887
- Validate IPAM config when creating a network. Automatically fix networks created prior to this release where
--ip-range
is larger than--subnet
. moby/moby#45759 - Containers connected only to internal networks will now have no default route set, making the
connect
syscall fail-fast. moby/moby#46603 - containerd image store: Add image events for
push
,pull
, andsave
. moby/moby#46405 - containerd image store: Add support for pulling legacy schema1 images. moby/moby#46513
- containerd image store: Add support for pushing all tags. moby/moby#46485
- containerd image store: Add support for registry token. moby/moby#46475
- containerd image store: Add support for showing the number of containers that use an image. moby/moby#46511
- containerd image store: Fix a bug related to the
ONBUILD
,MAINTAINER
, andHEALTHCHECK
Dockerfile instructions. moby/moby#46313 - containerd image store: Fix
Pulling from
progress message. moby/moby#46494 - containerd image store: Add support for referencing images via the truncated ID with
sha256:
prefix. moby/moby#46435 - containerd image store: Fix
docker images
showing intermediate layers by default. moby/moby#46423 - containerd image store: Fix checking if the specified platform exists when getting an image. moby/moby#46495
- containerd image store: Fix errors when multiple
ADD
orCOPY
instructions were used with the classic builder. moby/moby#46383 - containerd image store: Fix stack overflow errors when importing an image. moby/moby#46418
- containerd image store: Improve
docker pull
progress output. moby/moby#46412 - containerd image store: Print the tag, digest, and size after pushing an image. moby/moby#46384
- containerd image store: Remove panic from
UpdateConfig
. moby/moby#46433 - containerd image store: Return an error when an image tag resembles a digest. moby/moby#46492
- containerd image store:
docker image ls
now shows the correct image creation time and date. moby/moby#46719 - containerd image store: Fix an issue handling user namespace settings. moby/moby#46375
- containerd image store: Add support for pulling all tags (
docker pull -a
). moby/moby#46618 - containerd image store: Use the domain name in the image reference as the default registry authentication domain. moby/moby#46779
Packaging updates
- Upgrade API to v1.44. moby/moby#45468
- Upgrade Compose to
2.24.1
. docker/docker-ce-packaging#980 - Upgrade containerd to v1.7.12 (static binaries only). moby/moby#47070
- Upgrade Go runtime to 1.21.6. moby/moby#47053
- Upgrade runc to v1.1.11. moby/moby#47007
- Upgrade BuildKit to v0.12.4. moby/moby#46882
- Upgrade Buildx to v0.12.1. docker/docker-ce-packaging#979
Removed
- API: Remove VirtualSize field for the
GET /images/json
andGET /images/{id}/json
endpoints. moby/moby#45469 - Remove deprecated
devicemapper
storage driver. moby/moby#43637 - Remove deprecated orchestrator options. docker/cli#4366
- Remove support for Debian Upstart init system. moby/moby#45548, moby/moby#45551
- Remove the
--oom-score-adjust
daemon option. moby/moby#45484 - Remove warning for deprecated
~/.dockercfg
file. docker/cli#4281 - Remove
logentries
logging driver. moby/moby#46925
Deprecated
- Deprecate API versions older than 1.24. Deprecation notice
- Deprecate
IsAutomated
field andis-automated
filter fordocker search
. Deprecation notice - API: Deprecate
Container
andContainerConfig
properties for/images/{id}/json
(docker image inspect
). moby/moby#46939
Known limitations
Extended attributes for tar files
In this release, the code that handles tar
archives was changed to be more
strict and to produce an error when failing to write extended attributes
(xattr
). The tar
implementation for macOS generates additional extended
attributes by default when creating tar files. These attribute prefixes aren't
valid Linux xattr
namespace prefixes, which causes an error when Docker
attempts to process these files. For example, if you try to use a tar file with
an ADD
Dockerfile instruction, you might see an error message similar to the
following:
failed to solve: lsetxattr /sftp_key.ppk: operation not supported
Error messages related to extended attribute validation were improved to
include more context in
v25.0.1, but the limitation in Docker being
unable to process the files remains. Tar files created with the macOS tar
using default arguments will produce an error when the tar file is used with
Docker.
As a workaround, if you need to use tar files with Docker generated on macOS, you can either:
Use the
--no-xattr
flag for the macOStar
command to strip all the extended attributes. If you want to preserve extended attributes, this isn't a recommended option.Install and use
gnu-tar
to create the tarballs on macOS instead of the defaulttar
implementation. To installgnu-tar
using Homebrew:$ brew install gnu-tar
After installing, add the
gnu-tar
binary to yourPATH
, for example by updating your.zshrc
file:$ echo 'PATH=""/opt/homebrew/opt/gnu-tar/libexec/gnubin:$PATH""' >> ~/.zshrc $ source ~/.zshrc",,,
4279e138d8a1434a12e53a76eee438f1cd4d8027f7c5bbba4e2785eadd4228ce,"Docker daemon configuration overview
This page shows you how to customize the Docker daemon, dockerd
.
Note
This page is for users who've installed Docker Engine manually. If you're using Docker Desktop, refer to the settings page.
Configure the Docker daemon
There are two ways to configure the Docker daemon:
- Use a JSON configuration file. This is the preferred option, since it keeps all configurations in a single place.
- Use flags when starting
dockerd
.
You can use both of these options together as long as you don't specify the same option both as a flag and in the JSON file. If that happens, the Docker daemon won't start and prints an error message.
Configuration file
The following table shows the location where the Docker daemon expects to find the configuration file by default, depending on your system and how you're running the daemon.
| OS and configuration | File location |
|---|---|
| Linux, regular setup | /etc/docker/daemon.json |
| Linux, rootless mode | ~/.config/docker/daemon.json |
| Windows | C:\ProgramData\docker\config\daemon.json |
For rootless mode, the daemon respects the XDG_CONFIG_HOME
variable. If set,
the expected file location is $XDG_CONFIG_HOME/docker/daemon.json
.
You can also explicitly specify the location of the configuration file on
startup, using the dockerd --config-file
flag.
Learn about the available configuration options in the dockerd reference docs
Configuration using flags
You can also start the Docker daemon manually and configure it using flags. This can be useful for troubleshooting problems.
Here's an example of how to manually start the Docker daemon, using the same configurations as shown in the previous JSON configuration:
$ dockerd --debug \
--tls=true \
--tlscert=/var/docker/server.pem \
--tlskey=/var/docker/serverkey.pem \
--host tcp://192.168.59.3:2376
Learn about the available configuration options in the dockerd reference docs, or by running:
$ dockerd --help
Daemon data directory
The Docker daemon persists all data in a single directory. This tracks everything related to Docker, including containers, images, volumes, service definition, and secrets.
By default this directory is:
/var/lib/docker
on Linux.C:\ProgramData\docker
on Windows.
You can configure the Docker daemon to use a different directory, using the
data-root
configuration option. For example:
{
""data-root"": ""/mnt/docker-data""
}
Since the state of a Docker daemon is kept on this directory, make sure you use a dedicated directory for each daemon. If two daemons share the same directory, for example, an NFS share, you are going to experience errors that are difficult to troubleshoot.
Next steps
Many specific configuration options are discussed throughout the Docker documentation. Some places to go next include:",,,
4fc1731088369cc6ff6fecbd5b0c9e962b9249142ad6279014d58d299c28f362,"Configure a private marketplace for extensions
Learn how to configure and set up a private marketplace with a curated list of extensions for your Docker Desktop users.
Docker Extensions' private marketplace is designed specifically for organizations who don’t give developers root access to their machines. It makes use of Settings Management so administrators have complete control over the private marketplace.
Prerequisites
- Download and install Docker Desktop 4.26.0 or later.
- You must be an administrator for your organization.
- You have the ability to push the
extension-marketplace
folder andadmin-settings.json
file to the locations specified below through device management software such as Jamf.
Step one: Initialize the private marketplace
Create a folder locally for the content that will be deployed to your developers’ machines:
$ mkdir my-marketplace $ cd my-marketplace
Initialize the configuration files for your marketplace:
$ /Applications/Docker.app/Contents/Resources/bin/extension-admin init
$ C:\Program Files\Docker\Docker\resources\bin\extension-admin init
$ /opt/docker-desktop/extension-admin init
This creates 2 files:
admin-settings.json
, which activates the private marketplace feature once it’s applied to Docker Desktop on your developers’ machines.extensions.txt
, which determines which extensions to list in your private marketplace.
Step two: Set the behaviour
The generated admin-settings.json
file includes various settings you can modify.
Each setting has a value
that you can set, including a locked
field that lets you lock the setting and make it unchangeable by your developers.
extensionsEnabled
enables Docker Extensions.extensionsPrivateMarketplace
activates the private marketplace and ensures Docker Desktop connects to content defined and controlled by the administrator instead of the public Docker marketplace.onlyMarketplaceExtensions
allows or blocks developers from installing other extensions by using the command line. Teams developing new extensions must have this setting unlocked (""locked"": false
) to install and test extensions being developed.extensionsPrivateMarketplaceAdminContactURL
defines a contact link for developers to request new extensions in the private marketplace. Ifvalue
is empty then no link is shown to your developers on Docker Desktop, otherwise this can be either an HTTP link or a “mailto:” link. For example,""extensionsPrivateMarketplaceAdminContactURL"": { ""locked"": true, ""value"": ""mailto:admin@acme.com"" }
To find out more information about the admin-settings.json
file, see
Settings Management.
Step three: List allowed extensions
The generated extensions.txt
file defines the list of extensions that are available in your private marketplace.
Each line in the file is an allowed extension and follows the format of org/repo:tag
.
For example, if you want to permit the Disk Usage extension you would enter the following into your extensions.txt
file:
docker/disk-usage-extension:0.2.8
If no tag is provided, the latest tag available for the image is used. You can also comment out lines with #
so the extension is ignored.
This list can include different types of extension images:
- Extensions from the public marketplace or any public image stored in Docker Hub.
- Extension images stored in Docker Hub as private images. Developers need to be signed in and have pull access to these images.
- Extension images stored in a private registry. Developers need to be signed in and have pull access to these images.
Important
Your developers can only install the version of the extension that you’ve listed.
Step four: Generate the private marketplace
Once the list in extensions.txt
is ready, you can generate the marketplace:
$ /Applications/Docker.app/Contents/Resources/bin/extension-admin generate
$ C:\Program Files\Docker\Docker\resources\bin\extension-admin generate
$ /opt/docker-desktop/extension-admin generate
This creates an extension-marketplace
directory and downloads the marketplace metadata for all the allowed extensions.
The marketplace content is generated from extension image information as image labels, which is the same format as public extensions. It includes the extension title, description, screenshots, links, etc.
Step five: Test the private marketplace setup
It's recommended that you try the private marketplace on your Docker Desktop installation.
Run the following command in your terminal. This command automatically copies the generated files to the location where Docker Desktop reads the configuration files. Depending on your operating system, the location is:
- Mac:
/Library/Application\ Support/com.docker.docker
- Windows:
C:\ProgramData\DockerDesktop
- Linux:
/usr/share/docker-desktop
$ sudo /Applications/Docker.app/Contents/Resources/bin/extension-admin apply
$ C:\Program Files\Docker\Docker\resources\bin\extension-admin apply
$ sudo /opt/docker-desktop/extension-admin apply
- Mac:
Quit and re-open Docker Desktop.
Sign in with a Docker account.
When you select the Extensions tab, you should see the private marketplace listing only the extensions you have allowed in extensions.txt
.
Step six: Distribute the private marketplace
Once you’ve confirmed that the private marketplace configuration works, the final step is to distribute the files to the developers’ machines with the MDM software your organization uses. For example, Jamf.
The files to distribute are:
admin-settings.json
- the entire
extension-marketplace
folder and its subfolders
These files must be placed on developer's machines. Depending on your operating system, the target location is (as mentioned above):
- Mac:
/Library/Application\ Support/com.docker.docker
- Windows:
C:\ProgramData\DockerDesktop
- Linux:
/usr/share/docker-desktop
Make sure your developers are signed in to Docker Desktop in order for the private marketplace configuration to take effect. As an administrator, you should enforce sign-in.
Feedback
Give feedback or report any bugs you may find by emailing extensions@docker.com
.",,,
b7126b5bf967e7efbe339544ced2ccb97982473e2a41a5aae4f39aa580d2f29f,"Extension image labels
Extensions use image labels to provide additional information such as a title, description, screenshots, and more.
This information is then displayed as an overview of the extension, so users can choose to install it.
You can define
image labels in the extension's Dockerfile
.
Important
If any of the required labels are missing in the
Dockerfile
, Docker Desktop considers the extension invalid and doesn't list it in the Marketplace.
Here is the list of labels you can or need to specify when building your extension:
| Label | Required | Description | Example |
|---|---|---|---|
org.opencontainers.image.title | Yes | Human-readable title of the image (string). This appears in the UI for Docker Desktop. | my-extension |
org.opencontainers.image.description | Yes | Human-readable description of the software packaged in the image (string) | This extension is cool. |
org.opencontainers.image.vendor | Yes | Name of the distributing entity, organization, or individual. | Acme, Inc. |
com.docker.desktop.extension.api.version | Yes | Version of the Docker Extension manager that the extension is compatible with. It must follow semantic versioning. | A specific version like 0.1.0 or, a constraint expression: >= 0.1.0 , >= 1.4.7, < 2.0 . For your first extension, you can use docker extension version to know the SDK API version and specify >= <SDK_API_VERSION> . |
com.docker.desktop.extension.icon | Yes | The extension icon (format: .svg .png .jpg) | https://example.com/assets/image.svg |
com.docker.extension.screenshots | Yes | A JSON array of image URLs and an alternative text displayed to users (in the order they appear in your metadata) in your extension's details page. Note: The recommended size for screenshots is 2400x1600 pixels. | [{""alt"":""alternative text for image 1"", ""url"":""https://example.com/image1.png""}, {""alt"":""alternative text for image2"", ""url"":""https://example.com/image2.jpg""}] |
com.docker.extension.detailed-description | Yes | Additional information in plain text or HTML about the extension to display in the details dialog. | My detailed description or <h1>My detailed description</h1> |
com.docker.extension.publisher-url | Yes | The publisher website URL to display in the details dialog. | https://example.com |
com.docker.extension.additional-urls | No | A JSON array of titles and additional URLs displayed to users (in the order they appear in your metadata) in your extension's details page. Docker recommends you display the following links if they apply: documentation, support, terms of service, and privacy policy links. | [{""title"":""Documentation"",""url"":""https://example.com/docs""}, {""title"":""Support"",""url"":""https://example.com/bar/support""}, {""title"":""Terms of Service"",""url"":""https://example.com/tos""}, {""title"":""Privacy policy"",""url"":""https://example.com/privacy""}] |
com.docker.extension.changelog | Yes | Changelog in plain text or HTML containing the change for the current version only. | Extension changelog or <p>Extension changelog<ul> <li>New feature A</li> <li>Bug fix on feature B</li></ul></p> |
com.docker.extension.account-info | No | Whether the user needs to register to a SaaS platform to use some features of the extension. | required in case it does, leave it empty otherwise. |
com.docker.extension.categories | No | The list of Marketplace categories that your extension belongs to: ci-cd , container-orchestration , cloud-deployment , cloud-development , database , kubernetes , networking , image-registry , security , testing-tools , utility-tools ,volumes . If you don't specify this label, users won't be able to find your extension in the Extensions Marketplace when filtering by a category. Extensions published to the Marketplace before the 22nd of September 2022 have been auto-categorized by Docker. | Specified as comma separated values in case of having multiple categories e.g: kubernetes,security or a single value e.g. kubernetes . |
Tip
Docker Desktop applies CSS styles to the provided HTML content. You can make sure that it renders correctly within the Marketplace. It is recommended that you follow the styling guidelines.
Preview the extension in the Marketplace
You can validate that the image labels render as you expect.
When you create and install your unpublished extension, you can preview the extension in the Marketplace's Managed tab. You can see how the extension labels render in the list and in the details page of the extension.
Preview extensions already listed in Marketplace
When you install a local image of an extension already published in the Marketplace, for example with the tag
latest
, your local image is not detected as ""unpublished"".You can re-tag your image in order to have a different image name that's not listed as a published extension. Use
docker tag org/published-extension unpublished-extension
and thendocker extension install unpublished-extension
.",,,
f6bf4781b8a0c3287ed9ac256785226c9a6c04c9e632abed4acad1cecafe076e,"FAQs on SSO and enforcement
I currently have a Docker Team subscription. How do I enable SSO?
SSO is available with a Docker Business subscription. To enable SSO, you must first upgrade your subscription to a Docker Business subscription. To learn how to upgrade your existing account, see Upgrade your subscription.
Is DNS verification required to enable SSO?
Yes. You must verify a domain before using it with an SSO connection.
Does Docker SSO support authenticating through the command line?
When SSO is enforced, passwords are prevented from accessing the Docker CLI. You can still access the Docker CLI using a personal access token (PAT) for authentication.
Each user must create a PAT to access the CLI. To learn how to create a PAT, see Manage access tokens. Users who already used a PAT to sign in before SSO enforcement will still be able to use that PAT to authenticate.
How does SSO affect automation systems and CI/CD pipelines?
Before enforcing SSO, you must create PATs. These PATs are used instead of passwords for signing into automation systems and CI/CD pipelines.
What can organization users who authenticated with personal emails prior to enforcement expect?
Ensure your users have their organization email on their account, so that the accounts will be migrated to SSO for authentication.
Can I enable SSO and hold off on the enforcement option?
Yes, you can choose to not enforce, and users have the option to use either Docker ID (standard email and password) or domain-verified email address (SSO) at the sign-in screen.
SSO is enforced, but a user can sign in using a username and password. Why is this happening?
Guest users who are not part of your registered domain but have been invited to your organization do not sign in through your SSO Identity Provider. SSO enforcement only requires that users which do belong to your domain, must go through the SSO IdP.
Is there a way to test this functionality in a test tenant with Okta before going to production?
Yes, you can create a test organization. Companies can set up a new 5 seat Business plan on a new organization to test with. To do this, make sure to only enable SSO, not enforce it, or all domain email users will be forced to sign in to that test tenant.
Is the sign in required tracking at runtime or install time?
For Docker Desktop, if it's configured to require authentication to the organization, it tracks at runtime.
What is enforcing SSO versus enforcing sign-in?
Enforcing SSO and enforcing sign-in to Docker Desktop are different features that you can use separately or together.
Enforcing SSO ensures that users sign in using their SSO credentials instead of their Docker ID. One of the benefits is that SSO enables you to better manage user credentials.
Enforcing sign-in to Docker Desktop ensures that users always sign in to an account that's a member of your organization. The benefits are that your organization's security settings are always applied to the user's session and your users always receive the benefits of your subscription. For more details, see Enforce sign-in for Desktop.",,,
051e8ce20e405bd593538321ec3308e388a0a910e91d2e6c04b4aad2cb72670f,"Install the extension for your organization
To use the Docker for GitHub copilot extension, you first need to install the extension for your organization, and manage policies for Copilot in your organization.
Prerequisites
Before you start, ensure that you're signed in to your GitHub account on GitHub.com.
Install
To install the Docker for GitHub Copilot extension for your GitHub organization:
Go to the Docker for GitHub Copilot app in the GitHub Marketplace.
Select the Add button at the top of the page.
Under Pricing and setup, select the organization that you want to install the extension for and select Install it for free.
Select the Complete order and begin installation button.
Select the repositories where you want to use the Docker Extension for GitHub Copilot and finish with Install.
Manage policies
If you're enabling the extension for a GitHub organization, you also need to enable the Copilot Extensions policy. For instructions, see Setting a policy for GitHub Copilot Extensions in your organization.",,,
166c5588604afccaebf82b84c784d620bb5993e46e970950b9cf8dc719e250b3,"Dockerfile overview
Dockerfile
It all starts with a Dockerfile.
Docker builds images by reading the instructions from a Dockerfile. A Dockerfile is a text file containing instructions for building your source code. The Dockerfile instruction syntax is defined by the specification reference in the Dockerfile reference.
Here are the most common types of instructions:
| Instruction | Description |
|---|---|
FROM <image> | Defines a base for your image. |
RUN <command> | Executes any commands in a new layer on top of the current image and commits the result. RUN also has a shell form for running commands. |
WORKDIR <directory> | Sets the working directory for any RUN , CMD , ENTRYPOINT , COPY , and ADD instructions that follow it in the Dockerfile. |
COPY <src> <dest> | Copies new files or directories from <src> and adds them to the filesystem of the container at the path <dest> . |
CMD <command> | Lets you define the default program that is run once you start the container based on this image. Each Dockerfile only has one CMD , and only the last CMD instance is respected when multiple exist. |
Dockerfiles are crucial inputs for image builds and can facilitate automated, multi-layer image builds based on your unique configurations. Dockerfiles can start simple and grow with your needs to support more complex scenarios.
Filename
The default filename to use for a Dockerfile is Dockerfile
, without a file
extension. Using the default name allows you to run the docker build
command
without having to specify additional command flags.
Some projects may need distinct Dockerfiles for specific purposes. A common
convention is to name these <something>.Dockerfile
. You can specify the
Dockerfile filename using the --file
flag for the docker build
command.
Refer to the
docker build
CLI reference
to learn about the --file
flag.
Note
We recommend using the default (
Dockerfile
) for your project's primary Dockerfile.
Docker images
Docker images consist of layers. Each layer is the result of a build instruction in the Dockerfile. Layers are stacked sequentially, and each one is a delta representing the changes applied to the previous layer.
Example
Here's what a typical workflow for building applications with Docker looks like.
The following example code shows a small ""Hello World"" application written in Python, using the Flask framework.
from flask import Flask
app = Flask(__name__)
@app.route(""/"")
def hello():
return ""Hello World!""
In order to ship and deploy this application without Docker Build, you would need to make sure that:
- The required runtime dependencies are installed on the server
- The Python code gets uploaded to the server's filesystem
- The server starts your application, using the necessary parameters
The following Dockerfile creates a container image, which has all the dependencies installed and that automatically starts your application.
# syntax=docker/dockerfile:1
FROM ubuntu:22.04
# install app dependencies
RUN apt-get update && apt-get install -y python3 python3-pip
RUN pip install flask==3.0.*
# install app
COPY hello.py /
# final configuration
ENV FLASK_APP=hello
EXPOSE 8000
CMD [""flask"", ""run"", ""--host"", ""0.0.0.0"", ""--port"", ""8000""]
Here's a breakdown of what this Dockerfile does:
- Dockerfile syntax
- Base image
- Environment setup
- Comments
- Installing dependencies
- Copying files
- Setting environment variables
- Exposed ports
- Starting the application
Dockerfile syntax
The first line to add to a Dockerfile is a
# syntax
parser directive.
While optional, this directive instructs the Docker builder what syntax to use
when parsing the Dockerfile, and allows older Docker versions with
BuildKit enabled
to use a specific
Dockerfile frontend before
starting the build.
Parser directives
must appear before any other comment, whitespace, or Dockerfile instruction in
your Dockerfile, and should be the first line in Dockerfiles.
# syntax=docker/dockerfile:1
Tip
We recommend using
docker/dockerfile:1
, which always points to the latest release of the version 1 syntax. BuildKit automatically checks for updates of the syntax before building, making sure you are using the most current version.
Base image
The line following the syntax directive defines what base image to use:
FROM ubuntu:22.04
The
FROM
instruction sets your base
image to the 22.04 release of Ubuntu. All instructions that follow are executed
in this base image: an Ubuntu environment. The notation ubuntu:22.04
, follows
the name:tag
standard for naming Docker images. When you build images, you
use this notation to name your images. There are many public images you can
leverage in your projects, by importing them into your build steps using the
Dockerfile FROM
instruction.
Docker Hub contains a large set of official images that you can use for this purpose.
Environment setup
The following line executes a build command inside the base image.
# install app dependencies
RUN apt-get update && apt-get install -y python3 python3-pip
This
RUN
instruction executes a
shell in Ubuntu that updates the APT package index and installs Python tools in
the container.
Comments
Note the # install app dependencies
line. This is a comment. Comments in
Dockerfiles begin with the #
symbol. As your Dockerfile evolves, comments can
be instrumental to document how your Dockerfile works for any future readers
and editors of the file, including your future self!
Note
You might've noticed that comments are denoted using the same symbol as the syntax directive on the first line of the file. The symbol is only interpreted as a directive if the pattern matches a directive and appears at the beginning of the Dockerfile. Otherwise, it's treated as a comment.
Installing dependencies
The second RUN
instruction installs the flask
dependency required by the
Python application.
RUN pip install flask==3.0.*
A prerequisite for this instruction is that pip
is installed into the build
container. The first RUN
command installs pip
, which ensures that we can
use the command to install the flask web framework.
Copying files
The next instruction uses the
COPY
instruction to copy the
hello.py
file from the local build context into the root directory of our image.
COPY hello.py /
A
build context is the set of files that you can access
in Dockerfile instructions such as COPY
and ADD
.
After the COPY
instruction, the hello.py
file is added to the filesystem
of the build container.
Setting environment variables
If your application uses environment variables, you can set environment variables
in your Docker build using the
ENV
instruction.
ENV FLASK_APP=hello
This sets a Linux environment variable we'll need later. Flask, the framework used in this example, uses this variable to start the application. Without this, flask wouldn't know where to find our application to be able to run it.
Exposed ports
The
EXPOSE
instruction marks that
our final image has a service listening on port 8000
.
EXPOSE 8000
This instruction isn't required, but it is a good practice and helps tools and team members understand what this application is doing.
Starting the application
Finally,
CMD
instruction sets the
command that is run when the user starts a container based on this image.
CMD [""flask"", ""run"", ""--host"", ""0.0.0.0"", ""--port"", ""8000""]
This command starts the flask development server listening on all addresses
on port 8000
. The example here uses the ""exec form"" version of CMD
.
It's also possible to use the ""shell form"":
CMD flask run --host 0.0.0.0 --port 8000
There are subtle differences between these two versions,
for example in how they trap signals like SIGTERM
and SIGKILL
.
For more information about these differences, see
Shell and exec form
Building
To build a container image using the Dockerfile example from the
previous section, you use the docker build
command:
$ docker build -t test:latest .
The -t test:latest
option specifies the name and tag of the image.
The single dot (.
) at the end of the command sets the
build context to the current directory. This means that the
build expects to find the Dockerfile and the hello.py
file in the directory
where the command is invoked. If those files aren't there, the build fails.
After the image has been built, you can run the application as a container with
docker run
, specifying the image name:
$ docker run -p 127.0.0.1:8000:8000 test:latest
This publishes the container's port 8000 to http://localhost:8000
on the
Docker host.",,,
964ac2ae377b0b6ccd632611cdcb1ef4379361692dbd420112907f97f467f7e4,"How nodes work
Swarm mode lets you create a cluster of one or more Docker Engines called a swarm. A swarm consists of one or more nodes: physical or virtual machines running Docker Engine.
There are two types of nodes: managers and workers.
If you haven't already, read through the Swarm mode overview and key concepts.
Manager nodes
Manager nodes handle cluster management tasks:
- Maintaining cluster state
- Scheduling services
- Serving Swarm mode HTTP API endpoints
Using a Raft implementation, the managers maintain a consistent internal state of the entire swarm and all the services running on it. For testing purposes it is OK to run a swarm with a single manager. If the manager in a single-manager swarm fails, your services continue to run, but you need to create a new cluster to recover.
To take advantage of Swarm mode's fault-tolerance features, we recommend you implement an odd number of nodes according to your organization's high-availability requirements. When you have multiple managers you can recover from the failure of a manager node without downtime.
A three-manager swarm tolerates a maximum loss of one manager.
A five-manager swarm tolerates a maximum simultaneous loss of two manager nodes.
An odd number
N
of manager nodes in the cluster tolerates the loss of at most(N-1)/2
managers. Docker recommends a maximum of seven manager nodes for a swarm.Important
Adding more managers does NOT mean increased scalability or higher performance. In general, the opposite is true.
Worker nodes
Worker nodes are also instances of Docker Engine whose sole purpose is to execute containers. Worker nodes don't participate in the Raft distributed state, make scheduling decisions, or serve the swarm mode HTTP API.
You can create a swarm of one manager node, but you cannot have a worker node
without at least one manager node. By default, all managers are also workers.
In a single manager node cluster, you can run commands like docker service create
and the scheduler places all tasks on the local engine.
To prevent the scheduler from placing tasks on a manager node in a multi-node
swarm, set the availability for the manager node to Drain
. The scheduler
gracefully stops tasks on nodes in Drain
mode and schedules the tasks on an
Active
node. The scheduler does not assign new tasks to nodes with Drain
availability.
Refer to the
docker node update
command line reference to see how to change node availability.
Change roles
You can promote a worker node to be a manager by running docker node promote
.
For example, you may want to promote a worker node when you
take a manager node offline for maintenance. See
node promote.
You can also demote a manager node to a worker node. See node demote.",,,
1899fa33dc20f640a980c76646f9f44f53b06ca9ddc990f867e5859b3861c4b8,"Workarounds for common problems
Reboot
Restart your PC to stop / discard any vestige of the daemon running from the previously installed version.
Unset DOCKER_HOST
The DOCKER_HOST
environmental variable does not need to be set. If you use
bash, use the command unset ${!DOCKER_*}
to unset it. For other shells,
consult the shell's documentation.
Make sure Docker is running for webserver examples
For the hello-world-nginx
example and others, Docker Desktop must be
running to get to the webserver on http://localhost/
. Make sure that the
Docker whale is showing in the menu bar, and that you run the Docker commands in
a shell that is connected to the Docker Desktop Engine. Otherwise, you might start the webserver container but get a ""web page
not available"" error when you go to docker
.
How to solve port already allocated
errors
If you see errors like Bind for 0.0.0.0:8080 failed: port is already allocated
or listen tcp:0.0.0.0:8080: bind: address is already in use
...
These errors are often caused by some other software on Windows using those
ports. To discover the identity of this software, either use the resmon.exe
GUI and click ""Network"" and then ""Listening Ports"" or in a PowerShell use
netstat -aon | find /i ""listening ""
to discover the PID of the process
currently using the port (the PID is the number in the rightmost column). Decide
whether to shut the other process down, or to use a different port in your
docker app.
Docker Desktop fails to start when anti-virus software is installed
Some anti-virus software may be incompatible with Hyper-V and Microsoft Windows 10 builds. The conflict typically occurs after a Windows update and manifests as an error response from the Docker daemon and a Docker Desktop start failure.
For a temporary workaround, uninstall the anti-virus software, or explore other workarounds suggested on Docker Desktop forums.",,,
840ebc3621e97a969132481a0698d2c93efbc1be252490f96b58aea6ec335144,"Onboard your organization
Learn how to onboard your organization using Docker Hub or the Docker Admin Console.
Onboarding your organization lets administrators gain visibility into user activity and enforce security settings. In addition, members of your organization receive increased pull limits and other organization wide benefits. For more details, see Docker subscriptions and features.
In this guide, you'll learn how to do the following:
- Identify your users to help you efficiently allocate your subscription seats
- Invite members and owners to your organization
- Secure authentication and authorization for your organization using Single Sign-On (SSO) and System for Cross-domain Identity Management (SCIM)
- Enforce sign-on for Docker Desktop to ensure security best practices
Prerequisites
Before you start to onboard your organization, ensure that you:
Have a Docker Team or Business subscription. See Docker Pricing for details.
Note
When purchasing a self-serve subscription, the on-screen instructions guide you through creating an organization. If you have purchased a subscription through Docker Sales and you have not yet created an organization, see Create an organization.
Familiarize yourself with Docker concepts and terminology in the administration overview and FAQs.
Step 1: Identify your Docker users
Identifying your users will ensure that you allocate your subscription seats efficiently and that all your Docker users receive the benefits of your subscription.
- Identify the Docker users in your organization.
- If your organization uses device management software, like MDM or Jamf, you may use the device management software to help identify Docker users. See your device management software's documentation for details. You can identify Docker users by checking if Docker Desktop is installed at the following location on each user's machine:
- Mac:
/Applications/Docker.app
- Windows:
C:\Program Files\Docker\Docker
- Linux:
/opt/docker-desktop
- Mac:
- If your organization doesn't use device management software or your users haven't installed Docker Desktop yet, you may survey your users.
- If your organization uses device management software, like MDM or Jamf, you may use the device management software to help identify Docker users. See your device management software's documentation for details. You can identify Docker users by checking if Docker Desktop is installed at the following location on each user's machine:
- Instruct all your organization's Docker users to update their existing Docker account's email address to an address that's in your organization's domain, or to create a new account using an email address in your organization's domain.
- To update an account's email address, instruct your users to sign in to Docker Hub, and update the email address to their email address in your organization's domain.
- To create a new account, instruct your users to go sign up using their email address in your organization's domain.
- Ask your Docker sales representative or contact sales to get a list of Docker accounts that use an email address in your organization's domain.
Step 2: Invite owners
When you create an organization, you are the only owner. It is optional to add additional owners. Owners can help you onboard and manage your organization.
To add an owner, invite a user and assign them the owner role. For more details, see Invite members.
Step 3: Invite members
When you add users to your organization, you gain visibility into their activity and you can enforce security settings. In addition, members of your organization receive increased pull limits and other organization wide benefits.
To add a member, invite a user and assign them the member role. For more details, see Invite members.
Step 4: Manage members with SSO and SCIM
Configuring SSO and SCIM is optional and only available to Docker Business subscribers. To upgrade a Docker Team subscription to a Docker Business subscription, see Upgrade your subscription.
You can manage your members in your identity provider and automatically provision them to your Docker organization with SSO and SCIM. See the following for more details.
Configure SSO to authenticate and add members when they sign in to Docker through your identity provider.
Optional. Enforce SSO to ensure that when users sign in to Docker, they must use SSO.
Note
Enforcing single sign-on (SSO) and Step 5: Enforce sign-in for Docker Desktop are different features. For more details, see Enforcing sign-in versus enforcing single sign-on (SSO).
Configure SCIM to automatically provision, add, and de-provision members to Docker through your identity provider.
Step 5: Enforce sign-in for Docker Desktop
By default, members of your organization can use Docker Desktop without signing in. When users don’t sign in as a member of your organization, they don’t receive the benefits of your organization’s subscription and they can circumvent Docker’s security features.
There are multiple ways you can enforce sign-in, depending on your company's setup and preferences:
What's next
- Manage Docker products to configure access and view usage.
- Configure Hardened Docker Desktop to improve your organization’s security posture for containerized development.
- Audit your domains to ensure that all Docker users in your domain are part of your organization.
Your Docker subscription provides many more additional features. To learn more, see Docker subscriptions and features.",,,
732bb666f6980e3aa373c569ae4a04feac5cc6d6e7071112232b50f5151162ab,"Push to multiple registries with GitHub Actions
The following workflow will connect you to Docker Hub and GitHub Container Registry, and push the image to both registries:
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Login to GitHub Container Registry
uses: docker/login-action@v3
with:
registry: ghcr.io
username: ${{ github.repository_owner }}
password: ${{ secrets.GITHUB_TOKEN }}
- name: Set up QEMU
uses: docker/setup-qemu-action@v3
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Build and push
uses: docker/build-push-action@v6
with:
platforms: linux/amd64,linux/arm64
push: true
tags: |
user/app:latest
user/app:1.0.0
ghcr.io/user/app:latest
ghcr.io/user/app:1.0.0",,,
ce7a667df9a4dc27865f09a21cab9d37f2febd9d0dafdca396e2726bbbab0978,"Network and VM FAQs
How can I limit the type of internet access allowed by the container when it runs, to prevent it from being able to exfiltrate data or download malicious code?
There is no built-in mechanism for that but it can be addressed by process-level firewall on the host. Hook into the com.docker.vpnkit
user-space process and apply rules where it can connect to (DNS URL white list; packet/payload filter) and which ports/protocols it is allowed to use.
Can I prevent users binding ports on 0.0.0.0?
There is no direct way to enforce that through Docker Desktop but it would inherit any firewall rules enforced on the host.
What options exist to lock containerized network settings to a system? If not supported, are there any consequences to manipulating the settings?
The Docker network settings are entirely local within the VM and have no effect on the system.
Can I apply rules on container network traffic via a local firewall or VPN client?
For network connectivity, Docker Desktop uses a user-space process (com.docker.vpnkit
), which inherits constraints like firewall rules, VPN, HTTP proxy properties etc, from the user that launched it.
Does running Docker Desktop for Windows with Hyper-V backend allow users to create arbitrary VMs?
No. The DockerDesktopVM
name is hard coded in the service code, so you cannot use Docker Desktop to create or manipulate any other VM.
Can I prevent our users creating other VMs when using Docker Desktop on Mac?
On Mac it is an unprivileged operation to start a VM, so that is not enforced by Docker Desktop.
How does Docker Desktop achieve network level isolation when Hyper-V and/or WSL2 is used?
The VM processes are the same for both WSL 2 (running inside the docker-desktop
distribution) and Hyper-V (running inside the DockerDesktopVM
). Host/VM communication uses AF_VSOCK
hypervisor sockets (shared memory). It does not use Hyper-V network switches or network interfaces. All host networking is performed using normal TCP/IP sockets from the com.docker.vpnkit.exe
and com.docker.backend.exe
processes. For more information see
How Docker Desktop networking works under the hood.",,,
98761b7042f8dc28bbc7bf0cb7b949858426f036f1d793b4c2657d2383bbe339,"Local and tar exporters
Table of contents
The local
and tar
exporters output the root filesystem of the build result
into a local directory. They're useful for producing artifacts that aren't
container images.
local
exports files and directories.tar
exports the same, but bundles the export into a tarball.
Synopsis
Build a container image using the local
exporter:
$ docker buildx build --output type=local[,parameters] .
$ docker buildx build --output type=tar[,parameters] .
The following table describes the available parameters:
| Parameter | Type | Default | Description |
|---|---|---|---|
dest | String | Path to copy files to |
Further reading
For more information on the local
or tar
exporters, see the
BuildKit README.",,,
55aac4c63fd9e62094d74fe973cf0e347d83ffc073b1aebc167c3a2e61a3db97,"SBOM attestations
Software Bill of Materials (SBOM) attestations describe what software artifacts an image contains, and artifacts used to create the image. Metadata included in an SBOM for describing software artifacts may include:
- Name of the artifact
- Version
- License type
- Authors
- Unique package identifier
There are benefits to indexing contents of an image during the build, as opposed to scanning a final image. When scanning happens as part of the build, you're able to detect software you use to build the image, that may not show up in the final image.
The SBOMs generated by BuildKit follow the SPDX standard. SBOMs attach to the final image as a JSON-encoded SPDX document, using the format defined by the in-toto SPDX predicate.
Create SBOM attestations
To create an SBOM attestation, pass the --attest type=sbom
option to the
docker buildx build
command:
$ docker buildx build --tag <namespace>/<image>:<version> \
--attest type=sbom --push .
Alternatively, you can use the shorthand --sbom=true
option instead of --attest type=sbom
.
For an example on how to add SBOM attestations with GitHub Actions, see Add attestations with GitHub Actions.
Verify SBOM attestations
Always validate the generated SBOM for your image before you push your image to a registry.
To validate, you can build the image using the local
exporter.
Building with the local
exporter saves the build result to your local filesystem instead of creating an image.
Attestations are written to a JSON file in the root directory of your export.
$ docker buildx build \
--sbom=true \
--output type=local,dest=out .
The SBOM file appears in the root directory of the output, named sbom.spdx.json
:
$ ls -1 ./out | grep sbom
sbom.spdx.json
Arguments
By default, BuildKit only scans the final stage of an image. The resulting SBOM doesn't include build-time dependencies installed in earlier stages, or that exist in the build context. This may cause you to overlook vulnerabilities in those dependencies, which could impact the security of your final build artifacts.
For instance, you might use
multi-stage builds,
with a FROM scratch
stanza for your final stage to achieve a smaller image size.
FROM alpine AS build
# build the software ...
FROM scratch
COPY --from=build /path/to/bin /bin
ENTRYPOINT [ ""/bin"" ]
Scanning the resulting image built using this Dockerfile example would not
reveal build-time dependencies used in the build
stage.
To include build-time dependencies from your Dockerfile, you can set the build
arguments BUILDKIT_SBOM_SCAN_CONTEXT
and BUILDKIT_SBOM_SCAN_STAGE
. This
expands the scanning scope to include the build context and additional stages.
You can set the arguments as global arguments (after declaring the Dockerfile
syntax directive, before the first FROM
command) or individually in each
stage. If set globally, the value propagates to each stage in the Dockerfile.
The BUILDKIT_SBOM_SCAN_CONTEXT
and BUILDKIT_SBOM_SCAN_STAGE
build arguments
are special values. You can't perform variable substitution using these
arguments, and you can't set them using environment variables from within the
Dockerfile. The only way to set these values is using explicit ARG
command in
the Dockerfile.
Scan build context
To scan the build context, set the BUILDKIT_SBOM_SCAN_CONTEXT
to true
.
# syntax=docker/dockerfile:1
ARG BUILDKIT_SBOM_SCAN_CONTEXT=true
FROM alpine AS build
# ...
You can use the --build-arg
CLI option to override the value specified in the
Dockerfile.
$ docker buildx build --tag <image>:<version> \
--attest type=sbom \
--build-arg BUILDKIT_SBOM_SCAN_CONTEXT=false .
Note that passing the option as a CLI argument only, without having declared it
using ARG
in the Dockerfile, will have no effect. You must specify the ARG
in the Dockerfile, whereby you can override the context scanning behavior using
--build-arg
.
Scan stages
To scan more than just the final stage, set the BUILDKIT_SBOM_SCAN_STAGE
argument to true, either globally or in the specific stages that you want to
scan. The following table demonstrates the different possible settings for this
argument.
| Value | Description |
|---|---|
BUILDKIT_SBOM_SCAN_STAGE=true | Enables scanning for the current stage |
BUILDKIT_SBOM_SCAN_STAGE=false | Disables scanning for the current stage |
BUILDKIT_SBOM_SCAN_STAGE=base,bin | Enables scanning for the stages named base and bin |
Only stages that are built will be scanned. Stages that aren't dependencies of the target stage won't be built, or scanned.
The following Dockerfile example uses multi-stage builds to build a static website with Hugo.
# syntax=docker/dockerfile:1
FROM alpine as hugo
ARG BUILDKIT_SBOM_SCAN_STAGE=true
WORKDIR /src
COPY <<config.yml ./
title: My Hugo website
config.yml
RUN apk add --upgrade hugo && hugo
FROM scratch
COPY --from=hugo /src/public /
Setting ARG BUILDKIT_SBOM_SCAN_STAGE=true
in the hugo
stage ensures that the final SBOM
includes the information that Alpine Linux and Hugo were used to create the website.
Building this image with the local
exporter creates two JSON files:
$ docker buildx build \
--sbom=true \
--output type=local,dest=out .
$ ls -1 out | grep sbom
sbom-hugo.spdx.json
sbom.spdx.json
Inspecting SBOMs
To explore created SBOMs exported through the image
exporter, you can use
imagetools inspect
.
Using the --format
option, you can specify a template for the output. All
SBOM-related data is available under the .SBOM
attribute. For example, to get
the raw contents of an SBOM in SPDX format:
$ docker buildx imagetools inspect <namespace>/<image>:<version> \
--format ""{{ json .SBOM.SPDX }}""
{
""SPDXID"": ""SPDXRef-DOCUMENT"",
...
}
Tip
If the image is multi-platform, you can check the SBOM for a platform-specific index using
--format '{{ json (index .SBOM ""linux/amd64"").SPDX }}'
.
You can also construct more complex expressions using the full functionality of Go templates. For example, you can list all the installed packages and their version identifiers:
$ docker buildx imagetools inspect <namespace>/<image>:<version> \
--format ""{{ range .SBOM.SPDX.packages }}{{ .name }}@{{ .versionInfo }}{{ println }}{{ end }}""
adduser@3.118ubuntu2
apt@2.0.9
base-files@11ubuntu5.6
base-passwd@3.5.47
...
SBOM generator
BuildKit generates the SBOM using a scanner plugin. By default, it uses is the BuildKit Syft scanner plugin. This plugin is built on top of Anchore's Syft, an open source tool for generating an SBOM.
You can select a different plugin to use with the generator
option, specifying
an image that implements the
BuildKit SBOM scanner protocol.
$ docker buildx build --attest type=sbom,generator=<image> .
Tip
The Docker Scout SBOM generator is available. See Docker Scout SBOMs.
SBOM attestation example
The following JSON example shows what an SBOM attestation might look like.
{
""_type"": ""https://in-toto.io/Statement/v0.1"",
""predicateType"": ""https://spdx.dev/Document"",
""subject"": [
{
""name"": ""pkg:docker/<registry>/<image>@<tag/digest>?platform=<platform>"",
""digest"": {
""sha256"": ""e8275b2b76280af67e26f068e5d585eb905f8dfd2f1918b3229db98133cb4862""
}
}
],
""predicate"": {
""SPDXID"": ""SPDXRef-DOCUMENT"",
""creationInfo"": {
""created"": ""2022-12-16T15:27:25.517047753Z"",
""creators"": [""Organization: Anchore, Inc"", ""Tool: syft-v0.60.3""],
""licenseListVersion"": ""3.18""
},
""dataLicense"": ""CC0-1.0"",
""documentNamespace"": ""https://anchore.com/syft/dir/run/src/core/sbom-cba61a72-fa95-4b60-b63f-03169eac25ca"",
""name"": ""/run/src/core/sbom"",
""packages"": [
{
""SPDXID"": ""SPDXRef-b074348b8f56ea64"",
""downloadLocation"": ""NOASSERTION"",
""externalRefs"": [
{
""referenceCategory"": ""SECURITY"",
""referenceLocator"": ""cpe:2.3:a:org:repo:\\(devel\\):*:*:*:*:*:*:*"",
""referenceType"": ""cpe23Type""
},
{
""referenceCategory"": ""PACKAGE_MANAGER"",
""referenceLocator"": ""pkg:golang/github.com/org/repo@(devel)"",
""referenceType"": ""purl""
}
],
""filesAnalyzed"": false,
""licenseConcluded"": ""NONE"",
""licenseDeclared"": ""NONE"",
""name"": ""github.com/org/repo"",
""sourceInfo"": ""acquired package info from go module information: bin/server"",
""versionInfo"": ""(devel)""
},
{
""SPDXID"": ""SPDXRef-1b96f57f8fed62d8"",
""checksums"": [
{
""algorithm"": ""SHA256"",
""checksumValue"": ""0c13f1f3c1636491f716c2027c301f21f9dbed7c4a2185461ba94e3e58443408""
}
],
""downloadLocation"": ""NOASSERTION"",
""externalRefs"": [
{
""referenceCategory"": ""SECURITY"",
""referenceLocator"": ""cpe:2.3:a:go-chi:chi\\/v5:v5.0.0:*:*:*:*:*:*:*"",
""referenceType"": ""cpe23Type""
},
{
""referenceCategory"": ""SECURITY"",
""referenceLocator"": ""cpe:2.3:a:go_chi:chi\\/v5:v5.0.0:*:*:*:*:*:*:*"",
""referenceType"": ""cpe23Type""
},
{
""referenceCategory"": ""SECURITY"",
""referenceLocator"": ""cpe:2.3:a:go:chi\\/v5:v5.0.0:*:*:*:*:*:*:*"",
""referenceType"": ""cpe23Type""
},
{
""referenceCategory"": ""PACKAGE_MANAGER"",
""referenceLocator"": ""pkg:golang/github.com/go-chi/chi/v5@v5.0.0"",
""referenceType"": ""purl""
}
],
""filesAnalyzed"": false,
""licenseConcluded"": ""NONE"",
""licenseDeclared"": ""NONE"",
""name"": ""github.com/go-chi/chi/v5"",
""sourceInfo"": ""acquired package info from go module information: bin/server"",
""versionInfo"": ""v5.0.0""
}
],
""relationships"": [
{
""relatedSpdxElement"": ""SPDXRef-1b96f57f8fed62d8"",
""relationshipType"": ""CONTAINS"",
""spdxElementId"": ""SPDXRef-043f7360d3c66bc31ba45388f16423aa58693289126421b71d884145f8837fe1""
},
{
""relatedSpdxElement"": ""SPDXRef-b074348b8f56ea64"",
""relationshipType"": ""CONTAINS"",
""spdxElementId"": ""SPDXRef-043f7360d3c66bc31ba45388f16423aa58693289126421b71d884145f8837fe1""
}
],
""spdxVersion"": ""SPDX-2.2""
}
}",,,
4fda52de612bcfa5aa238a454bf9bfff9f8e22cd89542bbecde458eeba3f6f7e,"BTRFS storage driver
Btrfs is a copy-on-write filesystem that supports many advanced storage technologies, making it a good fit for Docker. Btrfs is included in the mainline Linux kernel.
Docker's btrfs
storage driver leverages many Btrfs features for image and
container management. Among these features are block-level operations, thin
provisioning, copy-on-write snapshots, and ease of administration. You can
combine multiple physical block devices into a single Btrfs filesystem.
This page refers to Docker's Btrfs storage driver as btrfs
and the overall
Btrfs Filesystem as Btrfs.
Note
The
btrfs
storage driver is only supported with Docker Engine CE on SLES, Ubuntu, and Debian systems.
Prerequisites
btrfs
is supported if you meet the following prerequisites:
btrfs
is only recommended with Docker CE on Ubuntu or Debian systems.Changing the storage driver makes any containers you have already created inaccessible on the local system. Use
docker save
to save containers, and push existing images to Docker Hub or a private repository, so that you do not need to re-create them later.btrfs
requires a dedicated block storage device such as a physical disk. This block device must be formatted for Btrfs and mounted into/var/lib/docker/
. The configuration instructions below walk you through this procedure. By default, the SLES/
filesystem is formatted with Btrfs, so for SLES, you do not need to use a separate block device, but you can choose to do so for performance reasons.btrfs
support must exist in your kernel. To check this, run the following command:$ grep btrfs /proc/filesystems btrfs
To manage Btrfs filesystems at the level of the operating system, you need the
btrfs
command. If you don't have this command, install thebtrfsprogs
package (SLES) orbtrfs-tools
package (Ubuntu).
Configure Docker to use the btrfs storage driver
This procedure is essentially identical on SLES and Ubuntu.
Stop Docker.
Copy the contents of
/var/lib/docker/
to a backup location, then empty the contents of/var/lib/docker/
:$ sudo cp -au /var/lib/docker /var/lib/docker.bk $ sudo rm -rf /var/lib/docker/*
Format your dedicated block device or devices as a Btrfs filesystem. This example assumes that you are using two block devices called
/dev/xvdf
and/dev/xvdg
. Double-check the block device names because this is a destructive operation.$ sudo mkfs.btrfs -f /dev/xvdf /dev/xvdg
There are many more options for Btrfs, including striping and RAID. See the Btrfs documentation.
Mount the new Btrfs filesystem on the
/var/lib/docker/
mount point. You can specify any of the block devices used to create the Btrfs filesystem.$ sudo mount -t btrfs /dev/xvdf /var/lib/docker
Note
Make the change permanent across reboots by adding an entry to
/etc/fstab
.Copy the contents of
/var/lib/docker.bk
to/var/lib/docker/
.$ sudo cp -au /var/lib/docker.bk/* /var/lib/docker/
Configure Docker to use the
btrfs
storage driver. This is required even though/var/lib/docker/
is now using a Btrfs filesystem. Edit or create the file/etc/docker/daemon.json
. If it is a new file, add the following contents. If it is an existing file, add the key and value only, being careful to end the line with a comma if it isn't the final line before an ending curly bracket (}
).{ ""storage-driver"": ""btrfs"" }
See all storage options for each storage driver in the daemon reference documentation
Start Docker. When it's running, verify that
btrfs
is being used as the storage driver.$ docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 17.03.1-ce Storage Driver: btrfs Build Version: Btrfs v4.4 Library Version: 101 <...>
When you are ready, remove the
/var/lib/docker.bk
directory.
Manage a Btrfs volume
One of the benefits of Btrfs is the ease of managing Btrfs filesystems without the need to unmount the filesystem or restart Docker.
When space gets low, Btrfs automatically expands the volume in chunks of roughly 1 GB.
To add a block device to a Btrfs volume, use the btrfs device add
and
btrfs filesystem balance
commands.
$ sudo btrfs device add /dev/svdh /var/lib/docker
$ sudo btrfs filesystem balance /var/lib/docker
Note
While you can do these operations with Docker running, performance suffers. It might be best to plan an outage window to balance the Btrfs filesystem.
How the btrfs
storage driver works
The btrfs
storage driver works differently from other
storage drivers in that your entire /var/lib/docker/
directory is stored on a
Btrfs volume.
Image and container layers on-disk
Information about image layers and writable container layers is stored in
/var/lib/docker/btrfs/subvolumes/
. This subdirectory contains one directory
per image or container layer, with the unified filesystem built from a layer
plus all its parent layers. Subvolumes are natively copy-on-write and have space
allocated to them on-demand from an underlying storage pool. They can also be
nested and snapshotted. The diagram below shows 4 subvolumes. 'Subvolume 2' and
'Subvolume 3' are nested, whereas 'Subvolume 4' shows its own internal directory
tree.
Only the base layer of an image is stored as a true subvolume. All the other layers are stored as snapshots, which only contain the differences introduced in that layer. You can create snapshots of snapshots as shown in the diagram below.
On disk, snapshots look and feel just like subvolumes, but in reality they are much smaller and more space-efficient. Copy-on-write is used to maximize storage efficiency and minimize layer size, and writes in the container's writable layer are managed at the block level. The following image shows a subvolume and its snapshot sharing data.
For maximum efficiency, when a container needs more space, it is allocated in chunks of roughly 1 GB in size.
Docker's btrfs
storage driver stores every image layer and container in its
own Btrfs subvolume or snapshot. The base layer of an image is stored as a
subvolume whereas child image layers and containers are stored as snapshots.
This is shown in the diagram below.
The high level process for creating images and containers on Docker hosts
running the btrfs
driver is as follows:
The image's base layer is stored in a Btrfs subvolume under
/var/lib/docker/btrfs/subvolumes
.Subsequent image layers are stored as a Btrfs snapshot of the parent layer's subvolume or snapshot, but with the changes introduced by this layer. These differences are stored at the block level.
The container's writable layer is a Btrfs snapshot of the final image layer, with the differences introduced by the running container. These differences are stored at the block level.
How container reads and writes work with btrfs
Reading files
A container is a space-efficient snapshot of an image. Metadata in the snapshot points to the actual data blocks in the storage pool. This is the same as with a subvolume. Therefore, reads performed against a snapshot are essentially the same as reads performed against a subvolume.
Writing files
As a general caution, writing and updating a large number of small files with Btrfs can result in slow performance.
Consider three scenarios where a container opens a file for write access with Btrfs.
Writing new files
Writing a new file to a container invokes an allocate-on-demand operation to allocate new data block to the container's snapshot. The file is then written to this new space. The allocate-on-demand operation is native to all writes with Btrfs and is the same as writing new data to a subvolume. As a result, writing new files to a container's snapshot operates at native Btrfs speeds.
Modifying existing files
Updating an existing file in a container is a copy-on-write operation (redirect-on-write is the Btrfs terminology). The original data is read from the layer where the file currently exists, and only the modified blocks are written into the container's writable layer. Next, the Btrfs driver updates the filesystem metadata in the snapshot to point to this new data. This behavior incurs minor overhead.
Deleting files or directories
If a container deletes a file or directory that exists in a lower layer, Btrfs masks the existence of the file or directory in the lower layer. If a container creates a file and then deletes it, this operation is performed in the Btrfs filesystem itself and the space is reclaimed.
Btrfs and Docker performance
There are several factors that influence Docker's performance under the btrfs
storage driver.
Note
Many of these factors are mitigated by using Docker volumes for write-heavy workloads, rather than relying on storing data in the container's writable layer. However, in the case of Btrfs, Docker volumes still suffer from these draw-backs unless
/var/lib/docker/volumes/
isn't backed by Btrfs.
Page caching
Btrfs doesn't support page cache sharing. This means that each process
accessing the same file copies the file into the Docker host's memory. As a
result, the btrfs
driver may not be the best choice for high-density use cases
such as PaaS.
Small writes
Containers performing lots of small writes (this usage pattern matches what
happens when you start and stop many containers in a short period of time, as
well) can lead to poor use of Btrfs chunks. This can prematurely fill the Btrfs
filesystem and lead to out-of-space conditions on your Docker host. Use btrfs filesys show
to closely monitor the amount of free space on your Btrfs device.
Sequential writes
Btrfs uses a journaling technique when writing to disk. This can impact the performance of sequential writes, reducing performance by up to 50%.
Fragmentation
Fragmentation is a natural byproduct of copy-on-write filesystems like Btrfs. Many small random writes can compound this issue. Fragmentation can manifest as CPU spikes when using SSDs or head thrashing when using spinning disks. Either of these issues can harm performance.
If your Linux kernel version is 3.9 or higher, you can enable the autodefrag
feature when mounting a Btrfs volume. Test this feature on your own workloads
before deploying it into production, as some tests have shown a negative impact
on performance.
SSD performance
Btrfs includes native optimizations for SSD media. To enable these features,
mount the Btrfs filesystem with the -o ssd
mount option. These optimizations
include enhanced SSD write performance by avoiding optimization such as seek
optimizations that don't apply to solid-state media.
Balance Btrfs filesystems often
Use operating system utilities such as a cron
job to balance the Btrfs
filesystem regularly, during non-peak hours. This reclaims unallocated blocks
and helps to prevent the filesystem from filling up unnecessarily. You can't
rebalance a totally full Btrfs filesystem unless you add additional physical
block devices to the filesystem.
See the Btrfs Wiki.
Use fast storage
Solid-state drives (SSDs) provide faster reads and writes than spinning disks.
Use volumes for write-heavy workloads
Volumes provide the best and most predictable performance for write-heavy workloads. This is because they bypass the storage driver and don't incur any of the potential overheads introduced by thin provisioning and copy-on-write. Volumes have other benefits, such as allowing you to share data among containers and persisting even when no running container is using them.",,,
fefaa4e23f8a55aabfb23969337d7d5c17d8e38fdd0a7c12d890cabf2231fd99,"Install Docker Desktop on Windows
Docker Desktop terms
Commercial use of Docker Desktop in larger enterprises (more than 250 employees OR more than $10 million USD in annual revenue) requires a paid subscription.
This page contains the download URL, information about system requirements, and instructions on how to install Docker Desktop for Windows.
For checksums, see Release notes
System requirements
Tip
Should I use Hyper-V or WSL?
Docker Desktop's functionality remains consistent on both WSL and Hyper-V, without a preference for either architecture. Hyper-V and WSL have their own advantages and disadvantages, depending on your specific set up and your planned use case.
- WSL version 1.1.3.0 or later.
- Windows 11 64-bit: Home or Pro version 22H2 or higher, or Enterprise or Education version 22H2 or higher.
- Windows 10 64-bit: Minimum required is Home or Pro 22H2 (build 19045) or higher, or Enterprise or Education 22H2 (build 19045) or higher.
- Turn on the WSL 2 feature on Windows. For detailed instructions, refer to the Microsoft documentation.
- The following hardware prerequisites are required to successfully run
WSL 2 on Windows 10 or Windows 11:
- 64-bit processor with Second Level Address Translation (SLAT)
- 4GB system RAM
- Enable hardware virtualization in BIOS. For more information, see Virtualization.
For more information on setting up WSL 2 with Docker Desktop, see WSL.
Note
Docker only supports Docker Desktop on Windows for those versions of Windows that are still within Microsoft’s servicing timeline. Docker Desktop is not supported on server versions of Windows, such as Windows Server 2019 or Windows Server 2022. For more information on how to run containers on Windows Server, see Microsoft's official documentation.
Important
To run Windows containers, you need Windows 10 or Windows 11 Professional or Enterprise edition. Windows Home or Education editions only allow you to run Linux containers.
Windows 11 64-bit: Enterprise, Pro, or Education version 22H2 or higher.
Windows 10 64-bit: Enterprise, Pro, or Education version 22H2 (build 19045) or higher.
Turn on Hyper-V and Containers Windows features.
The following hardware prerequisites are required to successfully run Client Hyper-V on Windows 10:
- 64 bit processor with Second Level Address Translation (SLAT)
- 4GB system RAM
- Turn on BIOS-level hardware virtualization support in the BIOS settings. For more information, see Virtualization.
Note
Docker only supports Docker Desktop on Windows for those versions of Windows that are still within Microsoft’s servicing timeline. Docker Desktop is not supported on server versions of Windows, such as Windows Server 2019 or Windows Server 2022. For more information on how to run containers on Windows Server, see Microsoft's official documentation.
Important
To run Windows containers, you need Windows 10 or Windows 11 Professional or Enterprise edition. Windows Home or Education editions only allow you to run Linux containers.
- WSL version 1.1.3.0 or later.
- Windows 11 64-bit: Home or Pro version 22H2 or higher, or Enterprise or Education version 22H2 or higher.
- Windows 10 64-bit: Minimum required is Home or Pro 22H2 (build 19045) or higher, or Enterprise or Education 22H2 (build 19045) or higher.
- Turn on the WSL 2 feature on Windows. For detailed instructions, refer to the Microsoft documentation.
- The following hardware prerequisites are required to successfully run
WSL 2 on Windows 10 or Windows 11:
- 64-bit processor with Second Level Address Translation (SLAT)
- 4GB system RAM
- Enable hardware virtualization in BIOS. For more information, see Virtualization.
Important
Windows containers are not supported.
Containers and images created with Docker Desktop are shared between all user accounts on machines where it is installed. This is because all Windows accounts use the same VM to build and run containers. Note that it is not possible to share containers and images between user accounts when using the Docker Desktop WSL 2 backend.
Running Docker Desktop inside a VMware ESXi or Azure VM is supported for Docker Business customers. It requires enabling nested virtualization on the hypervisor first. For more information, see Running Docker Desktop in a VM or VDI environment.
From the Docker Desktop menu, you can toggle which daemon (Linux or Windows) the Docker CLI talks to. Select Switch to Windows containers to use Windows containers, or select Switch to Linux containers to use Linux containers (the default).
For more information on Windows containers, refer to the following documentation:
Microsoft documentation on Windows containers.
Build and Run Your First Windows Server Container (Blog Post) gives a quick tour of how to build and run native Docker Windows containers on Windows 10 and Windows Server 2016 evaluation releases.
Getting Started with Windows Containers (Lab) shows you how to use the MusicStore application with Windows containers. The MusicStore is a standard .NET application and, forked here to use containers, is a good example of a multi-container application.
To understand how to connect to Windows containers from the local host, see I want to connect to a container from Windows
Note
When you switch to Windows containers, Settings only shows those tabs that are active and apply to your Windows containers.
If you set proxies or daemon configuration in Windows containers mode, these apply only on Windows containers. If you switch back to Linux containers, proxies and daemon configurations return to what you had set for Linux containers. Your Windows container settings are retained and become available again when you switch back.
Install Docker Desktop on Windows
Tip
See the FAQs on how to install and run Docker Desktop without needing administrator privileges.
Install interactively
Download the installer using the download button at the top of the page, or from the release notes.
Double-click
Docker Desktop Installer.exe
to run the installer. By default, Docker Desktop is installed atC:\Program Files\Docker\Docker
.When prompted, ensure the Use WSL 2 instead of Hyper-V option on the Configuration page is selected or not depending on your choice of backend.
If your system only supports one of the two options, you won't be able to select which backend to use.
Follow the instructions on the installation wizard to authorize the installer and proceed with the install.
When the installation is successful, select Close to complete the installation process.
If your administrator account is different to your user account, you must add the user to the docker-users group:
- Run Computer Management as an administrator.
- Navigate to Local Users and Groups > Groups > docker-users.
- Right-click to add the user to the group.
- Sign out and sign back in for the changes to take effect.
Install from the command line
After downloading Docker Desktop Installer.exe
, run the following command in a terminal to install Docker Desktop:
$ ""Docker Desktop Installer.exe"" install
If you’re using PowerShell you should run it as:
Start-Process 'Docker Desktop Installer.exe' -Wait install
If using the Windows Command Prompt:
start /w """" ""Docker Desktop Installer.exe"" install
By default, Docker Desktop is installed at C:\Program Files\Docker\Docker
.
The install
command accepts the following flags:
--quiet
: Suppresses information output when running the installer--accept-license
: Accepts the Docker Subscription Service Agreement now, rather than requiring it to be accepted when the application is first run--no-windows-containers
: Disables the Windows containers integration. This can improve security. For more information, see Windows containers.--allowed-org=<org name>
: Requires the user to sign in and be part of the specified Docker Hub organization when running the application--backend=<backend name>
: Selects the default backend to use for Docker Desktop,hyper-v
,windows
orwsl-2
(default)--installation-dir=<path>
: Changes the default installation location (C:\Program Files\Docker\Docker
)--admin-settings
: Automatically creates anadmin-settings.json
file which is used by admins to control certain Docker Desktop settings on client machines within their organization. For more information, see Settings Management.- It must be used together with the
--allowed-org=<org name>
flag. - For example:
--allowed-org=<org name> --admin-settings=""{'configurationFileVersion': 2, 'enhancedContainerIsolation': {'value': true, 'locked': false}}""
- It must be used together with the
--proxy-http-mode=<mode>
: Sets the HTTP Proxy mode,system
(default) ormanual
--override-proxy-http=<URL>
: Sets the URL of the HTTP proxy that must be used for outgoing HTTP requests, requires--proxy-http-mode
to bemanual
--override-proxy-https=<URL>
: Sets the URL of the HTTP proxy that must be used for outgoing HTTPS requests, requires--proxy-http-mode
to bemanual
--override-proxy-exclude=<hosts/domains>
: Bypasses proxy settings for the hosts and domains. Uses a comma-separated list.--proxy-enable-kerberosntlm
: Enables Kerberos and NTLM proxy authentication. If you are enabling this, ensure your proxy server is properly configured for Kerberos/NTLM authentication. Available with Docker Desktop 4.32 and later.--hyper-v-default-data-root=<path>
: Specifies the default location for the Hyper-V VM disk.--windows-containers-default-data-root=<path>
: Specifies the default location for the Windows containers.--wsl-default-data-root=<path>
: Specifies the default location for the WSL distribution disk.--always-run-service
: After installation completes, startscom.docker.service
and sets the service startup type to Automatic. This circumvents the need for administrator privileges, which are otherwise necessary to startcom.docker.service
.com.docker.service
is required by Windows containers and Hyper-V backend.
Note
If you're using PowerShell, you need to use the
ArgumentList
parameter before any flags. For example:Start-Process 'Docker Desktop Installer.exe' -Wait -ArgumentList 'install', '--accept-license'
If your admin account is different to your user account, you must add the user to the docker-users group:
$ net localgroup docker-users <user> /add
Start Docker Desktop
Docker Desktop does not start automatically after installation. To start Docker Desktop:
Search for Docker, and select Docker Desktop in the search results.
The Docker menu ( ) displays the Docker Subscription Service Agreement.
Here’s a summary of the key points:
- Docker Desktop is free for small businesses (fewer than 250 employees AND less than $10 million in annual revenue), personal use, education, and non-commercial open source projects.
- Otherwise, it requires a paid subscription for professional use.
- Paid subscriptions are also required for government entities.
- The Docker Pro, Team, and Business subscriptions include commercial use of Docker Desktop.
Select Accept to continue. Docker Desktop starts after you accept the terms.
Note that Docker Desktop won't run if you do not agree to the terms. You can choose to accept the terms at a later date by opening Docker Desktop.
For more information, see Docker Desktop Subscription Service Agreement. It is recommended that you read the FAQs.
Tip
As an IT administrator, you can use endpoint management (MDM) software to identify the number of Docker Desktop instances and their versions within your environment. This can provide accurate license reporting, help ensure your machines use the latest version of Docker Desktop, and enable you to enforce sign-in.
Where to go next
- Explore Docker's subscriptions to see what Docker can offer you.
- Get started with Docker.
- Explore Docker Desktop and all its features.
- Troubleshooting describes common problems, workarounds, and how to get support.
- FAQs provide answers to frequently asked questions.
- Release notes lists component updates, new features, and improvements associated with Docker Desktop releases.
- Back up and restore data provides instructions on backing up and restoring data related to Docker.",,,
847069caf292fcd085cb7a21e2143c287421dac2d686398dcc0f389b8cee4a4b,"Dockerfile release notes
This page contains information about the new features, improvements, known issues, and bug fixes in Dockerfile reference.
For usage, see the Dockerfile frontend syntax page.
1.14.0
2025-02-19The full release note for this release is available on GitHub.
# syntax=docker/dockerfile:1.14.0
COPY --chmod
now allows non-octal values. This feature was previously in the labs channel and is now available in the main release. moby/buildkit#5734- Fix handling of OSVersion platform property if one is set by the base image moby/buildkit#5714
- Fix errors where a named context metadata could be resolved even if it was not reachable by the current build configuration, leading to build errors moby/buildkit#5688
1.14.0-labs
2025-02-19The full release note for this release is available on GitHub.
# syntax=docker.io/docker/dockerfile-upstream:1.14.0-labs
- New
RUN --device=name,[required]
flag lets builds request CDI devices are available to the build step. Requires BuildKit v0.20.0+ moby/buildkit#4056, moby/buildkit#5738
1.13.0
2025-01-20The full release note for this release is available on GitHub.
# syntax=docker/dockerfile:1.13.0
- New
TARGETOSVERSION
,BUILDOSVERSION
builtin build-args are available for Windows builds, andTARGETPLATFORM
value now also containsOSVersion
value. moby/buildkit#5614 - Allow syntax forwarding for external frontends for files starting with a Byte Order Mark (BOM). moby/buildkit#5645
- Default
PATH
in Windows Containers has been updated withpowershell.exe
directory. moby/buildkit#5446 - Fix Dockerfile directive parsing to not allow invalid syntax. moby/buildkit#5646
- Fix case where
ONBUILD
command may have run twice on inherited stage. moby/buildkit#5593 - Fix possible missing named context replacement for child stages in Dockerfile. moby/buildkit#5596
1.13.0-labs
2025-01-20The full release note for this release is available on GitHub.
# syntax=docker.io/docker/dockerfile-upstream:1.13.0-labs
- Fix support for non-octal values for
COPY --chmod
. moby/buildkit#5626
1.12.0
2024-11-27The full release note for this release is available on GitHub.
# syntax=docker/dockerfile:1.12.0
- Fix incorrect description in History line of image configuration with multiple
ARG
instructions. moby/buildkit#5508
1.11.1
2024-11-08The full release note for this release is available on GitHub.
# syntax=docker/dockerfile:1.11.1
- Fix regression when using the
ONBUILD
instruction in stages inherited within the same Dockerfile. moby/buildkit#5490
1.11.0
2024-10-30The full release note for this release is available on GitHub.
# syntax=docker/dockerfile:1.11.0
- The
ONBUILD
instruction now supports commands that refer to other stages or images withfrom
, such asCOPY --from
orRUN mount=from=...
. moby/buildkit#5357 - The
SecretsUsedInArgOrEnv
build check has been improved to reduce false positives. moby/buildkit#5208 - A new
InvalidDefinitionDescription
build check recommends formatting comments for build arguments and stages descriptions. This is an experimental check. moby/buildkit#5208, moby/buildkit#5414 - Multiple fixes for the
ONBUILD
instruction's progress and error handling. moby/buildkit#5397 - Improved error reporting for missing flag errors. moby/buildkit#5369
- Enhanced progress output for secret values mounted as environment variables. moby/buildkit#5336
- Added built-in build argument
TARGETSTAGE
to expose the name of the (final) target stage for the current build. moby/buildkit#5431
1.11.0-labs
COPY --chmod
now supports non-octal values. moby/buildkit#5380
1.10.0
2024-09-10The full release note for this release is available on GitHub.
# syntax=docker/dockerfile:1.10.0
- Build secrets can now be mounted as environment variables using the
env=VARIABLE
option. moby/buildkit#5215 - The
# check
directive now allows new experimental attribute for enabling experimental validation rules likeCopyIgnoredFile
. moby/buildkit#5213 - Improve validation of unsupported modifiers for variable substitution. moby/buildkit#5146
ADD
andCOPY
instructions now support variable interpolation for build arguments for the--chmod
option values. moby/buildkit#5151- Improve validation of the
--chmod
option forCOPY
andADD
instructions. moby/buildkit#5148 - Fix missing completions for size and destination attributes on mounts. moby/buildkit#5245
- OCI annotations are now set to the Dockerfile frontend release image. moby/buildkit#5197
1.9.0
2024-07-11The full release note for this release is available on GitHub.
# syntax=docker/dockerfile:1.9.0
- Add new validation rules:
SecretsUsedInArgOrEnv
InvalidDefaultArgInFrom
RedundantTargetPlatform
CopyIgnoredFile
(experimental)FromPlatformFlagConstDisallowed
- Many performance improvements for working with big Dockerfiles. moby/buildkit#5067, moby/buildkit#5029
- Fix possible panic when building Dockerfile without defined stages. moby/buildkit#5150
- Fix incorrect JSON parsing that could cause some incorrect JSON values to pass without producing an error. moby/buildkit#5107
- Fix a regression where
COPY --link
with a destination path of.
could fail. moby/buildkit#5080 - Fix validation of
ADD --checksum
when used with a Git URL. moby/buildkit#5085
1.8.1
2024-06-18The full release note for this release is available on GitHub.
# syntax=docker/dockerfile:1.8.1
Bug fixes and enhancements
- Fix handling of empty strings on variable expansion. moby/buildkit#5052
- Improve formatting of build warnings. moby/buildkit#5037, moby/buildkit#5045, moby/buildkit#5046
- Fix possible invalid output for
UndeclaredVariable
warning for multi-stage builds. moby/buildkit#5048
1.8.0
2024-06-11The full release note for this release is available on GitHub.
# syntax=docker/dockerfile:1.8.0
- Many new validation rules have been added to verify that your Dockerfile is using best practices. These rules are validated during build and new
check
frontend method can be used to only trigger validation without completing the whole build. - New directive
#check
and build argumentBUILDKIT_DOCKERFILE_CHECK
lets you control the behavior or build checks. moby/buildkit#4962 - Using a single-platform base image that does not match your expected platform is now validated. moby/buildkit#4924
- Errors from the expansion of
ARG
definitions in global scope are now handled properly. moby/buildkit#4856 - Expansion of the default value of
ARG
now only happens if it is not overwritten by the user. Previously, expansion was completed and value was later ignored, which could result in an unexpected expansion error. moby/buildkit#4856 - Performance of parsing huge Dockerfiles with many stages has been improved. moby/buildkit#4970
- Fix some Windows path handling consistency errors. moby/buildkit#4825
1.7.0
2024-03-06Stable
# syntax=docker/dockerfile:1.7
- Variable expansion now allows string substitutions and trimming. moby/buildkit#4427, moby/buildkit#4287
- Named contexts with local sources now correctly transfer only the files used in the Dockerfile instead of the full source directory. moby/buildkit#4161
- Dockerfile now better validates the order of stages and returns nice errors with stack traces if stages are in incorrect order. moby/buildkit#4568, moby/buildkit#4567
- History commit messages now contain flags used with
COPY
andADD
. moby/buildkit#4597 - Progress messages for
ADD
commands from Git and HTTP sources have been improved. moby/buildkit#4408
Labs
# syntax=docker/dockerfile:1.7-labs
- New
--parents
flag has been added toCOPY
for copying files while keeping the parent directory structure. moby/buildkit#4598, moby/buildkit#3001, moby/buildkit#4720, moby/buildkit#4728, docs - New
--exclude
flag can be used inCOPY
andADD
commands to apply filter to copied files. moby/buildkit#4561, docs
1.6.0
2023-06-13New
- Add
--start-interval
flag to theHEALTHCHECK
instruction.
The following features have graduated from the labs channel to stable:
- The
ADD
instruction can now import files directly from Git URLs - The
ADD
instruction now supports--checksum
flag to validate the contents of the remote URL contents
Bug fixes and enhancements
- Variable substitution now supports additional POSIX compatible variants without
:
. moby/buildkit#3611 - Exported Windows images now contain OSVersion and OSFeatures values from base image. moby/buildkit#3619
- Changed the permissions for Heredocs to 0644. moby/buildkit#3992
1.5.2
2023-02-14Bug fixes and enhancements
- Fix building from Git reference that is missing branch name but contains a subdir
- 386 platform image is now included in the release
1.5.1
2023-01-18Bug fixes and enhancements
- Fix possible panic when warning conditions appear in multi-platform builds
1.5.0 (labs)
2023-01-10{{% experimental %}} The ""labs"" channel provides early access to Dockerfile features that are not yet available in the stable channel. {{% /experimental %}}
New
ADD
command now supports--checksum
flag to validate the contents of the remote URL contents
1.5.0
2023-01-10New
ADD
command can now import files directly from Git URLs
Bug fixes and enhancements
- Named contexts now support
oci-layout://
protocol for including images from local OCI layout structure - Dockerfile now supports secondary requests for listing all build targets or printing outline of accepted parameters for a specific build target
- Dockerfile
#syntax
directive that redirects to an external frontend image now allows the directive to be also set with//
comments or JSON. The file may also contain a shebang header - Named context can now be initialized with an empty scratch image
- Named contexts can now be initialized with an SSH Git URL
- Fix handling of
ONBUILD
when importing Schema1 images
1.4.3
2022-08-23Bug fixes and enhancements
- Fix creation timestamp not getting reset when building image from
docker-image://
named context - Fix passing
--platform
flag ofFROM
command when loadingdocker-image://
named context
1.4.2
2022-05-06Bug fixes and enhancements
- Fix loading certain environment variables from an image passed with built context
1.4.1
2022-04-08Bug fixes and enhancements
- Fix named context resolution for cross-compilation cases from input when input is built for a different platform
1.4.0
2022-03-09New
COPY --link
andADD --link
allow copying files with increased cache efficiency and rebase images without requiring them to be rebuilt.--link
copies files to a separate layer and then uses new LLB MergeOp implementation to chain independent layers together- Heredocs support have been promoted from labs channel to stable. This feature allows writing multiline inline scripts and files
- Additional named build contexts can be passed to build to add or overwrite a stage or an image inside the build. A source for the context can be a local source, image, Git, or HTTP URL
BUILDKIT_SANDBOX_HOSTNAME
build-arg can be used to set the default hostname for theRUN
steps
Bug fixes and enhancements
- When using a cross-compilation stage, the target platform for a step is now seen on progress output
- Fix some cases where Heredocs incorrectly removed quotes from content
1.3.1
2021-10-04Bug fixes and enhancements
- Fix parsing ""required"" mount key without a value
1.3.0 (labs)
2021-07-16{{% experimental %}} The ""labs"" channel provides early access to Dockerfile features that are not yet available in the stable channel. {{% /experimental %}}
New
RUN
andCOPY
commands now support Here-document syntax allowing writing multiline inline scripts and files
1.3.0
2021-07-16New
RUN
command allows--network
flag for requesting a specific type of network conditions.--network=host
requires allowingnetwork.host
entitlement. This feature was previously only available on labs channel
Bug fixes and enhancements
ADD
command with a remote URL input now correctly handles the--chmod
flag- Values for
RUN --mount
flag now support variable expansion, except for thefrom
field - Allow
BUILDKIT_MULTI_PLATFORM
build arg to force always creating multi-platform image, even if only contains single platform
1.2.1 (labs)
2020-12-12{{% experimental %}} The ""labs"" channel provides early access to Dockerfile features that are not yet available in the stable channel. {{% /experimental %}}
Bug fixes and enhancements
RUN
command allows--network
flag for requesting a specific type of network conditions.--network=host
requires allowingnetwork.host
entitlement
1.2.1
2020-12-12Bug fixes and enhancements
- Revert ""Ensure ENTRYPOINT command has at least one argument""
- Optimize processing
COPY
calls on multi-platform cross-compilation builds
1.2.0 (labs)
2020-12-03{{% experimental %}} The ""labs"" channel provides early access to Dockerfile features that are not yet available in the stable channel. {{% /experimental %}}
Bug fixes and enhancements
- Experimental channel has been renamed to labs
1.2.0
2020-12-03New
RUN --mount
syntax for creating secret, ssh, bind, and cache mounts have been moved to mainline channelARG
command now supports defining multiple build args on the same line similarly toENV
Bug fixes and enhancements
- Metadata load errors are now handled as fatal to avoid incorrect build results
- Allow lowercase Dockerfile name
--chown
flag inADD
now allows parameter expansionENTRYPOINT
requires at least one argument to avoid creating broken images
1.1.7
2020-04-18Bug fixes and enhancements
- Forward
FrontendInputs
to the gateway
1.1.2 (experimental)
2019-07-31{{% experimental %}} The ""labs"" channel provides early access to Dockerfile features that are not yet available in the stable channel. {{% /experimental %}}
Bug fixes and enhancements
- Allow setting security mode for a process with
RUN --security=sandbox|insecure
- Allow setting uid/gid for cache mounts
- Avoid requesting internally linked paths to be pulled to build context
- Ensure missing cache IDs default to target paths
- Allow setting namespace for cache mounts with
BUILDKIT_CACHE_MOUNT_NS
build arg
1.1.2
2019-07-31Bug fixes and enhancements
- Fix workdir creation with correct user and don't reset custom ownership
- Fix handling empty build args also used as
ENV
- Detect circular dependencies
1.1.0
2019-04-27New
ADD/COPY
commands now support implementation based onllb.FileOp
and do not require helper image if builtin file operations support is available--chown
flag forCOPY
command now supports variable expansion
Bug fixes and enhancements
- To find the files ignored from the build context Dockerfile frontend will
first look for a file
<path/to/Dockerfile>.dockerignore
and if it is not found.dockerignore
file will be looked up from the root of the build context. This allows projects with multiple Dockerfiles to use different.dockerignore
definitions",,,
802eee9fb51d061d127326672b3633d4a39d28664c944668922aa3ccc56fff3d,"Image Management
Table of contents
Availability:
Beta
Images and image indexes are the foundation of container images within a repository. The following diagram shows the relationship between images and image indexes.
This structure enables multi-architecture support through a single reference. It is important to note that images are not always referenced by an image index. The following objects are shown in the diagram.
- Image index: An image that points to multiple architecture-specific images (like AMD and ARM), letting a single reference work across different platforms.
- Image: Individual container images that contain the actual configuration and layers for a specific architecture and operating system.
Manage repository images and image indexes
Sign in to Docker Hub.
Select Repositories.
In the list, select a repository.
Select Image Management.
Search, filter, or sort the items.
- Search: In the search box above the list, specify your search.
- Filter: In the Filter by drop-down, select Tagged, Image index, or Image.
- Sort: Select the column title for Size, Last pushed, or Last pulled.
Note
Images that haven't been pulled in over 6 months are marked as Stale in the Status column.
Optional. Delete one or more items.
- Select the checkboxes next to the items in the list. Selecting any top-level index also removes any underlying images that aren't referenced elsewhere.
- Select Preview and delete.
- In the window that appears, verify the items that will be deleted and the amount of storage you will reclaim.
- Select Delete forever.",,,
012aaa9da8a3be2ef3298adc8bf0a6d57882a5428346827641258420e53056df,"Testcontainers
Testcontainers is a set of open source libraries that provides easy and lightweight APIs for bootstrapping local development and test dependencies with real services wrapped in Docker containers. Using Testcontainers, you can write tests that depend on the same services you use in production without mocks or in-memory services.
Quickstart
Supported languages
Testcontainers provide support for the most popular languages, and Docker sponsors the development of the following Testcontainers implementations:
The rest are community-driven and maintained by independent contributors.
Prerequisites
Testcontainers requires a Docker-API compatible container runtime. During development, Testcontainers is actively tested against recent versions of Docker on Linux, as well as against Docker Desktop on Mac and Windows. These Docker environments are automatically detected and used by Testcontainers without any additional configuration being necessary.
It is possible to configure Testcontainers to work for other Docker setups, such as a remote Docker host or Docker alternatives. However, these are not actively tested in the main development workflow, so not all Testcontainers features might be available and additional manual configuration might be necessary.
If you have further questions about configuration details for your setup or whether it supports running Testcontainers-based tests, contact the Testcontainers team and other users from the Testcontainers community on Slack.",,,
dda1510c4d8c0e8c469cd76d38eda6bbe19cfffd86d5a3397e8f0538cf8f9fe8,"Use Docker Engine plugins
This document describes the Docker Engine plugins generally available in Docker Engine. To view information on plugins managed by Docker, refer to Docker Engine plugin system.
You can extend the capabilities of the Docker Engine by loading third-party plugins. This page explains the types of plugins and provides links to several volume and network plugins for Docker.
Types of plugins
Plugins extend Docker's functionality. They come in specific types. For example, a volume plugin might enable Docker volumes to persist across multiple Docker hosts and a network plugin might provide network plumbing.
Currently Docker supports authorization, volume and network driver plugins. In the future it will support additional plugin types.
Installing a plugin
Follow the instructions in the plugin's documentation.
Finding a plugin
The sections below provide an overview of available third-party plugins.
Network plugins
| Plugin | Description |
|---|---|
| Contiv Networking | An open source network plugin to provide infrastructure and security policies for a multi-tenant micro services deployment, while providing an integration to physical network for non-container workload. Contiv Networking implements the remote driver and IPAM APIs available in Docker 1.9 onwards. |
| Kuryr Network Plugin | A network plugin is developed as part of the OpenStack Kuryr project and implements the Docker networking (libnetwork) remote driver API by utilizing Neutron, the OpenStack networking service. It includes an IPAM driver as well. |
| Kathará Network Plugin | Docker Network Plugin used by Kathará, an open source container-based network emulation system for showing interactive demos/lessons, testing production networks in a sandbox environment, or developing new network protocols. |
Volume plugins
| Plugin | Description |
|---|---|
| Azure File Storage plugin | Lets you mount Microsoft Azure File Storage shares to Docker containers as volumes using the SMB 3.0 protocol. Learn more. |
| BeeGFS Volume Plugin | An open source volume plugin to create persistent volumes in a BeeGFS parallel file system. |
| Blockbridge plugin | A volume plugin that provides access to an extensible set of container-based persistent storage options. It supports single and multi-host Docker environments with features that include tenant isolation, automated provisioning, encryption, secure deletion, snapshots and QoS. |
| Contiv Volume Plugin | An open source volume plugin that provides multi-tenant, persistent, distributed storage with intent based consumption. It has support for Ceph and NFS. |
| Convoy plugin | A volume plugin for a variety of storage back-ends including device mapper and NFS. It's a simple standalone executable written in Go and provides the framework to support vendor-specific extensions such as snapshots, backups and restore. |
| DigitalOcean Block Storage plugin | Integrates DigitalOcean's block storage solution into the Docker ecosystem by automatically attaching a given block storage volume to a DigitalOcean droplet and making the contents of the volume available to Docker containers running on that droplet. |
| DRBD plugin | A volume plugin that provides highly available storage replicated by DRBD. Data written to the docker volume is replicated in a cluster of DRBD nodes. |
| Flocker plugin | A volume plugin that provides multi-host portable volumes for Docker, enabling you to run databases and other stateful containers and move them around across a cluster of machines. |
| Fuxi Volume Plugin | A volume plugin that is developed as part of the OpenStack Kuryr project and implements the Docker volume plugin API by utilizing Cinder, the OpenStack block storage service. |
| gce-docker plugin | A volume plugin able to attach, format and mount Google Compute persistent-disks. |
| GlusterFS plugin | A volume plugin that provides multi-host volumes management for Docker using GlusterFS. |
| Horcrux Volume Plugin | A volume plugin that allows on-demand, version controlled access to your data. Horcrux is an open-source plugin, written in Go, and supports SCP, Minio and Amazon S3. |
| HPE 3Par Volume Plugin | A volume plugin that supports HPE 3Par and StoreVirtual iSCSI storage arrays. |
| Infinit volume plugin | A volume plugin that makes it easy to mount and manage Infinit volumes using Docker. |
| IPFS Volume Plugin | An open source volume plugin that allows using an ipfs filesystem as a volume. |
| Keywhiz plugin | A plugin that provides credentials and secret management using Keywhiz as a central repository. |
| Linode Volume Plugin | A plugin that adds the ability to manage Linode Block Storage as Docker Volumes from within a Linode. |
| Local Persist Plugin | A volume plugin that extends the default local driver's functionality by allowing you specify a mountpoint anywhere on the host, which enables the files to always persist, even if the volume is removed via docker volume rm . |
| NetApp Plugin (nDVP) | A volume plugin that provides direct integration with the Docker ecosystem for the NetApp storage portfolio. The nDVP package supports the provisioning and management of storage resources from the storage platform to Docker hosts, with a robust framework for adding additional platforms in the future. |
| Netshare plugin | A volume plugin that provides volume management for NFS 3/4, AWS EFS and CIFS file systems. |
| Nimble Storage Volume Plugin | A volume plug-in that integrates with Nimble Storage Unified Flash Fabric arrays. The plug-in abstracts array volume capabilities to the Docker administrator to allow self-provisioning of secure multi-tenant volumes and clones. |
| OpenStorage Plugin | A cluster-aware volume plugin that provides volume management for file and block storage solutions. It implements a vendor neutral specification for implementing extensions such as CoS, encryption, and snapshots. It has example drivers based on FUSE, NFS, NBD and EBS to name a few. |
| Portworx Volume Plugin | A volume plugin that turns any server into a scale-out converged compute/storage node, providing container granular storage and highly available volumes across any node, using a shared-nothing storage backend that works with any docker scheduler. |
| Quobyte Volume Plugin | A volume plugin that connects Docker to Quobyte's data center file system, a general-purpose scalable and fault-tolerant storage platform. |
| REX-Ray plugin | A volume plugin which is written in Go and provides advanced storage functionality for many platforms including VirtualBox, EC2, Google Compute Engine, OpenStack, and EMC. |
| Virtuozzo Storage and Ploop plugin | A volume plugin with support for Virtuozzo Storage distributed cloud file system as well as ploop devices. |
| VMware vSphere Storage Plugin | Docker Volume Driver for vSphere enables customers to address persistent storage requirements for Docker containers in vSphere environments. |
Authorization plugins
| Plugin | Description |
|---|---|
| Casbin AuthZ Plugin | An authorization plugin based on Casbin, which supports access control models like ACL, RBAC, ABAC. The access control model can be customized. The policy can be persisted into file or DB. |
| HBM plugin | An authorization plugin that prevents from executing commands with certains parameters. |
| Twistlock AuthZ Broker | A basic extendable authorization plugin that runs directly on the host or inside a container. This plugin allows you to define user policies that it evaluates during authorization. Basic authorization is provided if Docker daemon is started with the --tlsverify flag (username is extracted from the certificate common name). |
Troubleshooting a plugin
If you are having problems with Docker after loading a plugin, ask the authors of the plugin for help. The Docker team may not be able to assist you.
Writing a plugin
If you are interested in writing a plugin for Docker, or seeing how they work under the hood, see the Docker plugins reference.",,,
2068dff6c0ce95a20bc432a5704cad6ee3b7a1214ca800603bb46b1617592f97,"Extension architecture
Extensions are applications that run inside the Docker Desktop. They're packaged as Docker images, distributed through Docker Hub, and installed by users either through the Marketplace within the Docker Desktop Dashboard or the Docker Extensions CLI.
Extensions can be composed of three (optional) components:
- A frontend (or User Interface): A web application displayed in a tab of the dashboard in Docker Desktop
- A backend: One or many containerized services running in the Docker Desktop VM
- Executables: Shell scripts or binaries that Docker Desktop copies on the host when installing the extension
An extension doesn't necessarily need to have all these components, but at least one of them depending on the extension features.
To configure and run those components, Docker Desktop uses a metadata.json
file. See the
metadata section for more details.
The frontend
The frontend is basically a web application made from HTML, Javascript, and CSS. It can be built with a simple HTML file, some vanilla Javascript or any frontend framework, such as React or Vue.js.
When Docker Desktop installs the extension, it extracts the UI folder from the extension image, as defined by the
ui
section in the metadata.json
. See the
ui metadata section for more details.
Every time users click on the Extensions tab, Docker Desktop initializes the extension's UI as if it was the first time. When they navigate away from the tab, both the UI itself and all the sub-processes started by it (if any) are terminated.
The frontend can invoke docker
commands, communicate with the extension backend, or invoke extension executables
deployed on the host, through the
Extensions SDK.
Tip
The
docker extension init
generates a React based extension. But you can still use it as a starting point for your own extension and use any other frontend framework, like Vue, Angular, Svelte, etc. or event stay with vanilla Javascript.
Learn more about building a frontend for your extension.
The backend
Alongside a frontend application, extensions can also contain one or many backend services. In most cases, the Extension does not need a backend, and features can be implemented just by invoking docker commands through the SDK. However, there are some cases when an extension requires a backend service, for example:
- To run long-running processes that must outlive the frontend
- To store data in a local database and serve them back with a REST API
- To store the extension state, like when a button starts a long-running process, so that if you navigate away from the extension and come back, the frontend can pick up where it left off
- To access specific resources in the Docker Desktop VM, for example by mounting folders in the compose file
Tip
The
docker extension init
generates a Go backend. But you can still use it as a starting point for your own extension and use any other language like Node.js, Python, Java, .Net, or any other language and framework.
Usually, the backend is made of one container that runs within the Docker Desktop VM. Internally, Docker Desktop creates
a Docker Compose project, creates the container from the image
option of the vm
section of the metadata.json
, and
attaches it to the Compose project. See the
ui metadata section for more details.
In some cases, a compose.yaml
file can be used instead of an image
. This is useful when the backend container
needs more specific options, such as mounting volumes or requesting
capabilities
that can't be expressed just with a Docker image. The compose.yaml
file can also be used to add multiple containers
needed by the extension, like a database or a message broker.
Note that, if the Compose file defines many services, the SDK can only contact the first of them.
Note
In some cases, it is useful to also interact with the Docker engine from the backend. See How to use the Docker socket from the backend.
To communicate with the backend, the Extension SDK provides
functions to make GET
,
POST
, PUT
, HEAD
, and DELETE
requests from the frontend. Under the hood, the communication is done through a socket
or named pipe, depending on the operating system. If the backend was listening to a port, it would be difficult to
prevent collision with other applications running on the host or in a container already. Also, some users are
running Docker Desktop in constrained environments where they can't open ports on their machines.
Finally, the backend can be built with any technology, as long as it can run in a container and listen on a socket.
Learn more about adding a backend to your extension.
Executables
In addition to the frontend and the backend, extensions can also contain executables. Executables are binaries or shell scripts that are installed on the host when the extension is installed. The frontend can invoke them with the extension SDK.
These executables are useful when the extension needs to interact with a third-party CLI tool, like AWS, kubectl
, etc.
Shipping those executables with the extension ensure that the CLI tool is always available, at the right version, on
the users' machine.
When Docker Desktop installs the extension, it copies the executables on the host as defined by the host
section in
the metadata.json
. See the
ui metadata section for more details.
However, since they're executed on the users' machine, they have to be available to the platform they're running on.
For example, if you want to ship the kubectl
executable, you need to provide a different version for Windows, Mac,
and Linux. Multi arch images will also need to include binaries built for the right arch (AMD / ARM)
See the host metadata section for more details.
Learn how to invoke host binaries.",,,
8ef499bdd960058f42c3bffa47f18d7a43399a98a080fe6646f1fec6890213ec,"Deploy a stack to a swarm
When running Docker Engine in swarm mode, you can use docker stack deploy
to
deploy a complete application stack to the swarm. The deploy
command accepts
a stack description in the form of a
Compose file.
Note
The
docker stack deploy
command uses the legacy Compose file version 3 format, used by Compose V1. The latest format, defined by the Compose specification isn't compatible with thedocker stack deploy
command.For more information about the evolution of Compose, see History of Compose.
To run through this tutorial, you need:
A Docker Engine running in Swarm mode. If you're not familiar with Swarm mode, you might want to read Swarm mode key concepts and How services work.
Note
If you're trying things out on a local development environment, you can put your engine into Swarm mode with
docker swarm init
.If you've already got a multi-node swarm running, keep in mind that all
docker stack
anddocker service
commands must be run from a manager node.A current version of Docker Compose.
Set up a Docker registry
Because a swarm consists of multiple Docker Engines, a registry is required to distribute images to all of them. You can use the Docker Hub or maintain your own. Here's how to create a throwaway registry, which you can discard afterward.
Start the registry as a service on your swarm:
$ docker service create --name registry --publish published=5000,target=5000 registry:2
Check its status with
docker service ls
:$ docker service ls ID NAME REPLICAS IMAGE COMMAND l7791tpuwkco registry 1/1 registry:2@sha256:1152291c7f93a4ea2ddc95e46d142c31e743b6dd70e194af9e6ebe530f782c17
Once it reads
1/1
underREPLICAS
, it's running. If it reads0/1
, it's probably still pulling the image.Check that it's working with
curl
:$ curl http://127.0.0.1:5000/v2/ {}
Create the example application
The app used in this guide is based on the hit counter app in the Get started with Docker Compose guide. It consists of a Python app which maintains a counter in a Redis instance and increments the counter whenever you visit it.
Create a directory for the project:
$ mkdir stackdemo $ cd stackdemo
Create a file called
app.py
in the project directory and paste this in:from flask import Flask from redis import Redis app = Flask(__name__) redis = Redis(host='redis', port=6379) @app.route('/') def hello(): count = redis.incr('hits') return 'Hello World! I have been seen {} times.\n'.format(count) if __name__ == ""__main__"": app.run(host=""0.0.0.0"", port=8000, debug=True)
Create a file called
requirements.txt
and paste these two lines in:flask redis
Create a file called
Dockerfile
and paste this in:# syntax=docker/dockerfile:1 FROM python:3.4-alpine ADD . /code WORKDIR /code RUN pip install -r requirements.txt CMD [""python"", ""app.py""]
Create a file called
compose.yaml
and paste this in:services: web: image: 127.0.0.1:5000/stackdemo build: . ports: - ""8000:8000"" redis: image: redis:alpine
The image for the web app is built using the Dockerfile defined above. It's also tagged with
127.0.0.1:5000
- the address of the registry created earlier. This is important when distributing the app to the swarm.
Test the app with Compose
Start the app with
docker compose up
. This builds the web app image, pulls the Redis image if you don't already have it, and creates two containers.You see a warning about the Engine being in swarm mode. This is because Compose doesn't take advantage of swarm mode, and deploys everything to a single node. You can safely ignore this.
$ docker compose up -d WARNING: The Docker Engine you're using is running in swarm mode. Compose does not use swarm mode to deploy services to multiple nodes in a swarm. All containers are scheduled on the current node. To deploy your application across the swarm, use `docker stack deploy`. Creating network ""stackdemo_default"" with the default driver Building web ...(build output)... Creating stackdemo_redis_1 Creating stackdemo_web_1
Check that the app is running with
docker compose ps
:$ docker compose ps Name Command State Ports ----------------------------------------------------------------------------------- stackdemo_redis_1 docker-entrypoint.sh redis ... Up 6379/tcp stackdemo_web_1 python app.py Up 0.0.0.0:8000->8000/tcp
You can test the app with
curl
:$ curl http://localhost:8000 Hello World! I have been seen 1 times. $ curl http://localhost:8000 Hello World! I have been seen 2 times. $ curl http://localhost:8000 Hello World! I have been seen 3 times.
Bring the app down:
$ docker compose down --volumes Stopping stackdemo_web_1 ... done Stopping stackdemo_redis_1 ... done Removing stackdemo_web_1 ... done Removing stackdemo_redis_1 ... done Removing network stackdemo_default
Push the generated image to the registry
To distribute the web app's image across the swarm, it needs to be pushed to the registry you set up earlier. With Compose, this is very simple:
$ docker compose push
Pushing web (127.0.0.1:5000/stackdemo:latest)...
The push refers to a repository [127.0.0.1:5000/stackdemo]
5b5a49501a76: Pushed
be44185ce609: Pushed
bd7330a79bcf: Pushed
c9fc143a069a: Pushed
011b303988d2: Pushed
latest: digest: sha256:a81840ebf5ac24b42c1c676cbda3b2cb144580ee347c07e1bc80e35e5ca76507 size: 1372
The stack is now ready to be deployed.
Deploy the stack to the swarm
Create the stack with
docker stack deploy
:$ docker stack deploy --compose-file compose.yaml stackdemo Ignoring unsupported options: build Creating network stackdemo_default Creating service stackdemo_web Creating service stackdemo_redis
The last argument is a name for the stack. Each network, volume and service name is prefixed with the stack name.
Check that it's running with
docker stack services stackdemo
:$ docker stack services stackdemo ID NAME MODE REPLICAS IMAGE orvjk2263y1p stackdemo_redis replicated 1/1 redis:3.2-alpine@sha256:f1ed3708f538b537eb9c2a7dd50dc90a706f7debd7e1196c9264edeea521a86d s1nf0xy8t1un stackdemo_web replicated 1/1 127.0.0.1:5000/stackdemo@sha256:adb070e0805d04ba2f92c724298370b7a4eb19860222120d43e0f6351ddbc26f
Once it's running, you should see
1/1
underREPLICAS
for both services. This might take some time if you have a multi-node swarm, as images need to be pulled.As before, you can test the app with
curl
:$ curl http://localhost:8000 Hello World! I have been seen 1 times. $ curl http://localhost:8000 Hello World! I have been seen 2 times. $ curl http://localhost:8000 Hello World! I have been seen 3 times.
With Docker's built-in routing mesh, you can access any node in the swarm on port
8000
and get routed to the app:$ curl http://address-of-other-node:8000 Hello World! I have been seen 4 times.
Bring the stack down with
docker stack rm
:$ docker stack rm stackdemo Removing service stackdemo_web Removing service stackdemo_redis Removing network stackdemo_default
Bring the registry down with
docker service rm
:$ docker service rm registry
If you're just testing things out on a local machine and want to bring your Docker Engine out of Swarm mode, use
docker swarm leave
:$ docker swarm leave --force Node left the swarm.",,,
02fc3f286607df31437ca3054de2f0ec9266d4d11ecb7bdfdf521fa79ff9f44b,"Get started
If you're new to Docker, this section guides you through the essential resources to get started.
Follow the guides to help you get started and learn how Docker can optimize your development workflows.
For more advanced concepts and scenarios in Docker, see Guides.
Foundations of Docker
Install Docker and jump into discovering what Docker is.
Learn the foundational concepts and workflows of Docker.",,,
6cb6684210a855c796ae1f7211372b78917ea0d15f36336eaaeeff3eb212d0d6,"Use a logging driver plugin
Docker logging plugins allow you to extend and customize Docker's logging capabilities beyond those of the built-in logging drivers. A logging service provider can implement their own plugins and make them available on Docker Hub, or a private registry. This topic shows how a user of that logging service can configure Docker to use the plugin.
Install the logging driver plugin
To install a logging driver plugin, use docker plugin install <org/image>
,
using the information provided by the plugin developer.
You can list all installed plugins using docker plugin ls
, and you can inspect
a specific plugin using docker inspect
.
Configure the plugin as the default logging driver
When the plugin is installed, you can configure the Docker daemon to use it as
the default by setting the plugin's name as the value of the log-driver
key in the daemon.json
, as detailed in the
logging overview. If the
logging driver supports additional options, you can set those as the values of
the log-opts
array in the same file.
Configure a container to use the plugin as the logging driver
After the plugin is installed, you can configure a container to use the plugin
as its logging driver by specifying the --log-driver
flag to docker run
, as
detailed in the
logging overview.
If the logging driver supports additional options, you can specify them using
one or more --log-opt
flags with the option name as the key and the option
value as the value.",,,
e8904ea04dbf027ae9ad9c01280f6827f516cb86f9613509fb502e0a6f1df961,"Use CA certificates with Docker
Caution
Best practices should be followed when using Man-in-the-Middle (MITM) CA certificates in production containers. If compromised, attackers could intercept sensitive data, spoof a trusted service, or perform man-in-the-middle attacks. Consult your security team before you proceed.
If your company uses a proxy that inspects HTTPS traffic, you might need to add the required root certificates to your host machine and your Docker containers or images. This is because Docker and its containers, when pulling images or making network requests, need to trust the proxy’s certificates.
On the host, adding the root certificate ensures that any Docker commands (like
docker pull
) work without issues. For containers, you'll need to add the root
certificate to the container's trust store either during the build process or
at runtime. This ensures that applications running inside the containers can
communicate through the proxy without encountering security warnings or
connection failures.
Add CA certificate to the host
The following sections describe how to install CA certificates on your macOS or Windows host. For Linux, refer to the documentation for your distribution.
macOS
- Download the CA certificate for your MITM proxy software.
- Open the Keychain Access app.
- In Keychain Access, select System, then switch to the Certificates tab.
- Drag-and-drop the downloaded certificate into the list of certificates. Enter your password if prompted.
- Find the newly added certificate, double-click it, and expand the Trust section.
- Set Always Trust for the certificate. Enter your password if prompted.
- Start Docker Desktop and verify that
docker pull
works, assuming Docker Desktop is configured to use the MITM proxy.
Windows
Choose whether you want to install the certificate using the Microsoft Management Console (MMC) or your web browser.
- Download CA certificate for the MITM proxy software.
- Open the Microsoft Management Console (
mmc.exe
). - Add the Certificates Snap-In in the MMC.
- Select File → Add/Remove Snap-in, and then select Certificates → Add >.
- Select Computer Account and then Next.
- Select Local computer and then select Finish.
- Import the CA certificate:
- From the MMC, expand Certificates (Local Computer).
- Expand the Trusted Root Certification Authorities section.
- Right-click Certificates and select All Tasks and Import….
- Follow the prompts to import your CA certificate.
- Select Finish and then Close.
- Start Docker Desktop and verify that
docker pull
succeeds (assuming Docker Desktop is already configured to use the MITM proxy server).
Note
Depending on the SDK and/or runtime/framework in use, further steps may be required beyond adding the CA certificate to the operating system's trust store.
- Download the CA certificate for your MITM proxy software.
- Open your web browser, go to Settings and open Manage certificates
- Select the Trusted Root Certification Authorities tab.
- Select Import, then browse for the downloaded CA certificate.
- Select Open, then choose Place all certificates in the following store.
- Ensure Trusted Root Certification Authorities is selected and select Next.
- Select Finish and then Close.
- Start Docker Desktop and verify that
docker pull
succeeds (assuming Docker Desktop is already configured to use the MITM proxy server).
Add CA certificates to Linux images and containers
If you need to run containerized workloads that rely on internal or custom certificates, such as in environments with corporate proxies or secure services, you must ensure that the containers trust these certificates. Without adding the necessary CA certificates, applications inside your containers may encounter failed requests or security warnings when attempting to connect to HTTPS endpoints.
By adding CA certificates to images at build time, you ensure that any containers started from the image will trust the specified certificates. This is particularly important for applications that require seamless access to internal APIs, databases, or other services during production.
In cases where rebuilding the image isn't feasible, you can instead add certificates to containers directly. However, certificates added at runtime won’t persist if the container is destroyed or recreated, so this method is typically used for temporary fixes or testing scenarios.
Add certificates to images
Note
The following commands are for an Ubuntu base image. If your build uses a different Linux distribution, use equivalent commands for package management (
apt-get
,update-ca-certificates
, and so on).
To add ca certificate to a container image when you're building it, add the following instructions to your Dockerfile.
# Install the ca-certificate package
RUN apt-get update && apt-get install -y ca-certificates
# Copy the CA certificate from the context to the build container
COPY your_certificate.crt /usr/local/share/ca-certificates/
# Update the CA certificates in the container
RUN update-ca-certificates
Add certificates to containers
Note
The following commands are for an Ubuntu-based container. If your container uses a different Linux distribution, use equivalent commands for package management (
apt-get
,update-ca-certificates
, and so on).
To add a CA certificate to a running Linux container:
Download the CA certificate for your MITM proxy software.
If the certificate is in a format other than
.crt
, convert it to.crt
format:Example command$ openssl x509 -in cacert.der -inform DER -out myca.crt
Copy the certificate into the running container:
$ docker cp myca.crt <containerid>:/tmp
Attach to the container:
$ docker exec -it <containerid> sh
Ensure the
ca-certificates
package is installed (required for updating certificates):# apt-get update && apt-get install -y ca-certificates
Copy the certificate to the correct location for CA certificates:
# cp /tmp/myca.crt /usr/local/share/ca-certificates/root_cert.crt
Update the CA certificates:
# update-ca-certificates
Example outputUpdating certificates in /etc/ssl/certs... rehash: warning: skipping ca-certificates.crt, it does not contain exactly one certificate or CRL 1 added, 0 removed; done.
Verify that the container can communicate via the MITM proxy:
# curl https://example.com
Example output<!doctype html> <html> <head> <title>Example Domain</title> ...",,,
a285f69383eac1279f269d75efd0b252d78cceaefe95f99828ca3c0360af36a6,"Add image annotations with GitHub Actions
Annotations let you specify arbitrary metadata for OCI image components, such as manifests, indexes, and descriptors.
To add annotations when building images with GitHub Actions, use the
metadata-action to automatically create OCI-compliant annotations. The
metadata action creates an annotations
output that you can reference, both
with
build-push-action and
bake-action.
name: ci
on:
push:
env:
IMAGE_NAME: user/app
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Extract metadata
id: meta
uses: docker/metadata-action@v5
with:
images: ${{ env.IMAGE_NAME }}
- name: Build and push
uses: docker/build-push-action@v6
with:
tags: ${{ steps.meta.outputs.tags }}
annotations: ${{ steps.meta.outputs.annotations }}
push: true
name: ci
on:
push:
env:
IMAGE_NAME: user/app
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Extract metadata
id: meta
uses: docker/metadata-action@v5
with:
images: ${{ env.IMAGE_NAME }}
- name: Build
uses: docker/bake-action@v6
with:
files: |
./docker-bake.hcl
cwd://${{ steps.meta.outputs.bake-file-tags }}
cwd://${{ steps.meta.outputs.bake-file-annotations }}
push: true
Configure annotation level
By default, annotations are placed on image manifests. To configure the
annotation level, set
the DOCKER_METADATA_ANNOTATIONS_LEVELS
environment variable on the
metadata-action
step to a comma-separated list of all the levels that you
want to annotate. For example, setting DOCKER_METADATA_ANNOTATIONS_LEVELS
to
index
results in annotations on the image index instead of the manifests.
The following example creates annotations on both the image index and manifests.
name: ci
on:
push:
env:
IMAGE_NAME: user/app
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKERHUB_USERNAME }}
password: ${{ secrets.DOCKERHUB_TOKEN }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
- name: Extract metadata
id: meta
uses: docker/metadata-action@v5
with:
images: ${{ env.IMAGE_NAME }}
env:
DOCKER_METADATA_ANNOTATIONS_LEVELS: manifest,index
- name: Build and push
uses: docker/build-push-action@v6
with:
tags: ${{ steps.meta.outputs.tags }}
annotations: ${{ steps.meta.outputs.annotations }}
push: true
Note
The build must produce the components that you want to annotate. For example, to annotate an image index, the build must produce an index. If the build produces only a manifest and you specify
index
orindex-descriptor
, the build fails.",,,
807a2b31867f54ec575be572f722a1f0644708c3ac8c20967f4883b8e52db271,"Troubleshoot Docker Desktop
This page contains information on how to diagnose and troubleshoot Docker Desktop, and how to check the logs.
Warning
If you're experiencing malware detection issues on Mac, follow the steps documented in docker/for-mac#7527.
Troubleshoot menu
To navigate to Troubleshoot either:
- Select the Docker menu Docker menu and then Troubleshoot.
- Select the Troubleshoot icon near the top-right corner of Docker Dashboard.
The Troubleshooting menu contains the following options:
Restart Docker Desktop.
Reset Kubernetes cluster. Select to delete all stacks and Kubernetes resources. For more information, see Kubernetes.
Clean / Purge data. This option resets all Docker data without a reset to factory defaults. Selecting this option results in the loss of existing settings.
Reset to factory defaults: Choose this option to reset all options on Docker Desktop to their initial state, the same as when Docker Desktop was first installed.
If you are a Mac or Linux user, you also have the option to Uninstall Docker Desktop from your system.
Tip
If you need to contact support, select the Question mark icon near the top-right corner of Docker Dashboard, and then select Contact support. Users with a paid Docker subscription can use this option to send a support request.
Diagnose
Tip
If you do not find a solution in troubleshooting, browse the GitHub repositories or create a new issue:
Diagnose from the app
- From Troubleshoot, select Get support. This opens the in-app Support page and starts collecting the diagnostics.
- When the diagnostics collection process is complete, select Upload to get a Diagnostic ID.
- When the diagnostics are uploaded, Docker Desktop prints a diagnostic ID. Copy this ID.
- Use your diagnostics ID to get help:
- If you have a paid Docker subscription, select Contact support. This opens the Docker Desktop support form. Fill in the information required and add the ID you copied in step three to the Diagnostics ID field. Then, select Submit ticket to request Docker Desktop support.
Note
You must be signed in to Docker Desktop to access the support form. For information on what's covered as part of Docker Desktop support, see Support.
- If you don't have a paid Docker subscription, select Report a Bug to open a new Docker Desktop issue on GitHub. Complete the information required and ensure you add the diagnostic ID you copied in step three.
- If you have a paid Docker subscription, select Contact support. This opens the Docker Desktop support form. Fill in the information required and add the ID you copied in step three to the Diagnostics ID field. Then, select Submit ticket to request Docker Desktop support.
Diagnose from an error message
- When an error message appears, select Gather diagnostics.
- When the diagnostics are uploaded, Docker Desktop prints a diagnostic ID. Copy this ID.
- Use your diagnostics ID to get help:
- If you have a paid Docker subscription, select Contact support. This opens the Docker Desktop support form. Fill in the information required and add the ID you copied in step three to the Diagnostics ID field. Then, select Submit ticket to request Docker Desktop support.
Note
You must be signed in to Docker Desktop to access the support form. For information on what's covered as part of Docker Desktop support, see Support.
- If you don't have a paid Docker subscription, you can open a new Docker Desktop issue on GitHub for Mac, Windows, or Linux. Complete the information required and ensure you add the diagnostic ID printed in step two.
- If you have a paid Docker subscription, select Contact support. This opens the Docker Desktop support form. Fill in the information required and add the ID you copied in step three to the Diagnostics ID field. Then, select Submit ticket to request Docker Desktop support.
Diagnose from the terminal
In some cases, it's useful to run the diagnostics yourself, for instance, if Docker Desktop cannot start.
Locate the
com.docker.diagnose
tool:$ C:\Program Files\Docker\Docker\resources\com.docker.diagnose.exe
Create and upload the diagnostics ID. In PowerShell, run:
$ & ""C:\Program Files\Docker\Docker\resources\com.docker.diagnose.exe"" gather -upload
After the diagnostics have finished, the terminal displays your diagnostics ID and the path to the diagnostics file. The diagnostics ID is composed of your user ID and a timestamp. For example BE9AFAAF-F68B-41D0-9D12-84760E6B8740/20190905152051
.
Locate the
com.docker.diagnose
tool:$ /Applications/Docker.app/Contents/MacOS/com.docker.diagnose
Create and upload the diagnostics ID. Run:
$ /Applications/Docker.app/Contents/MacOS/com.docker.diagnose gather -upload
After the diagnostics have finished, the terminal displays your diagnostics ID and the path to the diagnostics file. The diagnostics ID is composed of your user ID and a timestamp. For example BE9AFAAF-F68B-41D0-9D12-84760E6B8740/20190905152051
.
Locate the
com.docker.diagnose
tool:$ /opt/docker-desktop/bin/com.docker.diagnose
Create and upload the diagnostics ID. Run:
$ /opt/docker-desktop/bin/com.docker.diagnose gather -upload
After the diagnostics have finished, the terminal displays your diagnostics ID and the path to the diagnostics file. The diagnostics ID is composed of your user ID and a timestamp. For example BE9AFAAF-F68B-41D0-9D12-84760E6B8740/20190905152051
.
To view the contents of the diagnostic file:
Unzip the file. In PowerShell, copy and paste the path to the diagnostics file into the following command and then run it. It should be similar to the following example:
$ Expand-Archive -LiteralPath ""C:\Users\testUser\AppData\Local\Temp\5DE9978A-3848-429E-8776-950FC869186F\20230607101602.zip"" -DestinationPath ""C:\Users\testuser\AppData\Local\Temp\5DE9978A-3848-429E-8776-950FC869186F\20230607101602""
Open the file in your preferred text editor. Run:
$ code <path-to-file>
Run:
$ open /tmp/<your-diagnostics-ID>.zip
Run:
$ unzip –l /tmp/<your-diagnostics-ID>.zip
Use your diagnostics ID to get help
If you have a paid Docker subscription, select Contact support. This opens the Docker Desktop support form. Fill in the information required and add the ID you copied in step three to the Diagnostics ID field. Then, select Submit ticket to request Docker Desktop support.
If you don't have a paid Docker subscription, create an issue on GitHub:
Self-diagnose tool
Docker Desktop contains a self-diagnose tool which can help you identify some common problems.
Locate the
com.docker.diagnose
tool.$ C:\Program Files\Docker\Docker\resources\com.docker.diagnose.exe
In PowerShell, run the self-diagnose tool:
$ & ""C:\Program Files\Docker\Docker\resources\com.docker.diagnose.exe"" check
Locate the
com.docker.diagnose
tool.$ /Applications/Docker.app/Contents/MacOS/com.docker.diagnose
Run the self-diagnose tool:
$ /Applications/Docker.app/Contents/MacOS/com.docker.diagnose check
Locate the
com.docker.diagnose
tool.Run the self-diagnose tool:
$ /opt/docker-desktop/bin/com.docker.diagnose check
The tool runs a suite of checks and displays PASS or FAIL next to each check. If there are any failures, it highlights the most relevant at the end of the report.
You can then create an issue on GitHub:
Check the logs
In addition to using the diagnose option to submit logs, you can browse the logs yourself.
In PowerShell, run:
$ code $Env:LOCALAPPDATA\Docker\log
This opens up all the logs in your preferred text editor for you to explore.
From terminal
To watch the live flow of Docker Desktop logs in the command line, run the following script from your preferred shell.
$ pred='process matches "".*(ocker|vpnkit).*"" || (process in {""taskgated-helper"", ""launchservicesd"", ""kernel""} && eventMessage contains[c] ""docker"")'
$ /usr/bin/log stream --style syslog --level=debug --color=always --predicate ""$pred""
Alternatively, to collect the last day of logs (1d
) in a file, run:
$ /usr/bin/log show --debug --info --style syslog --last 1d --predicate ""$pred"" >/tmp/logs.txt
From the Console app
Mac provides a built-in log viewer, named Console, which you can use to check Docker logs.
The Console lives in /Applications/Utilities
. You can search for it with
Spotlight Search.
To read the Docker app log messages, type docker
in the Console window search bar and press Enter. Then select ANY
to expand the drop-down list next to your docker
search entry, and select Process
.
You can use the Console Log Query to search logs, filter the results in various ways, and create reports.
You can access Docker Desktop logs by running the following command:
$ journalctl --user --unit=docker-desktop
You can also find the logs for the internal components included in Docker
Desktop at $HOME/.docker/desktop/log/
.
View the Docker daemon logs
Refer to the Read the daemon logs section to learn how to view the Docker Daemon logs.
Further resources
- View specific troubleshoot topics.
- Implement workarounds for common problems
- View information on known issues",,,
5078b9f616cdfae0698c4f5be6c45abe2052efbf4436a3533657264ba786fefd,"How does it work?
Docker implements Enhanced Container Isolation by using the Sysbox container runtime. Sysbox is a fork of the standard OCI runc runtime that was modified to enhance standard container isolation and workloads. For more details see Under the hood.
When
Enhanced Container Isolation is enabled, containers
created by users through docker run
or docker create
are automatically
launched using Sysbox instead of the standard OCI runc runtime. Users need not
do anything else and can continue to use containers as usual. For exceptions,
see
FAQs.
Even containers that use the insecure --privileged
flag can now be run
securely with Enhanced Container Isolation, such that they can no longer be used
to breach the Docker Desktop Virtual Machine (VM) or other containers.
Note
When Enhanced Container Isolation is enabled in Docker Desktop, the Docker CLI
--runtime
flag is ignored. Docker's default runtime continues to berunc
, but all user containers are implicitly launched with Sysbox.
Enhanced Container Isolation is not the same as Docker Engine's userns-remap mode or Rootless Docker.
Under the hood
Sysbox enhances container isolation by using techniques such as:
- Enabling the Linux user-namespace on all containers (root user in the container maps to an unprivileged user in the Linux VM).
- Restricting the container from mounting sensitive VM directories.
- Vetting sensitive system-calls between the container and the Linux kernel.
- Mapping filesystem user/group IDs between the container's user-namespace and the Linux VM.
- Emulating portions of the
/proc
and/sys
filesystems inside the container.
Some of these are made possible by recent advances in the Linux kernel which Docker Desktop now incorporates. Sysbox applies these techniques with minimal functional or performance impact to containers.
These techniques complement Docker's traditional container security mechanisms such as using other Linux namespaces, cgroups, restricted Linux Capabilities, Seccomp, and AppArmor. They add a strong layer of isolation between the container and the Linux kernel inside the Docker Desktop VM.
For more information, see Key features and benefits.
Enhanced Container Isolation versus user namespace remapping
The Docker Engine includes a feature called userns-remap mode that enables the user namespace in all containers. However it suffers from a few limitations and it's not supported within Docker Desktop.
Userns-remap mode is similar to Enhanced Container Isolation in that both improve container isolation by leveraging the Linux user-namespace.
However, Enhanced Container Isolation is much more advanced since it assigns exclusive user-namespace mappings per container automatically and adds several other container isolation features meant to secure Docker Desktop in organizations with stringent security requirements.
Enhanced Container Isolation versus Rootless Docker
Rootless Docker lets Docker Engine, and by extension the containers, to run without root privileges natively on a Linux host. This lets non-root users to install and run Docker natively on Linux.
Rootless Docker is not supported within Docker Desktop. While it's a valuable feature when running Docker natively on Linux, its value within Docker Desktop is reduced since Docker Desktop runs the Docker Engine within a Linux VM. That is, Docker Desktop already lets non-root host users to run Docker and isolates the Docker Engine from the host using a virtual machine.
Unlike Rootless Docker, Enhanced Container Isolation does not run Docker Engine within a Linux user-namespace. Rather it runs the containers generated by that engine within a user-namespace. This has the advantage of bypassing the limitations of Rootless Docker and creates a stronger boundary between the containers and the Docker Engine.
Enhanced Container Isolation is meant to ensure containers launched with Docker Desktop can't easily breach the Docker Desktop Linux VM and therefore modify security settings within it.",,,
356b570c3b8f4002d4dce0c9e9bb01d541da48c26da5b1c76880bfbfd0db2bdd,"Compose FAQs
What is the difference between docker compose
and docker-compose
Version one of the Docker Compose command-line binary was first released in 2014. It was written in Python, and is invoked with docker-compose
. Typically, Compose V1 projects include a top-level version element in the compose.yaml file, with values ranging from 2.0 to 3.8, which refer to the specific file formats.
Version two of the Docker Compose command-line binary was announced in 2020, is written in Go, and is invoked with docker compose
. Compose V2 ignores the version top-level element in the compose.yaml file.
For further information, see History and development of Compose.
What's the difference between up
, run
, and start
?
Typically, you want docker compose up
. Use up
to start or restart all the
services defined in a compose.yaml
. In the default ""attached""
mode, you see all the logs from all the containers. In ""detached"" mode (-d
),
Compose exits after starting the containers, but the containers continue to run
in the background.
The docker compose run
command is for running ""one-off"" or ""adhoc"" tasks. It
requires the service name you want to run and only starts containers for services
that the running service depends on. Use run
to run tests or perform
an administrative task such as removing or adding data to a data volume
container. The run
command acts like docker run -ti
in that it opens an
interactive terminal to the container and returns an exit status matching the
exit status of the process in the container.
The docker compose start
command is useful only to restart containers
that were previously created but were stopped. It never creates new
containers.
Why do my services take 10 seconds to recreate or stop?
The docker compose stop
command attempts to stop a container by sending a SIGTERM
. It then waits
for a
default timeout of 10 seconds. After the timeout,
a SIGKILL
is sent to the container to forcefully kill it. If you
are waiting for this timeout, it means that your containers aren't shutting down
when they receive the SIGTERM
signal.
There has already been a lot written about this problem of processes handling signals in containers.
To fix this problem, try the following:
Make sure you're using the exec form of
CMD
andENTRYPOINT
in your Dockerfile.For example use
[""program"", ""arg1"", ""arg2""]
not""program arg1 arg2""
. Using the string form causes Docker to run your process usingbash
which doesn't handle signals properly. Compose always uses the JSON form, so don't worry if you override the command or entrypoint in your Compose file.If you are able, modify the application that you're running to add an explicit signal handler for
SIGTERM
.Set the
stop_signal
to a signal which the application knows how to handle:services: web: build: . stop_signal: SIGINT
If you can't modify the application, wrap the application in a lightweight init system (like s6) or a signal proxy (like dumb-init or tini). Either of these wrappers takes care of handling
SIGTERM
properly.
How do I run multiple copies of a Compose file on the same host?
Compose uses the project name to create unique identifiers for all of a
project's containers and other resources. To run multiple copies of a project,
set a custom project name using the -p
command line option
or the
COMPOSE_PROJECT_NAME
environment variable.
Can I use JSON instead of YAML for my Compose file?
Yes. YAML is a superset of JSON so any JSON file should be valid YAML. To use a JSON file with Compose, specify the filename to use, for example:
$ docker compose -f docker-compose.json up
Should I include my code with COPY
/ADD
or a volume?
You can add your code to the image using COPY
or ADD
directive in a
Dockerfile
. This is useful if you need to relocate your code along with the
Docker image, for example when you're sending code to another environment
(production, CI, etc).
Use a volume
if you want to make changes to your code and see them
reflected immediately, for example when you're developing code and your server
supports hot code reloading or live-reload.
There may be cases where you want to use both. You can have the image
include the code using a COPY
, and use a volume
in your Compose file to
include the code from the host during development. The volume overrides
the directory contents of the image.",,,
b37277a130da3b9e8e3c2bced4ac8e48edfe2a891994219d1616f6c808b36f68,"Enterprise deployment FAQs
MSI
What happens to user data if they have an older Docker Desktop installation (i.e. .exe
)?
If they have an older .exe
installation, users must
uninstall this version before using the new MSI version. This deletes all Docker containers, images, volumes, and other Docker-related data local to the machine, and removes the files generated by the application. For older versions, users should
backup any containers that they want to keep.
For Docker Desktop versions 4.30 and later of the exe
installer, a -keep-data
flag is available. It removes Docker Desktop but keeps underlying data, such as the VMs that run containers.
& 'C:\Program Files\Docker\Docker\Docker Desktop Installer.exe' uninstall -keep-data
What happens if the user's machine has an older .exe
installation?
The new MSI installer checks if a previous version was installed and doesn't proceed with the installation. Instead, it prompts the user to uninstall their current/old version first, before retrying to install the MSI version.
My installation failed, how do I find out what happened?
MSI installations can sometimes fail unexpectedly and not provide users with much information about what went wrong.
To debug a failed installation, run the install again with verbose logging enabled:
msiexec /i ""DockerDesktop.msi"" /L*V "".\msi.log""
After the installation has failed, open the log file and search for occurrences of value 3
. This is the exit code Windows Installer outputs when it has failed. Just above the line, you will find the reason for the failure.
Why does the installer prompt for a reboot at the end of every fresh installation?
The installer prompts for a reboot because it assumes that changes have been made to the system that require a reboot to finish their configuration.
For example, if you select the WSL engine, the installer adds the required Windows features. After these features are installed, the system reboots to complete configurations so the WSL engine is functional.
You can suppress reboots by using the /norestart
option when launching the installer from the command line:
msiexec /i ""DockerDesktop.msi"" /L*V "".\msi.log"" /norestart
Why isn't the docker-users
group populated when the MSI is installed with Intune or another MDM solution?
It's common for MDM solutions to install applications in the context of the system account. This means that the docker-users
group isn't populated with the user's account, as the system account doesn't have access to the user's context.
As an example, you can reproduce this by running the installer with psexec
in an elevated command prompt:
psexec -i -s msiexec /i ""DockerDesktop.msi""
The installation should complete successfully, but the docker-users
group won't be populated.
As a workaround, you can create a script that runs in the context of the user account.
The script would be responsible for creating the docker-users
group and populating it with the correct user.
Here's an example script that creates the docker-users
group and adds the current user to it (requirements may vary depending on environment):
$Group = ""docker-users""
$CurrentUser = [System.Security.Principal.WindowsIdentity]::GetCurrent().Name
# Create the group
New-LocalGroup -Name $Group
# Add the user to the group
Add-LocalGroupMember -Group $Group -Member $CurrentUser
Note
After adding a new user to the
docker-users
group, the user must sign out and then sign back in for the changes to take effect.",,,
92f65af46fdc8183075694470743b76a0bee23725fbc20bc4cf4d1e6ce8399d0,"Manage company organizations
You can manage the organizations in a company in the Docker Admin Console.
View all organizations
- Sign in to the Admin Console.
- Select your company on the Choose profile page.
- Under Organizations, select Overview.
The organization overview page displays all organizations under your company.
Add seats to an organization
When you have a self-serve subscription that has no pending subscription changes, you can add seats using the following steps. If you have a sales-assisted subscription, you can contact Docker support or sales to add seats.
For more information about adding seats, see Manage seats.
Add organizations to a company
You must be a company owner to add an organization to a company. You must also be an organization owner of the organization you want to add. There is no limit to the number of organizations you can have under a company layer. All organizations must have a Business subscription.
Important
Once you add an organization to a company, you can't remove it from the company.
- Sign in to the Admin Console.
- Select your company on the Choose profile page.
- Select Organizations, then Overview.
- Select Add organization.
- Choose the organization you want to add from the drop-down menu.
- Select Add organization to confirm.
Manage an organization
- Sign in to the Admin Console.
- Select your company on the Choose profile page.
- Select the organization that you want to manage.
For more details about managing an organization, see Organization administration.",,,
9757280545115e1a3677e68b9c06c8e74fd6a3ff7b52c472d386b58f45a81c1f,"Delete a repository
Warning
Deleting a repository deletes all the images it contains and its build settings. This action can't be undone.
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears.
Select the Settings tab.
Select Delete repository.
Enter the name of your repository to confirm.
Select Delete Repository Forever.",,,
9bf99db6e102ec026c2308505a749d45e2b9306460247fc3d45fd6bcf5709612,"Docker Engine 18.04 release notes
Table of contents
18.04.0-ce
2018-04-10
Builder
- Fix typos in builder and client. moby/moby#36424
Client
- Print Stack API and Kubernetes versions in version command. docker/cli#898
- Fix Kubernetes duplication in version command. docker/cli#953
- Use HasAvailableFlags instead of HasFlags for Options in help. docker/cli#959
- Add support for mandatory variables to stack deploy. docker/cli#893
- Fix docker stack services command Port output. docker/cli#943
- Deprecate unencrypted storage. docker/cli#561
- Don't set a default filename for ConfigFile. docker/cli#917
- Fix compose network name. docker/cli#941
Logging
- Silent login: use credentials from cred store to login. docker/cli#139
- Add support for compressibility of log file. moby/moby#29932
- Fix empty LogPath with non-blocking logging mode. moby/moby#36272
Networking
- Prevent explicit removal of ingress network. moby/moby#36538
Runtime
- Devmapper cleanup improvements. moby/moby#36307
- Devmapper.Mounted: remove. moby/moby#36437
- Devmapper/Remove(): use Rmdir, ignore errors. moby/moby#36438
- LCOW - Change platform parser directive to FROM statement flag. moby/moby#35089
- Split daemon service code to windows file. moby/moby#36653
- Windows: Block pulling uplevel images. moby/moby#36327
- Windows: Hyper-V containers are broken after 36586 was merged. moby/moby#36610
- Windows: Move kernel_windows to use golang registry functions. moby/moby#36617
- Windows: Pass back system errors on container exit. moby/moby#35967
- Windows: Remove servicing mode. moby/moby#36267
- Windows: Report Version and UBR. moby/moby#36451
- Bump Runc to 1.0.0-rc5. moby/moby#36449
- Mount failure indicates the path that failed. moby/moby#36407
- Change return for errdefs.getImplementer(). moby/moby#36489
- Client: fix hijackedconn reading from buffer. moby/moby#36663
- Content encoding negotiation added to archive request. moby/moby#36164
- Daemon/stats: more resilient cpu sampling. moby/moby#36519
- Daemon/stats: remove obnoxious types file. moby/moby#36494
- Daemon: use context error rather than inventing new one. moby/moby#36670
- Enable CRIU on non-amd64 architectures (v2). moby/moby#36676
- Fixes intermittent client hang after closing stdin to attached container moby/moby#36517
- Fix daemon panic on container export after restart moby/moby#36586
- Follow-up fixes on multi-stage moby's Dockerfile. moby/moby#36425
- Freeze busybox and latest glibc in Docker image. moby/moby#36375
- If container will run as non root user, drop permitted, effective caps early. moby/moby#36587
- Layer: remove metadata store interface. moby/moby#36504
- Minor optimizations to dockerd. moby/moby#36577
- Whitelist statx syscall. moby/moby#36417
- Add missing error return for plugin creation. moby/moby#36646
- Fix AppArmor not being applied to Exec processes. moby/moby#36466
- Daemon/logger/ring.go: log error not instance. moby/moby#36475
- Fix stats collector spinning CPU if no stats are collected. moby/moby#36609
- Fix(distribution): digest cache should not be moved if it was an auth. moby/moby#36509
- Make sure plugin container is removed on failure. moby/moby#36715
- Bump to containerd 1.0.3. moby/moby#36749
- Don't sort plugin mount slice. moby/moby#36711
Swarm Mode
- Fixes for synchronizing the dispatcher shutdown with in-progress rpcs. moby/moby#36371
- Increase raft ElectionTick to 10xHeartbeatTick. moby/moby#36672
- Make Swarm manager Raft quorum parameters configurable in daemon config. moby/moby#36726
- Ingress network should not be attachable. docker/swarmkit#2523
- [manager/state] Add fernet as an option for raft encryption. docker/swarmkit#2535
- Log GRPC server errors. docker/swarmkit#2541
- Log leadership changes at the manager level. docker/swarmkit#2542
- Remove the containerd executor. docker/swarmkit#2568
- Agent: backoff session when no remotes are available. docker/swarmkit#2570
- [ca/manager] Remove root CA key encryption support entirely. docker/swarmkit#2573
- Fix agent logging race. docker/swarmkit#2578
- Adding logic to restore networks in order. docker/swarmkit#2571",,,
19e4f8d320fa72dc6e728eee1c4dd2a11552dad336d6670b483ccbc302cbf4b5,"Packet filtering and firewalls
On Linux, Docker creates iptables
and ip6tables
rules to implement network
isolation, port publishing and filtering.
Because these rules are required for the correct functioning of Docker bridge networks, you should not modify the rules created by Docker.
But, if you are running Docker on a host exposed to the internet, you will probably want to add iptables policies that prevent unauthorized access to containers or other services running on your host. This page describes how to achieve that, and the caveats you need to be aware of.
Note
Docker creates
iptables
rules for bridge networks.No
iptables
rules are created foripvlan
,macvlan
orhost
networking.
Docker and iptables chains
In the filter
table, Docker sets the default policy to DROP
, and creates the
following custom iptables
chains:
DOCKER-USER
- A placeholder for user-defined rules that will be processed before rules
in the
DOCKER-FORWARD
andDOCKER
chains.
- A placeholder for user-defined rules that will be processed before rules
in the
DOCKER-FORWARD
- The first stage of processing for Docker's networks. Rules that pass packets that are not related to established connections to the other Docker chains, as well as rules to accept packets that are part of established connections.
DOCKER
- Rules that determine whether a packet that is not part of an established connection should be accepted, based on the port forwarding configuration of running containers.
DOCKER-ISOLATION-STAGE-1
andDOCKER-ISOLATION-STAGE-2
- Rules to isolate Docker networks from each other.
DOCKER-INGRESS
- Rules related to Swarm networking.
In the FORWARD
chain, Docker adds rules that unconditionally jump to the
DOCKER-USER
, DOCKER-FORWARD
and DOCKER-INGRESS
chains.
In the nat
table, Docker creates chain DOCKER
and adds rules to implement
masquerading and port-mapping.
Add iptables policies before Docker's rules
Packets that get accepted or rejected by rules in these custom chains will not
be seen by user-defined rules appended to the FORWARD
chain. So, to add
additional rules to filter these packets, use the DOCKER-USER
chain.
Rules appended to the FORWARD
chain will be processed after Docker's rules.
Match the original IP and ports for requests
When packets arrive to the DOCKER-USER
chain, they have already passed through
a Destination Network Address Translation (DNAT) filter. That means that the
iptables
flags you use can only match internal IP addresses and ports of
containers.
If you want to match traffic based on the original IP and port in the network
request, you must use the
conntrack
iptables extension.
For example:
$ sudo iptables -I DOCKER-USER -p tcp -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
$ sudo iptables -I DOCKER-USER -p tcp -m conntrack --ctorigdst 198.51.100.2 --ctorigdstport 80 -j ACCEPT
Important
Using the
conntrack
extension may result in degraded performance.
Port publishing and mapping
By default, for both IPv4 and IPv6, the daemon blocks access to ports that have not been published. Published container ports are mapped to host IP addresses. To do this, it uses iptables to perform Network Address Translation (NAT), Port Address Translation (PAT), and masquerading.
For example, docker run -p 8080:80 [...]
creates a mapping
between port 8080 on any address on the Docker host, and the container's
port 80. Outgoing connections from the container will masquerade, using
the Docker host's IP address.
Restrict external connections to containers
By default, all external source IPs are allowed to connect to ports that have been published to the Docker host's addresses.
To allow only a specific IP or network to access the containers, insert a
negated rule at the top of the DOCKER-USER
filter chain. For example, the
following rule drops packets from all IP addresses except 192.0.2.2
:
$ iptables -I DOCKER-USER -i ext_if ! -s 192.0.2.2 -j DROP
You will need to change ext_if
to correspond with your
host's actual external interface. You could instead allow connections from a
source subnet. The following rule only allows access from the subnet 192.0.2.0/24
:
$ iptables -I DOCKER-USER -i ext_if ! -s 192.0.2.0/24 -j DROP
Finally, you can specify a range of IP addresses to accept using --src-range
(Remember to also add -m iprange
when using --src-range
or --dst-range
):
$ iptables -I DOCKER-USER -m iprange -i ext_if ! --src-range 192.0.2.1-192.0.2.3 -j DROP
You can combine -s
or --src-range
with -d
or --dst-range
to control both
the source and destination. For instance, if the Docker host has addresses
2001:db8:1111::2
and 2001:db8:2222::2
, you can make rules specific to
2001:db8:1111::2
and leave 2001:db8:2222::2
open.
iptables
is complicated. There is a lot more information at
Netfilter.org HOWTO.
Direct routing
Port mapping ensures that published ports are accessible on the host's network addresses, which are likely to be routable for any external clients. No routes are normally set up in the host's network for container addresses that exist within a host.
But, particularly with IPv6 you may prefer to avoid using NAT and instead arrange for external routing to container addresses (""direct routing"").
To access containers on a bridge network from outside the Docker host, you must set up routing to the bridge network via an address on the Docker host. This can be achieved using static routes, Border Gateway Protocol (BGP), or any other means appropriate for your network.
Within a local layer 2 network, remote hosts can set up static routes to a container network using the Docker daemon host's address on the local network. Those hosts can access containers directly. For remote hosts outside the local network, direct access to containers requires router configuration to enable the necessary routing.
Gateway modes
The bridge network driver has the following options:
com.docker.network.bridge.gateway_mode_ipv6
com.docker.network.bridge.gateway_mode_ipv4
Each of these can be set to one of the gateway modes:
nat
nat-unprotected
routed
isolated
The default is nat
, NAT and masquerading rules are set up for each
published container port. Packets leaving the host will use a host address.
With mode routed
, no NAT or masquerading rules are set up, but iptables
are still set up so that only published container ports are accessible.
Outgoing packets from the container will use the container's address,
not a host address.
In nat
mode, when a port is published to a specific host address, that
port is only accessible via the host interface with that address. So,
for example, publishing a port to an address on the loopback interface
means remote hosts cannot access it.
However, using direct routing, published container ports are always accessible from remote hosts, unless the Docker host's firewall has additional restrictions. Hosts on the local layer-2 network can set up direct routing without needing any additional network configuration. Hosts outside the local network can only use direct routing to the container if the network's routers are configured to enable it.
In nat-unprotected
mode, unpublished container ports are also
accessible using direct routing, no port filtering rules are set up.
This mode is included for compatibility with legacy default behaviour.
The gateway mode also affects communication between containers that are connected to different Docker networks on the same host.
- In
nat
andnat-unprotected
modes, containers in other bridge networks can only access published ports via the host addresses they are published to. Direct routing from other networks is not allowed. - In
routed
mode containers in other networks can use direct routing to access ports, without going via a host address.
In routed
mode, a host port in a -p
or --publish
port mapping is
not used, and the host address is only used to decide whether to apply
the mapping to IPv4 or IPv6. So, when a mapping only applies to routed
mode, only addresses 0.0.0.0
or ::
should be used, and a host port
should not be given. If a specific address or port is given, it will
have no effect on the published port and a warning message will be
logged.
Mode isolated
can only be used when the network is also created with
CLI flag --internal
, or equivalent. An address is normally assigned to the
bridge device in an internal
network. So, processes on the docker host can
access the network, and containers in the network can access host services
listening on that bridge address (including services listening on ""any"" host
address, 0.0.0.0
or ::
). No address is assigned to the bridge when the
network is created with gateway mode isolated
.
Example
Create a network suitable for direct routing for IPv6, with NAT enabled for IPv4:
$ docker network create --ipv6 --subnet 2001:db8::/64 -o com.docker.network.bridge.gateway_mode_ipv6=routed mynet
Create a container with a published port:
$ docker run --network=mynet -p 8080:80 myimage
Then:
- Only container port 80 will be open, for IPv4 and IPv6. It is accessible from anywhere, if there is routing to the container's address, and access is not blocked by the host's firewall.
- For IPv6, using
routed
mode, port 80 will be open on the container's IP address. Port 8080 will not be opened on the host's IP addresses, and outgoing packets will use the container's IP address. - For IPv4, using the default
nat
mode, the container's port 80 will be accessible via port 8080 on the host's IP addresses, as well as directly. Connections originating from the container will masquerade, using the host's IP address.
In docker inspect
, this port mapping will be shown as follows. Note that
there is no HostPort
for IPv6, because it is using routed
mode:
$ docker container inspect <id> --format ""{{json .NetworkSettings.Ports}}""
{""80/tcp"":[{""HostIp"":""0.0.0.0"",""HostPort"":""8080""},{""HostIp"":""::"",""HostPort"":""""}]}
Alternatively, to make the mapping IPv6-only, disabling IPv4 access to the
container's port 80, use the unspecified IPv6 address [::]
and do not
include a host port number:
$ docker run --network mynet -p '[::]::80'
Setting the default bind address for containers
By default, when a container's ports are mapped without any specific host
address, the Docker daemon binds published container ports to all host
addresses (0.0.0.0
and [::]
).
For example, the following command publishes port 8080 to all network interfaces on the host, on both IPv4 and IPv6 addresses, potentially making them available to the outside world.
docker run -p 8080:80 nginx
You can change the default binding address for published container ports so that
they're only accessible to the Docker host by default. To do that, you can
configure the daemon to use the loopback address (127.0.0.1
) instead.
Warning
In releases older than 28.0.0, hosts within the same L2 segment (for example, hosts connected to the same network switch) can reach ports published to localhost. For more information, see moby/moby#45610
To configure this setting for user-defined bridge networks, use
the com.docker.network.bridge.host_binding_ipv4
driver option when you create the network.
$ docker network create mybridge \
-o ""com.docker.network.bridge.host_binding_ipv4=127.0.0.1""
Note
- Setting the default binding address to
::
means port bindings with no host address specified will work for any IPv6 address on the host. But,0.0.0.0
means any IPv4 or IPv6 address.- Changing the default bind address doesn't have any effect on Swarm services. Swarm services are always exposed on the
0.0.0.0
network interface.
Default bridge
To set the default binding for the default bridge network, configure the ""ip""
key in the daemon.json
configuration file:
{
""ip"": ""127.0.0.1""
}
This changes the default binding address to 127.0.0.1
for published container
ports on the default bridge network.
Restart the daemon for this change to take effect.
Alternatively, you can use the dockerd --ip
flag when starting the daemon.
Docker on a router
On Linux, Docker needs ""IP Forwarding"" enabled on the host. So, it enables
the sysctl
settings net.ipv4.ip_forward
and net.ipv6.conf.all.forwarding
it they are not already enabled when it starts. When it does that, it also
sets the policy of the iptables FORWARD
chain to DROP
.
If Docker sets the policy for the FORWARD
chain to DROP
. This will prevent
your Docker host from acting as a router, it is the recommended setting when
IP Forwarding is enabled.
To stop Docker from setting the FORWARD
chain's policy to DROP
, include
""ip-forward-no-drop"": true
in /etc/docker/daemon.json
, or add option
--ip-forward-no-drop
to the dockerd
command line.
Alternatively, you may add ACCEPT
rules to the DOCKER-USER
chain for the
packets you want to forward. For example:
$ iptables -I DOCKER-USER -i src_if -o dst_if -j ACCEPT
Warning
In releases older than 28.0.0, Docker always set the default policy of the IPv6
FORWARD
chain toDROP
. In release 28.0.0 and newer, it will only set that policy if it enables IPv6 forwarding itself. This has always been the behaviour for IPv4 forwarding.If IPv6 forwarding is enabled on your host before Docker starts, check your host's configuration to make sure it is still secure.
Prevent Docker from manipulating iptables
It is possible to set the iptables
or ip6tables
keys to false
in
daemon configuration, but
this option is not appropriate for most users. It is likely to break
container networking for the Docker Engine.
All ports of all containers will be accessible from the network, and none will be mapped from Docker host IP addresses.
It is not possible to completely prevent Docker from creating iptables
rules, and creating rules after-the-fact is extremely involved and beyond
the scope of these instructions.
Integration with firewalld
If you are running Docker with the iptables
option set to true
, and
firewalld is enabled on your system, Docker
automatically creates a firewalld
zone called docker
, with target ACCEPT
.
All network interfaces created by Docker (for example, docker0
) are inserted
into the docker
zone.
Docker also creates a forwarding policy called docker-forwarding
that allows
forwarding from ANY
zone to the docker
zone.
Docker and ufw
Uncomplicated Firewall (ufw) is a frontend that ships with Debian and Ubuntu, and it lets you manage firewall rules. Docker and ufw use iptables in ways that make them incompatible with each other.
When you publish a container's ports using Docker, traffic to and from that
container gets diverted before it goes through the ufw firewall settings.
Docker routes container traffic in the nat
table, which means that packets
are diverted before it reaches the INPUT
and OUTPUT
chains that ufw uses.
Packets are routed before the firewall rules can be applied,
effectively ignoring your firewall configuration.",,,
113f7d73ca65298aca718b07324a54e36464c3d59e24cabe9884047a5978a1a2,"Deploy a service to the swarm
After you create a swarm, you can deploy a service to the swarm. For this tutorial, you also added worker nodes, but that is not a requirement to deploy a service.
Open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named
manager1
.Run the following command:
$ docker service create --replicas 1 --name helloworld alpine ping docker.com 9uk4639qpg7npwf3fn2aasksr
- The
docker service create
command creates the service. - The
--name
flag names the servicehelloworld
. - The
--replicas
flag specifies the desired state of 1 running instance. - The arguments
alpine ping docker.com
define the service as an Alpine Linux container that executes the commandping docker.com
.
- The
Run
docker service ls
to see the list of running services:$ docker service ls ID NAME SCALE IMAGE COMMAND 9uk4639qpg7n helloworld 1/1 alpine ping docker.com
Next steps
Now you're ready to inspect the service.",,,
8a36da213e1fb5b6a4581179db74816a54c7cc9b4799c8c571006479c38d0798,"Build cache invalidation
When building an image, Docker steps through the instructions in your Dockerfile, executing each in the order specified. For each instruction, the builder checks whether it can reuse the instruction from the build cache.
General rules
The basic rules of build cache invalidation are as follows:
The builder begins by checking if the base image is already cached. Each subsequent instruction is compared against the cached layers. If no cached layer matches the instruction exactly, the cache is invalidated.
In most cases, comparing the Dockerfile instruction with the corresponding cached layer is sufficient. However, some instructions require additional checks and explanations.
For the
ADD
andCOPY
instructions, and forRUN
instructions with bind mounts (RUN --mount=type=bind
), the builder calculates a cache checksum from file metadata to determine whether cache is valid. During cache lookup, cache is invalidated if the file metadata has changed for any of the files involved.The modification time of a file (
mtime
) is not taken into account when calculating the cache checksum. If only themtime
of the copied files have changed, the cache is not invalidated.Aside from the
ADD
andCOPY
commands, cache checking doesn't look at the files in the container to determine a cache match. For example, when processing aRUN apt-get -y update
command the files updated in the container aren't examined to determine if a cache hit exists. In that case just the command string itself is used to find a match.
Once the cache is invalidated, all subsequent Dockerfile commands generate new images and the cache isn't used.
If your build contains several layers and you want to ensure the build cache is reusable, order the instructions from less frequently changed to more frequently changed where possible.
RUN instructions
The cache for RUN
instructions isn't invalidated automatically between builds.
Suppose you have a step in your Dockerfile to install curl
:
FROM alpine:3.21 AS install
RUN apk add curl
This doesn't mean that the version of curl
in your image is always up-to-date.
Rebuilding the image one week later will still get you the same packages as before.
To force a re-execution of the RUN
instruction, you can:
- Make sure that a layer before it has changed
- Clear the build cache ahead of the build using
docker builder prune
- Use the
--no-cache
or--no-cache-filter
options
The --no-cache-filter
option lets you specify a specific build stage to
invalidate the cache for:
$ docker build --no-cache-filter install .
Build secrets
The contents of build secrets are not part of the build cache. Changing the value of a secret doesn't result in cache invalidation.
If you want to force cache invalidation after changing a secret value, you can pass a build argument with an arbitrary value that you also change when changing the secret. Build arguments do result in cache invalidation.
FROM alpine
ARG CACHEBUST
RUN --mount=type=secret,id=TOKEN,env=TOKEN \
some-command ...
$ TOKEN=""tkn_pat123456"" docker build --secret id=TOKEN --build-arg CACHEBUST=1 .
Properties of secrets such as IDs and mount paths do participate in the cache checksum, and result in cache invalidation if changed.",,,
57d7070c24bca78cb73aca966614740aeb6557d744b30e8cf8a2e49fd79e3555,"Docker Build Overview
Docker Build implements a client-server architecture, where:
- Client: Buildx is the client and the user interface for running and managing builds.
- Server: BuildKit is the server, or builder, that handles the build execution.
When you invoke a build, the Buildx client sends a build request to the BuildKit backend. BuildKit resolves the build instructions and executes the build steps. The build output is either sent back to the client or uploaded to a registry, such as Docker Hub.
Buildx and BuildKit are both installed with Docker Desktop and Docker Engine
out-of-the-box. When you invoke the docker build
command, you're using Buildx
to run a build using the default BuildKit bundled with Docker.
Buildx
Buildx is the CLI tool that you use to run builds. The docker build
command
is a wrapper around Buildx. When you invoke docker build
, Buildx interprets
the build options and sends a build request to the BuildKit backend.
The Buildx client can do more than just run builds. You can also use Buildx to create and manage BuildKit backends, referred to as builders. It also supports features for managing images in registries, and for running multiple builds concurrently.
Docker Buildx is installed by default with Docker Desktop. You can also build the CLI plugin from source, or grab a binary from the GitHub repository and install it manually. See Buildx README on GitHub for more information.
Note
While
docker build
invokes Buildx under the hood, there are subtle differences between this command and the canonicaldocker buildx build
. For details, see Difference betweendocker build
anddocker buildx build
.
BuildKit
BuildKit is the daemon process that executes the build workloads.
A build execution starts with the invocation of a docker build
command.
Buildx interprets your build command and sends a build request to the BuildKit
backend. The build request includes:
- The Dockerfile
- Build arguments
- Export options
- Caching options
BuildKit resolves the build instructions and executes the build steps. While BuildKit is executing the build, Buildx monitors the build status and prints the progress to the terminal.
If the build requires resources from the client, such as local files or build secrets, BuildKit requests the resources that it needs from Buildx.
This is one way in which BuildKit is more efficient compared to the legacy builder used in earlier versions of Docker. BuildKit only requests the resources that the build needs when they're needed. The legacy builder, in comparison, always takes a copy of the local filesystem.
Examples of resources that BuildKit can request from Buildx include:
- Local filesystem build contexts
- Build secrets
- SSH sockets
- Registry authentication tokens
For more information about BuildKit, see BuildKit.",,,
6f94eb6306c41c2fb5782417165f2ad1941acf6562e4fc002153397bc39dd5bc,"Understand permission requirements for Docker Desktop on Mac
This page contains information about the permission requirements for running and installing Docker Desktop on Mac.
It also provides clarity on running containers as root
as opposed to having root
access on the host.
Permission requirements
Docker Desktop for Mac is run as an unprivileged user. However, Docker Desktop requires certain functionalities to perform a limited set of privileged configurations such as:
- Installing symlinks in
/usr/local/bin
. - Binding privileged ports that are less than 1024. The so-called ""privileged ports"" are not generally used as a security boundary, however operating systems still prevent unprivileged processes from binding them which breaks commands like
docker run -p 127.0.0.1:80:80 docker/getting-started
. - Ensuring
localhost
andkubernetes.docker.internal
are defined in/etc/hosts
. Some old macOS installs don't havelocalhost
in/etc/hosts
, which causes Docker to fail. Defining the DNS namekubernetes.docker.internal
allows Docker to share Kubernetes contexts with containers. - Securely caching the Registry Access Management policy which is read-only for the developer.
Depending on which version of Docker Desktop for Mac is used, privileged access is granted either during installation, first run, or only when it's needed.
From version 4.18 and later, Docker Desktop for Mac provides greater control over functionality that's enabled during installation.
The first time Docker Desktop for Mac launches, it presents an installation window where you can choose to either use the default settings, which work for most developers and requires you to grant privileged access, or use advanced settings.
If you work in an environment with elevated security requirements, for instance where local administrative access is prohibited, then you can use the advanced settings to remove the need for granting privileged access. You can configure:
- The location of the Docker CLI tools either in the system or user directory
- The default Docker socket
- Privileged port mapping
Depending on which advanced settings you configure, you must enter your password to confirm.
You can change these configurations at a later date from the Advanced page in Settings.
Versions 4.15 to 4.17 of Docker Desktop for Mac don't require the privileged process to run permanently. Whenever elevated privileges are needed for a configuration, Docker Desktop prompts you with information on the task it needs to perform. Most configurations are applied once, subsequent runs don't prompt for privileged access anymore. The only time Docker Desktop may start the privileged process is for binding privileged ports that aren't allowed by default on the host OS.
Versions prior to 4.15 of Docker Desktop for Mac require root
access to be granted on the first run. The first time that Docker Desktop launches you receive an admin prompt to grant permission for the installation of the com.docker.vmnetd
privileged helper service. For subsequent runs, root
privileges aren't required. Following the principle of least privilege, this approach allows root
access to be used only for the operations for which it's absolutely necessary, while still being able to use Docker Desktop as an unprivileged user.
All privileged operations are run using the privileged helper process com.docker.vmnetd
.
Installing symlinks
The Docker binaries are installed by default in /Applications/Docker.app/Contents/Resources/bin
. Docker Desktop creates symlinks for the binaries in /usr/local/bin
, which means they're automatically included in PATH
on most systems.
With version 4.18 and later, you can choose whether to install symlinks either in /usr/local/bin
or $HOME/.docker/bin
during installation of Docker Desktop.
If /usr/local/bin
is chosen, and this location is not writable by unprivileged users, Docker Desktop requires authorization to confirm this choice before the symlinks to Docker binaries are created in /usr/local/bin
. If $HOME/.docker/bin
is chosen, authorization is not required, but then you must
manually add $HOME/.docker/bin
to their PATH.
You are also given the option to enable the installation of the /var/run/docker.sock
symlink. Creating this symlink ensures various Docker clients relying on the default Docker socket path work without additional changes.
As the /var/run
is mounted as a tmpfs, its content is deleted on restart, symlink to the Docker socket included. To ensure the Docker socket exists after restart, Docker Desktop sets up a launchd
startup task that creates the symlink by running ln -s -f /Users/<user>/.docker/run/docker.sock /var/run/docker.sock
. This ensures the you aren't prompted on each startup to create the symlink. If you don't enable this option at installation, the symlink and the startup task is not created and you may have to explicitly set the DOCKER_HOST
environment variable to /Users/<user>/.docker/run/docker.sock
in the clients it is using. The Docker CLI relies on the current context to retrieve the socket path, the current context is set to desktop-linux
on Docker Desktop startup.
For versions prior to 4.18, installing symlinks in /usr/local/bin
is a privileged configuration Docker Desktop performs on the first startup. Docker Desktop checks if symlinks exists and takes the following actions:
- Creates the symlinks without the admin prompt if
/usr/local/bin
is writable by unprivileged users. - Triggers an admin prompt for you to authorize the creation of symlinks in
/usr/local/bin
. If you authorizes this, symlinks to Docker binaries are created in/usr/local/bin
. If you reject the prompt, are not willing to run configurations requiring elevated privileges, or don't have admin rights on your machine, Docker Desktop creates the symlinks in~/.docker/bin
and edits your shell profile to ensure this location is in your PATH. This requires all open shells to be reloaded. The rejection is recorded for future runs to avoid prompting you again. For any failure to ensure binaries are on your PATH, you may need to manually add to their PATH the/Applications/Docker.app/Contents/Resources/bin
or use the full path to Docker binaries.
A particular case is the installation of the /var/run/docker.sock
symlink. Creating this symlink ensures various Docker clients relying on the default Docker socket path work without additional changes. As the /var/run
is mounted as a tmpfs, its content is deleted on restart, symlink to Docker socket included.
To ensure the Docker socket exists after restart, Docker Desktop sets up a launchd
startup task that creates a symlink by running ln -s -f /Users/<user>/.docker/run/docker.sock /var/run/docker.sock
. This ensures that you are not prompted on each startup to create the symlink. If you reject the prompt, the symlink and the startup task are not created and you may have to explicitly set the DOCKER_HOST
to /Users/<user>/.docker/run/docker.sock
in the clients it is using. The Docker CLI relies on the current context to retrieve the socket path, the current context is set to desktop-linux
on Docker Desktop startup.
Binding privileged ports
With version 4.18 and later you can choose to enable privileged port mapping during installation, or from the Advanced page in Settings post-installation. Docker Desktop requires authorization to confirm this choice.
For versions below 4.18 , if you run a container that requires binding privileged ports, Docker Desktop first attempts to bind it directly as an unprivileged process. If the OS prevents this and it fails, Docker Desktop checks if the com.docker.vmnetd
privileged helper process is running to bind the privileged port through it.
If the privileged helper process is not running, Docker Desktop prompts you for authorization to run it under launchd. This configures the privileged helper to run as in the versions of Docker Desktop prior to 4.15. However, the functionality provided by this privileged helper now only supports port binding and caching the Registry Access Management policy. If you decline the launch of the privileged helper process, binding the privileged port cannot be done and the Docker CLI returns an error:
$ docker run -p 127.0.0.1:80:80 docker/getting-started
docker: Error response from daemon: Ports are not available: exposing port
TCP 127.0.0.1:80 -> 0.0.0.0:0: failed to connect to /var/run/com.docker.vmnetd.sock:
is vmnetd running?: dial unix /var/run/com.docker.vmnetd.sock: connect: connection
refused.
ERRO[0003] error waiting for container: context canceled
Note
The command may fail with the same error if you take too long to authorize the prompt to start the helper process, as it may timeout.
Ensuring localhost
and kubernetes.docker.internal
are defined
With versions 4.18 and later, it is your responsibility to ensure that localhost is resolved to 127.0.0.1
and if Kubernetes is used, that kubernetes.docker.internal
is resolved to 127.0.0.1
.
On first run, Docker Desktop checks if localhost
is resolved to 127.0.0.1
. In case the resolution fails, it prompts you to allow adding the mapping to /etc/hosts
. Similarly, when the Kubernetes cluster is installed, it checks that kubernetes.docker.internal
is resolved to 127.0.0.1
and prompts you to do so.
Installing from the command line
In version 4.11 and later of Docker Desktop for Mac, privileged configurations are applied during the installation with the --user
flag on the
install command. In this case, you are not prompted to grant root privileges on the first run of Docker Desktop. Specifically, the --user
flag:
- Uninstalls the previous
com.docker.vmnetd
if present - Sets up symlinks
- Ensures that
localhost
is resolved to127.0.0.1
The limitation of this approach is that Docker Desktop can only be run by one user-account per machine, namely the one specified in the -–user
flag.
Privileged helper
In the limited situations when the privileged helper is needed, for example binding privileged ports or caching the Registry Access Management policy, the privileged helper is started by launchd
and runs in the background unless it is disabled at runtime as previously described. The Docker Desktop backend communicates with the privileged helper over the UNIX domain socket /var/run/com.docker.vmnetd.sock
. The functionalities it performs are:
- Binding privileged ports that are less than 1024.
- Securely caching the Registry Access Management policy which is read-only for the developer.
- Uninstalling the privileged helper.
The removal of the privileged helper process is done in the same way as removing launchd
processes.
$ ps aux | grep vmnetd
root 28739 0.0 0.0 34859128 228 ?? Ss 6:03PM 0:00.06 /Library/PrivilegedHelperTools/com.docker.vmnetd
user 32222 0.0 0.0 34122828 808 s000 R+ 12:55PM 0:00.00 grep vmnetd
$ sudo launchctl unload -w /Library/LaunchDaemons/com.docker.vmnetd.plist
Password:
$ ps aux | grep vmnetd
user 32242 0.0 0.0 34122828 716 s000 R+ 12:55PM 0:00.00 grep vmnetd
$ rm /Library/LaunchDaemons/com.docker.vmnetd.plist
$ rm /Library/PrivilegedHelperTools/com.docker.vmnetd
Containers running as root within the Linux VM
With Docker Desktop, the Docker daemon and containers run in a lightweight Linux
VM managed by Docker. This means that although containers run by default as
root
, this doesn't grant root
access to the Mac host machine. The Linux VM
serves as a security boundary and limits what resources can be accessed from the
host. Any directories from the host bind mounted into Docker containers still
retain their original permissions.
Enhanced Container Isolation
In addition, Docker Desktop supports Enhanced Container Isolation mode (ECI), available to Business customers only, which further secures containers without impacting developer workflows.
ECI automatically runs all containers within a Linux user-namespace, such that root in the container is mapped to an unprivileged user inside the Docker Desktop VM. ECI uses this and other advanced techniques to further secure containers within the Docker Desktop Linux VM, such that they are further isolated from the Docker daemon and other services running inside the VM.",,,
f141cb3b3e863f4520f45ffe019aed4096057db0b59526b9f0a26e760658b837,"Using profiles with Compose
Profiles help you adjust your Compose application for different environments or use cases by selectively activating services. Services can be assigned to one or more profiles; unassigned services start by default, while assigned ones only start when their profile is active. This setup means specific services, like those for debugging or development, to be included in a single compose.yml
file and activated only as needed.
Assigning profiles to services
Services are associated with profiles through the
profiles
attribute which takes an
array of profile names:
services:
frontend:
image: frontend
profiles: [frontend]
phpmyadmin:
image: phpmyadmin
depends_on: [db]
profiles: [debug]
backend:
image: backend
db:
image: mysql
Here the services frontend
and phpmyadmin
are assigned to the profiles
frontend
and debug
respectively and as such are only started when their
respective profiles are enabled.
Services without a profiles
attribute are always enabled. In this
case running docker compose up
would only start backend
and db
.
Valid profiles names follow the regex format of [a-zA-Z0-9][a-zA-Z0-9_.-]+
.
Tip
The core services of your application shouldn't be assigned
profiles
so they are always enabled and automatically started.
Start specific profiles
To start a specific profile supply the --profile
command-line option or
use the
COMPOSE_PROFILES
environment variable:
$ docker compose --profile debug up
$ COMPOSE_PROFILES=debug docker compose up
Both commands start the services with the debug
profile enabled.
In the previous compose.yaml
file, this starts the services
db
, backend
and phpmyadmin
.
Start multiple profiles
You can also enable
multiple profiles, e.g. with docker compose --profile frontend --profile debug up
the profiles frontend
and debug
will be enabled.
Multiple profiles can be specified by passing multiple --profile
flags or
a comma-separated list for the COMPOSE_PROFILES
environment variable:
$ docker compose --profile frontend --profile debug up
$ COMPOSE_PROFILES=frontend,debug docker compose up
If you want to enable all profiles at the same time, you can run docker compose --profile ""*""
.
Auto-starting profiles and dependency resolution
When a service with assigned profiles
is explicitly targeted on the command
line its profiles are started automatically so you don't need to start them
manually. This can be used for one-off services and debugging tools.
As an example consider the following configuration:
services:
backend:
image: backend
db:
image: mysql
db-migrations:
image: backend
command: myapp migrate
depends_on:
- db
profiles:
- tools
# Only start backend and db
$ docker compose up -d
# This runs db-migrations (and,if necessary, start db)
# by implicitly enabling the profiles `tools`
$ docker compose run db-migrations
But keep in mind that docker compose
only automatically starts the
profiles of the services on the command line and not of any dependencies.
This means that any other services the targeted service depends_on
should either:
- Share a common profile
- Always be started, by omitting
profiles
or having a matching profile started explicitly
services:
web:
image: web
mock-backend:
image: backend
profiles: [""dev""]
depends_on:
- db
db:
image: mysql
profiles: [""dev""]
phpmyadmin:
image: phpmyadmin
profiles: [""debug""]
depends_on:
- db
# Only start ""web""
$ docker compose up -d
# Start mock-backend (and, if necessary, db)
# by implicitly enabling profiles `dev`
$ docker compose up -d mock-backend
# This fails because profiles ""dev"" is not enabled
$ docker compose up phpmyadmin
Although targeting phpmyadmin
automatically starts the profiles debug
, it doesn't automatically start the profiles required by db
which is dev
.
To fix this you either have to add the debug
profile to the db
service:
db:
image: mysql
profiles: [""debug"", ""dev""]
or start the dev
profile explicitly:
# Profiles ""debug"" is started automatically by targeting phpmyadmin
$ docker compose --profile dev up phpmyadmin
$ COMPOSE_PROFILES=dev docker compose up phpmyadmin
Stop specific profiles
As with starting specific profiles, you can use the --profile
command-line option or
use the
COMPOSE_PROFILES
environment variable:
$ docker compose --profile debug down
$ COMPOSE_PROFILES=debug docker compose down
Both commands stop and remove services with the debug
profile. In the following compose.yaml
file, this stops the services db
and phpmyadmin
.
services:
frontend:
image: frontend
profiles: [frontend]
phpmyadmin:
image: phpmyadmin
depends_on: [db]
profiles: [debug]
backend:
image: backend
db:
image: mysql
Note
Running
docker compose down
only stopsbackend
anddb
.",,,
9edb226d0c93e9b20e29470fab43e5f7ef717483065aa6ccd0e72394c04724c6,"Advanced configuration options for ECI
Docker socket mount permissions
By default, when Enhanced Container Isolation (ECI) is enabled, Docker Desktop does not allow bind-mounting the Docker Engine socket into containers:
$ docker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock docker:cli
docker: Error response from daemon: enhanced container isolation: docker socket mount denied for container with image ""docker.io/library/docker""; image is not in the allowed list; if you wish to allow it, configure the docker socket image list in the Docker Desktop settings.
This prevents malicious containers from gaining access to the Docker Engine, as such access could allow them to perform supply chain attacks. For example, build and push malicious images into the organization's repositories or similar.
However, some legitimate use cases require containers to have access to the Docker Engine socket. For example, the popular Testcontainers framework sometimes bind-mounts the Docker Engine socket into containers to manage them or perform post-test cleanup. Similarly, some Buildpack frameworks, for example Paketo, require Docker socket bind-mounts into containers.
Administrators can optionally configure ECI to allow bind mounting the Docker Engine socket into containers, but in a controlled way.
This can be done via the Docker Socket mount permissions section in the
admin-settings.json
file. For example:
{
""configurationFileVersion"": 2,
""enhancedContainerIsolation"": {
""locked"": true,
""value"": true,
""dockerSocketMount"": {
""imageList"": {
""images"": [
""docker.io/localstack/localstack:*"",
""docker.io/testcontainers/ryuk:*"",
""docker:cli""
],
""allowDerivedImages"": true
},
""commandList"": {
""type"": ""deny"",
""commands"": [""push""]
}
}
}
}
Tip
You can now also configure these settings in the Docker Admin Console.
As shown above, there are two configurations for bind-mounting the Docker
socket into containers: the imageList
and the commandList
. These are
described below.
Image list
The imageList
is a list of container images that are allowed to bind-mount the
Docker socket. By default the list is empty, no containers are allowed to
bind-mount the Docker socket when ECI is enabled. However, an administrator can add
images to the list, using either of these formats:
| Image Reference Format | Description |
|---|---|
<image_name>[:<tag>] | Name of the image, with optional tag. If the tag is omitted, the :latest tag is used. If the tag is the wildcard * , then it means ""any tag for that image."" |
<image_name>@<digest> | Name of the image, with a specific repository digest (e.g., as reported by docker buildx imagetools inspect <image> ). This means only the image that matches that name and digest is allowed. |
The image name follows the standard convention, so it can point to any registry and repository.
In the previous example, the image list was configured with three images:
""imageList"": {
""images"": [
""docker.io/localstack/localstack:*"",
""docker.io/testcontainers/ryuk:*"",
""docker:cli""
]
}
This means that containers that use either the docker.io/localstack/localstack
or the docker.io/testcontainers/ryuk
image (with any tag), or the docker:cli
image, are allowed to bind-mount the Docker socket when ECI is enabled. Thus,
the following works:
$ docker run -it -v /var/run/docker.sock:/var/run/docker.sock docker:cli sh
/ #
Tip
Be restrictive with the images you allow, as described in Recommendations.
In general, it's easier to specify the image using the tag wildcard format, for example <image-name>:*
, because then imageList
doesn't need to be updated whenever a new version of the
image is used. Alternatively, you can use an immutable tag, for example :latest
,
but it does not always work as well as the wildcard because, for example,
Testcontainers uses specific versions of the image, not necessarily the latest
one.
When ECI is enabled, Docker Desktop periodically downloads the image digests for the allowed images from the appropriate registry and stores them in memory. Then, when a container is started with a Docker socket bind-mount, Docker Desktop checks if the container's image digest matches one of the allowed digests. If so, the container is allowed to start, otherwise it's blocked.
Due to the digest comparison, it's not possible to bypass the Docker socket mount permissions by re-tagging a disallowed image to the name of an allowed one. In other words, if a user does:
$ docker image rm <allowed_image>
$ docker tag <disallowed_image> <allowed_image>
$ docker run -v /var/run/docker.sock:/var/run/docker.sock <allowed_image>
then the tag operation succeeds, but the docker run
command fails
because the image digest of the disallowed image won't match that of the allowed
ones in the repository.
Docker Socket Mount Permissions for derived images
As described in the prior section, administrators can configure the list of container
images that are allowed to mount the Docker socket via the imageList
.
This works for most scenarios, but not always, because it requires knowing upfront
the name of the image(s) on which the Docker socket mounts should be allowed.
Some container tools such as
Paketo buildpacks,
build ephemeral local images that require Docker socket bind mounts. Since the name of
those ephemeral images is not known upfront, the imageList
is not sufficient.
To overcome this, starting with Docker Desktop version 4.34, the Docker Socket
mount permissions not only apply to the images listed in the imageList
; they
also apply to any local images derived (i.e., built from) an image in the
imageList
.
That is, if a local image called ""myLocalImage"" is built from ""myBaseImage""
(i.e., has a Dockerfile with a FROM myBaseImage
), then if ""myBaseImage"" is in
the imageList
, both ""myBaseImage"" and ""myLocalImage"" are allowed to mount the
Docker socket.
For example, to enable Paketo buildpacks to work with Docker Desktop and ECI,
simply add the following image to the imageList
:
""imageList"": {
""images"": [
""paketobuildpacks/builder:base""
],
""allowDerivedImages"": true
}
When the buildpack runs, it will create an ephemeral image derived from
paketobuildpacks/builder:base
and mount the Docker socket to it. ECI will
allow this because it will notice that the ephemeral image is derived from an
allowed image.
The behavior is disabled by default and must be explicitly enabled by setting
""allowDerivedImages"": true
as shown above. In general it is recommended that
you disable this setting unless you know it's required.
A few caveats:
Setting
""allowedDerivedImages"" :true
will impact the startup time of containers by up to 1 extra second, as Docker Desktop needs to perform some more checks on the container image.The
allowDerivedImages
setting only applies to local-only images built from an allowed image. That is, the derived image must not be present in a remote repository because if it were, you would just list its name in theimageList
.For derived image checking to work, the parent image (i.e., the image in the
imageList
) must be present locally (i.e., must have been explicitly pulled from a repository). This is usually not a problem as the tools that need this feature (e.g., Paketo buildpacks) will do the pre-pull of the parent image.For Docker Desktop versions 4.34 and 4.35 only: The
allowDerivedImages
setting applies to all images in theimageList
specified with an explicit tag (e.g.,<name>:<tag>
). It does not apply to images specified using the tag wildcard (e.g.,<name>:*
) described in the prior section. In Docker Desktop 4.36 and later, this caveat no longer applies, meaning that theallowDerivedImages
settings applies to images specified with or without a wildcard tag. This makes it easier to manage the ECI Docker socket image list.
Allowing all containers to mount the Docker socket
In Docker Desktop version 4.36 and later, it's possible to configure the image
list to allow any container to mount the Docker socket. You do this by adding
""*""
to the imageList
:
""imageList"": {
""images"": [
""*""
]
}
This tells Docker Desktop to allow all containers to mount the Docker socket which increases flexibility but reduces security. It also improves container startup time when using Enhanced Container Isolation.
It is recommended that you use this only in scenarios where explicitly listing allowed container images is not flexible enough.
Command list
In addition to the imageList
described in the prior sections, ECI can further
restrict the commands that a container can issue via a bind mounted Docker
socket. This is done via the Docker socket mount permission commandList
, and
acts as a complementary security mechanism to the imageList
(i.e., like a
second line of defense).
For example, say the imageList
is configured to allow image docker:cli
to
mount the Docker socket, and a container is started with it:
$ docker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock sh
/ #
By default, this allows the container to issue any command via that Docker socket (e.g., build and push images to the organization's repositories), which is generally not desirable.
To improve security, the commandList
can be configured to restrict the
commands that the processes inside the container can issue on the bind-mounted
Docker socket. The commandList
can be configured as a ""deny"" list (default) or
an ""allow"" list, depending on your preference.
Each command in the list is specified by its name, as reported by docker --help
(e.g., ""ps"", ""build"", ""pull"", ""push"", etc.) In addition, the following
command wildcards are allowed to block an entire group of commands:
| Command wildcard | Description |
|---|---|
| ""container*"" | Refers to all ""docker container ..."" commands |
| ""image*"" | Refers to all ""docker image ..."" commands |
| ""volume*"" | Refers to all ""docker volume ..."" commands |
| ""network*"" | Refers to all ""docker network ..."" commands |
| ""build*"" | Refers to all ""docker build ..."" commands |
| ""system*"" | Refers to all ""docker system ..."" commands |
For example, the following configuration blocks the build
and push
commands
on the Docker socket:
""commandList"": {
""type"": ""deny"",
""commands"": [""build"", ""push""]
}
Thus, if inside the container, you issue either of those commands on the bind-mounted Docker socket, they will be blocked:
/ # docker push myimage
Error response from daemon: enhanced container isolation: docker command ""/v1.43/images/myimage/push?tag=latest"" is blocked; if you wish to allow it, configure the docker socket command list in the Docker Desktop settings or admin-settings.
Similarly:
/ # curl --unix-socket /var/run/docker.sock -XPOST http://localhost/v1.43/images/myimage/push?tag=latest
Error response from daemon: enhanced container isolation: docker command ""/v1.43/images/myimage/push?tag=latest"" is blocked; if you wish to allow it, configure the docker socket command list in the Docker Desktop settings or admin-settings.
Note that if the commandList
had been configured as an ""allow"" list, then the
effect would be the opposite: only the listed commands would have been allowed.
Whether to configure the list as an allow or deny list depends on the use case.
Recommendations
Be restrictive on the list of container images for which you allow bind-mounting of the Docker socket (i.e., the
imageList
). Generally, only allow this for images that are absolutely needed and that you trust.Use the tag wildcard format if possible in the
imageList
(e.g.,<image_name>:*
), as this eliminates the need to update theadmin-settings.json
file due to image tag changes.In the
commandList
, block commands that you don't expect the container to execute. For example, for local testing (e.g., Testcontainers), containers that bind-mount the Docker socket typically create / run / remove containers, volumes, and networks, but don't typically build images or push them into repositories (though some may legitimately do this). What commands to allow or block depends on the use case.- Note that all ""docker"" commands issued by the container via the bind-mounted Docker socket will also execute under enhanced container isolation (i.e., the resulting container uses a the Linux user-namespace, sensitive system calls are vetted, etc.)
Caveats and limitations
When Docker Desktop is restarted, it's possible that an image that is allowed to mount the Docker socket is unexpectedly blocked from doing so. This can happen when the image digest changes in the remote repository (e.g., a "":latest"" image was updated) and the local copy of that image (e.g., from a prior
docker pull
) no longer matches the digest in the remote repository. In this case, remove the local image and pull it again (e.g.,docker rm <image>
anddocker pull <image>
).It's not possible to allow Docker socket bind-mounts on containers using local-only images (i.e., images that are not on a registry) unless they are derived from an allowed image or you've allowed all containers to mount the Docker socket. That is because Docker Desktop pulls the digests for the allowed images from the registry, and then uses that to compare against the local copy of the image.
The
commandList
configuration applies to all containers that are allowed to bind-mount the Docker socket. Therefore it can't be configured differently per container.The following commands are not yet supported in the
commandList
:
| Unsupported command | Description |
|---|---|
compose | Docker Compose |
dev | Dev environments |
extension | Manages Docker Extensions |
feedback | Send feedback to Docker |
init | Creates Docker-related starter files |
manifest | Manages Docker image manifests |
plugin | Manages plugins |
sbom | View Software Bill of Materials (SBOM) |
scout | Docker Scout |
trust | Manage trust on Docker images |
Note
Docker socket mount permissions do not apply when running ""true"" Docker-in-Docker (i.e., when running the Docker Engine inside a container). In this case there's no bind-mount of the host's Docker socket into the container, and therefore no risk of the container leveraging the configuration and credentials of the host's Docker Engine to perform malicious activity. Enhanced Container Isolation is capable of running Docker-in-Docker securely, without giving the outer container true root permissions in the Docker Desktop VM.",,,
c352e3b79f0851f5774eff91857523e5076d91e8d144ff25243954ce7d92d763,"Docker security non-events
This page lists security vulnerabilities which Docker mitigated, such that
processes run in Docker containers were never vulnerable to the bug—even before
it was fixed. This assumes containers are run without adding extra capabilities
or not run as --privileged
.
The list below is not even remotely complete. Rather, it is a sample of the few bugs we've actually noticed to have attracted security review and publicly disclosed vulnerabilities. In all likelihood, the bugs that haven't been reported far outnumber those that have. Luckily, since Docker's approach to secure by default through apparmor, seccomp, and dropping capabilities, it likely mitigates unknown bugs just as well as it does known ones.
Bugs mitigated:
- CVE-2013-1956,
1957,
1958,
1959,
1979,
CVE-2014-4014,
5206,
5207,
7970,
7975,
CVE-2015-2925,
8543,
CVE-2016-3134,
3135, etc.:
The introduction of unprivileged user namespaces lead to a huge increase in the
attack surface available to unprivileged users by giving such users legitimate
access to previously root-only system calls like
mount()
. All of these CVEs are examples of security vulnerabilities due to introduction of user namespaces. Docker can use user namespaces to set up containers, but then disallows the process inside the container from creating its own nested namespaces through the default seccomp profile, rendering these vulnerabilities unexploitable. - CVE-2014-0181,
CVE-2015-3339:
These are bugs that require the presence of a setuid binary. Docker disables
setuid binaries inside containers via the
NO_NEW_PRIVS
process flag and other mechanisms. - CVE-2014-4699:
A bug in
ptrace()
could allow privilege escalation. Docker disablesptrace()
inside the container using apparmor, seccomp and by droppingCAP_PTRACE
. Three times the layers of protection there! - CVE-2014-9529:
A series of crafted
keyctl()
calls could cause kernel DoS / memory corruption. Docker disableskeyctl()
inside containers using seccomp. - CVE-2015-3214,
4036: These are
bugs in common virtualization drivers which could allow a guest OS user to
execute code on the host OS. Exploiting them requires access to virtualization
devices in the guest. Docker hides direct access to these devices when run
without
--privileged
. Interestingly, these seem to be cases where containers are ""more secure"" than a VM, going against common wisdom that VMs are ""more secure"" than containers. - CVE-2016-0728:
Use-after-free caused by crafted
keyctl()
calls could lead to privilege escalation. Docker disableskeyctl()
inside containers using the default seccomp profile. - CVE-2016-2383:
A bug in eBPF -- the special in-kernel DSL used to express things like seccomp
filters -- allowed arbitrary reads of kernel memory. The
bpf()
system call is blocked inside Docker containers using (ironically) seccomp. - CVE-2016-3134,
4997,
4998:
A bug in setsockopt with
IPT_SO_SET_REPLACE
,ARPT_SO_SET_REPLACE
, andARPT_SO_SET_REPLACE
causing memory corruption / local privilege escalation. These arguments are blocked byCAP_NET_ADMIN
, which Docker does not allow by default.
Bugs not mitigated:
- CVE-2015-3290,
5157: Bugs in
the kernel's non-maskable interrupt handling allowed privilege escalation.
Can be exploited in Docker containers because the
modify_ldt()
system call is not currently blocked using seccomp. - CVE-2016-5195:
A race condition was found in the way the Linux kernel's memory subsystem
handled the copy-on-write (COW) breakage of private read-only memory mappings,
which allowed unprivileged local users to gain write access to read-only memory.
Also known as ""dirty COW.""
Partial mitigations: on some operating systems this vulnerability is mitigated
by the combination of seccomp filtering of
ptrace
and the fact that/proc/self/mem
is read-only.",,,
6d1762c04b17a60f7ad41f61909fe631cc91f4fce10f0e69bef3d4833831db19,"Legacy container links
Warning
The
--link
flag is a legacy feature of Docker. It may eventually be removed. Unless you absolutely need to continue using it, we recommend that you use user-defined networks to facilitate communication between two containers instead of using--link
. One feature that user-defined networks do not support that you can do with--link
is sharing environment variables between containers. However, you can use other mechanisms such as volumes to share environment variables between containers in a more controlled way.See Differences between user-defined bridges and the default bridge for some alternatives to using
--link
.
The information in this section explains legacy container links within the
Docker default bridge
network which is created automatically when you install
Docker.
Before the
Docker networks feature, you could use the
Docker link feature to allow containers to discover each other and securely
transfer information about one container to another container. With the
introduction of the Docker networks feature, you can still create links but they
behave differently between default bridge
network and
user defined networks.
This section briefly discusses connecting via a network port and then goes into
detail on container linking in default bridge
network.
Connect using network port mapping
Let's say you used this command to run a simple Python Flask application:
$ docker run -d -P training/webapp python app.py
Note
Containers have an internal network and an IP address. Docker can have a variety of network configurations. You can see more information on Docker networking here.
When that container was created, the -P
flag was used to automatically map
any network port inside it to a random high port within an ephemeral port
range on your Docker host. Next, when docker ps
was run, you saw that port
5000 in the container was bound to port 49155 on the host.
$ docker ps nostalgic_morse
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
bc533791f3f5 training/webapp:latest python app.py 5 seconds ago Up 2 seconds 0.0.0.0:49155->5000/tcp nostalgic_morse
You also saw how you can bind a container's ports to a specific port using
the -p
flag. Here port 80 of the host is mapped to port 5000 of the
container:
$ docker run -d -p 80:5000 training/webapp python app.py
And you saw why this isn't such a great idea because it constrains you to only one container on that specific port.
Instead, you may specify a range of host ports to bind a container port to that is different than the default ephemeral port range:
$ docker run -d -p 8000-9000:5000 training/webapp python app.py
This would bind port 5000 in the container to a randomly available port between 8000 and 9000 on the host.
There are also a few other ways you can configure the -p
flag. By
default the -p
flag binds the specified port to all interfaces on
the host machine. But you can also specify a binding to a specific
interface, for example only to the localhost
.
$ docker run -d -p 127.0.0.1:80:5000 training/webapp python app.py
This would bind port 5000 inside the container to port 80 on the
localhost
or 127.0.0.1
interface on the host machine.
Or, to bind port 5000 of the container to a dynamic port but only on the
localhost
, you could use:
$ docker run -d -p 127.0.0.1::5000 training/webapp python app.py
You can also bind UDP and SCTP (typically used by telecom protocols such as SIGTRAN, Diameter, and S1AP/X2AP) ports by adding a trailing /udp
or /sctp
. For example:
$ docker run -d -p 127.0.0.1:80:5000/udp training/webapp python app.py
You also learned about the useful docker port
shortcut which showed us the
current port bindings. This is also useful for showing you specific port
configurations. For example, if you've bound the container port to the
localhost
on the host machine, then the docker port
output reflects that.
$ docker port nostalgic_morse 5000
127.0.0.1:49155
Note
The
-p
flag can be used multiple times to configure multiple ports.
Connect with the linking system
Note
This section covers the legacy link feature in the default
bridge
network. Refer to differences between user-defined bridges and the default bridge for more information on links in user-defined networks.
Network port mappings are not the only way Docker containers can connect to one another. Docker also has a linking system that allows you to link multiple containers together and send connection information from one to another. When containers are linked, information about a source container can be sent to a recipient container. This allows the recipient to see selected data describing aspects of the source container.
The importance of naming
To establish links, Docker relies on the names of your containers.
You've already seen that each container you create has an automatically
created name; indeed you've become familiar with our old friend
nostalgic_morse
during this guide. You can also name containers
yourself. This naming provides two useful functions:
It can be useful to name containers that do specific functions in a way that makes it easier for you to remember them, for example naming a container containing a web application
web
.It provides Docker with a reference point that allows it to refer to other containers, for example, you can specify to link the container
web
to containerdb
.
You can name your container by using the --name
flag, for example:
$ docker run -d -P --name web training/webapp python app.py
This launches a new container and uses the --name
flag to
name the container web
. You can see the container's name using the
docker ps
command.
$ docker ps -l
CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES
aed84ee21bde training/webapp:latest python app.py 12 hours ago Up 2 seconds 0.0.0.0:49154->5000/tcp web
You can also use docker inspect
to return the container's name.
Note
Container names must be unique. That means you can only call one container
web
. If you want to re-use a container name you must delete the old container (withdocker container rm
) before you can create a new container with the same name. As an alternative you can use the--rm
flag with thedocker run
command. This deletes the container immediately after it is stopped.
Communication across links
Links allow containers to discover each other and securely transfer information
about one container to another container. When you set up a link, you create a
conduit between a source container and a recipient container. The recipient can
then access select data about the source. To create a link, you use the --link
flag. First, create a new container, this time one containing a database.
$ docker run -d --name db training/postgres
This creates a new container called db
from the training/postgres
image, which contains a PostgreSQL database.
Now, you need to delete the web
container you created previously so you can replace it
with a linked one:
$ docker container rm -f web
Now, create a new web
container and link it with your db
container.
$ docker run -d -P --name web --link db:db training/webapp python app.py
This links the new web
container with the db
container you created
earlier. The --link
flag takes the form:
--link <name or id>:alias
Where name
is the name of the container we're linking to and alias
is an
alias for the link name. That alias is used shortly.
The --link
flag also takes the form:
--link <name or id>
In this case the alias matches the name. You could write the previous example as:
$ docker run -d -P --name web --link db training/webapp python app.py
Next, inspect your linked containers with docker inspect
:
$ docker inspect -f ""{{ .HostConfig.Links }}"" web
[/db:/web/db]
You can see that the web
container is now linked to the db
container
web/db
. Which allows it to access information about the db
container.
So what does linking the containers actually do? You've learned that a link allows a
source container to provide information about itself to a recipient container. In
our example, the recipient, web
, can access information about the source db
. To do
this, Docker creates a secure tunnel between the containers that doesn't need to
expose any ports externally on the container; when we started the
db
container we did not use either the -P
or -p
flags. That's a big benefit of
linking: we don't need to expose the source container, here the PostgreSQL database, to
the network.
Docker exposes connectivity information for the source container to the recipient container in two ways:
- Environment variables,
- Updating the
/etc/hosts
file.
Environment variables
Docker creates several environment variables when you link containers. Docker
automatically creates environment variables in the target container based on
the --link
parameters. It also exposes all environment variables
originating from Docker from the source container. These include variables from:
- the
ENV
commands in the source container's Dockerfile - the
-e
,--env
, and--env-file
options on thedocker run
command when the source container is started
These environment variables enable programmatic discovery from within the target container of information related to the source container.
Warning
It is important to understand that all environment variables originating from Docker within a container are made available to any container that links to it. This could have serious security implications if sensitive data is stored in them.
Docker sets an <alias>_NAME
environment variable for each target container
listed in the --link
parameter. For example, if a new container called
web
is linked to a database container called db
via --link db:webdb
,
then Docker creates a WEBDB_NAME=/web/webdb
variable in the web
container.
Docker also defines a set of environment variables for each port exposed by the
source container. Each variable has a unique prefix in the form <name>_PORT_<port>_<protocol>
The components in this prefix are:
- the alias
<name>
specified in the--link
parameter (for example,webdb
) - the
<port>
number exposed - a
<protocol>
which is either TCP or UDP
Docker uses this prefix format to define three distinct environment variables:
- The
prefix_ADDR
variable contains the IP Address from the URL, for exampleWEBDB_PORT_5432_TCP_ADDR=172.17.0.82
. - The
prefix_PORT
variable contains just the port number from the URL for exampleWEBDB_PORT_5432_TCP_PORT=5432
. - The
prefix_PROTO
variable contains just the protocol from the URL for exampleWEBDB_PORT_5432_TCP_PROTO=tcp
.
If the container exposes multiple ports, an environment variable set is defined for each one. This means, for example, if a container exposes 4 ports that Docker creates 12 environment variables, 3 for each port.
Additionally, Docker creates an environment variable called <alias>_PORT
.
This variable contains the URL of the source container's first exposed port.
The 'first' port is defined as the exposed port with the lowest number.
For example, consider the WEBDB_PORT=tcp://172.17.0.82:5432
variable. If
that port is used for both tcp and udp, then the tcp one is specified.
Finally, Docker also exposes each Docker originated environment variable
from the source container as an environment variable in the target. For each
variable Docker creates an <alias>_ENV_<name>
variable in the target
container. The variable's value is set to the value Docker used when it
started the source container.
Returning back to our database example, you can run the env
command to list the specified container's environment variables.
$ docker run --rm --name web2 --link db:db training/webapp env
<...>
DB_NAME=/web2/db
DB_PORT=tcp://172.17.0.5:5432
DB_PORT_5432_TCP=tcp://172.17.0.5:5432
DB_PORT_5432_TCP_PROTO=tcp
DB_PORT_5432_TCP_PORT=5432
DB_PORT_5432_TCP_ADDR=172.17.0.5
<...>
You can see that Docker has created a series of environment variables with
useful information about the source db
container. Each variable is prefixed
with
DB_
, which is populated from the alias
you specified above. If the alias
were db1
, the variables would be prefixed with DB1_
. You can use these
environment variables to configure your applications to connect to the database
on the db
container. The connection is secure and private; only the
linked web
container can communicate with the db
container.
Important notes on Docker environment variables
Unlike host entries in the
/etc/hosts
file,
IP addresses stored in the environment variables are not automatically updated
if the source container is restarted. We recommend using the host entries in
/etc/hosts
to resolve the IP address of linked containers.
These environment variables are only set for the first process in the
container. Some daemons, such as sshd
, scrub them when spawning shells
for connection.
Updating the /etc/hosts
file
In addition to the environment variables, Docker adds a host entry for the
source container to the /etc/hosts
file. Here's an entry for the web
container:
$ docker run -t -i --rm --link db:webdb training/webapp /bin/bash
root@aed84ee21bde:/opt/webapp# cat /etc/hosts
172.17.0.7 aed84ee21bde
<...>
172.17.0.5 webdb 6e5cdeb2d300 db
You can see two relevant host entries. The first is an entry for the web
container that uses the Container ID as a host name. The second entry uses the
link alias to reference the IP address of the db
container. In addition to
the alias you provide, the linked container's name, if unique from the alias
provided to the --link
parameter, and the linked container's hostname are
also added to /etc/hosts
for the linked container's IP address. You can ping
that host via any of these entries:
root@aed84ee21bde:/opt/webapp# apt-get install -yqq inetutils-ping
root@aed84ee21bde:/opt/webapp# ping webdb
PING webdb (172.17.0.5): 48 data bytes
56 bytes from 172.17.0.5: icmp_seq=0 ttl=64 time=0.267 ms
56 bytes from 172.17.0.5: icmp_seq=1 ttl=64 time=0.250 ms
56 bytes from 172.17.0.5: icmp_seq=2 ttl=64 time=0.256 ms
Note
In the example, you had to install
ping
because it was not included in the container initially.
Here, you used the ping
command to ping the db
container using its host entry,
which resolves to 172.17.0.5
. You can use this host entry to configure an application
to make use of your db
container.
Note
You can link multiple recipient containers to a single source. For example, you could have multiple (differently named) web containers attached to your
db
container.
If you restart the source container, the /etc/hosts
files on the linked containers
are automatically updated with the source container's new IP address,
allowing linked communication to continue.
$ docker restart db
db
$ docker run -t -i --rm --link db:db training/webapp /bin/bash
root@aed84ee21bde:/opt/webapp# cat /etc/hosts
172.17.0.7 aed84ee21bde
<...>
172.17.0.9 db",,,
380114b85313226d3873238bde558f6637589032141f7f95b7bfe9412983f880,"Deploy on Kubernetes with Docker Desktop
Docker Desktop includes a standalone Kubernetes server and client, as well as Docker CLI integration, enabling local Kubernetes development and testing directly on your machine.
The Kubernetes server runs as a single or multi-node cluster, within Docker container(s). This lightweight setup helps you explore Kubernetes features, test workloads, and work with container orchestration in parallel with other Docker functionalities.
Kubernetes on Docker Desktop runs alongside other workloads, including Swarm services and standalone containers.
What happens when I enable Kubernetes in Docker Desktop?
When you enable Kubernetes in Docker Desktop, the following actions are triggered in the Docker Desktop backend and VM:
- Generation of certificates and cluster configuration
- Download and installation of Kubernetes internal components
- Cluster bootup
- Installation of additional controllers for networking and storage
Turning the Kubernetes server on or off in Docker Desktop does not affect your other workloads.
Install and turn on Kubernetes
- Open the Docker Desktop Dashboard and navigate to Settings.
- Select the Kubernetes tab.
- Toggle on Enable Kubernetes.
- Choose your cluster provisioning method.
- Select Apply & Restart to save the settings.
This sets up the images required to run the Kubernetes server as containers, and installs the kubectl
command-line tool on your system at /usr/local/bin/kubectl
(Mac) or C:\Program Files\Docker\Docker\Resources\bin\kubectl.exe
(Windows).
Note
Docker Desktop for Linux does not include
kubectl
by default. You can install it separately by following the Kubernetes installation guide. Ensure thekubectl
binary is installed at/usr/local/bin/kubectl
.
When Kubernetes is enabled, its status is displayed in the Docker Desktop Dashboard footer and the Docker menu.
You can check which version of Kubernetes you're on with:
$ kubectl version
Cluster provisioning method
Docker Desktop Kubernetes can be provisioned with either the kubeadm
or kind
provisioners.
kubeadm
is the older provisioner. It supports a single-node cluster, you can't select the kubernetes
version, it's slower to provision than kind
, and it's not supported by
Enhanced Container Isolation (ECI),
meaning that if ECI is enabled the cluster works but it's not protected by ECI.
kind
is the newer provisioner, and it's available if you are signed in and are
using Docker Desktop version 4.38 or later. It supports multi-node clusters (for
a more realistic Kubernetes setup), you can choose the Kubernetes version, it's
faster to provision than kubeadm
, and it's supported by ECI (i.e., when ECI is
enabled, the Kubernetes cluster runs in unprivileged Docker containers, thus
making it more secure). Note however that kind
requires that Docker Desktop be
configured to use the
containerd image store (the default image
store in Docker Desktop 4.34 and later).
The following table summarizes this comparison.
| Feature | kubeadm | kind |
|---|---|---|
| Availability | Docker Desktop 4.0+ | Docker Desktop 4.38+ (requires sign in) |
| Multi-node cluster support | No | Yes |
| Kubernetes version selector | No | Yes |
| Speed to provision | ~1 min | ~30 seconds |
| Supported by ECI | No | Yes |
| Works with containerd image store | Yes | Yes |
| Works with Docker image store | Yes | No |
Additional settings
Kubernetes dashboard
Once Kubernetes is installed and set up, you can select the Deploy the Kubernetes Dashboard into cluster setting so you can manage and monitor your Kubernetes clusters and applications more easily.
Viewing system containers
By default, Kubernetes system containers are hidden. To inspect these containers, enable Show system containers (advanced).
You can now view the running Kubernetes containers with docker ps
or in the Docker Desktop Dashboard.
Using the kubectl command
Kubernetes integration automatically installs the Kubernetes CLI command
at /usr/local/bin/kubectl
on Mac and at C:\Program Files\Docker\Docker\Resources\bin\kubectl.exe
on Windows. This location may not be in your shell's PATH
variable, so you may need to type the full path of the command or add it to
the PATH
.
If you have already installed kubectl
and it is
pointing to some other environment, such as minikube
or a Google Kubernetes Engine cluster, ensure you change the context so that kubectl
is pointing to docker-desktop
:
$ kubectl config get-contexts
$ kubectl config use-context docker-desktop
Tip
If the
kubectl
config get-contexts command returns an empty result, try:
- Running the command in the Command Prompt or PowerShell.
- Setting the
KUBECONFIG
environment variable to point to your.kube/config
file.
Verify installation
To confirm that Kubernetes is running, list the available nodes:
$ kubectl get nodes
NAME STATUS ROLES AGE VERSION
docker-desktop Ready control-plane 3h v1.29.1
If you installed kubectl
using Homebrew, or by some other method, and
experience conflicts, remove /usr/local/bin/kubectl
.
For more information about kubectl
, see the
kubectl
documentation.
Upgrade your cluster
Kubernetes clusters are not automatically upgraded with Docker Desktop updates. To upgrade the cluster, you must manually select Reset Kubernetes Cluster in settings.
Troubleshooting
- If Kubernetes fails to start, make sure Docker Desktop is running with enough allocated resources. Check Settings > Resources.
- If the
kubectl
commands return errors, confirm the context is set todocker-desktop
You can then try checking the logs of the Kubernetes system containers if you have enabled that setting.$ kubectl config use-context docker-desktop
- If you're experiencing cluster issues after updating, reset your Kubernetes cluster. Resetting a Kubernetes cluster can help resolve issues by essentially reverting the cluster to a clean state, and clearing out misconfigurations, corrupted data, or stuck resources that may be causing problems. If the issue still persists, you may need to clean and purge data, and then restart Docker Desktop.
Turn off and uninstall Kubernetes
To turn off Kubernetes in Docker Desktop:
- From the Docker Desktop Dashboard, select the Settings icon.
- Select the Kubernetes tab.
- Deselect the Enable Kubernetes checkbox.
- Select Apply & Restart to save the settings. This stops and removes Kubernetes containers, and also removes the
/usr/local/bin/kubectl
command.",,,
3378d7192c67e8c100954e1ffcd7500b7312006fc86c19831a09af6f8ab0e9e6,"Install the Docker Compose plugin
This page contains instructions on how to install the Docker Compose plugin on Linux from the command line.
To install the Docker Compose plugin on Linux, you can either:
Note
These instructions assume you already have Docker Engine and Docker CLI installed and now want to install the Docker Compose plugin. For the Docker Compose standalone, see Install the Docker Compose Standalone.
Install using the repository
Set up the repository. Find distribution-specific instructions in:
Ubuntu | CentOS | Debian | Raspberry Pi OS | Fedora | RHEL | SLES.
Update the package index, and install the latest version of Docker Compose:
For Ubuntu and Debian, run:
$ sudo apt-get update $ sudo apt-get install docker-compose-plugin
For RPM-based distributions, run:
$ sudo yum update $ sudo yum install docker-compose-plugin
Verify that Docker Compose is installed correctly by checking the version.
$ docker compose version
Expected output:
Docker Compose version vN.N.N
Where
vN.N.N
is placeholder text standing in for the latest version.
Update Docker Compose
To update the Docker Compose plugin, run the following commands:
For Ubuntu and Debian, run:
$ sudo apt-get update $ sudo apt-get install docker-compose-plugin
For RPM-based distributions, run:
$ sudo yum update $ sudo yum install docker-compose-plugin
Install the plugin manually
Note
This option requires you to manage upgrades manually. It is recommended that you set up Docker's repository for easier maintenance.
To download and install the Docker Compose CLI plugin, run:
$ DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker} $ mkdir -p $DOCKER_CONFIG/cli-plugins $ curl -SL https://github.com/docker/compose/releases/download/v2.33.1/docker-compose-linux-x86_64 -o $DOCKER_CONFIG/cli-plugins/docker-compose
This command downloads and installs the latest release of Docker Compose for the active user under
$HOME
directory.To install:
- Docker Compose for all users on your system, replace
~/.docker/cli-plugins
with/usr/local/lib/docker/cli-plugins
. - A different version of Compose, substitute
v2.33.1
with the version of Compose you want to use. - For a different architecture, substitute
x86_64
with the architecture you want.
- Docker Compose for all users on your system, replace
Apply executable permissions to the binary:
$ chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose
or, if you chose to install Compose for all users:
$ sudo chmod +x /usr/local/lib/docker/cli-plugins/docker-compose
Test the installation.
$ docker compose version
Expected output:
Docker Compose version v2.33.1",,,
463a7aa2e14a63d234cbb95640ee5c0b241ded29322b567b8212dfbb669bdc47,"Content library
Docker Hub's content library is the world's largest collection of container images, extensions, and plugins. It provides a central location to discover pre-built images and tools designed to streamline your container workflows, making it easier to share and collaborate.
In this section, learn about:
- Search: Discover how to browse and search Docker Hub's extensive resources.
- Trusted content: Dive into Docker Official Images, Verified Publisher content, and Sponsored Open Source images, all vetted for security and reliability to streamline your workflows.
- Catalogs: Explore specialized collections like the generative AI catalog.
- Mirroring: Learn how to create a mirror of Docker Hub's container image library as a pull-through cache.",,,
d95201768f0a0db7b81f122dc16d2ed95d0706e9b438ec5a75e9ee68a6e33478,"Best practices for optimizing Docker Hub usage
Use the following steps to help optimize and manage your Docker Hub usage for both individuals and organizations:
Use the Docker Hub usage data to identify which accounts consume the most data, determine peak usage times, and identify which images are related to the most data usage. In addition, look for usage trends, such as the following:
- Inefficient pull behavior: Identify frequently accessed repositories to assess whether you can optimize caching practices or consolidate usage to reduce pulls.
- Inefficient automated systems: Check which automated tools, such as CI/CD pipelines, may be causing higher pull rates, and configure them to avoid unnecessary image pulls.
Optimize image pulls by:
- Using caching: Implement local image caching via mirroring or within your CI/CD pipelines to reduce redundant pulls.
- Automating manual workflows: Avoid unnecessary pulls by configuring automated systems to pull only when a new version of an image is available.
Optimize your storage by:
- Regularly auditing and removing entire repositories with untagged, unused, or outdated images.
- Using Image Management to remove stale and outdated images within a repository.
For organizations, monitor and enforce organizational policies by doing the following:
- Routinely view Docker Hub usage to monitor usage.
- Enforce sign-in to ensure that you can monitor the usage of your users and users receive higher usage limits.
- Look for duplicate user accounts in Docker and remove accounts from your organization as needed.",,,
fbf135ed2f4f1f777c2f2247dd31a66720a6ad86909cdbedc5fcd1eea41d78b7,"Deploy services to a swarm
Swarm services use a declarative model, which means that you define the desired state of the service, and rely upon Docker to maintain this state. The state includes information such as (but not limited to):
- The image name and tag the service containers should run
- How many containers participate in the service
- Whether any ports are exposed to clients outside the swarm
- Whether the service should start automatically when Docker starts
- The specific behavior that happens when the service is restarted (such as whether a rolling restart is used)
- Characteristics of the nodes where the service can run (such as resource constraints and placement preferences)
For an overview of Swarm mode, see Swarm mode key concepts. For an overview of how services work, see How services work.
Create a service
To create a single-replica service with no extra configuration, you only need to supply the image name. This command starts an Nginx service with a randomly-generated name and no published ports. This is a naive example, since you can't interact with the Nginx service.
$ docker service create nginx
The service is scheduled on an available node. To confirm that the service
was created and started successfully, use the docker service ls
command:
$ docker service ls
ID NAME MODE REPLICAS IMAGE PORTS
a3iixnklxuem quizzical_lamarr replicated 1/1 docker.io/library/nginx@sha256:41ad9967ea448d7c2b203c699b429abe1ed5af331cd92533900c6d77490e0268
Created services do not always run right away. A service can be in a pending state if its image is unavailable, if no node meets the requirements you configure for the service, or for other reasons. See Pending services for more information.
To provide a name for your service, use the --name
flag:
$ docker service create --name my_web nginx
Just like with standalone containers, you can specify a command that the
service's containers should run, by adding it after the image name. This example
starts a service called helloworld
which uses an alpine
image and runs the
command ping docker.com
:
$ docker service create --name helloworld alpine ping docker.com
You can also specify an image tag for the service to use. This example modifies
the previous one to use the alpine:3.6
tag:
$ docker service create --name helloworld alpine:3.6 ping docker.com
For more details about image tag resolution, see Specify the image version the service should use.
gMSA for Swarm
Note
This example only works for a Windows container.
Swarm now allows using a Docker config as a gMSA credential spec - a requirement for Active Directory-authenticated applications. This reduces the burden of distributing credential specs to the nodes they're used on.
The following example assumes a gMSA and its credential spec (called credspec.json) already exists, and that the nodes being deployed to are correctly configured for the gMSA.
To use a config as a credential spec, first create the Docker config containing the credential spec:
$ docker config create credspec credspec.json
Now, you should have a Docker config named credspec, and you can create a service using this credential spec. To do so, use the --credential-spec flag with the config name, like this:
$ docker service create --credential-spec=""config://credspec"" <your image>
Your service uses the gMSA credential spec when it starts, but unlike a typical Docker config (used by passing the --config flag), the credential spec is not mounted into the container.
Create a service using an image on a private registry
If your image is available on a private registry which requires login, use the
--with-registry-auth
flag with docker service create
, after logging in. If
your image is stored on registry.example.com
, which is a private registry, use
a command like the following:
$ docker login registry.example.com
$ docker service create \
--with-registry-auth \
--name my_service \
registry.example.com/acme/my_image:latest
This passes the login token from your local client to the swarm nodes where the service is deployed, using the encrypted WAL logs. With this information, the nodes are able to log into the registry and pull the image.
Provide credential specs for managed service accounts
In Enterprise Edition 3.0, security is improved through the centralized distribution and management of Group Managed Service Account(gMSA) credentials using Docker config functionality. Swarm now allows using a Docker config as a gMSA credential spec, which reduces the burden of distributing credential specs to the nodes on which they are used.
Note
This option is only applicable to services using Windows containers.
Credential spec files are applied at runtime, eliminating the need for host-based credential spec files or registry entries - no gMSA credentials are written to disk on worker nodes. You can make credential specs available to Docker Engine running swarm kit worker nodes before a container starts. When deploying a service using a gMSA-based config, the credential spec is passed directly to the runtime of containers in that service.
The --credential-spec
must be in one of the following formats:
file://<filename>
: The referenced file must be present in theCredentialSpecs
subdirectory in the docker data directory, which defaults toC:\ProgramData\Docker\
on Windows. For example, specifyingfile://spec.json
loadsC:\ProgramData\Docker\CredentialSpecs\spec.json
.registry://<value-name>
: The credential spec is read from the Windows registry on the daemon’s host.config://<config-name>
: The config name is automatically converted to the config ID in the CLI. The credential spec contained in the specifiedconfig
is used.
The following simple example retrieves the gMSA name and JSON contents from your Active Directory (AD) instance:
$ name=""mygmsa""
$ contents=""{...}""
$ echo $contents > contents.json
Make sure that the nodes to which you are deploying are correctly configured for the gMSA.
To use a config as a credential spec, create a Docker config in a credential spec file named credpspec.json
.
You can specify any name for the name of the config
.
$ docker config create --label com.docker.gmsa.name=mygmsa credspec credspec.json
Now you can create a service using this credential spec. Specify the --credential-spec
flag with the config name:
$ docker service create --credential-spec=""config://credspec"" <your image>
Your service uses the gMSA credential spec when it starts, but unlike a typical Docker config (used by passing the --config flag), the credential spec is not mounted into the container.
Update a service
You can change almost everything about an existing service using the
docker service update
command. When you update a service, Docker stops its
containers and restarts them with the new configuration.
Since Nginx is a web service, it works much better if you publish port 80
to clients outside the swarm. You can specify this when you create the service,
using the -p
or --publish
flag. When updating an existing service, the flag
is --publish-add
. There is also a --publish-rm
flag to remove a port that
was previously published.
Assuming that the my_web
service from the previous section still exists, use
the following command to update it to publish port 80.
$ docker service update --publish-add 80 my_web
To verify that it worked, use docker service ls
:
$ docker service ls
ID NAME MODE REPLICAS IMAGE PORTS
4nhxl7oxw5vz my_web replicated 1/1 docker.io/library/nginx@sha256:41ad9967ea448d7c2b203c699b429abe1ed5af331cd92533900c6d77490e0268 *:0->80/tcp
For more information on how publishing ports works, see publish ports.
You can update almost every configuration detail about an existing service, including the image name and tag it runs. See Update a service's image after creation.
Remove a service
To remove a service, use the docker service remove
command. You can remove a
service by its ID or name, as shown in the output of the docker service ls
command. The following command removes the my_web
service.
$ docker service remove my_web
Service configuration details
The following sections provide details about service configuration. This topic does not cover every flag or scenario. In almost every instance where you can define a configuration at service creation, you can also update an existing service's configuration in a similar way.
See the command-line references for
docker service create
and
docker service update
, or run
one of those commands with the --help
flag.
Configure the runtime environment
You can configure the following options for the runtime environment in the container:
- Environment variables using the
--env
flag - The working directory inside the container using the
--workdir
flag - The username or UID using the
--user
flag
The following service's containers have an environment variable $MYVAR
set to myvalue
, run from the /tmp/
directory, and run as the
my_user
user.
$ docker service create --name helloworld \
--env MYVAR=myvalue \
--workdir /tmp \
--user my_user \
alpine ping docker.com
Update the command an existing service runs
To update the command an existing service runs, you can use the --args
flag.
The following example updates an existing service called helloworld
so that
it runs the command ping docker.com
instead of whatever command it was running
before:
$ docker service update --args ""ping docker.com"" helloworld
Specify the image version a service should use
When you create a service without specifying any details about the version of
the image to use, the service uses the version tagged with the latest
tag.
You can force the service to use a specific version of the image in a few
different ways, depending on your desired outcome.
An image version can be expressed in several different ways:
If you specify a tag, the manager (or the Docker client, if you use content trust) resolves that tag to a digest. When the request to create a container task is received on a worker node, the worker node only sees the digest, not the tag.
$ docker service create --name=""myservice"" ubuntu:16.04
Some tags represent discrete releases, such as
ubuntu:16.04
. Tags like this almost always resolve to a stable digest over time. It is recommended that you use this kind of tag when possible.Other types of tags, such as
latest
ornightly
, may resolve to a new digest often, depending on how often an image's author updates the tag. It is not recommended to run services using a tag which is updated frequently, to prevent different service replica tasks from using different image versions.If you don't specify a version at all, by convention the image's
latest
tag is resolved to a digest. Workers use the image at this digest when creating the service task.Thus, the following two commands are equivalent:
$ docker service create --name=""myservice"" ubuntu $ docker service create --name=""myservice"" ubuntu:latest
If you specify a digest directly, that exact version of the image is always used when creating service tasks.
$ docker service create \ --name=""myservice"" \ ubuntu:16.04@sha256:35bc48a1ca97c3971611dc4662d08d131869daa692acb281c7e9e052924e38b1
When you create a service, the image's tag is resolved to the specific digest
the tag points to at the time of service creation. Worker nodes for that
service use that specific digest forever unless the service is explicitly
updated. This feature is particularly important if you do use often-changing tags
such as latest
, because it ensures that all service tasks use the same version
of the image.
Note
If content trust is enabled, the client actually resolves the image's tag to a digest before contacting the swarm manager, to verify that the image is signed. Thus, if you use content trust, the swarm manager receives the request pre-resolved. In this case, if the client cannot resolve the image to a digest, the request fails.
If the manager can't resolve the tag to a digest, each worker node is responsible for resolving the tag to a digest, and different nodes may use different versions of the image. If this happens, a warning like the following is logged, substituting the placeholders for real information.
unable to pin image <IMAGE-NAME> to digest: <REASON>
To see an image's current digest, issue the command
docker inspect <IMAGE>:<TAG>
and look for the RepoDigests
line. The
following is the current digest for ubuntu:latest
at the time this content
was written. The output is truncated for clarity.
$ docker inspect ubuntu:latest
""RepoDigests"": [
""ubuntu@sha256:35bc48a1ca97c3971611dc4662d08d131869daa692acb281c7e9e052924e38b1""
],
After you create a service, its image is never updated unless you explicitly run
docker service update
with the --image
flag as described below. Other update
operations such as scaling the service, adding or removing networks or volumes,
renaming the service, or any other type of update operation do not update the
service's image.
Update a service's image after creation
Each tag represents a digest, similar to a Git hash. Some tags, such as
latest
, are updated often to point to a new digest. Others, such as
ubuntu:16.04
, represent a released software version and are not expected to
update to point to a new digest often if at all. When you create a service, it
is constrained to create tasks using a specific digest of an image until you
update the service using service update
with the --image
flag.
When you run service update
with the --image
flag, the swarm manager queries
Docker Hub or your private Docker registry for the digest the tag currently
points to and updates the service tasks to use that digest.
Note
If you use content trust, the Docker client resolves image and the swarm manager receives the image and digest, rather than a tag.
Usually, the manager can resolve the tag to a new digest and the service updates, redeploying each task to use the new image. If the manager can't resolve the tag or some other problem occurs, the next two sections outline what to expect.
If the manager resolves the tag
If the swarm manager can resolve the image tag to a digest, it instructs the worker nodes to redeploy the tasks and use the image at that digest.
If a worker has cached the image at that digest, it uses it.
If not, it attempts to pull the image from Docker Hub or the private registry.
If it succeeds, the task is deployed using the new image.
If the worker fails to pull the image, the service fails to deploy on that worker node. Docker tries again to deploy the task, possibly on a different worker node.
If the manager cannot resolve the tag
If the swarm manager cannot resolve the image to a digest, all is not lost:
The manager instructs the worker nodes to redeploy the tasks using the image at that tag.
If the worker has a locally cached image that resolves to that tag, it uses that image.
If the worker does not have a locally cached image that resolves to the tag, the worker tries to connect to Docker Hub or the private registry to pull the image at that tag.
If this succeeds, the worker uses that image.
If this fails, the task fails to deploy and the manager tries again to deploy the task, possibly on a different worker node.
Publish ports
When you create a swarm service, you can publish that service's ports to hosts outside the swarm in two ways:
You can rely on the routing mesh. When you publish a service port, the swarm makes the service accessible at the target port on every node, regardless of whether there is a task for the service running on that node or not. This is less complex and is the right choice for many types of services.
You can publish a service task's port directly on the swarm node where that service is running. This bypasses the routing mesh and provides the maximum flexibility, including the ability for you to develop your own routing framework. However, you are responsible for keeping track of where each task is running and routing requests to the tasks, and load-balancing across the nodes.
Keep reading for more information and use cases for each of these methods.
Publish a service's ports using the routing mesh
To publish a service's ports externally to the swarm, use the
--publish <PUBLISHED-PORT>:<SERVICE-PORT>
flag. The swarm makes the service
accessible at the published port on every swarm node. If an external host
connects to that port on any swarm node, the routing mesh routes it to a task.
The external host does not need to know the IP addresses or internally-used
ports of the service tasks to interact with the service. When a user or process
connects to a service, any worker node running a service task may respond. For
more details about swarm service networking, see
Manage swarm service networks.
Example: Run a three-task Nginx service on 10-node swarm
Imagine that you have a 10-node swarm, and you deploy an Nginx service running three tasks on a 10-node swarm:
$ docker service create --name my_web \
--replicas 3 \
--publish published=8080,target=80 \
nginx
Three tasks run on up to three nodes. You don't need to know which nodes
are running the tasks; connecting to port 8080 on any of the 10 nodes
connects you to one of the three nginx
tasks. You can test this using curl
.
The following example assumes that localhost
is one of the swarm nodes. If
this is not the case, or localhost
does not resolve to an IP address on your
host, substitute the host's IP address or resolvable host name.
The HTML output is truncated:
$ curl localhost:8080
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...truncated...
</html>
Subsequent connections may be routed to the same swarm node or a different one.
Publish a service's ports directly on the swarm node
Using the routing mesh may not be the right choice for your application if you
need to make routing decisions based on application state or you need total
control of the process for routing requests to your service's tasks. To publish
a service's port directly on the node where it is running, use the mode=host
option to the --publish
flag.
Note
If you publish a service's ports directly on the swarm node using
mode=host
and also setpublished=<PORT>
this creates an implicit limitation that you can only run one task for that service on a given swarm node. You can work around this by specifyingpublished
without a port definition, which causes Docker to assign a random port for each task.In addition, if you use
mode=host
and you do not use the--mode=global
flag ondocker service create
, it is difficult to know which nodes are running the service to route work to them.
Example: Run an nginx
web server service on every swarm node
nginx is an open source reverse proxy, load balancer, HTTP cache, and a web server. If you run nginx as a service using the routing mesh, connecting to the nginx port on any swarm node shows you the web page for (effectively) a random swarm node running the service.
The following example runs nginx as a service on each node in your swarm and exposes nginx port locally on each swarm node.
$ docker service create \
--mode global \
--publish mode=host,target=80,published=8080 \
--name=nginx \
nginx:latest
You can reach the nginx server on port 8080 of every swarm node. If you add a node to the swarm, a nginx task is started on it. You cannot start another service or container on any swarm node which binds to port 8080.
Note
This is a purely illustrative example. Creating an application-layer routing framework for a multi-tiered service is complex and out of scope for this topic.
Connect the service to an overlay network
You can use overlay networks to connect one or more services within the swarm.
First, create overlay network on a manager node using the docker network create
command with the --driver overlay
flag.
$ docker network create --driver overlay my-network
After you create an overlay network in swarm mode, all manager nodes have access to the network.
You can create a new service and pass the --network
flag to attach the service
to the overlay network:
$ docker service create \
--replicas 3 \
--network my-network \
--name my-web \
nginx
The swarm extends my-network
to each node running the service.
You can also connect an existing service to an overlay network using the
--network-add
flag.
$ docker service update --network-add my-network my-web
To disconnect a running service from a network, use the --network-rm
flag.
$ docker service update --network-rm my-network my-web
For more information on overlay networking and service discovery, refer to Attach services to an overlay network and Docker swarm mode overlay network security model.
Grant a service access to secrets
To create a service with access to Docker-managed secrets, use the --secret
flag. For more information, see
Manage sensitive strings (secrets) for Docker services
Customize a service's isolation mode
Important
This setting applies to Windows hosts only and is ignored for Linux hosts.
Docker allows you to specify a swarm service's isolation mode. The isolation mode can be one of the following:
default
: Use the default isolation mode configured for the Docker host, as configured by the-exec-opt
flag orexec-opts
array indaemon.json
. If the daemon does not specify an isolation technology,process
is the default for Windows Server, andhyperv
is the default (and only) choice for Windows 10.process
: Run the service tasks as a separate process on the host.Note
process
isolation mode is only supported on Windows Server. Windows 10 only supportshyperv
isolation mode.hyperv
: Run the service tasks as isolatedhyperv
tasks. This increases overhead but provides more isolation.
You can specify the isolation mode when creating or updating a new service using
the --isolation
flag.
Control service placement
Swarm services provide a few different ways for you to control scale and placement of services on different nodes.
You can specify whether the service needs to run a specific number of replicas or should run globally on every worker node. See Replicated or global services.
You can configure the service's CPU or memory requirements, and the service only runs on nodes which can meet those requirements.
Placement constraints let you configure the service to run only on nodes with specific (arbitrary) metadata set, and cause the deployment to fail if appropriate nodes do not exist. For instance, you can specify that your service should only run on nodes where an arbitrary label
pci_compliant
is set totrue
.Placement preferences let you apply an arbitrary label with a range of values to each node, and spread your service's tasks across those nodes using an algorithm. Currently, the only supported algorithm is
spread
, which tries to place them evenly. For instance, if you label each node with a labelrack
which has a value from 1-10, then specify a placement preference keyed onrack
, then service tasks are placed as evenly as possible across all nodes with the labelrack
, after taking other placement constraints, placement preferences, and other node-specific limitations into account.Unlike constraints, placement preferences are best-effort, and a service does not fail to deploy if no nodes can satisfy the preference. If you specify a placement preference for a service, nodes that match that preference are ranked higher when the swarm managers decide which nodes should run the service tasks. Other factors, such as high availability of the service, also factor into which nodes are scheduled to run service tasks. For example, if you have N nodes with the rack label (and then some others), and your service is configured to run N+1 replicas, the +1 is scheduled on a node that doesn't already have the service on it if there is one, regardless of whether that node has the
rack
label or not.
Replicated or global services
Swarm mode has two types of services: replicated and global. For replicated services, you specify the number of replica tasks for the swarm manager to schedule onto available nodes. For global services, the scheduler places one task on each available node that meets the service's placement constraints and resource requirements.
You control the type of service using the --mode
flag. If you don't specify a
mode, the service defaults to replicated
. For replicated services, you specify
the number of replica tasks you want to start using the --replicas
flag. For
example, to start a replicated nginx service with 3 replica tasks:
$ docker service create \
--name my_web \
--replicas 3 \
nginx
To start a global service on each available node, pass --mode global
to
docker service create
. Every time a new node becomes available, the scheduler
places a task for the global service on the new node. For example to start a
service that runs alpine on every node in the swarm:
$ docker service create \
--name myservice \
--mode global \
alpine top
Service constraints let you set criteria for a node to meet before the scheduler
deploys a service to the node. You can apply constraints to the
service based upon node attributes and metadata or engine metadata. For more
information on constraints, refer to the docker service create
CLI reference.
Reserve memory or CPUs for a service
To reserve a given amount of memory or number of CPUs for a service, use the
--reserve-memory
or --reserve-cpu
flags. If no available nodes can satisfy
the requirement (for instance, if you request 4 CPUs and no node in the swarm
has 4 CPUs), the service remains in a pending state until an appropriate node is
available to run its tasks.
Out Of Memory Exceptions (OOME)
If your service attempts to use more memory than the swarm node has available, you may experience an Out Of Memory Exception (OOME) and a container, or the Docker daemon, might be killed by the kernel OOM killer. To prevent this from happening, ensure that your application runs on hosts with adequate memory and see Understand the risks of running out of memory.
Swarm services allow you to use resource constraints, placement preferences, and labels to ensure that your service is deployed to the appropriate swarm nodes.
Placement constraints
Use placement constraints to control the nodes a service can be assigned to. In
the following example, the service only runs on nodes with the
label region
set
to east
. If no appropriately-labelled nodes are available, tasks will wait in
Pending
until they become available. The --constraint
flag uses an equality
operator (==
or !=
). For replicated services, it is possible that all
services run on the same node, or each node only runs one replica, or that some
nodes don't run any replicas. For global services, the service runs on every
node that meets the placement constraint and any
resource requirements.
$ docker service create \
--name my-nginx \
--replicas 5 \
--constraint node.labels.region==east \
nginx
You can also use the constraint
service-level key in a compose.yaml
file.
If you specify multiple placement constraints, the service only deploys onto
nodes where they are all met. The following example limits the service to run on
all nodes where region
is set to east
and type
is not set to devel
:
$ docker service create \
--name my-nginx \
--mode global \
--constraint node.labels.region==east \
--constraint node.labels.type!=devel \
nginx
You can also use placement constraints in conjunction with placement preferences and CPU/memory constraints. Be careful not to use settings that are not possible to fulfill.
For more information on constraints, refer to the docker service create
CLI reference.
Placement preferences
While
placement constraints limit the nodes a service
can run on, placement preferences try to place tasks on appropriate nodes
in an algorithmic way (currently, only spread evenly). For instance, if you
assign each node a rack
label, you can set a placement preference to spread
the service evenly across nodes with the rack
label, by value. This way, if
you lose a rack, the service is still running on nodes on other racks.
Placement preferences are not strictly enforced. If no node has the label you specify in your preference, the service is deployed as though the preference were not set.
Note
Placement preferences are ignored for global services.
The following example sets a preference to spread the deployment across nodes
based on the value of the datacenter
label. If some nodes have
datacenter=us-east
and others have datacenter=us-west
, the service is
deployed as evenly as possible across the two sets of nodes.
$ docker service create \
--replicas 9 \
--name redis_2 \
--placement-pref 'spread=node.labels.datacenter' \
redis:7.4.0
Note
Nodes which are missing the label used to spread still receive task assignments. As a group, these nodes receive tasks in equal proportion to any of the other groups identified by a specific label value. In a sense, a missing label is the same as having the label with a null value attached to it. If the service should only run on nodes with the label being used for the spread preference, the preference should be combined with a constraint.
You can specify multiple placement preferences, and they are processed in the order they are encountered. The following example sets up a service with multiple placement preferences. Tasks are spread first over the various datacenters, and then over racks (as indicated by the respective labels):
$ docker service create \
--replicas 9 \
--name redis_2 \
--placement-pref 'spread=node.labels.datacenter' \
--placement-pref 'spread=node.labels.rack' \
redis:7.4.0
You can also use placement preferences in conjunction with placement constraints or CPU/memory constraints. Be careful not to use settings that are not possible to fulfill.
This diagram illustrates how placement preferences work:
When updating a service with docker service update
, --placement-pref-add
appends a new placement preference after all existing placement preferences.
--placement-pref-rm
removes an existing placement preference that matches the
argument.
Configure a service's update behavior
When you create a service, you can specify a rolling update behavior for how the
swarm should apply changes to the service when you run docker service update
.
You can also specify these flags as part of the update, as arguments to
docker service update
.
The --update-delay
flag configures the time delay between updates to a service
task or sets of tasks. You can describe the time T
as a combination of the
number of seconds Ts
, minutes Tm
, or hours Th
. So 10m30s
indicates a 10
minute 30 second delay.
By default the scheduler updates 1 task at a time. You can pass the
--update-parallelism
flag to configure the maximum number of service tasks
that the scheduler updates simultaneously.
When an update to an individual task returns a state of RUNNING
, the scheduler
continues the update by continuing to another task until all tasks are updated.
If at any time during an update a task returns FAILED
, the scheduler pauses
the update. You can control the behavior using the --update-failure-action
flag for docker service create
or docker service update
.
In the example service below, the scheduler applies updates to a maximum of 2
replicas at a time. When an updated task returns either RUNNING
or FAILED
,
the scheduler waits 10 seconds before stopping the next task to update:
$ docker service create \
--replicas 10 \
--name my_web \
--update-delay 10s \
--update-parallelism 2 \
--update-failure-action continue \
alpine
The --update-max-failure-ratio
flag controls what fraction of tasks can fail
during an update before the update as a whole is considered to have failed. For
example, with --update-max-failure-ratio 0.1 --update-failure-action pause
,
after 10% of the tasks being updated fail, the update is paused.
An individual task update is considered to have failed if the task doesn't
start up, or if it stops running within the monitoring period specified with
the --update-monitor
flag. The default value for --update-monitor
is 30
seconds, which means that a task failing in the first 30 seconds after it's
started counts towards the service update failure threshold, and a failure
after that is not counted.
Roll back to the previous version of a service
In case the updated version of a service doesn't function as expected, it's
possible to manually roll back to the previous version of the service using
docker service update
's --rollback
flag. This reverts the service
to the configuration that was in place before the most recent
docker service update
command.
Other options can be combined with --rollback
; for example,
--update-delay 0s
, to execute the rollback without a delay between tasks:
$ docker service update \
--rollback \
--update-delay 0s
my_web
You can configure a service to roll back automatically if a service update fails to deploy. See Automatically roll back if an update fails.
Manual rollback is handled at the server side, which allows manually-initiated
rollbacks to respect the new rollback parameters. Note that --rollback
cannot
be used in conjunction with other flags to docker service update
.
Automatically roll back if an update fails
You can configure a service in such a way that if an update to the service causes redeployment to fail, the service can automatically roll back to the previous configuration. This helps protect service availability. You can set one or more of the following flags at service creation or update. If you do not set a value, the default is used.
| Flag | Default | Description |
|---|---|---|
--rollback-delay | 0s | Amount of time to wait after rolling back a task before rolling back the next one. A value of 0 means to roll back the second task immediately after the first rolled-back task deploys. |
--rollback-failure-action | pause | When a task fails to roll back, whether to pause or continue trying to roll back other tasks. |
--rollback-max-failure-ratio | 0 | The failure rate to tolerate during a rollback, specified as a floating-point number between 0 and 1. For instance, given 5 tasks, a failure ratio of .2 would tolerate one task failing to roll back. A value of 0 means no failure are tolerated, while a value of 1 means any number of failure are tolerated. |
--rollback-monitor | 5s | Duration after each task rollback to monitor for failure. If a task stops before this time period has elapsed, the rollback is considered to have failed. |
--rollback-parallelism | 1 | The maximum number of tasks to roll back in parallel. By default, one task is rolled back at a time. A value of 0 causes all tasks to be rolled back in parallel. |
The following example configures a redis
service to roll back automatically
if a docker service update
fails to deploy. Two tasks can be rolled back in
parallel. Tasks are monitored for 20 seconds after rollback to be sure they do
not exit, and a maximum failure ratio of 20% is tolerated. Default values are
used for --rollback-delay
and --rollback-failure-action
.
$ docker service create --name=my_redis \
--replicas=5 \
--rollback-parallelism=2 \
--rollback-monitor=20s \
--rollback-max-failure-ratio=.2 \
redis:latest
Give a service access to volumes or bind mounts
For best performance and portability, you should avoid writing important data directly into a container's writable layer. You should instead use data volumes or bind mounts. This principle also applies to services.
You can create two types of mounts for services in a swarm, volume
mounts or
bind
mounts. Regardless of which type of mount you use, configure it using the
--mount
flag when you create a service, or the --mount-add
or --mount-rm
flag when updating an existing service. The default is a data volume if you
don't specify a type.
Data volumes
Data volumes are storage that exist independently of a container. The lifecycle of data volumes under swarm services is similar to that under containers. Volumes outlive tasks and services, so their removal must be managed separately. Volumes can be created before deploying a service, or if they don't exist on a particular host when a task is scheduled there, they are created automatically according to the volume specification on the service.
To use existing data volumes with a service use the --mount
flag:
$ docker service create \
--mount src=<VOLUME-NAME>,dst=<CONTAINER-PATH> \
--name myservice \
<IMAGE>
If a volume with the name <VOLUME-NAME>
doesn't exist when a task is
scheduled to a particular host, then one is created. The default volume
driver is local
. To use a different volume driver with this create-on-demand
pattern, specify the driver and its options with the --mount
flag:
$ docker service create \
--mount type=volume,src=<VOLUME-NAME>,dst=<CONTAINER-PATH>,volume-driver=<DRIVER>,volume-opt=<KEY0>=<VALUE0>,volume-opt=<KEY1>=<VALUE1>
--name myservice \
<IMAGE>
For more information on how to create data volumes and the use of volume drivers, see Use volumes.
Bind mounts
Bind mounts are file system paths from the host where the scheduler deploys the container for the task. Docker mounts the path into the container. The file system path must exist before the swarm initializes the container for the task.
The following examples show bind mount syntax:
To mount a read-write bind:
$ docker service create \ --mount type=bind,src=<HOST-PATH>,dst=<CONTAINER-PATH> \ --name myservice \ <IMAGE>
To mount a read-only bind:
$ docker service create \ --mount type=bind,src=<HOST-PATH>,dst=<CONTAINER-PATH>,readonly \ --name myservice \ <IMAGE>
Important
Bind mounts can be useful but they can also cause problems. In most cases, it is recommended that you architect your application such that mounting paths from the host is unnecessary. The main risks include the following:
If you bind mount a host path into your service’s containers, the path must exist on every swarm node. The Docker swarm mode scheduler can schedule containers on any machine that meets resource availability requirements and satisfies all constraints and placement preferences you specify.
The Docker swarm mode scheduler may reschedule your running service containers at any time if they become unhealthy or unreachable.
Host bind mounts are non-portable. When you use bind mounts, there is no guarantee that your application runs the same way in development as it does in production.
Create services using templates
You can use templates for some flags of service create
, using the syntax
provided by the Go's
text/template
package.
The following flags are supported:
--hostname
--mount
--env
Valid placeholders for the Go template are:
| Placeholder | Description |
|---|---|
.Service.ID | Service ID |
.Service.Name | Service name |
.Service.Labels | Service labels |
.Node.ID | Node ID |
.Node.Hostname | Node hostname |
.Task.Name | Task name |
.Task.Slot | Task slot |
Template example
This example sets the template of the created containers based on the service's name and the ID of the node where the container is running:
$ docker service create --name hosttempl \
--hostname=""{{.Node.ID}}-{{.Service.Name}}""\
busybox top
To see the result of using the template, use the docker service ps
and
docker inspect
commands.
$ docker service ps va8ew30grofhjoychbr6iot8c
ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS
wo41w8hg8qan hosttempl.1 busybox:latest@sha256:29f5d56d12684887bdfa50dcd29fc31eea4aaf4ad3bec43daf19026a7ce69912 2e7a8a9c4da2 Running Running about a minute ago
$ docker inspect --format=""{{.Config.Hostname}}"" hosttempl.1.wo41w8hg8qanxwjwsg4kxpprj",,,
7fd12464b36f81cb94886599cd0c431120f7369bb4a1a8615d5e7b6b0f27b26d,"Specify a project name
In Compose, the default project name is derived from the base name of the project directory. However, you have the flexibility to set a custom project name.
This page offers examples of scenarios where custom project names can be helpful, outlines the various methods to set a project name, and provides the order of precedence for each approach.
Note
The default project directory is the base directory of the Compose file. A custom value can also be set for it using the
--project-directory
command line option.
Example use cases
Compose uses a project name to isolate environments from each other. There are multiple contexts where a project name is useful:
- On a development host: Create multiple copies of a single environment, useful for running stable copies for each feature branch of a project.
- On a CI server: Prevent interference between builds by setting the project name to a unique build number.
- On a shared or development host: Avoid interference between different projects that might share the same service names.
Set a project name
Project names must contain only lowercase letters, decimal digits, dashes, and underscores, and must begin with a lowercase letter or decimal digit. If the base name of the project directory or current directory violates this constraint, alternative mechanisms are available.
The precedence order for each method, from highest to lowest, is as follows:
- The
-p
command line flag. - The COMPOSE_PROJECT_NAME environment variable.
- The
top-level
name:
attribute in your Compose file. Or the lastname:
if you specify multiple Compose files in the command line with the-f
flag. - The base name of the project directory containing your Compose file. Or the base name of the first Compose file if you
specify multiple Compose files in the command line with the
-f
flag. - The base name of the current directory if no Compose file is specified.
What's next?
- Read up on working with multiple Compose files.
- Explore some sample apps.",,,
871b8b5f7defb4710f024b44e1c18b68e87dcb2fbe80c319ee29dfddbe830bf4,"Update billing information
You can update the billing information for your personal account or for an organization. When you update your billing information, these changes apply to future billing invoices. Note that you can't update an existing invoice, including paid and unpaid invoices.
The billing information provided appears on all your billing invoices. The email address provided is where Docker sends all invoices and other billing-related communication.
Important
Starting July 1, 2024, Docker will begin collecting sales tax on subscription fees in compliance with state regulations for customers in the United States. For our global customers subject to VAT, the implementation will start rolling out on July 1, 2024. Note that while the roll out begins on this date, VAT charges may not apply to all applicable subscriptions immediately.
To ensure that tax assessments are correct, make sure that your billing information and VAT/Tax ID, if applicable, are updated. If you're exempt from sales tax, see Register a tax certificate.
Manage billing information
Personal account
To update your billing information:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- Select Billing information from the left-hand navigation.
- On your billing information card, select Change.
- Update your billing contact and billing address information.
- Optional. To add or update a VAT ID, select the I'm purchasing as a business checkbox and enter your Tax ID.
- Select Update.
To update your billing information:
- Sign in to Docker Hub.
- Select your avatar in the top-right corner.
- From the drop-down menu, select Billing.
- Select Billing Address and enter your updated billing information.
- Optional. To add or update a VAT ID, enter your Tax ID/VAT.
- Select Submit.
Organization
Note
You must be an organization owner to make changes to the billing information.
To update your billing information:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- Select Billing information from the left-hand navigation.
- On your billing information card, select Change.
- Update your billing contact and billing address information.
- Optional. To add or update a VAT ID, select the I'm purchasing as a business checkbox and enter your Tax ID.
- Select Update.
To update your billing information:
- Sign in to Docker Hub.
- Select your avatar in the top-right corner.
- From the drop-down menu select Billing.
- Select the organization that you want to change the payment method for.
- Select Billing Address.
- Optional. To add or update a VAT ID, enter your Tax ID/VAT.
- Select Submit.
Update your billing invoice email address
Docker sends the following billing-related emails:
- Confirmation of a new subscription.
- Confirmation of paid invoices.
- Notifications of credit or debit card payment failures.
- Notifications of credit or debit card expiration.
- Confirmation of a cancelled subscription
- Reminders of subscription renewals for annual subscribers. This is sent 14 days before the renewal date.
You can update the email address that receives billing invoices at any time.
Personal account
To update your billing email address:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- Select Billing information from the left-hand navigation.
- On your billing information card, select Change.
- Update your billing contact information and select Update.
To update your billing email address:
- Sign in to Docker Hub.
- Select your avatar in the top-right corner.
- From the drop-down menu select Billing.
- Select Billing Address.
- Update the email address in the Billing contact section.
- Select Submit.
Organizations
To update your billing email address:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- Select Billing information from the left-hand navigation.
- On your billing information card, select Change.
- Update your billing contact information and select Update.
To update your billing email address:
- Sign in to Docker Hub.
- Select your avatar in the top-right corner.
- From the drop-down menu select Billing.
- Select the name of the organization.
- Select Billing Address.
- Update the email address in the Billing contact section.
- Select Submit.",,,
45e18ef2ed59cd182f4fd26efdde0ef5a115d41b7a13f8a402c56b16147f5955,"Checking your build configuration
Build checks are a feature introduced in Dockerfile 1.8. It lets you validate your build configuration and conduct a series of checks prior to executing your build. Think of it as an advanced form of linting for your Dockerfile and build options, or a dry-run mode for builds.
You can find the list of checks available, and a description of each, in the Build checks reference.
How build checks work
Typically, when you run a build, Docker executes the build steps in your Dockerfile and build options as specified. With build checks, rather than executing the build steps, Docker checks the Dockerfile and options you provide and reports any issues it detects.
Build checks are useful for:
- Validating your Dockerfile and build options before running a build.
- Ensuring that your Dockerfile and build options are up-to-date with the latest best practices.
- Identifying potential issues or anti-patterns in your Dockerfile and build options.
Build with checks
Build checks are supported in:
- Buildx version 0.15.0 and later
- docker/build-push-action version 6.6.0 and later
- docker/bake-action version 5.6.0 and later
Invoking a build runs the checks by default, and displays any violations in the build output. For example, the following command both builds the image and runs the checks:
$ docker build .
[+] Building 3.5s (11/11) FINISHED
...
1 warning found (use --debug to expand):
- Lint Rule 'JSONArgsRecommended': JSON arguments recommended for CMD to prevent unintended behavior related to OS signals (line 7)
In this example, the build ran successfully, but a
JSONArgsRecommended warning
was reported, because CMD
instructions should use JSON array syntax.
With the GitHub Actions, the checks display in the diff view of pull requests.
name: Build and push Docker images
on:
push:
jobs:
build:
runs-on: ubuntu-latest
steps:
- name: Build and push
uses: docker/build-push-action@v6.6.0
More verbose output
Check warnings for a regular docker build
display a concise message
containing the rule name, the message, and the line number of where in the
Dockerfile the issue originated. If you want to see more detailed information
about the checks, you can use the --debug
flag. For example:
$ docker --debug build .
[+] Building 3.5s (11/11) FINISHED
...
1 warning found:
- JSONArgsRecommended: JSON arguments recommended for CMD to prevent unintended behavior related to OS signals (line 4)
JSON arguments recommended for ENTRYPOINT/CMD to prevent unintended behavior related to OS signals
More info: https://docs.docker.com/go/dockerfile/rule/json-args-recommended/
Dockerfile:4
--------------------
2 |
3 | FROM alpine
4 | >>> CMD echo ""Hello, world!""
5 |
--------------------
With the --debug
flag, the output includes a link to the documentation for
the check, and a snippet of the Dockerfile where the issue was found.
Check a build without building
To run build checks without actually building, you can use the docker build
command as you typically would, but with the addition of the --check
flag.
Here's an example:
$ docker build --check .
Instead of executing the build steps, this command only runs the checks and reports any issues it finds. If there are any issues, they will be reported in the output. For example:
[+] Building 1.5s (5/5) FINISHED
=> [internal] connecting to local controller
=> [internal] load build definition from Dockerfile
=> => transferring dockerfile: 253B
=> [internal] load metadata for docker.io/library/node:22
=> [auth] library/node:pull token for registry-1.docker.io
=> [internal] load .dockerignore
=> => transferring context: 50B
JSONArgsRecommended - https://docs.docker.com/go/dockerfile/rule/json-args-recommended/
JSON arguments recommended for ENTRYPOINT/CMD to prevent unintended behavior related to OS signals
Dockerfile:7
--------------------
5 |
6 | COPY index.js .
7 | >>> CMD node index.js
8 |
--------------------
This output with --check
shows the
verbose message
for the check.
Unlike a regular build, if any violations are reported when using the --check
flag, the command exits with a non-zero status code.
Fail build on check violations
Check violations for builds are reported as warnings, with exit code 0, by
default. You can configure Docker to fail the build when violations are
reported, using a check=error=true
directive in your Dockerfile. This will
cause the build to error out when after the build checks are run, before the
actual build gets executed.
|
|
Without the # check=error=true
directive, this build would complete with an
exit code of 0. However, with the directive, build check violation results in
non-zero exit code:
$ docker build .
[+] Building 1.5s (5/5) FINISHED
...
1 warning found (use --debug to expand):
- JSONArgsRecommended: JSON arguments recommended for CMD to prevent unintended behavior related to OS signals (line 5)
Dockerfile:1
--------------------
1 | >>> # syntax=docker/dockerfile:1
2 | # check=error=true
3 |
--------------------
ERROR: lint violation found for rules: JSONArgsRecommended
$ echo $?
1
You can also set the error directive on the CLI by passing the
BUILDKIT_DOCKERFILE_CHECK
build argument:
$ docker build --check --build-arg ""BUILDKIT_DOCKERFILE_CHECK=error=true"" .
Skip checks
By default, all checks are run when you build an image. If you want to skip
specific checks, you can use the check=skip
directive in your Dockerfile.
The skip
parameter takes a CSV string of the check IDs you want to skip.
For example:
# syntax=docker/dockerfile:1
# check=skip=JSONArgsRecommended,StageNameCasing
FROM alpine AS BASE_STAGE
CMD echo ""Hello, world!""
Building this Dockerfile results in no check violations.
You can also skip checks by passing the BUILDKIT_DOCKERFILE_CHECK
build
argument with a CSV string of check IDs you want to skip. For example:
$ docker build --check --build-arg ""BUILDKIT_DOCKERFILE_CHECK=skip=JSONArgsRecommended,StageNameCasing"" .
To skip all checks, use the skip=all
parameter:
# syntax=docker/dockerfile:1
# check=skip=all
Combine error and skip parameters for check directives
To both skip specific checks and error on check violations, pass both the
skip
and error
parameters separated by a semi-colon (;
) to the check
directive in your Dockerfile or in a build argument. For example:
# syntax=docker/dockerfile:1
# check=skip=JSONArgsRecommended,StageNameCasing;error=true
$ docker build --check --build-arg ""BUILDKIT_DOCKERFILE_CHECK=skip=JSONArgsRecommended,StageNameCasing;error=true"" .
Experimental checks
Before checks are promoted to stable, they may be available as experimental checks. Experimental checks are disabled by default. To see the list of experimental checks available, refer to the Build checks reference.
To enable all experimental checks, set the BUILDKIT_DOCKERFILE_CHECK
build
argument to experimental=all
:
$ docker build --check --build-arg ""BUILDKIT_DOCKERFILE_CHECK=experimental=all"" .
You can also enable experimental checks in your Dockerfile using the check
directive:
# syntax=docker/dockerfile:1
# check=experimental=all
To selectively enable experimental checks, you can pass a CSV string of the
check IDs you want to enable, either to the check
directive in your Dockerfile
or as a build argument. For example:
# syntax=docker/dockerfile:1
# check=experimental=JSONArgsRecommended,StageNameCasing
Note that the experimental
directive takes precedence over the skip
directive, meaning that experimental checks will run regardless of the skip
directive you have set. For example, if you set skip=all
and enable
experimental checks, the experimental checks will still run:
# syntax=docker/dockerfile:1
# check=skip=all;experimental=all
Further reading
For more information about using build checks, see:",,,
a654d5e442db00a4888033be46553301e72c70e4a4e94eee607d32f0109eca22,"Custom Dockerfile syntax
Dockerfile frontend
BuildKit supports loading frontends dynamically from container images. To use
an external Dockerfile frontend, the first line of your
Dockerfile
needs to set the
syntax
directive
pointing to the specific image you want to use:
# syntax=[remote image reference]
For example:
# syntax=docker/dockerfile:1
# syntax=docker.io/docker/dockerfile:1
# syntax=example.com/user/repo:tag@sha256:abcdef...
You can also use the predefined BUILDKIT_SYNTAX
build argument to set the
frontend image reference on the command line:
$ docker build --build-arg BUILDKIT_SYNTAX=docker/dockerfile:1 .
This defines the location of the Dockerfile syntax that is used to build the Dockerfile. The BuildKit backend allows seamlessly using external implementations that are distributed as Docker images and execute inside a container sandbox environment.
Custom Dockerfile implementations allow you to:
- Automatically get bug fixes without updating the Docker daemon
- Make sure all users are using the same implementation to build your Dockerfile
- Use the latest features without updating the Docker daemon
- Try out new features or third-party features before they are integrated in the Docker daemon
- Use alternative build definitions, or create your own
- Build your own Dockerfile frontend with custom features
Note
BuildKit ships with a built-in Dockerfile frontend, but it's recommended to use an external image to make sure that all users use the same version on the builder and to pick up bug fixes automatically without waiting for a new version of BuildKit or Docker Engine.
Official releases
Docker distributes official versions of the images that can be used for building
Dockerfiles under docker/dockerfile
repository on Docker Hub. There are two
channels where new images are released: stable
and labs
.
Stable channel
The stable
channel follows
semantic versioning.
For example:
docker/dockerfile:1
- kept updated with the latest1.x.x
minor and patch release.docker/dockerfile:1.2
- kept updated with the latest1.2.x
patch release, and stops receiving updates once version1.3.0
is released.docker/dockerfile:1.2.1
- immutable: never updated.
We recommend using docker/dockerfile:1
, which always points to the latest
stable release of the version 1 syntax, and receives both ""minor"" and ""patch""
updates for the version 1 release cycle. BuildKit automatically checks for
updates of the syntax when performing a build, making sure you are using the
most current version.
If a specific version is used, such as 1.2
or 1.2.1
, the Dockerfile needs
to be updated manually to continue receiving bugfixes and new features. Old
versions of the Dockerfile remain compatible with the new versions of the
builder.
Labs channel
The labs
channel provides early access to Dockerfile features that are not yet
available in the stable
channel. labs
images are released at the same time
as stable releases, and follow the same version pattern, but use the -labs
suffix, for example:
docker/dockerfile:labs
- latest release onlabs
channel.docker/dockerfile:1-labs
- same asdockerfile:1
, with experimental features enabled.docker/dockerfile:1.2-labs
- same asdockerfile:1.2
, with experimental features enabled.docker/dockerfile:1.2.1-labs
- immutable: never updated. Same asdockerfile:1.2.1
, with experimental features enabled.
Choose a channel that best fits your needs. If you want to benefit from
new features, use the labs
channel. Images in the labs
channel contain
all the features in the stable
channel, plus early access features.
Stable features in the labs
channel follow
semantic versioning,
but early access features don't, and newer releases may not be backwards
compatible. Pin the version to avoid having to deal with breaking changes.
Other resources
For documentation on labs
features, master builds, and nightly feature
releases, refer to the description in
the BuildKit source repository on GitHub.
For a full list of available images, visit the
docker/dockerfile
repository on Docker Hub,
and the
docker/dockerfile-upstream
repository on Docker Hub
for development builds.",,,
e4775d7e754c2b870175144917d13e69e1599366048f1aa86c4f5897aa2a1927,"Invoke host binaries
In some cases, your extension may need to invoke some command from the host. For example, you might want to invoke the CLI of your cloud provider to create a new resource, or the CLI of a tool your extension provides, or even a shell script that you want to run on the host.
You could do that by executing the CLI from a container with the extension SDK. But this CLI needs to access the host's filesystem, which isn't easy nor fast if it runs in a container.
However host binaries invoke from the extension executables (as binaries, shell scripts) shipped as part of your extension and deployed to the host. As extensions can run on multiple platforms, this means that you need to ship the executables for all the platforms you want to support.
Learn more about extensions architecture.
Note
Only executables shipped as part of the extension can be invoked with the SDK.
In this example, the CLI is a simple Hello world
script that must be invoked with a parameter and returns a
string.
Add the executables to the extension
Create a bash
script for macOS and Linux, in the file binaries/unix/hello.sh
with the following content:
#!/bin/sh
echo ""Hello, $1!""
Create a batch script
for Windows in another file binaries/windows/hello.cmd
with the following content:
@echo off
echo ""Hello, %1!""
Then update the Dockerfile
to copy the binaries
folder into the extension's container filesystem and make the
files executable.
# Copy the binaries into the right folder
COPY --chmod=0755 binaries/windows/hello.cmd /windows/hello.cmd
COPY --chmod=0755 binaries/unix/hello.sh /linux/hello.sh
COPY --chmod=0755 binaries/unix/hello.sh /darwin/hello.sh
Invoke the executable from the UI
In your extension, use the Docker Desktop Client object to
invoke the shell script
provided by the extension with the ddClient.extension.host.cli.exec()
function.
In this example, the binary returns a string as result, obtained by result?.stdout
, as soon as the extension view is rendered.
export function App() {
const ddClient = createDockerDesktopClient();
const [hello, setHello] = useState("""");
useEffect(() => {
const run = async () => {
let binary = ""hello.sh"";
if (ddClient.host.platform === 'win32') {
binary = ""hello.cmd"";
}
const result = await ddClient.extension.host?.cli.exec(binary, [""world""]);
setHello(result?.stdout);
};
run();
}, [ddClient]);
return (
<div>
{hello}
</div>
);
}
Important
We don't have an example for Vue yet. Fill out the form and let us know if you'd like a sample with Vue.
Important
We don't have an example for Angular yet. Fill out the form and let us know if you'd like a sample with Angular.
Important
We don't have an example for Svelte yet. Fill out the form and let us know if you'd like a sample with Svelte.
Configure the metadata file
The host binaries must be specified in the metadata.json
file so that Docker Desktop copies them on to the host when installing
the extension. Once the extension is uninstalled, the binaries that were copied are removed as well.
{
""vm"": {
...
},
""ui"": {
...
},
""host"": {
""binaries"": [
{
""darwin"": [
{
""path"": ""/darwin/hello.sh""
}
],
""linux"": [
{
""path"": ""/linux/hello.sh""
}
],
""windows"": [
{
""path"": ""/windows/hello.cmd""
}
]
}
]
}
}
The path
must reference the path of the binary inside the container.",,,
db62550095512a4c76e0b26e7426650df06d2a1c6a3fb9f21abab59956760230,"Building with Bake from a Compose file
Table of contents
Bake supports the Compose file format to parse a Compose file and translate each service to a target.
# compose.yaml
services:
webapp-dev:
build: &build-dev
dockerfile: Dockerfile.webapp
tags:
- docker.io/username/webapp:latest
cache_from:
- docker.io/username/webapp:cache
cache_to:
- docker.io/username/webapp:cache
webapp-release:
build:
<<: *build-dev
x-bake:
platforms:
- linux/amd64
- linux/arm64
db:
image: docker.io/username/db
build:
dockerfile: Dockerfile.db
$ docker buildx bake --print
{
""group"": {
""default"": {
""targets"": [""db"", ""webapp-dev"", ""webapp-release""]
}
},
""target"": {
""db"": {
""context"": ""."",
""dockerfile"": ""Dockerfile.db"",
""tags"": [""docker.io/username/db""]
},
""webapp-dev"": {
""context"": ""."",
""dockerfile"": ""Dockerfile.webapp"",
""tags"": [""docker.io/username/webapp:latest""],
""cache-from"": [
{
""ref"": ""docker.io/username/webapp:cache"",
""type"": ""registry""
}
],
""cache-to"": [
{
""ref"": ""docker.io/username/webapp:cache"",
""type"": ""registry""
}
]
},
""webapp-release"": {
""context"": ""."",
""dockerfile"": ""Dockerfile.webapp"",
""tags"": [""docker.io/username/webapp:latest""],
""cache-from"": [
{
""ref"": ""docker.io/username/webapp:cache"",
""type"": ""registry""
}
],
""cache-to"": [
{
""ref"": ""docker.io/username/webapp:cache"",
""type"": ""registry""
}
],
""platforms"": [""linux/amd64"", ""linux/arm64""]
}
}
}
The compose format has some limitations compared to the HCL format:
- Specifying variables or global scope attributes is not yet supported
inherits
service field is not supported, but you can use YAML anchors to reference other services, as demonstrated in the previous example with&build-dev
.
.env
file
You can declare default environment variables in an environment file named
.env
. This file will be loaded from the current working directory,
where the command is executed and applied to compose definitions passed
with -f
.
# compose.yaml
services:
webapp:
image: docker.io/username/webapp:${TAG:-v1.0.0}
build:
dockerfile: Dockerfile
# .env
TAG=v1.1.0
$ docker buildx bake --print
{
""group"": {
""default"": {
""targets"": [""webapp""]
}
},
""target"": {
""webapp"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""tags"": [""docker.io/username/webapp:v1.1.0""]
}
}
}
Note
System environment variables take precedence over environment variables in
.env
file.
Extension field with x-bake
Where some fields are not available in the compose specification, you can use
the
special extension field
x-bake
in your compose file to evaluate extra fields:
# compose.yaml
services:
addon:
image: ct-addon:bar
build:
context: .
dockerfile: ./Dockerfile
args:
CT_ECR: foo
CT_TAG: bar
x-bake:
tags:
- ct-addon:foo
- ct-addon:alp
platforms:
- linux/amd64
- linux/arm64
cache-from:
- user/app:cache
- type=local,src=path/to/cache
cache-to:
- type=local,dest=path/to/cache
pull: true
aws:
image: ct-fake-aws:bar
build:
dockerfile: ./aws.Dockerfile
args:
CT_ECR: foo
CT_TAG: bar
x-bake:
secret:
- id=mysecret,src=./secret
- id=mysecret2,src=./secret2
platforms: linux/arm64
output: type=docker
no-cache: true
$ docker buildx bake --print
{
""group"": {
""default"": {
""targets"": [""addon"", ""aws""]
}
},
""target"": {
""addon"": {
""context"": ""."",
""dockerfile"": ""./Dockerfile"",
""args"": {
""CT_ECR"": ""foo"",
""CT_TAG"": ""bar""
},
""tags"": [""ct-addon:foo"", ""ct-addon:alp""],
""cache-from"": [
{
""ref"": ""user/app:cache"",
""type"": ""registry""
},
{
""src"": ""path/to/cache"",
""type"": ""local""
}
],
""cache-to"": [
{
""dest"": ""path/to/cache"",
""type"": ""local""
}
],
""platforms"": [""linux/amd64"", ""linux/arm64""],
""pull"": true
},
""aws"": {
""context"": ""."",
""dockerfile"": ""./aws.Dockerfile"",
""args"": {
""CT_ECR"": ""foo"",
""CT_TAG"": ""bar""
},
""tags"": [""ct-fake-aws:bar""],
""secret"": [
{
""id"": ""mysecret"",
""src"": ""./secret""
},
{
""id"": ""mysecret2"",
""src"": ""./secret2""
}
],
""platforms"": [""linux/arm64""],
""output"": [
{
""type"": ""docker""
}
],
""no-cache"": true
}
}
}
Complete list of valid fields for x-bake
:
cache-from
cache-to
contexts
no-cache
no-cache-filter
output
platforms
pull
secret
ssh
tags",,,
eef1fbf08049a4116526fa6961cd710d6679e404a2ea4bd71b957500cb8781d1,"Docker Engine 19.03 release notes
19.03.15
2021-02-01
Security
- CVE-2021-21285 Prevent an invalid image from crashing docker daemon
- CVE-2021-21284 Lock down file permissions to prevent remapped root from accessing docker state
- Ensure AppArmor and SELinux profiles are applied when building with BuildKit
Client
- Check contexts before importing them to reduce risk of extracted files escaping context store
19.03.14
2020-12-01
Security
- CVE-2020-15257: Update bundled static binaries of containerd to v1.3.9 moby/moby#41731. Package managers should update the containerd.io package.
Builder
- Beta versions of apparmor are now parsed correctly preventing build failures moby/moby#41542
Networking
- Fix panic when swarmkit service keeps failing to start moby/moby#41635
Runtime
- Return correct errors instead of spurious -EINVAL moby/moby#41293
Rootless
- Lock state dir for preventing automatic clean-up by systemd-tmpfiles moby/moby#41635
- dockerd-rootless.sh: support new containerd shim socket path convention moby/moby#41557
Logging
- gcplogs: Fix memory/connection leak moby/moby#41522
- awslogs: Support for AWS imdsv2 moby/moby#41494
19.03.13
2020-09-16
Builder
- buildkit: Fix nil dereference in cache logic moby/moby#41279
- buildkit: Treat Unix sockets as regular files during COPY/ADD moby/moby#41269
- buildkit: Ignore system and security xattrs in calculation to ensure consistent COPY caching regardless of SELinux environment moby/moby#41222
- buildkit: Make
--cache-from
behavior more reliable moby/moby#41222 - buildkit: Fix infinite loop burning CPU when exporting cache moby/moby#41185
Client
- Bump Golang 1.13.15 docker/cli#2674
- Fix config file permission issues (~/.docker/config.json) docker/cli#2631
- build: Fix panic on terminals with zero height docker/cli#2719
- windows: Fix potential issue with newline character in console docker/cli#2623
Networking
- Clean up network sandbox on failure moby/moby#41081
- Fix shallow error messages by forwarding deadline-related errors to user moby/moby#41312
- Fix leaking of netns file descriptors moby/moby#41287
Rootless
- Fix port forwarder resource leak moby/moby#41277
Runtime
- Bump Golang 1.13.15 moby/moby#41334
- Update to containerd 1.3.7 moby/moby#40408
Windows
- Fix slow Windows container start time when using servercore image moby/moby#41192
19.03.12
2020-06-18
Client
- Fix bug preventing logout from registry when using multiple config files (e.g. Windows vs WSL2 when using Docker Desktop) docker/cli#2592
- Fix regression preventing context metadata to be read docker/cli#2586
- Bump Golang 1.13.12 docker/cli#2575
Networking
- Fix regression preventing daemon start up in a systemd-nspawn environment moby/moby#41124 moby/libnetwork#2567
- Fix the retry logic for creating overlay networks in swarm moby/moby#41124 moby/libnetwork#2565
Runtime
- Bump Golang 1.13.12 moby/moby#41082
19.03.11
2020-06-01
Network
Disable IPv6 Router Advertisements to prevent address spoofing. CVE-2020-13401
Description
In the Docker default configuration, the container network interface is a virtual ethernet link going to the host (veth interface).
In this configuration, an attacker able to run a process as root in a container can send and receive arbitrary packets to the host using the CAP_NET_RAW
capability (present in the default configuration).
If IPv6 is not totally disabled on the host (via ipv6.disable=1
on the kernel cmdline), it will be either unconfigured or configured on some interfaces, but it’s pretty likely that ipv6 forwarding is disabled, that is, /proc/sys/net/ipv6/conf//forwarding == 0
. Also by default, /proc/sys/net/ipv6/conf//accept_ra == 1
. The combination of these 2 sysctls means that the host accepts router advertisements and configures the IPv6 stack using them.
By sending “rogue” router advertisements from a container, an attacker can reconfigure the host to redirect part or all of the IPv6 traffic of the host to the attacker-controlled container.
Even if there was no IPv6 traffic before, if the DNS returns A (IPv4) and AAAA (IPv6) records, many HTTP libraries will try to connect via IPv6 first then fallback to IPv4, giving an opportunity to the attacker to respond. If by chance the host has a vulnerability like last year’s RCE in apt (CVE-2019-3462), the attacker can now escalate to the host.
As CAP_NET_ADMIN
is not present by default for Docker containers, the attacker can’t configure the IPs they want to MitM, they can’t use iptables to NAT or REDIRECT the traffic, and they can’t use IP_TRANSPARENT
.
The attacker can however still use CAP_NET_RAW
and implement a tcp/ip stack in user space.
See kubernetes/kubernetes#91507 for related issues.
19.03.10
2020-05-29
Client
- Fix version negotiation with older engine. docker/cli#2538
- Avoid setting SSH flags through hostname. docker/cli#2560
- Fix panic when DOCKER_CLI_EXPERIMENTAL is invalid. docker/cli#2558
- Avoid potential panic on s390x by upgrading Go to 1.13.11. docker/cli#2532
Networking
- Fix DNS fallback regression. moby/moby#41009
Runtime
- Avoid potential panic on s390x by upgrading Go to 1.13.11. moby/moby#40978
Packaging
- Fix ARM builds on ARM64. moby/moby#41027
19.03.9
2020-05-14
Builder
- buildkit: Fix concurrent map write panic when building multiple images in parallel. moby/moby#40780
- buildkit: Fix issue preventing chowning of non-root-owned files between stages with userns. moby/moby#40955
- Avoid creation of irrelevant temporary files on Windows. moby/moby#40877
Client
- Fix panic on single-character volumes. docker/cli#2471
- Lazy daemon feature detection to avoid long timeouts on simple commands. docker/cli#2442
- docker context inspect on Windows is now faster. docker/cli#2516
- Bump Golang 1.13.10. docker/cli#2431
- Bump gopkg.in/yaml.v2 to v2.2.8. docker/cli#2470
Logging
- Avoid situation preventing container logs to rotate due to closing a closed log file. moby/moby#40921
Networking
- Fix potential panic upon restart. moby/moby#40809
- Assign the correct network value to the default bridge Subnet field. moby/moby#40565
Runtime
- Fix docker crash when creating namespaces with UID in /etc/subuid and /etc/subgid. moby/moby#40562
- Improve ARM platform matching. moby/moby#40758
- overlay2: show backing filesystem. moby/moby#40652
- Update CRIU to v3.13 ""Silicon Willet"". moby/moby#40850
- Only show registry v2 schema1 deprecation warning upon successful fallback, as opposed to any registry error. moby/moby#40681
- Use FILE_SHARE_DELETE for log files on Windows. moby/moby#40563
- Bump Golang 1.13.10. moby/moby#40803
Rootless
- Now rootlesskit-docker-proxy returns detailed error message on exposing privileged ports. moby/moby#40863
- Supports numeric ID in /etc/subuid and /etc/subgid. moby/moby#40951
Security
- apparmor: add missing rules for userns. moby/moby#40564
- SElinux: fix ENOTSUP errors not being detected when relabeling. moby/moby#40946
Swarm
- Increase refill rate for logger to avoid hanging on service logs. moby/moby#40628
- Fix issue where single swarm manager is stuck in Down state after reboot. moby/moby#40831
- tasks.db no longer grows indefinitely. moby/moby#40831
19.03.8
2020-03-10
Runtime
- Improve mitigation for CVE-2019-14271 for some nscd configuration.
19.03.7
2020-03-03
Builder
- builder-next: Fix deadlock issues in corner cases. moby/moby#40557
Runtime
- overlay: remove modprobe execs. moby/moby#40462
- selinux: display better error messages when setting file labels. moby/moby#40547
- Speed up initial stats collection. moby/moby#40549
- rootless: use certs.d from XDG_CONFIG_HOME. moby/moby#40461
- Bump Golang 1.12.17. moby/moby#40533
- Bump google.golang.org/grpc to v1.23.1. moby/moby#40566
- Update containerd binary to v1.2.13. moby/moby#40540
- Prevent showing stopped containers as running in an edge case. moby/moby#40555
- Prevent potential lock. moby/moby#40604
Client
- Bump Golang 1.12.17. docker/cli#2342
- Bump google.golang.org/grpc to v1.23.1. docker/cli#1884 docker/cli#2373
19.03.6
2020-02-12
Builder
- builder-next: Allow modern sign hashes for ssh forwarding. docker/engine#453
- builder-next: Clear onbuild rules after triggering. docker/engine#453
- builder-next: Fix issue with directory permissions when usernamespaces is enabled. moby/moby#40440
- Bump hcsshim to fix docker build failing on Windows 1903. docker/engine#429
Networking
- Shorten controller ID in exec-root to not hit UNIX_PATH_MAX. docker/engine#424
- Fix panic in drivers/overlay/encryption.go. docker/engine#424
- Fix hwaddr set race between us and udev. docker/engine#439
Runtime
- Bump Golang 1.12.16. moby/moby#40433
- Update containerd binary to v1.2.12. moby/moby#40433
- Update to runc v1.0.0-rc10. moby/moby#40433
- Fix possible runtime panic in Lgetxattr. docker/engine#454
- rootless: fix proxying UDP packets. docker/engine#434
19.03.5
2019-11-14
Builder
- builder-next: Added
entitlements
in builder config. docker/engine#412 - Fix builder-next: permission errors on using build secrets or ssh forwarding with userns-remap. docker/engine#420
- Fix builder-next: copying a symlink inside an already copied directory. docker/engine#420
Packaging
- Support RHEL 8 packages
Runtime
- Bump Golang to 1.12.12. docker/engine#418
- Update to RootlessKit to v0.7.0 to harden slirp4netns with mount namespace and seccomp. docker/engine#397
- Fix to propagate GetContainer error from event processor. docker/engine#407
- Fix push of OCI image. docker/engine#405
19.03.4
2019-10-17
Networking
- Rollback libnetwork changes to fix
DOCKER-USER
iptables chain issue. docker/engine#404
Known Issues
Existing
- In some circumstances with large clusters, Docker information might, as part of the Swarm section,
include the error
code = ResourceExhausted desc = grpc: received message larger than max (5351376 vs. 4194304)
. This does not indicate any failure or misconfiguration by the user, and requires no response. - Orchestrator port conflict can occur when redeploying all services as new. Due to many Swarm manager
requests in a short amount of time, some services are not able to receive traffic and are causing a
404
error after being deployed.- Workaround: restart all tasks via
docker service update --force
.
- Workaround: restart all tasks via
- CVE-2018-15664 symlink-exchange attack with directory traversal. Workaround until proper fix is available in upcoming patch release:
docker pause
container before doing file operations. moby/moby#39252 docker cp
regression due to CVE mitigation. An error is produced when the source ofdocker cp
is set to/
.
19.03.3
2019-10-08
Security
- Patched
runc
in containerd. CVE-2017-18367
Builder
Fix builder-next: resolve digest for third party registries. docker/engine#339
Fix builder-next: user namespace builds when daemon started with socket activation. docker/engine#373
Fix builder-next; session: release forwarded ssh socket connection per connection. docker/engine#373
Fix build-next: llbsolver: error on multiple cache importers. docker/engine#373
Client
Added support for Docker Template 0.1.6.
Mitigate against YAML files that have excessive aliasing. docker/cli#2119
Runtime
Bump Golang to 1.12.10. docker/engine#387
Bump containerd to 1.2.10. docker/engine#385
Distribution: modify warning logic when pulling v2 schema1 manifests. docker/engine#368
Fix
POST /images/create
returning a 500 status code when providing an incorrect platform option. docker/engine#365Fix
POST /build
returning a 500 status code when providing an incorrect platform option. docker/engine#365Fix panic on 32-bit ARMv7 caused by misaligned struct member. docker/engine#363
Fix to return ""invalid parameter"" when linking to non-existing container. docker/engine#352
Fix overlay2: busy error on mount when using kernel >= 5.2. docker/engine#332
Fix
docker rmi
stuck in certain misconfigured systems, e.g. dead NFS share. docker/engine#335Fix handling of blocked I/O of exec'd processes. docker/engine#296
Fix jsonfile logger: follow logs stuck when
max-size
is set andmax-file=1
. docker/engine#378
Known Issues
New
DOCKER-USER
iptables chain is missing: docker/for-linux#810. Users cannot perform additional container network traffic filtering on top of this iptables chain. You are not affected by this issue if you are not customizing iptable chains on top ofDOCKER-USER
.- Workaround: Insert the iptables chain after the docker daemon starts.
For example:
iptables -N DOCKER-USER iptables -I FORWARD -j DOCKER-USER iptables -A DOCKER-USER -j RETURN
- Workaround: Insert the iptables chain after the docker daemon starts.
For example:
Existing
- In some circumstances with large clusters, docker information might, as part of the Swarm section,
include the error
code = ResourceExhausted desc = grpc: received message larger than max (5351376 vs. 4194304)
. This does not indicate any failure or misconfiguration by the user, and requires no response. - Orchestrator port conflict can occur when redeploying all services as new. Due to many swarm manager
requests in a short amount of time, some services are not able to receive traffic and are causing a
404
error after being deployed.- Workaround: restart all tasks via
docker service update --force
.
- Workaround: restart all tasks via
- CVE-2018-15664 symlink-exchange attack with directory traversal. Workaround until proper fix is available in upcoming patch release:
docker pause
container before doing file operations. moby/moby#39252 docker cp
regression due to CVE mitigation. An error is produced when the source ofdocker cp
is set to/
.
19.03.2
2019-09-03
Builder
Fix
COPY --from
to non-existing directory on Windows. moby/moby#39695Fix builder-next: metadata commands not having created time in history. moby/moby#39456
Fix builder-next: close progress on layer export error. moby/moby#39782
Update buildkit to 588c73e1e4. moby/moby#39781
Client
Fix Windows absolute path detection on non-Windows docker/cli#1990
Fix to zsh completion script for
docker login --username
.Fix context: produce consistent output on
context create
. docker/cli#1985Fix support for HTTP proxy env variable. docker/cli#2059
Logging
- Fix for reading journald logs. moby/moby#37819 moby/moby#38859
Networking
- Prevent panic on network attached to a container with disabled networking. moby/moby#39589
Runtime
Bump Golang to 1.12.8.
Fix a potential engine panic when using XFS disk quota for containers. moby/moby#39644
Swarm
- Fix an issue where nodes with several tasks could not be removed. docker/swarmkit#2867
Known issues
In some circumstances with large clusters, docker information might, as part of the Swarm section, include the error
code = ResourceExhausted desc = grpc: received message larger than max (5351376 vs. 4194304)
. This does not indicate any failure or misconfiguration by the user, and requires no response.Orchestrator port conflict can occur when redeploying all services as new. Due to many swarm manager requests in a short amount of time, some services are not able to receive traffic and are causing a
404
error after being deployed.- Workaround: restart all tasks via
docker service update --force
.
- Workaround: restart all tasks via
Traffic cannot egress the HOST because of missing Iptables rules in the FORWARD chain The missing rules are :
/sbin/iptables --wait -C FORWARD -o docker_gwbridge -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT /sbin/iptables --wait -C FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
- Workaround: Add these rules back using a script and cron definitions. The script
must contain '-C' commands to check for the presence of a rule and '-A' commands to add
rules back. Run the script on a cron in regular intervals, for example, every
minutes. - Affected versions: 18.09.1, 19.03.0
- Workaround: Add these rules back using a script and cron definitions. The script
must contain '-C' commands to check for the presence of a rule and '-A' commands to add
rules back. Run the script on a cron in regular intervals, for example, every
CVE-2018-15664 symlink-exchange attack with directory traversal. Workaround until proper fix is available in upcoming patch release:
docker pause
container before doing file operations. moby/moby#39252docker cp
regression due to CVE mitigation. An error is produced when the source ofdocker cp
is set to/
.
19.03.1
2019-07-25
Security
- Fixed loading of nsswitch based config inside chroot under Glibc. CVE-2019-14271
Known issues
In some circumstances, in large clusters, docker information might, as part of the Swarm section, include the error
code = ResourceExhausted desc = grpc: received message larger than max (5351376 vs. 4194304)
. This does not indicate any failure or misconfiguration by the user, and requires no response.Orchestrator port conflict can occur when redeploying all services as new. Due to many swarm manager requests in a short amount of time, some services are not able to receive traffic and are causing a
404
error after being deployed.- Workaround: restart all tasks via
docker service update --force
.
- Workaround: restart all tasks via
Traffic cannot egress the HOST because of missing Iptables rules in the FORWARD chain The missing rules are :
/sbin/iptables --wait -C FORWARD -o docker_gwbridge -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT /sbin/iptables --wait -C FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
- Workaround: Add these rules back using a script and cron definitions. The script
must contain '-C' commands to check for the presence of a rule and '-A' commands to add
rules back. Run the script on a cron in regular intervals, for example, every
minutes. - Affected versions: 18.09.1, 19.03.0
- Workaround: Add these rules back using a script and cron definitions. The script
must contain '-C' commands to check for the presence of a rule and '-A' commands to add
rules back. Run the script on a cron in regular intervals, for example, every
CVE-2018-15664 symlink-exchange attack with directory traversal. Workaround until proper fix is available in upcoming patch release:
docker pause
container before doing file operations. moby/moby#39252docker cp
regression due to CVE mitigation. An error is produced when the source ofdocker cp
is set to/
.
19.03.0
2019-07-22
Builder
Fixed
COPY --from
to preserve ownership. moby/moby#38599builder-next:
- Added inline cache support
--cache-from
. docker/engine#215 - Outputs configuration allowed. moby/moby#38898
- Fixed gcr workaround token cache. docker/engine#212
stopprogress
called on download error. docker/engine#215- Buildkit now uses systemd's
resolv.conf
. docker/engine#260. - Setting buildkit outputs now allowed. docker/cli#1766
- Look for Dockerfile specific dockerignore file (for example, Dockerfile.dockerignore) for ignored paths. docker/engine#215
- Automatically detect if process execution is possible for x86, arm, and arm64 binaries. docker/engine#215
- Updated buildkit to 1f89ec1. docker/engine#260
- Use Dockerfile frontend version
docker/dockerfile:1.1
by default. docker/engine#215 - No longer rely on an external image for COPY/ADD operations. docker/engine#215
- Added inline cache support
Client
- Added
--pids-limit
flag todocker update
. docker/cli#1765 - Added systctl support for services. docker/cli#1754
- Added support for
template_driver
in compose files. docker/cli#1746 - Added
--device
support for Windows. docker/cli#1606 - Added support for Data Path Port configuration. docker/cli#1509
- Added fast context switch: commands. docker/cli#1501
- Support added for
--mount type=bind,bind-nonrecursive,...
docker/cli#1430 - Added maximum replicas per node. docker/cli#1612
- Added option to pull images quietly. docker/cli#882
- Added a separate
--domainname
flag. docker/cli#1130 - Added support for secret drivers in
docker stack deploy
. docker/cli#1783 - Added ability to use swarm
Configs
asCredentialSpecs
on services. docker/cli#1781 - Added
--security-opt systempaths=unconfined
support. docker/cli#1808 - Added basic framework for writing and running CLI plugins. docker/cli#1564 docker/cli#1898
- Bumped Docker App to v0.8.0. docker/docker-ce-packaging#341
- Added support for Docker buildx. docker/docker-ce-packaging#336
- Added support for Docker Assemble v0.36.0.
- Added support for Docker Cluster v1.0.0-rc2.
- Added support for Docker Template v0.1.4.
- Added support for Docker Registry v0.1.0-rc1.
- Bumped google.golang.org/grpc to v1.20.1. docker/cli#1884
- CLI changed to pass driver specific options to
docker run
. docker/cli#1767 - Bumped Golang 1.12.5. docker/cli#1875
docker system info
output now segregates information relevant to the client and daemon. docker/cli#1638- (Experimental) When targeting Kubernetes, added support for
x-pull-secret: some-pull-secret
in compose-files service configs. docker/cli#1617 - (Experimental) When targeting Kubernetes, added support for
x-pull-policy: <Never|Always|IfNotPresent>
in compose-files service configs. docker/cli#1617 - cp, save, export: Now preventing overwriting irregular files. docker/cli#1515
- npipe volume type on stack file now allowed. docker/cli#1195
- Fixed tty initial size error. docker/cli#1529
- Fixed problem with labels copying value from environment variables. docker/cli#1671
API
- Updated API version to v1.40. moby/moby#38089
- Added warnings to
/info
endpoint, and moved detection to the daemon. moby/moby#37502 - Added HEAD support for
/_ping
endpoint. moby/moby#38570 - Added
Cache-Control
headers to disable caching/_ping
endpoint. moby/moby#38569 - Added
containerd
,runc
, anddocker-init
versions to/version
. moby/moby#37974 - Added undocumented
/grpc
endpoint and registered BuildKit's controller. moby/moby#38990
Experimental
- Enabled checkpoint/restore of containers with TTY. moby/moby#38405
- LCOW: Added support for memory and CPU limits. moby/moby#37296
- Windows: Added ContainerD runtime. moby/moby#38541
- Windows: LCOW now requires Windows RS5+. moby/moby#39108
Security
- mount: added BindOptions.NonRecursive (API v1.40). moby/moby#38003
- seccomp: whitelisted
io_pgetevents()
. moby/moby#38895 - seccomp:
ptrace(2)
for 4.8+ kernels now allowed. moby/moby#38137
Runtime
- Running
dockerd
as a non-root user (Rootless mode) is now allowed. moby/moby#380050 - Rootless: optional support provided for
lxc-user-nic
SUID binary. docker/engine#208 - Added DeviceRequests to HostConfig to support NVIDIA GPUs. moby/moby#38828
- Added
--device
support for Windows. moby/moby#37638 - Added
memory.kernelTCP
support for linux. moby/moby#37043 - Windows credential specs can now be passed directly to the engine. moby/moby#38777
- Added pids-limit support in docker update. moby/moby#32519
- Added support for exact list of capabilities. moby/moby#38380
- daemon: Now use 'private' ipc mode by default. moby/moby#35621
- daemon: switched to semaphore-gated WaitGroup for startup tasks. moby/moby#38301
- Now use
idtools.LookupGroup
instead of parsing/etc/group
file for docker.sock ownership to fix:api.go doesn't respect nsswitch.conf
. moby/moby#38126 - cli: fixed images filter when using multi reference filter. moby/moby#38171
- Bumped Golang to 1.12.5. docker/engine#209
- Bumped
containerd
to 1.2.6. moby/moby#39016 - Bumped
runc
to 1.0.0-rc8, opencontainers/selinux v1.2.2. docker/engine#210 - Bumped
google.golang.org/grpc
to v1.20.1. docker/engine#215 - Performance optimized in aufs and layer store for massively parallel container creation/removal. moby/moby#39135 moby/moby#39209
- Root is now passed to chroot for chroot Tar/Untar (CVE-2018-15664) moby/moby#39292
- Fixed
docker --init
with /dev bind mount. moby/moby#37665 - The right device number is now fetched when greater than 255 and using the
--device-read-bps
option. moby/moby#39212 - Fixed
Path does not exist
error when path definitely exists. moby/moby#39251
Networking
- Moved IPVLAN driver out of experimental. moby/moby#38983
- Added support for 'dangling' filter. moby/moby#31551 docker/libnetwork#2230
- Load balancer sandbox is now deleted when a service is updated with
--network-rm
. docker/engine#213 - Windows: Now forcing a nil IP specified in
PortBindings
to IPv4zero (0.0.0.0). docker/libnetwork#2376
Swarm
- Added support for maximum replicas per node. moby/moby#37940
- Added support for GMSA CredentialSpecs from Swarmkit configs. moby/moby#38632
- Added support for sysctl options in services. moby/moby#37701
- Added support for filtering on node labels. moby/moby#37650
- Windows: Support added for named pipe mounts in docker service create + stack yml. moby/moby#37400
- VXLAN UDP Port configuration now supported. moby/moby#38102
- Now using Service Placement Constraints in Enforcer. docker/swarmkit#2857
- Increased max recv gRPC message size for nodes and secrets. docker/engine#256
Logging
- Enabled gcplogs driver on Windows. moby/moby#37717
- Added zero padding for RFC5424 syslog format. moby/moby#38335
- Added
IMAGE_NAME
attribute tojournald
log events. moby/moby#38032
Deprecation
- Deprecate image manifest v2 schema1 in favor of v2 schema2. Future version of Docker will remove support for v2 schema1 althogether. moby/moby#39365
- Removed v1.10 migrator. moby/moby#38265
- Now skipping deprecated storage-drivers in auto-selection. moby/moby#38019
- Deprecated
aufs
storage driver and added warning. moby/moby#38090 - Removed support for 17.09.
For more information on deprecated flags and APIs, refer to deprecation information for target removal dates.
Known issues
In some circumstances with large clusters, docker information might, as part of the Swarm section, include the error
code = ResourceExhausted desc = grpc: received message larger than max (5351376 vs. 4194304)
. This does not indicate any failure or misconfiguration by the user, and requires no response.Orchestrator port conflict can occur when redeploying all services as new. Due to many swarm manager requests in a short amount of time, some services are not able to receive traffic and are causing a
404
error after being deployed.- Workaround: restart all tasks via
docker service update --force
.
- Workaround: restart all tasks via
Traffic cannot egress the HOST because of missing Iptables rules in the FORWARD chain The missing rules are :
/sbin/iptables --wait -C FORWARD -o docker_gwbridge -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT /sbin/iptables --wait -C FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
- Workaround: Add these rules back using a script and cron definitions. The script
must contain '-C' commands to check for the presence of a rule and '-A' commands to add
rules back. Run the script on a cron in regular intervals, for example, every
minutes. - Affected versions: 18.09.1, 19.03.0
- Workaround: Add these rules back using a script and cron definitions. The script
must contain '-C' commands to check for the presence of a rule and '-A' commands to add
rules back. Run the script on a cron in regular intervals, for example, every
CVE-2018-15664 symlink-exchange attack with directory traversal. Workaround until proper fix is available in upcoming patch release:
docker pause
container before doing file operations. moby/moby#39252docker cp
regression due to CVE mitigation. An error is produced when the source ofdocker cp
is set to/
.",,,
3a0608cc91f4af6dcabb9814a30bcd0de459e062a60fb6dc9d72f1d099d44f35,"Bake file reference
The Bake file is a file for defining workflows that you run using docker buildx bake
.
File format
You can define your Bake file in the following file formats:
- HashiCorp Configuration Language (HCL)
- JSON
- YAML (Compose file)
By default, Bake uses the following lookup order to find the configuration file:
compose.yaml
compose.yml
docker-compose.yml
docker-compose.yaml
docker-bake.json
docker-bake.hcl
docker-bake.override.json
docker-bake.override.hcl
You can specify the file location explicitly using the --file
flag:
$ docker buildx bake --file ../docker/bake.hcl --print
If you don't specify a file explicitly, Bake searches for the file in the
current working directory. If more than one Bake file is found, all files are
merged into a single definition. Files are merged according to the lookup
order. That means that if your project contains both a compose.yaml
file and
a docker-bake.hcl
file, Bake loads the compose.yaml
file first, and then
the docker-bake.hcl
file.
If merged files contain duplicate attribute definitions, those definitions are either merged or overridden by the last occurrence, depending on the attribute. The following attributes are overridden by the last occurrence:
target.cache-to
target.dockerfile-inline
target.dockerfile
target.outputs
target.platforms
target.pull
target.tags
target.target
For example, if compose.yaml
and docker-bake.hcl
both define the tags
attribute, the docker-bake.hcl
is used.
$ cat compose.yaml
services:
webapp:
build:
context: .
tags:
- bar
$ cat docker-bake.hcl
target ""webapp"" {
tags = [""foo""]
}
$ docker buildx bake --print webapp
{
""group"": {
""default"": {
""targets"": [
""webapp""
]
}
},
""target"": {
""webapp"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""tags"": [
""foo""
]
}
}
}
All other attributes are merged. For example, if compose.yaml
and
docker-bake.hcl
both define unique entries for the labels
attribute, all
entries are included. Duplicate entries for the same label are overridden.
$ cat compose.yaml
services:
webapp:
build:
context: .
labels:
com.example.foo: ""foo""
com.example.name: ""Alice""
$ cat docker-bake.hcl
target ""webapp"" {
labels = {
""com.example.bar"" = ""bar""
""com.example.name"" = ""Bob""
}
}
$ docker buildx bake --print webapp
{
""group"": {
""default"": {
""targets"": [
""webapp""
]
}
},
""target"": {
""webapp"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""labels"": {
""com.example.foo"": ""foo"",
""com.example.bar"": ""bar"",
""com.example.name"": ""Bob""
}
}
}
}
Syntax
The Bake file supports the following property types:
target
: build targetsgroup
: collections of build targetsvariable
: build arguments and variablesfunction
: custom Bake functions
You define properties as hierarchical blocks in the Bake file. You can assign one or more attributes to a property.
The following snippet shows a JSON representation of a simple Bake file. This Bake file defines three properties: a variable, a group, and a target.
{
""variable"": {
""TAG"": {
""default"": ""latest""
}
},
""group"": {
""default"": {
""targets"": [""webapp""]
}
},
""target"": {
""webapp"": {
""dockerfile"": ""Dockerfile"",
""tags"": [""docker.io/username/webapp:${TAG}""]
}
}
}
In the JSON representation of a Bake file, properties are objects, and attributes are values assigned to those objects.
The following example shows the same Bake file in the HCL format:
variable ""TAG"" {
default = ""latest""
}
group ""default"" {
targets = [""webapp""]
}
target ""webapp"" {
dockerfile = ""Dockerfile""
tags = [""docker.io/username/webapp:${TAG}""]
}
HCL is the preferred format for Bake files. Aside from syntactic differences, HCL lets you use features that the JSON and YAML formats don't support.
The examples in this document use the HCL format.
Target
A target reflects a single docker build
invocation.
Consider the following build command:
$ docker build \
--file=Dockerfile.webapp \
--tag=docker.io/username/webapp:latest \
https://github.com/username/webapp
You can express this command in a Bake file as follows:
target ""webapp"" {
dockerfile = ""Dockerfile.webapp""
tags = [""docker.io/username/webapp:latest""]
context = ""https://github.com/username/webapp""
}
The following table shows the complete list of attributes that you can assign to a target:
| Name | Type | Description |
|---|---|---|
args | Map | Build arguments |
annotations | List | Exporter annotations |
attest | List | Build attestations |
cache-from | List | External cache sources |
cache-to | List | External cache destinations |
call | String | Specify the frontend method to call for the target. |
context | String | Set of files located in the specified path or URL |
contexts | Map | Additional build contexts |
description | String | Description of a target |
dockerfile-inline | String | Inline Dockerfile string |
dockerfile | String | Dockerfile location |
inherits | List | Inherit attributes from other targets |
labels | Map | Metadata for images |
matrix | Map | Define a set of variables that forks a target into multiple targets. |
name | String | Override the target name when using a matrix. |
no-cache-filter | List | Disable build cache for specific stages |
no-cache | Boolean | Disable build cache completely |
output | List | Output destinations |
platforms | List | Target platforms |
pull | Boolean | Always pull images |
secret | List | Secrets to expose to the build |
shm-size | List | Size of /dev/shm |
ssh | List | SSH agent sockets or keys to expose to the build |
tags | List | Image names and tags |
target | String | Target build stage |
ulimits | List | Ulimit options |
target.args
Use the args
attribute to define build arguments for the target.
This has the same effect as passing a
--build-arg
flag to the build command.
target ""default"" {
args = {
VERSION = ""0.0.0+unknown""
}
}
You can set args
attributes to use null
values.
Doing so forces the target
to use the ARG
value specified in the Dockerfile.
variable ""GO_VERSION"" {
default = ""1.20.3""
}
target ""webapp"" {
dockerfile = ""webapp.Dockerfile""
tags = [""docker.io/username/webapp""]
}
target ""db"" {
args = {
GO_VERSION = null
}
dockerfile = ""db.Dockerfile""
tags = [""docker.io/username/db""]
}
target.annotations
The annotations
attribute lets you add annotations to images built with bake.
The key takes a list of annotations, in the format of KEY=VALUE
.
target ""default"" {
output = [{ type = ""image"", name = ""foo"" }]
annotations = [""org.opencontainers.image.authors=dvdksn""]
}
By default, the annotation is added to image manifests. You can configure the level of the annotations by adding a prefix to the annotation, containing a comma-separated list of all the levels that you want to annotate. The following example adds annotations to both the image index and manifests.
target ""default"" {
output = [{ type = ""image"", name = ""foo"" }]
annotations = [""index,manifest:org.opencontainers.image.authors=dvdksn""]
}
Read about the supported levels in Specifying annotation levels.
target.attest
The attest
attribute lets you apply
build attestations to the target.
This attribute accepts the long-form CSV version of attestation parameters.
target ""default"" {
attest = [
{
type = ""provenance"",
mode = ""max"",
},
{
type = ""sbom"",
}
]
}
target.cache-from
Build cache sources.
The builder imports cache from the locations you specify.
It uses the
Buildx cache storage backends,
and it works the same way as the
--cache-from
flag.
This takes a list value, so you can specify multiple cache sources.
target ""app"" {
cache-from = [
{
type = ""s3"",
region = ""eu-west-1"",
bucket = ""mybucket""
},
{
type = ""registry"",
ref = ""user/repo:cache""
}
]
}
target.cache-to
Build cache export destinations.
The builder exports its build cache to the locations you specify.
It uses the
Buildx cache storage backends,
and it works the same way as the
--cache-to
flag.
This takes a list value, so you can specify multiple cache export targets.
target ""app"" {
cache-to = [
{
type = ""s3"",
region = ""eu-west-1"",
bucket = ""mybucket""
},
{
type = ""inline"",
}
]
}
target.call
Specifies the frontend method to use. Frontend methods let you, for example,
execute build checks only, instead of running a build. This is the same as the
--call
flag.
target ""app"" {
call = ""check""
}
Supported values are:
build
builds the target (default)check
: evaluates build checks for the targetoutline
: displays the target's build arguments and their default values if availabletargets
: lists all Bake targets in the loaded definition, along with its description.
For more information about frontend methods, refer to the CLI reference for
docker buildx build --call
.
target.context
Specifies the location of the build context to use for this target. Accepts a URL or a directory path. This is the same as the build context positional argument that you pass to the build command.
target ""app"" {
context = ""./src/www""
}
This resolves to the current working directory ("".""
) by default.
$ docker buildx bake --print -f - <<< 'target ""default"" {}'
[+] Building 0.0s (0/0)
{
""target"": {
""default"": {
""context"": ""."",
""dockerfile"": ""Dockerfile""
}
}
}
target.contexts
Additional build contexts.
This is the same as the
--build-context
flag.
This attribute takes a map, where keys result in named contexts that you can
reference in your builds.
You can specify different types of contexts, such local directories, Git URLs, and even other Bake targets. Bake automatically determines the type of a context based on the pattern of the context value.
| Context type | Example |
|---|---|
| Container image | docker-image://alpine@sha256:0123456789 |
| Git URL | https://github.com/user/proj.git |
| HTTP URL | https://example.com/files |
| Local directory | ../path/to/src |
| Bake target | target:base |
Pin an image version
# docker-bake.hcl
target ""app"" {
contexts = {
alpine = ""docker-image://alpine:3.13""
}
}
# Dockerfile
FROM alpine
RUN echo ""Hello world""
Use a local directory
# docker-bake.hcl
target ""app"" {
contexts = {
src = ""../path/to/source""
}
}
# Dockerfile
FROM scratch AS src
FROM golang
COPY --from=src . .
Use another target as base
Note
You should prefer to use regular multi-stage builds over this option. You can Use this feature when you have multiple Dockerfiles that can't be easily merged into one.
# docker-bake.hcl
target ""base"" {
dockerfile = ""baseapp.Dockerfile""
}
target ""app"" {
contexts = {
baseapp = ""target:base""
}
}
# Dockerfile
FROM baseapp
RUN echo ""Hello world""
target.description
Defines a human-readable description for the target, clarifying its purpose or functionality.
target ""lint"" {
description = ""Runs golangci-lint to detect style errors""
args = {
GOLANGCI_LINT_VERSION = null
}
dockerfile = ""lint.Dockerfile""
}
This attribute is useful when combined with the docker buildx bake --list=targets
option, providing a more informative output when listing the available build
targets in a Bake file.
target.dockerfile-inline
Uses the string value as an inline Dockerfile for the build target.
target ""default"" {
dockerfile-inline = ""FROM alpine\nENTRYPOINT [\""echo\"", \""hello\""]""
}
The dockerfile-inline
takes precedence over the dockerfile
attribute.
If you specify both, Bake uses the inline version.
target.dockerfile
Name of the Dockerfile to use for the build.
This is the same as the
--file
flag for the docker build
command.
target ""default"" {
dockerfile = ""./src/www/Dockerfile""
}
Resolves to ""Dockerfile""
by default.
$ docker buildx bake --print -f - <<< 'target ""default"" {}'
[+] Building 0.0s (0/0)
{
""target"": {
""default"": {
""context"": ""."",
""dockerfile"": ""Dockerfile""
}
}
}
target.entitlements
Entitlements are permissions that the build process requires to run.
Currently supported entitlements are:
network.host
: Allows the build to use commands that access the host network. In Dockerfile, useRUN --network=host
to run a command with host network enabled.security.insecure
: Allows the build to run commands in privileged containers that are not limited by the default security sandbox. Such container may potentially access and modify system resources. In Dockerfile, useRUN --security=insecure
to run a command in a privileged container.
target ""integration-tests"" {
# this target requires privileged containers to run nested containers
entitlements = [""security.insecure""]
}
Entitlements are enabled with a two-step process. First, a target must declare the entitlements it requires. Secondly, when invoking the bake
command, the user must grant the entitlements by passing the --allow
flag or confirming the entitlements when prompted in an interactive terminal. This is to ensure that the user is aware of the possibly insecure permissions they are granting to the build process.
target.inherits
A target can inherit attributes from other targets.
Use inherits
to reference from one target to another.
In the following example,
the app-dev
target specifies an image name and tag.
The app-release
target uses inherits
to reuse the tag name.
variable ""TAG"" {
default = ""latest""
}
target ""app-dev"" {
tags = [""docker.io/username/myapp:${TAG}""]
}
target ""app-release"" {
inherits = [""app-dev""]
platforms = [""linux/amd64"", ""linux/arm64""]
}
The inherits
attribute is a list,
meaning you can reuse attributes from multiple other targets.
In the following example, the app-release
target reuses attributes
from both the app-dev
and _release
targets.
target ""app-dev"" {
args = {
GO_VERSION = ""1.20""
BUILDX_EXPERIMENTAL = 1
}
tags = [""docker.io/username/myapp""]
dockerfile = ""app.Dockerfile""
labels = {
""org.opencontainers.image.source"" = ""https://github.com/username/myapp""
}
}
target ""_release"" {
args = {
BUILDKIT_CONTEXT_KEEP_GIT_DIR = 1
BUILDX_EXPERIMENTAL = 0
}
}
target ""app-release"" {
inherits = [""app-dev"", ""_release""]
platforms = [""linux/amd64"", ""linux/arm64""]
}
When inheriting attributes from multiple targets and there's a conflict,
the target that appears last in the inherits
list takes precedence.
The previous example defines the BUILDX_EXPERIMENTAL
argument twice for the app-release
target.
It resolves to 0
because the _release
target appears last in the inheritance chain:
$ docker buildx bake --print app-release
[+] Building 0.0s (0/0)
{
""group"": {
""default"": {
""targets"": [
""app-release""
]
}
},
""target"": {
""app-release"": {
""context"": ""."",
""dockerfile"": ""app.Dockerfile"",
""args"": {
""BUILDKIT_CONTEXT_KEEP_GIT_DIR"": ""1"",
""BUILDX_EXPERIMENTAL"": ""0"",
""GO_VERSION"": ""1.20""
},
""labels"": {
""org.opencontainers.image.source"": ""https://github.com/username/myapp""
},
""tags"": [
""docker.io/username/myapp""
],
""platforms"": [
""linux/amd64"",
""linux/arm64""
]
}
}
}
target.labels
Assigns image labels to the build.
This is the same as the --label
flag for docker build
.
target ""default"" {
labels = {
""org.opencontainers.image.source"" = ""https://github.com/username/myapp""
""com.docker.image.source.entrypoint"" = ""Dockerfile""
}
}
It's possible to use a null
value for labels.
If you do, the builder uses the label value specified in the Dockerfile.
target.matrix
A matrix strategy lets you fork a single target into multiple different variants, based on parameters that you specify. This works in a similar way to [Matrix strategies for GitHub Actions]. You can use this to reduce duplication in your bake definition.
The matrix
attribute is a map of parameter names to lists of values.
Bake builds each possible combination of values as a separate target.
Each generated target must have a unique name.
To specify how target names should resolve, use the name
attribute.
The following example resolves the app
target to app-foo
and app-bar
.
It also uses the matrix value to define the
target build stage.
target ""app"" {
name = ""app-${tgt}""
matrix = {
tgt = [""foo"", ""bar""]
}
target = tgt
}
$ docker buildx bake --print app
[+] Building 0.0s (0/0)
{
""group"": {
""app"": {
""targets"": [
""app-foo"",
""app-bar""
]
},
""default"": {
""targets"": [
""app""
]
}
},
""target"": {
""app-bar"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""target"": ""bar""
},
""app-foo"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""target"": ""foo""
}
}
}
Multiple axes
You can specify multiple keys in your matrix to fork a target on multiple axes. When using multiple matrix keys, Bake builds every possible variant.
The following example builds four targets:
app-foo-1-0
app-foo-2-0
app-bar-1-0
app-bar-2-0
target ""app"" {
name = ""app-${tgt}-${replace(version, ""."", ""-"")}""
matrix = {
tgt = [""foo"", ""bar""]
version = [""1.0"", ""2.0""]
}
target = tgt
args = {
VERSION = version
}
}
Multiple values per matrix target
If you want to differentiate the matrix on more than just a single value, you can use maps as matrix values. Bake creates a target for each map, and you can access the nested values using dot notation.
The following example builds two targets:
app-foo-1-0
app-bar-2-0
target ""app"" {
name = ""app-${item.tgt}-${replace(item.version, ""."", ""-"")}""
matrix = {
item = [
{
tgt = ""foo""
version = ""1.0""
},
{
tgt = ""bar""
version = ""2.0""
}
]
}
target = item.tgt
args = {
VERSION = item.version
}
}
target.name
Specify name resolution for targets that use a matrix strategy.
The following example resolves the app
target to app-foo
and app-bar
.
target ""app"" {
name = ""app-${tgt}""
matrix = {
tgt = [""foo"", ""bar""]
}
target = tgt
}
target.network
Specify the network mode for the whole build request. This will override the default network mode
for all the RUN
instructions in the Dockerfile. Accepted values are default
, host
, and none
.
Usually, a better approach to set the network mode for your build steps is to instead use RUN --network=<value>
in your Dockerfile. This way, you can set the network mode for individual build steps and everyone building
the Dockerfile gets consistent behavior without needing to pass additional flags to the build command.
If you set network mode to host
in your Bake file, you must also grant network.host
entitlement when
invoking the bake
command. This is because host
network mode requires elevated privileges and can be a security risk.
You can pass --allow=network.host
to the docker buildx bake
command to grant the entitlement, or you can
confirm the entitlement when prompted if you are using an interactive terminal.
target ""app"" {
# make sure this build does not access internet
network = ""none""
}
target.no-cache-filter
Don't use build cache for the specified stages.
This is the same as the --no-cache-filter
flag for docker build
.
The following example avoids build cache for the foo
build stage.
target ""default"" {
no-cache-filter = [""foo""]
}
target.no-cache
Don't use cache when building the image.
This is the same as the --no-cache
flag for docker build
.
target ""default"" {
no-cache = 1
}
target.output
Configuration for exporting the build output.
This is the same as the
--output
flag.
The following example configures the target to use a cache-only output,
target ""default"" {
output = [{ type = ""cacheonly"" }]
}
target.platforms
Set target platforms for the build target.
This is the same as the
--platform
flag.
The following example creates a multi-platform build for three architectures.
target ""default"" {
platforms = [""linux/amd64"", ""linux/arm64"", ""linux/arm/v7""]
}
target.pull
Configures whether the builder should attempt to pull images when building the target.
This is the same as the --pull
flag for docker build
.
The following example forces the builder to always pull all images referenced in the build target.
target ""default"" {
pull = true
}
target.secret
Defines secrets to expose to the build target.
This is the same as the
--secret
flag.
variable ""HOME"" {
default = null
}
target ""default"" {
secret = [
{ type = ""env"", id = ""KUBECONFIG"" },
{ type = ""file"", id = ""aws"", src = ""${HOME}/.aws/credentials"" },
]
}
This lets you mount the secret in your Dockerfile.
RUN --mount=type=secret,id=aws,target=/root/.aws/credentials \
aws cloudfront create-invalidation ...
RUN --mount=type=secret,id=KUBECONFIG,env=KUBECONFIG \
helm upgrade --install
target.shm-size
Sets the size of the shared memory allocated for build containers when using
RUN
instructions.
The format is <number><unit>
. number
must be greater than 0
. Unit is
optional and can be b
(bytes), k
(kilobytes), m
(megabytes), or g
(gigabytes). If you omit the unit, the system uses bytes.
This is the same as the --shm-size
flag for docker build
.
target ""default"" {
shm-size = ""128m""
}
Note
In most cases, it is recommended to let the builder automatically determine the appropriate configurations. Manual adjustments should only be considered when specific performance tuning is required for complex build scenarios.
target.ssh
Defines SSH agent sockets or keys to expose to the build.
This is the same as the
--ssh
flag.
This can be useful if you need to access private repositories during a build.
target ""default"" {
ssh = [{ id = ""default"" }]
}
FROM alpine
RUN --mount=type=ssh \
apk add git openssh-client \
&& install -m 0700 -d ~/.ssh \
&& ssh-keyscan github.com >> ~/.ssh/known_hosts \
&& git clone git@github.com:user/my-private-repo.git
target.tags
Image names and tags to use for the build target.
This is the same as the
--tag
flag.
target ""default"" {
tags = [
""org/repo:latest"",
""myregistry.azurecr.io/team/image:v1""
]
}
target.target
Set the target build stage to build.
This is the same as the
--target
flag.
target ""default"" {
target = ""binaries""
}
target.ulimits
Ulimits overrides the default ulimits of build's containers when using RUN
instructions and are specified with a soft and hard limit as such:
<type>=<soft limit>[:<hard limit>]
, for example:
target ""app"" {
ulimits = [
""nofile=1024:1024""
]
}
Note
If you do not provide a
hard limit
, thesoft limit
is used for both values. If noulimits
are set, they are inherited from the defaultulimits
set on the daemon.
Note
In most cases, it is recommended to let the builder automatically determine the appropriate configurations. Manual adjustments should only be considered when specific performance tuning is required for complex build scenarios.
Group
Groups allow you to invoke multiple builds (targets) at once.
group ""default"" {
targets = [""db"", ""webapp-dev""]
}
target ""webapp-dev"" {
dockerfile = ""Dockerfile.webapp""
tags = [""docker.io/username/webapp:latest""]
}
target ""db"" {
dockerfile = ""Dockerfile.db""
tags = [""docker.io/username/db""]
}
Groups take precedence over targets, if both exist with the same name.
The following bake file builds the default
group.
Bake ignores the default
target.
target ""default"" {
dockerfile-inline = ""FROM ubuntu""
}
group ""default"" {
targets = [""alpine"", ""debian""]
}
target ""alpine"" {
dockerfile-inline = ""FROM alpine""
}
target ""debian"" {
dockerfile-inline = ""FROM debian""
}
Variable
The HCL file format supports variable block definitions. You can use variables as build arguments in your Dockerfile, or interpolate them in attribute values in your Bake file.
variable ""TAG"" {
default = ""latest""
}
target ""webapp-dev"" {
dockerfile = ""Dockerfile.webapp""
tags = [""docker.io/username/webapp:${TAG}""]
}
You can assign a default value for a variable in the Bake file,
or assign a null
value to it. If you assign a null
value,
Buildx uses the default value from the Dockerfile instead.
You can override variable defaults set in the Bake file using environment variables.
The following example sets the TAG
variable to dev
,
overriding the default latest
value shown in the previous example.
$ TAG=dev docker buildx bake webapp-dev
Built-in variables
The following variables are built-ins that you can use with Bake without having to define them.
| Variable | Description |
|---|---|
BAKE_CMD_CONTEXT | Holds the main context when building using a remote Bake file. |
BAKE_LOCAL_PLATFORM | Returns the current platform’s default platform specification (e.g. linux/amd64 ). |
Use environment variable as default
You can set a Bake variable to use the value of an environment variable as a default value:
variable ""HOME"" {
default = ""$HOME""
}
Interpolate variables into attributes
To interpolate a variable into an attribute string value, you must use curly brackets. The following doesn't work:
variable ""HOME"" {
default = ""$HOME""
}
target ""default"" {
ssh = [""default=$HOME/.ssh/id_rsa""]
}
Wrap the variable in curly brackets where you want to insert it:
variable ""HOME"" {
default = ""$HOME""
}
target ""default"" {
- ssh = [""default=$HOME/.ssh/id_rsa""]
+ ssh = [""default=${HOME}/.ssh/id_rsa""]
}
Before you can interpolate a variable into an attribute, first you must declare it in the bake file, as demonstrated in the following example.
$ cat docker-bake.hcl
target ""default"" {
dockerfile-inline = ""FROM ${BASE_IMAGE}""
}
$ docker buildx bake
[+] Building 0.0s (0/0)
docker-bake.hcl:2
--------------------
1 | target ""default"" {
2 | >>> dockerfile-inline = ""FROM ${BASE_IMAGE}""
3 | }
4 |
--------------------
ERROR: docker-bake.hcl:2,31-41: Unknown variable; There is no variable named ""BASE_IMAGE""., and 1 other diagnostic(s)
$ cat >> docker-bake.hcl
variable ""BASE_IMAGE"" {
default = ""alpine""
}
$ docker buildx bake
[+] Building 0.6s (5/5) FINISHED
Function
A set of general-purpose functions provided by go-cty are available for use in HCL files:
# docker-bake.hcl
target ""webapp-dev"" {
dockerfile = ""Dockerfile.webapp""
tags = [""docker.io/username/webapp:latest""]
args = {
buildno = ""${add(123, 1)}""
}
}
In addition, user defined functions are also supported:
# docker-bake.hcl
function ""increment"" {
params = [number]
result = number + 1
}
target ""webapp-dev"" {
dockerfile = ""Dockerfile.webapp""
tags = [""docker.io/username/webapp:latest""]
args = {
buildno = ""${increment(123)}""
}
}
Note
See User defined HCL functions page for more details.",,,
b26d1827e284681968366139916d4d084b7e1701c75977ae0786d49acb54188d,"Functions
HCL functions are great for when you need to manipulate values in your build configuration in more complex ways than just concatenation or interpolation.
Standard library
Bake ships with built-in support for the
go-cty
standard library functions.
The following example shows the add
function.
variable ""TAG"" {
default = ""latest""
}
group ""default"" {
targets = [""webapp""]
}
target ""webapp"" {
args = {
buildno = ""${add(123, 1)}""
}
}
$ docker buildx bake --print webapp
{
""group"": {
""default"": {
""targets"": [""webapp""]
}
},
""target"": {
""webapp"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""args"": {
""buildno"": ""124""
}
}
}
}
User-defined functions
You can create user-defined functions that do just what you want, if the built-in standard library functions don't meet your needs.
The following example defines an increment
function.
function ""increment"" {
params = [number]
result = number + 1
}
group ""default"" {
targets = [""webapp""]
}
target ""webapp"" {
args = {
buildno = ""${increment(123)}""
}
}
$ docker buildx bake --print webapp
{
""group"": {
""default"": {
""targets"": [""webapp""]
}
},
""target"": {
""webapp"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""args"": {
""buildno"": ""124""
}
}
}
}
Variables in functions
You can make references to variables and standard library functions inside your functions.
You can't reference user-defined functions from other functions.
The following example uses a global variable (REPO
) in a custom function.
# docker-bake.hcl
variable ""REPO"" {
default = ""user/repo""
}
function ""tag"" {
params = [tag]
result = [""${REPO}:${tag}""]
}
target ""webapp"" {
tags = tag(""v1"")
}
Printing the Bake file with the --print
flag shows that the tag
function
uses the value of REPO
to set the prefix of the tag.
$ docker buildx bake --print webapp
{
""group"": {
""default"": {
""targets"": [""webapp""]
}
},
""target"": {
""webapp"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""tags"": [""user/repo:v1""]
}
}
}",,,
43001c916dbfcc6c13a4e3ff60fbadf6b6df25c783bb947ff32f89001ea85ca1,"JSON File logging driver
By default, Docker captures the standard output (and standard error) of all your containers,
and writes them in files using the JSON format. The JSON format annotates each line with its
origin (stdout
or stderr
) and its timestamp. Each log file contains information about
only one container.
{
""log"": ""Log line is here\n"",
""stream"": ""stdout"",
""time"": ""2019-01-01T11:11:11.111111111Z""
}
Warning
The
json-file
logging driver uses file-based storage. These files are designed to be exclusively accessed by the Docker daemon. Interacting with these files with external tools may interfere with Docker's logging system and result in unexpected behavior, and should be avoided.
Usage
To use the json-file
driver as the default logging driver, set the log-driver
and log-opts
keys to appropriate values in the daemon.json
file, which is
located in /etc/docker/
on Linux hosts or
C:\ProgramData\docker\config\
on Windows Server. If the file does not exist, create it first. For more information about
configuring Docker using daemon.json
, see
daemon.json.
The following example sets the log driver to json-file
and sets the max-size
and max-file
options to enable automatic log-rotation.
{
""log-driver"": ""json-file"",
""log-opts"": {
""max-size"": ""10m"",
""max-file"": ""3""
}
}
Note
log-opts
configuration options in thedaemon.json
configuration file must be provided as strings. Boolean and numeric values (such as the value formax-file
in the example above) must therefore be enclosed in quotes (""
).
Restart Docker for the changes to take effect for newly created containers. Existing containers don't use the new logging configuration automatically.
You can set the logging driver for a specific container by using the
--log-driver
flag to docker container create
or docker run
:
$ docker run \
--log-driver json-file --log-opt max-size=10m \
alpine echo hello world
Options
The json-file
logging driver supports the following logging options:
| Option | Description | Example value |
|---|---|---|
max-size | The maximum size of the log before it is rolled. A positive integer plus a modifier representing the unit of measure (k , m , or g ). Defaults to -1 (unlimited). | --log-opt max-size=10m |
max-file | The maximum number of log files that can be present. If rolling the logs creates excess files, the oldest file is removed. Only effective when max-size is also set. A positive integer. Defaults to 1. | --log-opt max-file=3 |
labels | Applies when starting the Docker daemon. A comma-separated list of logging-related labels this daemon accepts. Used for advanced log tag options. | --log-opt labels=production_status,geo |
labels-regex | Similar to and compatible with labels . A regular expression to match logging-related labels. Used for advanced
log tag options. | --log-opt labels-regex=^(production_status|geo) |
env | Applies when starting the Docker daemon. A comma-separated list of logging-related environment variables this daemon accepts. Used for advanced log tag options. | --log-opt env=os,customer |
env-regex | Similar to and compatible with env . A regular expression to match logging-related environment variables. Used for advanced
log tag options. | --log-opt env-regex=^(os|customer) |
compress | Toggles compression for rotated logs. Default is disabled . | --log-opt compress=true |
Examples
This example starts an alpine
container which can have a maximum of 3 log
files no larger than 10 megabytes each.
$ docker run -it --log-opt max-size=10m --log-opt max-file=3 alpine ash",,,
c796333eb372b03693d16281199654a24ab9a9df5c3d47220612d64381335a5e,"Delegations for content trust
Delegations in Docker Content Trust (DCT) allow you to control who can and cannot sign an image tag. A delegation will have a pair of private and public delegation keys. A delegation could contain multiple pairs of keys and contributors in order to a) allow multiple users to be part of a delegation, and b) to support key rotation.
The most important delegation within Docker Content Trust is targets/releases
.
This is seen as the canonical source of a trusted image tag, and without a
contributor's key being under this delegation, they will be unable to sign a tag.
Fortunately when using the $ docker trust
commands, we will automatically
initialize a repository, manage the repository keys, and add a collaborator's key to the
targets/releases
delegation via docker trust signer add
.
Configuring the Docker client
By default, the $ docker trust
commands expect the notary server URL to be the
same as the registry URL specified in the image tag (following a similar logic to
$ docker push
). When using Docker Hub or DTR, the notary
server URL is the same as the registry URL. However, for self-hosted
environments or 3rd party registries, you will need to specify an alternative
URL for the notary server. This is done with:
$ export DOCKER_CONTENT_TRUST_SERVER=https://<URL>:<PORT>
If you do not export this variable in self-hosted environments, you may see errors such as:
$ docker trust signer add --key cert.pem jeff registry.example.com/admin/demo
Adding signer ""jeff"" to registry.example.com/admin/demo...
<...>
Error: trust data missing for remote repository registry.example.com/admin/demo or remote repository not found: timestamp key trust data unavailable. Has a notary repository been initialized?
$ docker trust inspect registry.example.com/admin/demo --pretty
WARN[0000] Error while downloading remote metadata, using cached timestamp - this might not be the latest version available remotely
<...>
If you have enabled authentication for your notary server, or are using DTR, you will need to log in before you can push data to the notary server.
$ docker login registry.example.com/user/repo
Username: admin
Password:
Login Succeeded
$ docker trust signer add --key cert.pem jeff registry.example.com/user/repo
Adding signer ""jeff"" to registry.example.com/user/repo...
Initializing signed repository for registry.example.com/user/repo...
Successfully initialized ""registry.example.com/user/repo""
Successfully added signer: jeff to registry.example.com/user/repo
If you do not log in, you will see:
$ docker trust signer add --key cert.pem jeff registry.example.com/user/repo
Adding signer ""jeff"" to registry.example.com/user/repo...
Initializing signed repository for registry.example.com/user/repo...
you are not authorized to perform this operation: server returned 401.
Failed to add signer to: registry.example.com/user/repo
Configuring the Notary client
Some of the more advanced features of DCT require the Notary CLI. To install and configure the Notary CLI:
Download the client and ensure that it is available on your path.
Create a configuration file at
~/.notary/config.json
with the following content:
{
""trust_dir"" : ""~/.docker/trust"",
""remote_server"": {
""url"": ""https://registry.example.com"",
""root_ca"": ""../.docker/ca.pem""
}
}
The newly created configuration file contains information about the location of your local Docker trust data and the notary server URL.
For more detailed information about how to use notary outside of the Docker Content Trust use cases, refer to the Notary CLI documentation here
Creating delegation keys
A prerequisite to adding your first contributor is a pair of delegation keys.
These keys can either be generated locally using $ docker trust
, generated by
a certificate authority.
Using Docker Trust to generate keys
Docker trust has a built-in generator for a delegation key pair,
$ docker trust generate <name>
. Running this command will automatically load
the delegation private key in to the local Docker trust store.
$ docker trust key generate jeff
Generating key for jeff...
Enter passphrase for new jeff key with ID 9deed25:
Repeat passphrase for new jeff key with ID 9deed25:
Successfully generated and loaded private key. Corresponding public key available: /home/ubuntu/Documents/mytrustdir/jeff.pub
Manually generating keys
If you need to manually generate a private key (either RSA or ECDSA) and an X.509 certificate containing the public key, you can use local tools like openssl or cfssl along with a local or company-wide Certificate Authority.
Here is an example of how to generate a 2048-bit RSA portion key (all RSA keys must be at least 2048 bits):
$ openssl genrsa -out delegation.key 2048
Generating RSA private key, 2048 bit long modulus
....................................................+++
............+++
e is 65537 (0x10001)
They should keep delegation.key
private because it is used to sign tags.
Then they need to generate an x509 certificate containing the public key, which is what you need from them. Here is the command to generate a CSR (certificate signing request):
$ openssl req -new -sha256 -key delegation.key -out delegation.csr
Then they can send it to whichever CA you trust to sign certificates, or they can self-sign the certificate (in this example, creating a certificate that is valid for 1 year):
$ openssl x509 -req -sha256 -days 365 -in delegation.csr -signkey delegation.key -out delegation.crt
Then they need to give you delegation.crt
, whether it is self-signed or signed
by a CA.
Finally you will need to add the private key into your local Docker trust store.
$ docker trust key load delegation.key --name jeff
Loading key from ""delegation.key""...
Enter passphrase for new jeff key with ID 8ae710e:
Repeat passphrase for new jeff key with ID 8ae710e:
Successfully imported key from delegation.key
Viewing local delegation keys
To list the keys that have been imported in to the local Docker trust store we can use the Notary CLI.
$ notary key list
ROLE GUN KEY ID LOCATION
---- --- ------ --------
root f6c6a4b00fefd8751f86194c7d87a3bede444540eb3378c4a11ce10852ab1f96 /home/ubuntu/.docker/trust/private
jeff 9deed251daa1aa6f9d5f9b752847647cf8d705da0763aa5467650d0987ed5306 /home/ubuntu/.docker/trust/private
Managing delegations in a Notary Server
When the first delegation is added to the Notary Server using $ docker trust
,
we automatically initiate trust data for the repository. This includes creating
the notary target and snapshots keys, and rotating the snapshot key to be
managed by the notary server. More information on these keys can be found
here
When initiating a repository, you will need the key and the passphrase of a local
Notary Canonical Root Key. If you have not initiated a repository before, and
therefore don't have a Notary root key, $ docker trust
will create one for you.
Important
Be sure to protect and back up your Notary Canonical Root Key.
Initiating the repository
To upload the first key to a delegation, at the same time initiating a
repository, you can use the $ docker trust signer add
command. This will add
the contributor's public key to the targets/releases
delegation, and create a
second targets/<name>
delegation.
For DCT the name of the second delegation, in the below example
jeff
, is there to help you keep track of the owner of the keys. In more
advanced use cases of Notary additional delegations are used for hierarchy.
$ docker trust signer add --key cert.pem jeff registry.example.com/admin/demo
Adding signer ""jeff"" to registry.example.com/admin/demo...
Initializing signed repository for registry.example.com/admin/demo...
Enter passphrase for root key with ID f6c6a4b:
Enter passphrase for new repository key with ID b0014f8:
Repeat passphrase for new repository key with ID b0014f8:
Successfully initialized ""registry.example.com/admin/demo""
Successfully added signer: jeff to registry.example.com/admin/demo
You can see which keys have been pushed to the Notary server for each repository
with the $ docker trust inspect
command.
$ docker trust inspect --pretty registry.example.com/admin/demo
No signatures for registry.example.com/admin/demo
List of signers and their keys for registry.example.com/admin/demo
SIGNER KEYS
jeff 1091060d7bfd
Administrative keys for registry.example.com/admin/demo
Repository Key: b0014f8e4863df2d028095b74efcb05d872c3591de0af06652944e310d96598d
Root Key: 64d147e59e44870311dd2d80b9f7840039115ef3dfa5008127d769a5f657a5d7
You could also use the Notary CLI to list delegations and keys. Here you can
clearly see the keys were attached to targets/releases
and targets/jeff
.
$ notary delegation list registry.example.com/admin/demo
ROLE PATHS KEY IDS THRESHOLD
---- ----- ------- ---------
targets/jeff """" <all paths> 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1 1
targets/releases """" <all paths> 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1 1
Adding additional signers
Docker Trust allows you to configure multiple delegations per repository,
allowing you to manage the lifecycle of delegations. When adding additional
delegations with $ docker trust
the collaborators key is once again added to
the targets/release
role.
Note you will need the passphrase for the repository key; this would have been configured when you first initiated the repository.
$ docker trust signer add --key ben.pub ben registry.example.com/admin/demo
Adding signer ""ben"" to registry.example.com/admin/demo...
Enter passphrase for repository key with ID b0014f8:
Successfully added signer: ben to registry.example.com/admin/demo
Check to prove that there are now 2 delegations (Signer).
$ docker trust inspect --pretty registry.example.com/admin/demo
No signatures for registry.example.com/admin/demo
List of signers and their keys for registry.example.com/admin/demo
SIGNER KEYS
ben afa404703b25
jeff 1091060d7bfd
Administrative keys for registry.example.com/admin/demo
Repository Key: b0014f8e4863df2d028095b74efcb05d872c3591de0af06652944e310d96598d
Root Key: 64d147e59e44870311dd2d80b9f7840039115ef3dfa5008127d769a5f657a5d7
Adding keys to an existing delegation
To support things like key rotation and expiring / retiring keys you can publish
multiple contributor keys per delegation. The only prerequisite here is to make
sure you use the same the delegation name, in this case jeff
. Docker trust
will automatically handle adding this new key to targets/releases
.
Note
You will need the passphrase for the repository key; this would have been configured when you first initiated the repository.
$ docker trust signer add --key cert2.pem jeff registry.example.com/admin/demo
Adding signer ""jeff"" to registry.example.com/admin/demo...
Enter passphrase for repository key with ID b0014f8:
Successfully added signer: jeff to registry.example.com/admin/demo
Check to prove that the delegation (Signer) now contains multiple Key IDs.
$ docker trust inspect --pretty registry.example.com/admin/demo
No signatures for registry.example.com/admin/demo
List of signers and their keys for registry.example.com/admin/demo
SIGNER KEYS
jeff 1091060d7bfd, 5570b88df073
Administrative keys for registry.example.com/admin/demo
Repository Key: b0014f8e4863df2d028095b74efcb05d872c3591de0af06652944e310d96598d
Root Key: 64d147e59e44870311dd2d80b9f7840039115ef3dfa5008127d769a5f657a5d7
Removing a delegation
If you need to remove a delegation, including the contributor keys that are
attached to the targets/releases
role, you can use the
$ docker trust signer remove
command.
Note
Tags that were signed by the removed delegation will need to be resigned by an active delegation
$ docker trust signer remove ben registry.example.com/admin/demo
Removing signer ""ben"" from registry.example.com/admin/demo...
Enter passphrase for repository key with ID b0014f8:
Successfully removed ben from registry.example.com/admin/demo
Troubleshooting
If you see an error that there are no usable keys in
targets/releases
, you will need to add additional delegations usingdocker trust signer add
before resigning images.WARN[0000] role targets/releases has fewer keys than its threshold of 1; it will not be usable until keys are added to it
If you have added additional delegations already and are seeing an error message that there are no valid signatures in
targest/releases
, you will need to resign thetargets/releases
delegation file with the Notary CLI.WARN[0000] Error getting targets/releases: valid signatures did not meet threshold for targets/releases
Resigning the delegation file is done with the
$ notary witness
command$ notary witness registry.example.com/admin/demo targets/releases --publish
More information on the
$ notary witness
command can be found here
Removing a contributor's key from a delegation
As part of rotating keys for a delegation, you may want to remove an individual key but retain the delegation. This can be done with the Notary CLI.
Remember you will have to remove the key from both the targets/releases
role
and the role specific to that signer targets/<name>
.
We will need to grab the Key ID from the Notary Server
$ notary delegation list registry.example.com/admin/demo ROLE PATHS KEY IDS THRESHOLD ---- ----- ------- --------- targets/jeff """" <all paths> 8fb597cbaf196f0781628b2f52bff6b3912e4e8075720378fda60d17232bbcf9 1 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1 targets/releases """" <all paths> 8fb597cbaf196f0781628b2f52bff6b3912e4e8075720378fda60d17232bbcf9 1 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1
Remove from the
targets/releases
delegation$ notary delegation remove registry.example.com/admin/demo targets/releases 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1 --publish Auto-publishing changes to registry.example.com/admin/demo Enter username: admin Enter password: Enter passphrase for targets key with ID b0014f8: Successfully published changes for repository registry.example.com/admin/demo
Remove from the
targets/<name>
delegation$ notary delegation remove registry.example.com/admin/demo targets/jeff 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1 --publish Removal of delegation role targets/jeff with keys [5570b88df0736c468493247a07e235e35cf3641270c944d0e9e8899922fc6f99], to repository ""registry.example.com/admin/demo"" staged for next publish. Auto-publishing changes to registry.example.com/admin/demo Enter username: admin Enter password: Enter passphrase for targets key with ID b0014f8: Successfully published changes for repository registry.example.com/admin/demo
Check the remaining delegation list
$ notary delegation list registry.example.com/admin/demo ROLE PATHS KEY IDS THRESHOLD ---- ----- ------- --------- targets/jeff """" <all paths> 8fb597cbaf196f0781628b2f52bff6b3912e4e8075720378fda60d17232bbcf9 1 targets/releases """" <all paths> 8fb597cbaf196f0781628b2f52bff6b3912e4e8075720378fda60d17232bbcf9 1
Removing a local delegation private key
As part of rotating delegation keys, you may need to remove a local delegation
key from the local Docker trust store. This is done with the Notary CLI, using
the $ notary key remove
command.
We will need to get the Key ID from the local Docker Trust store
$ notary key list ROLE GUN KEY ID LOCATION ---- --- ------ -------- root f6c6a4b00fefd8751f86194c7d87a3bede444540eb3378c4a11ce10852ab1f96 /home/ubuntu/.docker/trust/private admin 8fb597cbaf196f0781628b2f52bff6b3912e4e8075720378fda60d17232bbcf9 /home/ubuntu/.docker/trust/private jeff 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1 /home/ubuntu/.docker/trust/private targets ...example.com/admin/demo c819f2eda8fba2810ec6a7f95f051c90276c87fddfc3039058856fad061c009d /home/ubuntu/.docker/trust/private
Remove the key from the local Docker Trust store
$ notary key remove 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1 Are you sure you want to remove 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1 (role jeff) from /home/ubuntu/.docker/trust/private? (yes/no) y Deleted 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1 (role jeff) from /home/ubuntu/.docker/trust/private.
Removing all trust data from a repository
You can remove all trust data from a repository, including repository, target, snapshot and all delegations keys using the Notary CLI.
This is often required by a container registry before a particular repository can be deleted.
$ notary delete registry.example.com/admin/demo --remote
Deleting trust data for repository registry.example.com/admin/demo
Enter username: admin
Enter password:
Successfully deleted local and remote trust data for repository registry.example.com/admin/demo
$ docker trust inspect --pretty registry.example.com/admin/demo
No signatures or cannot access registry.example.com/admin/demo",,,
1d852dfbfab32860ac40f9da996524c69afd300225a9a9a58afabf7738a1acfb,"Set or change pre-defined environment variables in Docker Compose
Compose already comes with pre-defined environment variables. It also inherits common Docker CLI environment variables, such as DOCKER_HOST
and DOCKER_CONTEXT
. See
Docker CLI environment variable reference for details.
This page contains information on how you can set or change the following pre-defined environment variables if you need to:
COMPOSE_PROJECT_NAME
COMPOSE_FILE
COMPOSE_PROFILES
COMPOSE_CONVERT_WINDOWS_PATHS
COMPOSE_PATH_SEPARATOR
COMPOSE_IGNORE_ORPHANS
COMPOSE_REMOVE_ORPHANS
COMPOSE_PARALLEL_LIMIT
COMPOSE_ANSI
COMPOSE_STATUS_STDOUT
COMPOSE_ENV_FILES
COMPOSE_MENU
COMPOSE_EXPERIMENTAL
Methods to override
You can set or change the pre-defined environment variables:
- With an
.env
file located in your working directory - From the command line
- From your shell
When changing or setting any environment variables, be aware of Environment variable precedence.
Configure
COMPOSE_PROJECT_NAME
Sets the project name. This value is prepended along with the service name to the container's name on startup.
For example, if your project name is myapp
and it includes two services db
and web
,
then Compose starts containers named myapp-db-1
and myapp-web-1
respectively.
Compose can set the project name in different ways. The level of precedence (from highest to lowest) for each method is as follows:
- The
-p
command line flag COMPOSE_PROJECT_NAME
- The top level
name:
variable from the config file (or the lastname:
from a series of config files specified using-f
) - The
basename
of the project directory containing the config file (or containing the first config file specified using-f
) - The
basename
of the current directory if no config file is specified
Project names must contain only lowercase letters, decimal digits, dashes, and
underscores, and must begin with a lowercase letter or decimal digit. If the
basename
of the project directory or current directory violates this
constraint, you must use one of the other mechanisms.
See also the
command-line options overview and
using -p
to specify a project name.
COMPOSE_FILE
Specifies the path to a Compose file. Specifying multiple Compose files is supported.
- Default behavior: If not provided, Compose looks for a file named
compose.yaml
in the current directory and, if not found, then Compose searches each parent directory recursively until a file by that name is found. - When specifying multiple Compose files, the path separators are, by default, on:
Mac and Linux:
:
(colon)Windows:
;
(semicolon) For example:COMPOSE_FILE=compose.yaml:compose.prod.yaml
COMPOSE_PATH_SEPARATOR
.
See also the
command-line options overview and
using -f
to specify name and path of one or more Compose files.
COMPOSE_PROFILES
Specifies one or more profiles to be enabled when docker compose up
is run.
Services with matching profiles are started as well as any services for which no profile has been defined.
For example, calling docker compose up
with COMPOSE_PROFILES=frontend
selects services with the
frontend
profile as well as any services without a profile specified.
If specifying multiple profiles, use a comma as a separator.
This following example enables all services matching both the frontend
and debug
profiles and services without a profile.
COMPOSE_PROFILES=frontend,debug
See also
Using profiles with Compose and the
--profile
command-line option.
COMPOSE_CONVERT_WINDOWS_PATHS
When enabled, Compose performs path conversion from Windows-style to Unix-style in volume definitions.
- Supported values:
true
or1
, to enablefalse
or0
, to disable
- Defaults to:
0
COMPOSE_PATH_SEPARATOR
Specifies a different path separator for items listed in COMPOSE_FILE
.
- Defaults to:
- On macOS and Linux to
:
- On Windows to
;
- On macOS and Linux to
COMPOSE_IGNORE_ORPHANS
When enabled, Compose doesn't try to detect orphaned containers for the project.
- Supported values:
true
or1
, to enablefalse
or0
, to disable
- Defaults to:
0
COMPOSE_REMOVE_ORPHANS
When enabled, Compose automatically removes orphaned containers when updating a service or stack. Orphaned containers are those that were created by a previous configuration but are no longer defined in the current compose.yaml
file.
- Supported values:
true
or1
, to enable automatic removal of orphaned containersfalse
or0
, to disable automatic removal. Compose displays a warning about orphaned containers instead.
- Defaults to:
0
COMPOSE_PARALLEL_LIMIT
Specifies the maximum level of parallelism for concurrent engine calls.
COMPOSE_ANSI
Specifies when to print ANSI control characters.
- Supported values:
auto
, Compose detects if TTY mode can be used. Otherwise, use plain text modenever
, use plain text modealways
or0
, use TTY mode
- Defaults to:
auto
COMPOSE_STATUS_STDOUT
When enabled, Compose writes its internal status and progress messages to stdout
instead of stderr
.
The default value is false to clearly separate the output streams between Compose messages and your container's logs.
- Supported values:
true
or1
, to enablefalse
or0
, to disable
- Defaults to:
0
COMPOSE_ENV_FILES
Lets you specify which environment files Compose should use if --env-file
isn't used.
When using multiple environment files, use a comma as a separator. For example:
COMPOSE_ENV_FILES=.env.envfile1, .env.envfile2
If COMPOSE_ENV_FILES
is not set, and you don't provide --env-file
in the CLI, Docker Compose uses the default behavior, which is to look for an .env
file in the project directory.
COMPOSE_MENU
When enabled, Compose displays a navigation menu where you can choose to open the Compose stack in Docker Desktop, switch on
watch
mode, or use
Docker Debug.
- Supported values:
true
or1
, to enablefalse
or0
, to disable
- Defaults to:
1
if you obtained Docker Compose through Docker Desktop, otherwise default is0
COMPOSE_EXPERIMENTAL
This is an opt-out variable. When turned off it deactivates the experimental features such as the navigation menu or Synchronized file shares.
- Supported values:
true
or1
, to enablefalse
or0
, to disable
- Defaults to:
1
Unsupported in Compose V2
The following environment variables have no effect in Compose V2. For more information, see Migrate to Compose V2.
COMPOSE_API_VERSION
By default the API version is negotiated with the server. UseDOCKER_API_VERSION
.
See the Docker CLI environment variable reference page.COMPOSE_HTTP_TIMEOUT
COMPOSE_TLS_VERSION
COMPOSE_FORCE_WINDOWS_HOST
COMPOSE_INTERACTIVE_NO_CLI
COMPOSE_DOCKER_CLI_BUILD
UseDOCKER_BUILDKIT
to select between BuildKit and the classic builder. IfDOCKER_BUILDKIT=0
thendocker compose build
uses the classic builder to build images.",,,
05780ee34f534b08a22438f9468f35f5dcde15b7ceafa258880ad0f7fb80ed11,"Use multiple Compose files
This section contains information on the ways you can work with multiple Compose files.
Using multiple Compose files lets you customize a Compose application for different environments or workflows. This is useful for large applications that may use dozens of containers, with ownership distributed across multiple teams. For example, if your organization or team uses a monorepo, each team may have their own “local” Compose file to run a subset of the application. They then need to rely on other teams to provide a reference Compose file that defines the expected way to run their own subset. Complexity moves from the code in to the infrastructure and the configuration file.
The quickest way to work with multiple Compose files is to
merge Compose files using the -f
flag in the command line to list out your desired Compose files. However,
merging rules means this can soon get quite complicated.
Docker Compose provides two other options to manage this complexity when working with multiple Compose files. Depending on your project's needs, you can:
- Extend a Compose file by referring to another Compose file and selecting the bits you want to use in your own application, with the ability to override some attributes.
- Include other Compose files directly in your Compose file.",,,
baf10129b2e27c0cfe0ea2a0b228eba3274d0932e0ac83cda00a090bd55a7067,"Using Docker Scout in continuous integration
You can analyze Docker images in continuous integration pipelines as you build them using a GitHub action or the Docker Scout CLI plugin.
Available integrations:
You can also add runtime integration as part of your CI/CD pipeline, which lets
you assign an image to an environment, such as production
or staging
, when
you deploy it. For more information, see
Environment monitoring.",,,
b277362ed17ec03bcb79a5f95bbffceb26faf7603a9bc62ed47ff754679d8de1,"Get started
Guides
Manuals
Reference
{ switch(e.key) { case 'k': if (e.metaKey || e.ctrlKey) { e.preventDefault(); $el.focus(); } break; } }"" class=""flex-grow px-2 bg-transparent min-w-0 text-white placeholder:text-white outline-none"" placeholder=Search tabindex=0>
K
Start typing to search… or try
Ask AI
Ask AI
Back
Search
Get started
Guides
Manuals
Reference
Search
Not finding what you're looking for?
Try Ask AI",,,
97ea8c13317eacbec1376e1901e669df856685c761eaf5a228f0c27914205a10,"Image management
Docker Hub provides powerful features for managing and organizing your repository content, ensuring that your images and artifacts are accessible, version-controlled, and easy to share. This section covers key image management tasks, including tagging, pushing images, transferring images between repositories, and supported software artifacts.
- Tags: Tags help you version and organize different iterations of your images within a single repository. This topic explains tagging and provides guidance on how to create, view, and delete tags in Docker Hub.
- Image Management: Manage your images and image indexes to optimize your repository storage.
- Software artifacts: Docker Hub supports OCI (Open Container Initiative) artifacts, allowing you to store, manage, and distribute a range of content beyond standard Docker images, including Helm charts, vulnerability reports, and more. This section provides an overview of OCI artifacts as well as some examples of pushing them to Docker Hub.
- Push images to Hub: Docker Hub enables you to push local images
to it, making them available for your team or the Docker community. Learn how
to configure your images and use the
docker push
command to upload them to Docker Hub. - Move images between repositories: Organizing content across different repositories can help streamline collaboration and resource management. This topic details how to move images from one Docker Hub repository to another, whether for personal consolidation or to share images with an organization.",,,
d4a44314702bcf3cb6c5f24734ca73b9b161356a12ccf8a856c59fec2a7dcde5,"ZFS storage driver
ZFS is a next generation filesystem that supports many advanced storage technologies such as volume management, snapshots, checksumming, compression and deduplication, replication and more.
It was created by Sun Microsystems (now Oracle Corporation) and is open sourced under the CDDL license. Due to licensing incompatibilities between the CDDL and GPL, ZFS cannot be shipped as part of the mainline Linux kernel. However, the ZFS On Linux (ZoL) project provides an out-of-tree kernel module and userspace tools which can be installed separately.
The ZFS on Linux (ZoL) port is healthy and maturing. However, at this point in
time it is not recommended to use the zfs
Docker storage driver for production
use unless you have substantial experience with ZFS on Linux.
Note
There is also a FUSE implementation of ZFS on the Linux platform. This is not recommended. The native ZFS driver (ZoL) is more tested, has better performance, and is more widely used. The remainder of this document refers to the native ZoL port.
Prerequisites
- ZFS requires one or more dedicated block devices, preferably solid-state drives (SSDs).
- The
/var/lib/docker/
directory must be mounted on a ZFS-formatted filesystem. - Changing the storage driver makes any containers you have already
created inaccessible on the local system. Use
docker save
to save containers, and push existing images to Docker Hub or a private repository, so that you do not need to re-create them later.
Note
There is no need to use
MountFlags=slave
becausedockerd
andcontainerd
are in different mount namespaces.
Configure Docker with the zfs
storage driver
Stop Docker.
Copy the contents of
/var/lib/docker/
to/var/lib/docker.bk
and remove the contents of/var/lib/docker/
.$ sudo cp -au /var/lib/docker /var/lib/docker.bk $ sudo rm -rf /var/lib/docker/*
Create a new
zpool
on your dedicated block device or devices, and mount it into/var/lib/docker/
. Be sure you have specified the correct devices, because this is a destructive operation. This example adds two devices to the pool.$ sudo zpool create -f zpool-docker -m /var/lib/docker /dev/xvdf /dev/xvdg
The command creates the
zpool
and names itzpool-docker
. The name is for display purposes only, and you can use a different name. Check that the pool was created and mounted correctly usingzfs list
.$ sudo zfs list NAME USED AVAIL REFER MOUNTPOINT zpool-docker 55K 96.4G 19K /var/lib/docker
Configure Docker to use
zfs
. Edit/etc/docker/daemon.json
and set thestorage-driver
tozfs
. If the file was empty before, it should now look like this:{ ""storage-driver"": ""zfs"" }
Save and close the file.
Start Docker. Use
docker info
to verify that the storage driver iszfs
.$ sudo docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 17.03.1-ce Storage Driver: zfs Zpool: zpool-docker Zpool Health: ONLINE Parent Dataset: zpool-docker Space Used By Parent: 249856 Space Available: 103498395648 Parent Quota: no Compression: off <...>
Manage zfs
Increase capacity on a running device
To increase the size of the zpool
, you need to add a dedicated block device to
the Docker host, and then add it to the zpool
using the zpool add
command:
$ sudo zpool add zpool-docker /dev/xvdh
Limit a container's writable storage quota
If you want to implement a quota on a per-image/dataset basis, you can set the
size
storage option to limit the amount of space a single container can use
for its writable layer.
Edit /etc/docker/daemon.json
and add the following:
{
""storage-driver"": ""zfs"",
""storage-opts"": [""size=256M""]
}
See all storage options for each storage driver in the daemon reference documentation
Save and close the file, and restart Docker.
How the zfs
storage driver works
ZFS uses the following objects:
- filesystems: thinly provisioned, with space allocated from the
zpool
on demand. - snapshots: read-only space-efficient point-in-time copies of filesystems.
- clones: Read-write copies of snapshots. Used for storing the differences from the previous layer.
The process of creating a clone:
- A read-only snapshot is created from the filesystem.
- A writable clone is created from the snapshot. This contains any differences from the parent layer.
Filesystems, snapshots, and clones all allocate space from the underlying
zpool
.
Image and container layers on-disk
Each running container's unified filesystem is mounted on a mount point in
/var/lib/docker/zfs/graph/
. Continue reading for an explanation of how the
unified filesystem is composed.
Image layering and sharing
The base layer of an image is a ZFS filesystem. Each child layer is a ZFS clone based on a ZFS snapshot of the layer below it. A container is a ZFS clone based on a ZFS Snapshot of the top layer of the image it's created from.
The diagram below shows how this is put together with a running container based on a two-layer image.
When you start a container, the following steps happen in order:
The base layer of the image exists on the Docker host as a ZFS filesystem.
Additional image layers are clones of the dataset hosting the image layer directly below it.
In the diagram, ""Layer 1"" is added by taking a ZFS snapshot of the base layer and then creating a clone from that snapshot. The clone is writable and consumes space on-demand from the zpool. The snapshot is read-only, maintaining the base layer as an immutable object.
When the container is launched, a writable layer is added above the image.
In the diagram, the container's read-write layer is created by making a snapshot of the top layer of the image (Layer 1) and creating a clone from that snapshot.
As the container modifies the contents of its writable layer, space is allocated for the blocks that are changed. By default, these blocks are 128k.
How container reads and writes work with zfs
Reading files
Each container's writable layer is a ZFS clone which shares all its data with the dataset it was created from (the snapshots of its parent layers). Read operations are fast, even if the data being read is from a deep layer. This diagram illustrates how block sharing works:
Writing files
Writing a new file: space is allocated on demand from the underlying zpool
and the blocks are written directly into the container's writable layer.
Modifying an existing file: space is allocated only for the changed blocks, and those blocks are written into the container's writable layer using a copy-on-write (CoW) strategy. This minimizes the size of the layer and increases write performance.
Deleting a file or directory:
- When you delete a file or directory that exists in a lower layer, the ZFS driver masks the existence of the file or directory in the container's writable layer, even though the file or directory still exists in the lower read-only layers.
- If you create and then delete a file or directory within the container's
writable layer, the blocks are reclaimed by the
zpool
.
ZFS and Docker performance
There are several factors that influence the performance of Docker using the
zfs
storage driver.
Memory: Memory has a major impact on ZFS performance. ZFS was originally designed for large enterprise-grade servers with a large amount of memory.
ZFS Features: ZFS includes a de-duplication feature. Using this feature may save disk space, but uses a large amount of memory. It is recommended that you disable this feature for the
zpool
you are using with Docker, unless you are using SAN, NAS, or other hardware RAID technologies.ZFS Caching: ZFS caches disk blocks in a memory structure called the adaptive replacement cache (ARC). The Single Copy ARC feature of ZFS allows a single cached copy of a block to be shared by multiple clones of a With this feature, multiple running containers can share a single copy of a cached block. This feature makes ZFS a good option for PaaS and other high-density use cases.
Fragmentation: Fragmentation is a natural byproduct of copy-on-write filesystems like ZFS. ZFS mitigates this by using a small block size of 128k. The ZFS intent log (ZIL) and the coalescing of writes (delayed writes) also help to reduce fragmentation. You can monitor fragmentation using
zpool status
. However, there is no way to defragment ZFS without reformatting and restoring the filesystem.Use the native ZFS driver for Linux: The ZFS FUSE implementation is not recommended, due to poor performance.
Performance best practices
Use fast storage: Solid-state drives (SSDs) provide faster reads and writes than spinning disks.
Use volumes for write-heavy workloads: Volumes provide the best and most predictable performance for write-heavy workloads. This is because they bypass the storage driver and do not incur any of the potential overheads introduced by thin provisioning and copy-on-write. Volumes have other benefits, such as allowing you to share data among containers and persisting even when no running container is using them.",,,
1c2c1657e969122215bf24056a2a59687fa6bc602d620c40959430b120a62a82,"Docker Engine 18.01 release notes
Table of contents
18.01.0-ce
2018-01-10
Builder
- Fix files not being deleted if user-namespaces are enabled moby/moby#35822
- Add support for expanding environment-variables in
docker commit --change ...
moby/moby#35582
Client
- Return errors from client in stack deploy configs docker/cli#757
- Fix description of filter flag in prune commands docker/cli#774
- Add ""pid"" to unsupported options list docker/cli#768
- Add support for experimental Cli configuration docker/cli#758
- Add support for generic resources to bash completion docker/cli#749
- Fix error in zsh completion script for docker exec docker/cli#751
- Add a debug message when client closes websocket attach connection moby/moby#35720
- Fix bash completion for
""docker swarm""
docker/cli#772
Documentation
- Correct references to
--publish
long syntax in docs docker/cli#746 - Corrected descriptions for MAC_ADMIN and MAC_OVERRIDE docker/cli#761
- Updated developer doc to explain external CLI moby/moby#35681
- Fix
""on-failure""
restart policy being documented as ""failure"" docker/cli#754 - Fix anchors to ""Storage driver options"" docker/cli#748
Experimental
- Add kubernetes support to
docker stack
command docker/cli#721
- Don't append the container id to custom directory checkpoints. moby/moby#35694
Logging
- Fix daemon crash when using the GELF log driver over TCP when the GELF server goes down moby/moby#35765
- Fix awslogs batch size calculation for large logs moby/moby#35726
Networking
- Windows: Fix to allow docker service to start on Windows VM docker/libnetwork#1916
- Fix for docker intercepting DNS requests on ICS network docker/libnetwork#2014
- Windows: Added a new network creation driver option docker/libnetwork#2021
Runtime
- Validate Mount-specs on container start to prevent missing host-path moby/moby#35833
- Fix overlay2 storage driver inside a user namespace moby/moby#35794
- Zfs: fix busy error on container stop moby/moby#35674
- Fix health checks not using the container's working directory moby/moby#35845
- Fix VFS graph driver failure to initialize because of failure to setup fs quota moby/moby#35827
- Fix containerd events being processed twice moby/moby#35896
Swarm mode
- Fix published ports not being updated if a service has the same number of host-mode published ports with Published Port 0 docker/swarmkit#2376
- Make the task termination order deterministic docker/swarmkit#2265",,,
8f5068ede482447908398e7ecf5dd9e25924f9e85763f282d7b533d1c9aea3d4,"Service accounts
Important
As of December 10, 2024, Enhanced Service Account add-ons are no longer available. Existing Service Account agreements will be honored until their current term expires, but new purchases or renewals of Enhanced Service Account add-ons are no longer available and customers must renew under a new subscription plan.
Docker recommends transitioning to Organization Access Tokens (OATs), which can provide similar functionality.
A service account is a Docker ID used for automated management of container images or containerized applications. Service accounts are typically used in automated workflows, and don't share Docker IDs with the members in the organization. Common use cases for service accounts include mirroring content on Docker Hub, or tying in image pulls from your CI/CD process.
Enhanced Service Account add-on tiers
Refer to the following table for details on the Enhanced Service Account add-ons:
| Tier | Pull Rates Per Day* |
|---|---|
| 1 | 5,000-10,000 |
| 2 | 10,000-25,000 |
| 3 | 25,000-50,000 |
| 4 | 50,000-100,000 |
| 5 | 100,000+ |
*The service account may exceed Pulls by up to 25% for up to 20 days during the year without incurring additional fees. Reports on consumption are available upon request.",,,
b0cd18b06c90cf31278ba3673b05e593291f5fe766ae273f7b9dfe48e502312f,"Create an exception using the GUI
The Docker Scout Dashboard and Docker Desktop provide a user-friendly interface for creating exceptions for vulnerabilities found in container images. Exceptions let you acknowledge accepted risks or address false positives in image analysis.
Prerequisites
To create an in the Docker Scout Dashboard or Docker Desktop, you need a Docker account with Editor or Owner permissions for the Docker organization that owns the image.
Steps
To create an exception for a vulnerability in an image using the Docker Scout Dashboard or Docker Desktop:
- Go to the Images page.
- Select the image tag that contains the vulnerability you want to create an exception for.
- Open the Image layers tab.
- Select the layer that contains the vulnerability you want to create an exception for.
- In the Vulnerabilities tab, find the vulnerability you want to create an exception for. Vulnerabilities are grouped by package. Find the package that contains the vulnerability you want to create an exception for, and then expand the package.
- Select the Create exception button next to the vulnerability.
Selecting the Create exception button opens the Create exception side panel. In this panel, you can provide the details of the exception:
Exception type: The type of exception. The only supported types are:
Accepted risk: The vulnerability is not addressed due to its minimal security risk, high remediation costs, dependence on an upstream fix, or similar.
False positive: The vulnerability presents no security risk in your specific use case, configuration, or because of measures in place that block exploitation
If you select False positive, you must provide a justification for why the vulnerability is a false positive:
Additional details: Any additional information that you want to provide about the exception.
Scope: The scope of the exception. The scope can be:
- Image: The exception applies to the selected image.
- All images in repository: The exception applies to all images in the repository.
- Specific repository: The exception applies to all images in the specified repositories.
- All images in my organization: The exception applies to all images in your organization.
Package scope: The scope of the exception. The package scope can be:
- Selected package: The exception applies to the selected package.
- Any packages: The exception applies to all packages vulnerable to this CVE.
When you've filled in the details, select the Create button to create the exception.
The exception is now created and factored into the analysis results for the images that you selected. The exception is also listed on the Exceptions tab of the Vulnerabilities page in the Docker Scout Dashboard.
- Open the Images view in Docker Desktop.
- Open the Hub tab.
- Select the image tag that contains the vulnerability you want to create an exception for.
- Select the layer that contains the vulnerability you want to create an exception for.
- In the Vulnerabilities tab, find the vulnerability you want to create an exception for.
- Select the Create exception button next to the vulnerability.
Selecting the Create exception button opens the Create exception side panel. In this panel, you can provide the details of the exception:
Exception type: The type of exception. The only supported types are:
Accepted risk: The vulnerability is not addressed due to its minimal security risk, high remediation costs, dependence on an upstream fix, or similar.
False positive: The vulnerability presents no security risk in your specific use case, configuration, or because of measures in place that block exploitation
If you select False positive, you must provide a justification for why the vulnerability is a false positive:
Additional details: Any additional information that you want to provide about the exception.
Scope: The scope of the exception. The scope can be:
- Image: The exception applies to the selected image.
- All images in repository: The exception applies to all images in the repository.
- Specific repository: The exception applies to all images in the specified repositories.
- All images in my organization: The exception applies to all images in your organization.
Package scope: The scope of the exception. The package scope can be:
- Selected package: The exception applies to the selected package.
- Any packages: The exception applies to all packages vulnerable to this CVE.
When you've filled in the details, select the Create button to create the exception.
The exception is now created and factored into the analysis results for the images that you selected. The exception is also listed on the Exceptions tab of the Vulnerabilities page in the Docker Scout Dashboard.",,,
4c8cb7c93efb87d56c4a6def98394d3134a246b93df35c16e044db7a4bad9165,"Format command and log output
Docker supports Go templates which you can use to manipulate the output format of certain commands and log drivers.
Docker provides a set of basic functions to manipulate template elements.
All of these examples use the docker inspect
command, but many other CLI
commands have a --format
flag, and many of the CLI command references
include examples of customizing the output format.
Note
When using the
--format
flag, you need observe your shell environment. In a POSIX shell, you can run the following with a single quote:$ docker inspect --format '{{join .Args "" , ""}}'
Otherwise, in a Windows shell (for example, PowerShell), you need to use single quotes, but escape the double quotes inside the parameters as follows:
$ docker inspect --format '{{join .Args \"" , \""}}'
join
join
concatenates a list of strings to create a single string.
It puts a separator between each element in the list.
$ docker inspect --format '{{join .Args "" , ""}}' container
table
table
specifies which fields you want to see its output.
$ docker image list --format ""table {{.ID}}\t{{.Repository}}\t{{.Tag}}\t{{.Size}}""
json
json
encodes an element as a json string.
$ docker inspect --format '{{json .Mounts}}' container
lower
lower
transforms a string into its lowercase representation.
$ docker inspect --format ""{{lower .Name}}"" container
split
split
slices a string into a list of strings separated by a separator.
$ docker inspect --format '{{split .Image "":""}}' container
title
title
capitalizes the first character of a string.
$ docker inspect --format ""{{title .Name}}"" container
upper
upper
transforms a string into its uppercase representation.
$ docker inspect --format ""{{upper .Name}}"" container
println
println
prints each value on a new line.
$ docker inspect --format='{{range .NetworkSettings.Networks}}{{println .IPAddress}}{{end}}' container
Hint
To find out what data can be printed, show all content as json:
$ docker container ls --format='{{json .}}'",,,
cd9da7e09ae8c3150191b0cb61103acc3e1f4f22cf783061512d59f1d2085231,"View container logs
The docker logs
command shows information logged by a running container. The
docker service logs
command shows information logged by all containers
participating in a service. The information that's logged and the format of the
log depends almost entirely on the container's endpoint command.
By default, docker logs
or docker service logs
shows the command's output
just as it would appear if you ran the command interactively in a terminal. Unix
and Linux commands typically open three I/O streams when they run, called
STDIN
, STDOUT
, and STDERR
. STDIN
is the command's input stream, which
may include input from the keyboard or input from another command. STDOUT
is
usually a command's normal output, and STDERR
is typically used to output
error messages. By default, docker logs
shows the command's STDOUT
and
STDERR
. To read more about I/O and Linux, see the
Linux Documentation Project article on I/O redirection.
In some cases, docker logs
may not show useful information unless you take
additional steps.
- If you use a
logging driver which sends logs to a file, an
external host, a database, or another logging back-end, and have
""dual logging""
disabled,
docker logs
may not show useful information. - If your image runs a non-interactive process such as a web server or a
database, that application may send its output to log files instead of
STDOUT
andSTDERR
.
In the first case, your logs are processed in other ways and you may choose not
to use docker logs
. In the second case, the official nginx
image shows one
workaround, and the official Apache httpd
image shows another.
The official nginx
image creates a symbolic link from /var/log/nginx/access.log
to /dev/stdout
, and creates another symbolic link
from /var/log/nginx/error.log
to /dev/stderr
, overwriting the log files and
causing logs to be sent to the relevant special device instead. See the
Dockerfile.
The official httpd
driver changes the httpd
application's configuration to
write its normal output directly to /proc/self/fd/1
(which is STDOUT
) and
its errors to /proc/self/fd/2
(which is STDERR
). See the
Dockerfile.
Next steps
- Configure logging drivers.
- Write a Dockerfile.",,,
d8f74f7f5c55ad2b683c78dfc5d123d79450ff86626cdc9f1ae696606fb33e58,"Docker Build Cloud release notes
Table of contents
This page contains information about the new features, improvements, known issues, and bug fixes in Docker Build Cloud releases.
2025-02-24
New
Added a new Build settings page where you can configure disk allocation, private resource access, and firewall settings for your cloud builders in your organization. These configurations help optimize storage, enable access to private registries, and secure outbound network traffic.",,,
a63608dfcdc9334b15dd6012bc83aeb519730cbf660321fedc27ca2298d843b7,"Docker container build driver
The Docker container driver allows creation of a managed and customizable BuildKit environment in a dedicated Docker container.
Using the Docker container driver has a couple of advantages over the default Docker driver. For example:
- Specify custom BuildKit versions to use.
- Build multi-arch images, see QEMU
- Advanced options for cache import and export
Synopsis
Run the following command to create a new builder, named container
, that uses
the Docker container driver:
$ docker buildx create \
--name container \
--driver=docker-container \
--driver-opt=[key=value,...]
container
The following table describes the available driver-specific options that you can
pass to --driver-opt
:
| Parameter | Type | Default | Description |
|---|---|---|---|
image | String | Sets the BuildKit image to use for the container. | |
memory | String | Sets the amount of memory the container can use. | |
memory-swap | String | Sets the memory swap limit for the container. | |
cpu-quota | String | Imposes a CPU CFS quota on the container. | |
cpu-period | String | Sets the CPU CFS scheduler period for the container. | |
cpu-shares | String | Configures CPU shares (relative weight) of the container. | |
cpuset-cpus | String | Limits the set of CPU cores the container can use. | |
cpuset-mems | String | Limits the set of CPU memory nodes the container can use. | |
default-load | Boolean | false | Automatically load images to the Docker Engine image store. |
network | String | Sets the network mode for the container. | |
cgroup-parent | String | /docker/buildx | Sets the cgroup parent of the container if Docker is using the ""cgroupfs"" driver. |
restart-policy | String | unless-stopped | Sets the container's restart policy. |
env.<key> | String | Sets the environment variable key to the specified value in the container. |
Before you configure the resource limits for the container, read about configuring runtime resource constraints for containers.
Usage
When you run a build, Buildx pulls the specified image
(by default,
moby/buildkit
).
When the container has started, Buildx submits the build submitted to the
containerized build server.
$ docker buildx build -t <image> --builder=container .
WARNING: No output specified with docker-container driver. Build result will only remain in the build cache. To push result image into registry use --push or to load image into docker use --load
#1 [internal] booting buildkit
#1 pulling image moby/buildkit:buildx-stable-1
#1 pulling image moby/buildkit:buildx-stable-1 1.9s done
#1 creating container buildx_buildkit_container0
#1 creating container buildx_buildkit_container0 0.5s done
#1 DONE 2.4s
...
Cache persistence
The docker-container
driver supports cache persistence, as it stores all the
BuildKit state and related cache into a dedicated Docker volume.
To persist the docker-container
driver's cache, even after recreating the
driver using docker buildx rm
and docker buildx create
, you can destroy the
builder using the --keep-state
flag:
For example, to create a builder named container
and then remove it while
persisting state:
# setup a builder
$ docker buildx create --name=container --driver=docker-container --use --bootstrap
container
$ docker buildx ls
NAME/NODE DRIVER/ENDPOINT STATUS BUILDKIT PLATFORMS
container * docker-container
container0 desktop-linux running v0.10.5 linux/amd64
$ docker volume ls
DRIVER VOLUME NAME
local buildx_buildkit_container0_state
# remove the builder while persisting state
$ docker buildx rm --keep-state container
$ docker volume ls
DRIVER VOLUME NAME
local buildx_buildkit_container0_state
# the newly created driver with the same name will have all the state of the previous one!
$ docker buildx create --name=container --driver=docker-container --use --bootstrap
container
QEMU
The docker-container
driver supports using
QEMU
(user mode) to build non-native platforms. Use the --platform
flag to specify
which architectures that you want to build for.
For example, to build a Linux image for amd64
and arm64
:
$ docker buildx build \
--builder=container \
--platform=linux/amd64,linux/arm64 \
-t <registry>/<image> \
--push .
Note
Emulation with QEMU can be much slower than native builds, especially for compute-heavy tasks like compilation and compression or decompression.
Custom network
You can customize the network that the builder container uses. This is useful if you need to use a specific network for your builds.
For example, let's
create a network
named foonet
:
$ docker network create foonet
Now create a
docker-container
builder
that will use this network:
$ docker buildx create --use \
--name mybuilder \
--driver docker-container \
--driver-opt ""network=foonet""
Boot and
inspect mybuilder
:
$ docker buildx inspect --bootstrap
Inspect the builder container and see what network is being used:
$ docker inspect buildx_buildkit_mybuilder0 --format={{.NetworkSettings.Networks}}
map[foonet:0xc00018c0c0]
Further reading
For more information on the Docker container driver, see the buildx reference.",,,
e11bd017615e6dc3048d683b9ef8fb424d4b8349d90e1196905de286e4e6334d,"FAQs on SSO and identity providers
Is it possible to use more than one IdP with Docker SSO?
No. You can only configure Docker SSO to work with a single IdP. A domain can only be associated with a single IdP. Docker supports Entra ID (formerly Azure AD) and identity providers that support SAML 2.0.
Is it possible to change my identity provider after configuring SSO?
Yes. You must delete your existing IdP configuration in your Docker SSO connection and then configure SSO using your new IdP. If you had already turned on enforcement, you should turn off enforcement before updating the provider SSO connection.
What information do I need from my identity provider to configure SSO?
To enable SSO in Docker, you need the following from your IdP:
SAML: Entity ID, ACS URL, Single Logout URL and the public X.509 certificate
Entra ID (formerly Azure AD): Client ID, Client Secret, AD Domain.
What happens if my existing certificate expires?
If your existing certificate has expired, you may need to contact your identity provider to retrieve a new X.509 certificate. Then, you need to update the certificate in the SSO configuration settings in Docker Hub or Docker Admin Console.
What happens if my IdP goes down when SSO is enabled?
If SSO is enforced, then it is not possible to access Docker Hub when your IdP is down. You can still access Docker Hub images from the CLI using your Personal Access Token.
If SSO is enabled but not enforced, then users could fallback to authenticate with username/password and trigger a reset password flow (if necessary).
How do I handle accounts using Docker Hub as a secondary registry? Do I need a bot account?
You can add a bot account to your IdP and create an access token for it to replace the other credentials.
Does a bot account need a seat to access an organization using SSO?
Yes, bot accounts need a seat, similar to a regular end user, having a non-aliased domain email enabled in the IdP and using a seat in Hub.
Does SAML SSO use Just-in-Time provisioning?
The SSO implementation uses Just-in-Time (JIT) provisioning by default. You can optionally disable JIT in the Admin Console if you enable auto-provisioning using SCIM. See Just-in-Time provisioning.
Is IdP-initiated sign-in available?
Docker SSO doesn't support IdP-initiated sign-in, only Service Provider Initiated (SP-initiated) sign-in.
Is it possible to connect Docker Hub directly with a Microsoft Entra (formerly Azure AD) group?
Yes, Entra ID (formerly Azure AD) is supported with SSO for Docker Business, both through a direct integration and through SAML.
My SSO connection with Entra ID isn't working and I receive an error that the application is misconfigured. How can I troubleshoot this?
Confirm that you've configured the necessary API permissions in Entra ID (formerly Azure AD) for your SSO connection. You need to grant admin consent within your Entra ID (formerly Azure AD) tenant. See Entra ID (formerly Azure AD) documentation.",,,
c6b9acfa2bf091088b51445de97e2272b4759f37addfa2c6b2f6c1a65529d268,"Generic environment integration with CLI
Table of contents
You can create a generic environment integration by running the Docker Scout
CLI client in your CI workflows. The CLI client is available as a binary on
GitHub and as a container image on Docker Hub. Use the client to invoke the
docker scout environment
command to assign your images to environments.
For more information about how to use the docker scout environment
command,
refer to the
CLI reference.
Examples
Before you start, set the following environment variables in your CI system:
DOCKER_SCOUT_HUB_USER
: your Docker Hub usernameDOCKER_SCOUT_HUB_PASSWORD
: your Docker Hub personal access token
Make sure the variables are accessible to your project.
version: 2.1
jobs:
record_environment:
machine:
image: ubuntu-2204:current
image: namespace/repo
steps:
- run: |
if [[ -z ""$CIRCLE_TAG"" ]]; then
tag=""$CIRCLE_TAG""
echo ""Running tag '$CIRCLE_TAG'""
else
tag=""$CIRCLE_BRANCH""
echo ""Running on branch '$CI_COMMIT_BRANCH'""
fi
echo ""tag = $tag""
- run: docker run -it \
-e DOCKER_SCOUT_HUB_USER=$DOCKER_SCOUT_HUB_USER \
-e DOCKER_SCOUT_HUB_PASSWORD=$DOCKER_SCOUT_HUB_PASSWORD \
docker/scout-cli:1.0.2 environment \
--org ""<MY_DOCKER_ORG>"" \
""<ENVIRONMENT>"" ${image}:${tag}
The following example uses the Docker executor.
variables:
image: namespace/repo
record_environment:
image: docker/scout-cli:1.0.2
script:
- |
if [[ -z ""$CI_COMMIT_TAG"" ]]; then
tag=""latest""
echo ""Running tag '$CI_COMMIT_TAG'""
else
tag=""$CI_COMMIT_REF_SLUG""
echo ""Running on branch '$CI_COMMIT_BRANCH'""
fi
echo ""tag = $tag""
- environment --org <MY_DOCKER_ORG> ""PRODUCTION"" ${image}:${tag}
trigger:
- main
resources:
- repo: self
variables:
tag: ""$(Build.BuildId)""
image: ""namespace/repo""
stages:
- stage: Docker Scout
displayName: Docker Scout environment integration
jobs:
- job: Record
displayName: Record environment
pool:
vmImage: ubuntu-latest
steps:
- task: Docker@2
- script: docker run -it \
-e DOCKER_SCOUT_HUB_USER=$DOCKER_SCOUT_HUB_USER \
-e DOCKER_SCOUT_HUB_PASSWORD=$DOCKER_SCOUT_HUB_PASSWORD \
docker/scout-cli:1.0.2 environment \
--org ""<MY_DOCKER_ORG>"" \
""<ENVIRONMENT>"" $(image):$(tag)
stage('Analyze image') {
steps {
// Install Docker Scout
sh 'curl -sSfL https://raw.githubusercontent.com/docker/scout-cli/main/install.sh | sh -s -- -b /usr/local/bin'
// Log into Docker Hub
sh 'echo $DOCKER_SCOUT_HUB_PASSWORD | docker login -u $DOCKER_SCOUT_HUB_USER --password-stdin'
// Analyze and fail on critical or high vulnerabilities
sh 'docker-scout environment --org ""<MY_DOCKER_ORG>"" ""<ENVIRONMENT>"" $IMAGE_TAG
}
}",,,
ad2ace04cc98ee5512c8de1767ab0e98c839aaccfa0a50bfeeb5861b8a8f9139,"Provenance attestations
The provenance attestations include facts about the build process, including details such as:
- Build timestamps
- Build parameters and environment
- Version control metadata
- Source code details
- Materials (files, scripts) consumed during the build
Provenance attestations follow the SLSA provenance schema, version 0.2.
For more information about how BuildKit populates these provenance properties, refer to SLSA definitions.
Create provenance attestations
To create a provenance attestation, pass the --attest type=provenance
option
to the docker buildx build
command:
$ docker buildx build --tag <namespace>/<image>:<version> \
--attest type=provenance,mode=[min,max] .
Alternatively, you can use the shorthand --provenance=true
option instead of --attest type=provenance
.
To specify the mode
parameter using the shorthand option, use: --provenance=mode=max
.
For an example on how to add provenance attestations with GitHub Actions, see Add attestations with GitHub Actions.
Mode
You can use the mode
parameter to define the level of detail to be included in
the provenance attestation. Supported values are mode=min
, and mode=max
(default).
Min
In min
mode, the provenance attestations include a minimal set of information,
such as:
- Build timestamps
- The frontend used
- Build materials
- Source repository and revision
- Build platform
- Reproducibility
Values of build arguments, the identities of secrets, and rich layer metadata is
not included mode=min
. The min
-level provenance is safe to use for all
builds, as it doesn't leak information from any part of the build environment.
The following JSON example shows the information included in a provenance
attestations created using the min
mode:
{
""_type"": ""https://in-toto.io/Statement/v0.1"",
""predicateType"": ""https://slsa.dev/provenance/v0.2"",
""subject"": [
{
""name"": ""pkg:docker/<registry>/<image>@<tag/digest>?platform=<platform>"",
""digest"": {
""sha256"": ""e8275b2b76280af67e26f068e5d585eb905f8dfd2f1918b3229db98133cb4862""
}
}
],
""predicate"": {
""builder"": { ""id"": """" },
""buildType"": ""https://mobyproject.org/buildkit@v1"",
""materials"": [
{
""uri"": ""pkg:docker/docker/dockerfile@1"",
""digest"": {
""sha256"": ""9ba7531bd80fb0a858632727cf7a112fbfd19b17e94c4e84ced81e24ef1a0dbc""
}
},
{
""uri"": ""pkg:docker/golang@1.19.4-alpine?platform=linux%2Farm64"",
""digest"": {
""sha256"": ""a9b24b67dc83b3383d22a14941c2b2b2ca6a103d805cac6820fd1355943beaf1""
}
}
],
""invocation"": {
""configSource"": { ""entryPoint"": ""Dockerfile"" },
""parameters"": {
""frontend"": ""gateway.v0"",
""args"": {
""cmdline"": ""docker/dockerfile:1"",
""source"": ""docker/dockerfile:1"",
""target"": ""binaries""
},
""locals"": [{ ""name"": ""context"" }, { ""name"": ""dockerfile"" }]
},
""environment"": { ""platform"": ""linux/arm64"" }
},
""metadata"": {
""buildInvocationID"": ""c4a87v0sxhliuewig10gnsb6v"",
""buildStartedOn"": ""2022-12-16T08:26:28.651359794Z"",
""buildFinishedOn"": ""2022-12-16T08:26:29.625483253Z"",
""reproducible"": false,
""completeness"": {
""parameters"": true,
""environment"": true,
""materials"": false
},
""https://mobyproject.org/buildkit@v1#metadata"": {
""vcs"": {
""revision"": ""a9ba846486420e07d30db1107411ac3697ecab68"",
""source"": ""git@github.com:<org>/<repo>.git""
}
}
}
}
}
Max
The max
mode includes all of the information included in the min
mode, as
well as:
- The LLB definition of the build. These show the exact steps taken to produce the image.
- Information about the Dockerfile, including a full base64-encoded version of the file.
- Source maps describing the relationship between build steps and image layers.
When possible, you should prefer mode=max
as it contains significantly more
detailed information for analysis.
Warning
Note that
mode=max
exposes the values of build arguments.If you're misusing build arguments to pass credentials, authentication tokens, or other secrets, you should refactor your build to pass the secrets using secret mounts instead. Secret mounts don't leak outside of the build and are never included in provenance attestations.
Inspecting Provenance
To explore created Provenance exported through the image
exporter, you can
use
imagetools inspect
.
Using the --format
option, you can specify a template for the output. All
provenance-related data is available under the .Provenance
attribute. For
example, to get the raw contents of the Provenance in the SLSA format:
$ docker buildx imagetools inspect <namespace>/<image>:<version> \
--format ""{{ json .Provenance.SLSA }}""
{
""buildType"": ""https://mobyproject.org/buildkit@v1"",
...
}
You can also construct more complex expressions using the full functionality of
Go templates. For example, for provenance generated with mode=max
, you can
extract the full source code of the Dockerfile used to build the image:
$ docker buildx imagetools inspect <namespace>/<image>:<version> \
--format '{{ range (index .Provenance.SLSA.metadata ""https://mobyproject.org/buildkit@v1#metadata"").source.infos }}{{ if eq .filename ""Dockerfile"" }}{{ .data }}{{ end }}{{ end }}' | base64 -d
FROM ubuntu:20.04
RUN apt-get update
...
Provenance attestation example
The following example shows what a JSON representation of a provenance
attestation with mode=max
looks like:
{
""_type"": ""https://in-toto.io/Statement/v0.1"",
""predicateType"": ""https://slsa.dev/provenance/v0.2"",
""subject"": [
{
""name"": ""pkg:docker/<registry>/<image>@<tag/digest>?platform=<platform>"",
""digest"": {
""sha256"": ""e8275b2b76280af67e26f068e5d585eb905f8dfd2f1918b3229db98133cb4862""
}
}
],
""predicate"": {
""builder"": { ""id"": """" },
""buildType"": ""https://mobyproject.org/buildkit@v1"",
""materials"": [
{
""uri"": ""pkg:docker/docker/dockerfile@1"",
""digest"": {
""sha256"": ""9ba7531bd80fb0a858632727cf7a112fbfd19b17e94c4e84ced81e24ef1a0dbc""
}
},
{
""uri"": ""pkg:docker/golang@1.19.4-alpine?platform=linux%2Farm64"",
""digest"": {
""sha256"": ""a9b24b67dc83b3383d22a14941c2b2b2ca6a103d805cac6820fd1355943beaf1""
}
}
],
""buildConfig"": {
""llbDefinition"": [
{
""id"": ""step4"",
""op"": {
""Op"": {
""exec"": {
""meta"": {
""args"": [""/bin/sh"", ""-c"", ""go mod download -x""],
""env"": [
""PATH=/go/bin:/usr/local/go/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"",
""GOLANG_VERSION=1.19.4"",
""GOPATH=/go"",
""CGO_ENABLED=0""
],
""cwd"": ""/src""
},
""mounts"": [
{ ""input"": 0, ""dest"": ""/"", ""output"": 0 },
{
""input"": -1,
""dest"": ""/go/pkg/mod"",
""output"": -1,
""mountType"": 3,
""cacheOpt"": { ""ID"": ""//go/pkg/mod"" }
},
{
""input"": 1,
""selector"": ""/go.mod"",
""dest"": ""/src/go.mod"",
""output"": -1,
""readonly"": true
},
{
""input"": 1,
""selector"": ""/go.sum"",
""dest"": ""/src/go.sum"",
""output"": -1,
""readonly"": true
}
]
}
},
""platform"": { ""Architecture"": ""arm64"", ""OS"": ""linux"" },
""constraints"": {}
},
""inputs"": [""step3:0"", ""step1:0""]
}
]
},
""metadata"": {
""buildInvocationID"": ""edf52vxjyf9b6o5qd7vgx0gru"",
""buildStartedOn"": ""2022-12-15T15:38:13.391980297Z"",
""buildFinishedOn"": ""2022-12-15T15:38:14.274565297Z"",
""reproducible"": false,
""completeness"": {
""parameters"": true,
""environment"": true,
""materials"": false
},
""https://mobyproject.org/buildkit@v1#metadata"": {
""vcs"": {
""revision"": ""a9ba846486420e07d30db1107411ac3697ecab68-dirty"",
""source"": ""git@github.com:<org>/<repo>.git""
},
""source"": {
""locations"": {
""step4"": {
""locations"": [
{
""ranges"": [
{ ""start"": { ""line"": 5 }, ""end"": { ""line"": 5 } },
{ ""start"": { ""line"": 6 }, ""end"": { ""line"": 6 } },
{ ""start"": { ""line"": 7 }, ""end"": { ""line"": 7 } },
{ ""start"": { ""line"": 8 }, ""end"": { ""line"": 8 } }
]
}
]
}
},
""infos"": [
{
""filename"": ""Dockerfile"",
""data"": ""RlJPTSBhbHBpbmU6bGF0ZXN0Cg=="",
""llbDefinition"": [
{
""id"": ""step0"",
""op"": {
""Op"": {
""source"": {
""identifier"": ""local://dockerfile"",
""attrs"": {
""local.differ"": ""none"",
""local.followpaths"": ""[\""Dockerfile\"",\""Dockerfile.dockerignore\"",\""dockerfile\""]"",
""local.session"": ""s4j58ngehdal1b5hn7msiqaqe"",
""local.sharedkeyhint"": ""dockerfile""
}
}
},
""constraints"": {}
}
},
{ ""id"": ""step1"", ""op"": { ""Op"": null }, ""inputs"": [""step0:0""] }
]
}
]
},
""layers"": {
""step2:0"": [
[
{
""mediaType"": ""application/vnd.docker.image.rootfs.diff.tar.gzip"",
""digest"": ""sha256:261da4162673b93e5c0e7700a3718d40bcc086dbf24b1ec9b54bca0b82300626"",
""size"": 3259190
},
{
""mediaType"": ""application/vnd.docker.image.rootfs.diff.tar.gzip"",
""digest"": ""sha256:bc729abf26b5aade3c4426d388b5ea6907fe357dec915ac323bb2fa592d6288f"",
""size"": 286218
},
{
""mediaType"": ""application/vnd.docker.image.rootfs.diff.tar.gzip"",
""digest"": ""sha256:7f1d6579712341e8062db43195deb2d84f63b0f2d1ed7c3d2074891085ea1b56"",
""size"": 116878653
},
{
""mediaType"": ""application/vnd.docker.image.rootfs.diff.tar.gzip"",
""digest"": ""sha256:652874aefa1343799c619d092ab9280b25f96d97939d5d796437e7288f5599c9"",
""size"": 156
}
]
]
}
}
}
}
}",,,
a53edc42aa2bbfaf586c60a8ed54d2bb4c6f030e24a6f1edbf74dd61ba9c3217,"Overview of installing Docker Compose
This page contains summary information about the available options for installing Docker Compose.
Installation scenarios
Scenario one: Install Docker Desktop
The easiest and recommended way to get Docker Compose is to install Docker Desktop. Docker Desktop includes Docker Compose along with Docker Engine and Docker CLI which are Compose prerequisites.
Docker Desktop is available on:
If you have already installed Docker Desktop, you can check which version of Compose you have by selecting About Docker Desktop from the Docker menu .
Note
After Docker Compose V1 was removed in Docker Desktop version 4.23.0 as it had reached end-of-life, the
docker-compose
command now points directly to the Docker Compose V2 binary, running in standalone mode. If you rely on Docker Desktop auto-update, the symlink might be broken and command unavailable, as the update doesn't ask for administrator password.This only affects Mac users. To fix this, either recreate the symlink:
$ sudo rm /usr/local/bin/docker-compose $ sudo ln -s /Applications/Docker.app/Contents/Resources/cli-plugins/docker-compose /usr/local/bin/docker-compose
Or enable Automatically check configuration which will detect and fix it for you.
Scenario two: Install the Docker Compose plugin
Important
This install scenario is only available on Linux.
If you already have Docker Engine and Docker CLI installed, you can install the Docker Compose plugin from the command line, by either:
Scenario three: Install the Docker Compose standalone
Warning
This install scenario is not recommended and is only supported for backward compatibility purposes.
You can install the Docker Compose standalone on Linux or on Windows Server.",,,
22859b79e5a4e049a25ae2106cf07d7dd1fd8cb9631f1a9c6f38fcedabed137a,"Integrate Docker Scout with Azure Container Registry
Integrating Docker Scout with Azure Container Registry (ACR) lets you view
image insights for images hosted in ACR repositories. After integrating Docker
Scout with ACR and activating Docker Scout for a repository, pushing an image
to the repository automatically triggers image analysis. You can view image
insights using the Docker Scout Dashboard, or the docker scout
CLI commands.
How it works
To help you integrate your Azure Container Registry with Docker Scout, you can use a custom Azure Resource Manager (ARM) template that automatically creates the necessary infrastructure in Azure for you:
- An EventGrid Topic and Subscription for Image push and delete events.
- A read-only authorization token for the registry, used to list repositories, and ingest the images.
When the resources have been created in Azure, you can enable the integration for image repositories in the integrated ACR instance. Once you've enabled a repository, pushing new images triggers image analysis automatically. The analysis results appear in the Docker Scout Dashboard.
If you enable the integration on a repository that already contains images, Docker Scout pulls and analyzes the latest image version automatically.
ARM template
The following table describes the configuration resources.
Note
Creating these resources incurs a small, recurring cost on the Azure account. The Cost column in the table represents an estimated monthly cost of the resources, when integrating an ACR registry that gets 100 images pushed per day.
The Egress cost varies depending on usage, but it’s around $0.1 per GB, and the first 100 GB are free.
| Azure | Resource | Cost |
|---|---|---|
| Event Grid system topic | Subscribe to Azure Container Registry events (image push and image delete) | Free |
| Event subscription | Send Event Grid events to Scout via a Webhook subscription | $0.60 for every 1M messages. First 100k for free. |
| Registry Token | Read-only token used for Scout to list the repositories, and pull images from the registry | Free |
The following JSON document shows the ARM template Docker Scout uses to create the Azure resources.
{
""$schema"": ""https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#"",
""contentVersion"": ""1.0.0.0"",
""parameters"": {
""DockerScoutWebhook"": {
""metadata"": {
""description"": ""EventGrid's subscription Webhook""
},
""type"": ""String""
},
""RegistryName"": {
""metadata"": {
""description"": ""Name of the registry to add Docker Scout""
},
""type"": ""String""
},
""systemTopics_dockerScoutRepository"": {
""defaultValue"": ""docker-scout-repository"",
""metadata"": {
""description"": ""EventGrid's topic name""
},
""type"": ""String""
}
},
""resources"": [
{
""apiVersion"": ""2023-06-01-preview"",
""identity"": {
""type"": ""None""
},
""location"": ""[resourceGroup().location]"",
""name"": ""[parameters('systemTopics_dockerScoutRepository')]"",
""properties"": {
""source"": ""[extensionResourceId(resourceGroup().Id , 'Microsoft.ContainerRegistry/Registries', parameters('RegistryName'))]"",
""topicType"": ""Microsoft.ContainerRegistry.Registries""
},
""type"": ""Microsoft.EventGrid/systemTopics""
},
{
""apiVersion"": ""2023-06-01-preview"",
""dependsOn"": [
""[resourceId('Microsoft.EventGrid/systemTopics', parameters('systemTopics_dockerScoutRepository'))]""
],
""name"": ""[concat(parameters('systemTopics_dockerScoutRepository'), '/image-change')]"",
""properties"": {
""destination"": {
""endpointType"": ""WebHook"",
""properties"": {
""endpointUrl"": ""[parameters('DockerScoutWebhook')]"",
""maxEventsPerBatch"": 1,
""preferredBatchSizeInKilobytes"": 64
}
},
""eventDeliverySchema"": ""EventGridSchema"",
""filter"": {
""enableAdvancedFilteringOnArrays"": true,
""includedEventTypes"": [
""Microsoft.ContainerRegistry.ImagePushed"",
""Microsoft.ContainerRegistry.ImageDeleted""
]
},
""labels"": [],
""retryPolicy"": {
""eventTimeToLiveInMinutes"": 1440,
""maxDeliveryAttempts"": 30
}
},
""type"": ""Microsoft.EventGrid/systemTopics/eventSubscriptions""
},
{
""apiVersion"": ""2023-01-01-preview"",
""name"": ""[concat(parameters('RegistryName'), '/docker-scout-readonly-token')]"",
""properties"": {
""credentials"": {},
""scopeMapId"": ""[resourceId('Microsoft.ContainerRegistry/registries/scopeMaps', parameters('RegistryName'), '_repositories_pull_metadata_read')]""
},
""type"": ""Microsoft.ContainerRegistry/registries/tokens""
}
],
""variables"": {}
}
Integrate a registry
Go to ACR integration page on the Docker Scout Dashboard.
In the How to integrate section, enter the Registry hostname of the registry you want to integrate.
Select Next.
Select Deploy to Azure to open the template deployment wizard in Azure.
You may be prompted to sign in to your Azure account if you're not already signed in.
In the template wizard, configure your deployment:
Resource group: enter the same resource group as you're using for the container registry. The Docker Scout resources must be deployed to the same resource group as the registry.
Registry name: the field is pre-filled with the subdomain of the registry hostname.
Select Review + create, and then Create to deploy the template.
Wait until the deployment is complete.
In the Deployment details section click on the newly created resource of the type Container registry token. Generate a new password for this token.
Alternatively, use the search function in Azure to navigate to the Container registry resource that you're looking to integrate, and generate the new password for the created access token.
Copy the generated password and head back to the Docker Scout Dashboard to finalize the integration.
Paste the generated password into the Registry token field.
Select Enable integration.
After selecting Enable integration, Docker Scout performs a connection test to verify the integration. If the verification was successful, you're redirected to the Azure registry summary page, which shows you all your Azure integrations for the current organization.
Next, activate Docker Scout for the repositories that you want to analyze in Repository settings.
After activating repositories, images that you push are analyzed by Docker Scout. The analysis results appear in the Docker Scout Dashboard. If your repository already contains images, Docker Scout pulls and analyzes the latest image version automatically.
Remove an integration
Important
Removing the integration in the Docker Scout Dashboard doesn't automatically remove the resources created in Azure.
To remove an ACR integration:
Go to the ACR integration page on the Docker Scout Dashboard.
Find the ACR integration that you want to remove, and select the Remove button.
In the dialog that opens, confirm by selecting Remove.
After removing the integration in the Docker Scout Dashboard, also remove the Azure resources related to the integration:
- The
docker-scout-readonly-token
token for the container registry. - The
docker-scout-repository
Event Grid System Topic.
- The",,,
9767032a837e759917f717ce4baf1a111508ab487896295912097812aa87c926,"Enable GPU access with Docker Compose
Compose services can define GPU device reservations if the Docker host contains such devices and the Docker Daemon is set accordingly. For this, make sure you install the prerequisites if you haven't already done so.
The examples in the following sections focus specifically on providing service containers access to GPU devices with Docker Compose.
You can use either docker-compose
or docker compose
commands. For more information, see
Migrate to Compose V2.
Enabling GPU access to service containers
GPUs are referenced in a compose.yaml
file using the
device attribute from the Compose Deploy specification, within your services that need them.
This provides more granular control over a GPU reservation as custom values can be set for the following device properties:
capabilities
. This value specifies as a list of strings (eg.capabilities: [gpu]
). You must set this field in the Compose file. Otherwise, it returns an error on service deployment.count
. This value, specified as an integer or the valueall
, represents the number of GPU devices that should be reserved (providing the host holds that number of GPUs). Ifcount
is set toall
or not specified, all GPUs available on the host are used by default.device_ids
. This value, specified as a list of strings, represents GPU device IDs from the host. You can find the device ID in the output ofnvidia-smi
on the host. If nodevice_ids
are set, all GPUs available on the host are used by default.driver
. This value is specified as a string, for exampledriver: 'nvidia'
options
. Key-value pairs representing driver specific options.
Important
You must set the
capabilities
field. Otherwise, it returns an error on service deployment.
count
anddevice_ids
are mutually exclusive. You must only define one field at a time.
For more information on these properties, see the Compose Deploy Specification.
Example of a Compose file for running a service with access to 1 GPU device
services:
test:
image: nvidia/cuda:12.3.1-base-ubuntu20.04
command: nvidia-smi
deploy:
resources:
reservations:
devices:
- driver: nvidia
count: 1
capabilities: [gpu]
Run with Docker Compose:
$ docker compose up
Creating network ""gpu_default"" with the default driver
Creating gpu_test_1 ... done
Attaching to gpu_test_1
test_1 | +-----------------------------------------------------------------------------+
test_1 | | NVIDIA-SMI 450.80.02 Driver Version: 450.80.02 CUDA Version: 11.1 |
test_1 | |-------------------------------+----------------------+----------------------+
test_1 | | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |
test_1 | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |
test_1 | | | | MIG M. |
test_1 | |===============================+======================+======================|
test_1 | | 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 |
test_1 | | N/A 23C P8 9W / 70W | 0MiB / 15109MiB | 0% Default |
test_1 | | | | N/A |
test_1 | +-------------------------------+----------------------+----------------------+
test_1 |
test_1 | +-----------------------------------------------------------------------------+
test_1 | | Processes: |
test_1 | | GPU GI CI PID Type Process name GPU Memory |
test_1 | | ID ID Usage |
test_1 | |=============================================================================|
test_1 | | No running processes found |
test_1 | +-----------------------------------------------------------------------------+
gpu_test_1 exited with code 0
On machines hosting multiple GPUs, the device_ids
field can be set to target specific GPU devices and count
can be used to limit the number of GPU devices assigned to a service container.
You can use count
or device_ids
in each of your service definitions. An error is returned if you try to combine both, specify an invalid device ID, or use a value of count that’s higher than the number of GPUs in your system.
$ nvidia-smi
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02 Driver Version: 450.80.02 CUDA Version: 11.0 |
|-------------------------------+----------------------+----------------------+
| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |
| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |
| | | MIG M. |
|===============================+======================+======================|
| 0 Tesla T4 On | 00000000:00:1B.0 Off | 0 |
| N/A 72C P8 12W / 70W | 0MiB / 15109MiB | 0% Default |
| | | N/A |
+-------------------------------+----------------------+----------------------+
| 1 Tesla T4 On | 00000000:00:1C.0 Off | 0 |
| N/A 67C P8 11W / 70W | 0MiB / 15109MiB | 0% Default |
| | | N/A |
+-------------------------------+----------------------+----------------------+
| 2 Tesla T4 On | 00000000:00:1D.0 Off | 0 |
| N/A 74C P8 12W / 70W | 0MiB / 15109MiB | 0% Default |
| | | N/A |
+-------------------------------+----------------------+----------------------+
| 3 Tesla T4 On | 00000000:00:1E.0 Off | 0 |
| N/A 62C P8 11W / 70W | 0MiB / 15109MiB | 0% Default |
| | | N/A |
+-------------------------------+----------------------+----------------------+
Access specific devices
To allow access only to GPU-0 and GPU-3 devices:
services:
test:
image: tensorflow/tensorflow:latest-gpu
command: python -c ""import tensorflow as tf;tf.test.gpu_device_name()""
deploy:
resources:
reservations:
devices:
- driver: nvidia
device_ids: ['0', '3']
capabilities: [gpu]",,,
d2984d60cb147a1e53df92f02727da414e2e865b560bb6ea33c9d0450a161c58,"Docker
Docker objects
▸ listContainers(options?
): Promise
<unknown
>
To get the list of containers:
const containers = await ddClient.docker.listContainers();
▸ listImages(options?
): Promise
<unknown
>
To get the list of local container images:
const images = await ddClient.docker.listImages();
See the Docker API reference for details about these methods.
Deprecated access to Docker objects
The methods below are deprecated and will be removed in a future version. Use the methods specified above.
const containers = await window.ddClient.listContainers();
const images = await window.ddClient.listImages();
Docker commands
Extensions can also directly execute the docker
command line.
▸ exec(cmd
, args
): Promise
<
ExecResult
>
const result = await ddClient.docker.cli.exec(""info"", [
""--format"",
'""{{ json . }}""',
]);
The result contains both the standard output and the standard error of the executed command:
{
""stderr"": ""..."",
""stdout"": ""...""
}
In this example, the command output is JSON. For convenience, the command result object also has methods to easily parse it:
result.lines(): string[]
splits output lines.result.parseJsonObject(): any
parses a well-formed json output.result.parseJsonLines(): any[]
parses each output line as a json object.
▸ exec(cmd
, args
, options
): void
The command above streams the output as a result of the execution of a Docker command. This is useful if you need to get the output as a stream or the output of the command is too long.
await ddClient.docker.cli.exec(""logs"", [""-f"", ""...""], {
stream: {
onOutput(data) {
if (data.stdout) {
console.error(data.stdout);
} else {
console.log(data.stderr);
}
},
onError(error) {
console.error(error);
},
onClose(exitCode) {
console.log(""onClose with exit code "" + exitCode);
},
splitOutputLines: true,
},
});
The child process created by the extension is killed (SIGTERM
) automatically when you close the dashboard in Docker Desktop or when you exit the extension UI.
If needed, you can also use the result of the exec(streamOptions)
call in order to kill (SIGTERM
) the process.
const logListener = await ddClient.docker.cli.exec(""logs"", [""-f"", ""...""], {
stream: {
// ...
},
});
// when done listening to logs or before starting a new one, kill the process
logListener.close();
This exec(streamOptions)
API can also be used to listen to docker events:
await ddClient.docker.cli.exec(
""events"",
[""--format"", ""{{ json . }}"", ""--filter"", ""container=my-container""],
{
stream: {
onOutput(data) {
if (data.stdout) {
const event = JSON.parse(data.stdout);
console.log(event);
} else {
console.log(data.stderr);
}
},
onClose(exitCode) {
console.log(""onClose with exit code "" + exitCode);
},
splitOutputLines: true,
},
}
);
Note
You cannot use this to chain commands in a single
exec()
invocation (likedocker kill $(docker ps -q)
or using pipe between commands).You need to invoke
exec()
for each command and parse results to pass parameters to the next command if needed.
See the Exec API reference for details about these methods.
Deprecated execution of Docker commands
This method is deprecated and will be removed in a future version. Use the one specified just below.
const output = await window.ddClient.execDockerCmd(
""info"",
""--format"",
'""{{ json . }}""'
);
window.ddClient.spawnDockerCmd(""logs"", [""-f"", ""...""], (data, error) => {
console.log(data.stdout);
});",,,
1e50da455adfacc8949ca53639cd34a2676c8475cd12a3dbbacd59a6b618f76d,"Overview of Dev Environments
Important
Dev Environments is no longer under active development.
While the current functionality remains available, it may take us longer to respond to support requests.
Dev Environments let you create a configurable developer environment with all the code and tools you need to quickly get up and running.
It uses tools built into code editors that allows Docker to access code mounted into a container rather than on your local host. This isolates the tools, files and running services on your machine allowing multiple versions of them to exist side by side.
You can use Dev Environments through the intuitive GUI in Docker Desktop Dashboard or straight from your terminal with the new
docker dev
CLI plugin.
Use Dev Environments
To use Dev Environments:
- Navigate to the Features in Development tab in Settings.
- On the Beta tab, select Turn on Dev Environments.
- Select Apply & restart.
The Dev Environments tab is now visible in Docker Desktop Dashboard.
How does it work?
Changes to Dev Environments with Docker Desktop 4.13
Docker has simplified how you configure your dev environment project. All you need to get started is a
compose-dev.yaml
file. If you have an existing project with a.docker/
folder this is automatically migrated the next time you launch.
Dev Environments is powered by Docker Compose. This allows Dev Environments to take advantage of all the benefits and features of Compose whilst adding an intuitive GUI where you can launch environments with the click of a button.
Every dev environment you want to run needs a compose-dev.yaml
file which configures your application's services and lives in your project directory. You don't need to be an expert in Docker Compose or write a compose-dev.yaml
file from scratch as Dev Environments creates a starter compose-dev.yaml
files based on the main language in your project.
You can also use the many sample dev environments as a starting point for how to integrate different services. Alternatively, see Set up a dev environment for more information.
What's next?
Learn how to:",,,
6502794bbf303617a41b764e54a4b8d17a5f069b9a2849e2da13deac0f6e25d2,"Create an account
You can create a free Docker account with your email address or by signing up with your Google or GitHub account. Once you've created your account with a unique Docker ID, you can access all Docker products, including Docker Hub. With Docker Hub, you can access repositories and explore images that are available from the community and verified publishers.
Your Docker ID becomes your username for hosted Docker services, and Docker forums.
Tip
Explore Docker's subscriptions to see what else Docker can offer you.
Create a Docker ID
Sign up with your email address
Go to the Docker sign-up page.
Enter a unique, valid email address.
Enter a username to use as your Docker ID. Once you create your Docker ID you can't reuse it in the future if you deactivate this account.
Your username:
- Must be between 4 and 30 characters long
- Can only contain numbers and lowercase letters
Enter a password that's at least 9 characters long.
Select Sign Up.
Open your email client. Docker sends a verification email to the address you provided.
Verify your email address to complete the registration process.
Note
You must verify your email address before you have full access to Docker's features.
Sign up with Google or GitHub
Important
To sign up with your social provider, you must verify your email address with your provider before you begin.
Go to the Docker sign-up page.
Select your social provider, Google or GitHub.
Select the social account you want to link to your Docker account.
Select Authorize Docker to let Docker to access your social account information. You will be re-routed to the sign-up page.
Enter a username to use as your Docker ID.
Your username:
- Must be between 4 and 30 characters long
- Can only contain numbers and lowercase letters
Select Sign up.
Sign in
Once you register your Docker ID and verify your email address, you can sign in to your Docker account. You can either:
- Sign in with your email address (or username) and password.
- Sign in with your social provider. For more information, see Sign in with your social provider.
- Sign in through the CLI using the
docker login
command. For more information, seedocker login
.
Warning
When you use the
docker login
command, your credentials are stored in your home directory in.docker/config.json
. The password is base64-encoded in this file.We recommend using one of the Docker credential helpers for secure storage of passwords. For extra security, you can also use a personal access token to sign in instead, which is still encoded in this file (without a Docker credential helper) but doesn't permit administrator actions (such as changing the password).
Sign in with your social provider
Important
To sign in with your social provider, you must verify your email address with your provider before you begin.
You can also sign in to your Docker account with your Google or GitHub account. If a Docker account exists with the same email address as the primary email for your social provider, your Docker account will automatically be linked to the social profile. This lets you sign in with your social provider.
If you try to sign in with your social provider and don't have a Docker account yet, a new account will be created for you. Follow the on-screen instructions to create a Docker ID using your social provider.
Reset your password at sign in
To reset your password, enter your email address on the Sign in page and continue to sign in. When prompted for your password, select Forgot password?.
Troubleshooting
If you have a paid Docker subscription, you can contact the Support team for assistance.
All Docker users can seek troubleshooting information and support through the following resources, where Docker or the community respond on a best effort basis:",,,
6aa54ce41e8c296234238aabd9f471912b441ef358ad740663ef919fd6060649,"Live restore
By default, when the Docker daemon terminates, it shuts down running containers. You can configure the daemon so that containers remain running if the daemon becomes unavailable. This functionality is called live restore. The live restore option helps reduce container downtime due to daemon crashes, planned outages, or upgrades.
Note
Live restore isn't supported for Windows containers, but it does work for Linux containers running on Docker Desktop for Windows.
Enable live restore
There are two ways to enable the live restore setting to keep containers alive when the daemon becomes unavailable. Only do one of the following.
Add the configuration to the daemon configuration file. On Linux, this defaults to
/etc/docker/daemon.json
. On Docker Desktop for Mac or Docker Desktop for Windows, select the Docker icon from the task bar, then click Settings -> Docker Engine.Use the following JSON to enable
live-restore
.{ ""live-restore"": true }
Restart the Docker daemon. On Linux, you can avoid a restart (and avoid any downtime for your containers) by reloading the Docker daemon. If you use
systemd
, then use the commandsystemctl reload docker
. Otherwise, send aSIGHUP
signal to thedockerd
process.
If you prefer, you can start the
dockerd
process manually with the--live-restore
flag. This approach isn't recommended because it doesn't set up the environment thatsystemd
or another process manager would use when starting the Docker process. This can cause unexpected behavior.
Live restore during upgrades
Live restore allows you to keep containers running across Docker daemon updates,
but is only supported when installing patch releases (YY.MM.x
), not for
major (YY.MM
) daemon upgrades.
If you skip releases during an upgrade, the daemon may not restore its connection to the containers. If the daemon can't restore the connection, it can't manage the running containers and you must stop them manually.
Live restore upon restart
The live restore option only works to restore containers if the daemon options, such as bridge IP addresses and graph driver, didn't change. If any of these daemon-level configuration options have changed, the live restore may not work and you may need to manually stop the containers.
Impact of live restore on running containers
If the daemon is down for a long time, running containers may fill up the FIFO log the daemon normally reads. A full log blocks containers from logging more data. The default buffer size is 64K. If the buffers fill, you must restart the Docker daemon to flush them.
On Linux, you can modify the kernel's buffer size by changing
/proc/sys/fs/pipe-max-size
. You can't modify the buffer size on Docker Desktop for
Mac or Docker Desktop for Windows.
Live restore and Swarm mode
The live restore option only pertains to standalone containers, and not to Swarm services. Swarm services are managed by Swarm managers. If Swarm managers are not available, Swarm services continue to run on worker nodes but can't be managed until enough Swarm managers are available to maintain a quorum.",,,
6c2a1d5ebe4acb602a1d7f3e68c35c94a4adb643aee5f5f3521c2b7f3208b430,"Migrate to Compose V2
From July 2023 Compose V1 stopped receiving updates. It’s also no longer available in new releases of Docker Desktop.
Compose V2, which was first released in 2020, is included with all currently supported versions of Docker Desktop. It offers an improved CLI experience, improved build performance with BuildKit, and continued new-feature development.
How do I switch to Compose V2?
The easiest and recommended way is to make sure you have the latest version of Docker Desktop, which bundles the Docker Engine and Docker CLI platform including Compose V2.
With Docker Desktop, Compose V2 is always accessible as docker compose
.
Additionally, the Use Compose V2 setting is turned on by default, which provides an alias from docker-compose
.
For manual installs on Linux, you can get Compose V2 by either:
What are the differences between Compose V1 and Compose V2?
docker-compose
vs docker compose
Unlike Compose V1, Compose V2 integrates into the Docker CLI platform and the recommended command-line syntax is docker compose
.
The Docker CLI platform provides a consistent and predictable set of options and flags, such as the DOCKER_HOST
environment variable or the --context
command-line flag.
This change lets you use all of the shared flags on the root docker
command.
For example, docker --log-level=debug --tls compose up
enables debug logging from the Docker Engine as well as ensuring that TLS is used for the connection.
Tip
Update scripts to use Compose V2 by replacing the hyphen (
-
) with a space, usingdocker compose
instead ofdocker-compose
.
Service container names
Compose generates container names based on the project name, service name, and scale/replica count.
In Compose V1, an underscore (_
) was used as the word separator.
In Compose V2, a hyphen (-
) is used as the word separator.
Underscores aren't valid characters in DNS hostnames. By using a hyphen instead, Compose V2 ensures service containers can be accessed over the network via consistent, predictable hostnames.
For example, running the Compose command -p myproject up --scale=1 svc
results in a container named myproject_svc_1
with Compose V1 and a container named myproject-svc-1
with Compose V2.
Tip
In Compose V2, the global
--compatibility
flag orCOMPOSE_COMPATIBILITY
environment variable preserves the Compose V1 behavior to use underscores (_
) as the word separator. As this option must be specified for every Compose V2 command run, it's recommended that you only use this as a temporary measure while transitioning to Compose V2.
Command-line flags and subcommands
Compose V2 supports almost all Compose V1 flags and subcommands, so in most cases, it can be used as a drop-in replacement in scripts.
Unsupported in V2
The following were deprecated in Compose V1 and aren't supported in Compose V2:
docker-compose scale
. Usedocker compose up --scale
instead.docker-compose rm --all
Different in V2
The following behave differently between Compose V1 and V2:
| Compose V1 | Compose V2 | |
|---|---|---|
--compatibility | Deprecated. Migrates YAML fields based on legacy schema version. | Uses _ as word separator for container names instead of - to match V1. |
ps --filter KEY-VALUE | Undocumented. Allows filtering by arbitrary service properties. | Only allows filtering by specific properties, e.g. --filter=status=running . |
Environment variables
Environment variable behavior in Compose V1 wasn't formally documented and behaved inconsistently in some edge cases.
For Compose V2, the
Environment variables section covers both
precedence as well as
.env
file interpolation and includes many examples covering tricky situations such as escaping nested quotes.
Check if:
- Your project uses multiple levels of environment variable overrides, for example
.env
file and--env
CLI flags. - Any
.env
file values have escape sequences or nested quotes. - Any
.env
file values contain literal$
signs in them. This is common with PHP projects. - Any variable values use advanced expansion syntax, for example
${VAR:?error}
.
Tip
Run
docker compose config
on the project to preview the configuration after Compose V2 has performed interpolation to verify that values appear as expected.Maintaining backwards compatibility with Compose V1 is typically achievable by ensuring that literal values (no interpolation) are single-quoted and values that should have interpolation applied are double-quoted.
What does this mean for my projects that use Compose V1?
For most projects, switching to Compose V2 requires no changes to the Compose YAML or your development workflow.
It's recommended that you adapt to the new preferred way of running Compose V2, which is to use docker compose
instead of docker-compose
.
This provides additional flexibility and removes the requirement for a docker-compose
compatibility alias.
However, Docker Desktop continues to support a docker-compose
alias to redirect commands to docker compose
for convenience and improved compatibility with third-party tools and scripts.
Is there anything else I need to know before I switch?
Migrating running projects
In both V1 and V2, running up
on a Compose project recreates service containers as necessary to reach the desired state based on comparing the actual state in the Docker Engine to the resolved project configuration including Compose YAML, environment variables, and command-line flags.
Because Compose V1 and V2
name service containers differently, running up
using V2 the first time on a project with running services originally launched by V1, results in service containers being recreated with updated names.
Note that even if --compatibility
flag is used to preserve the V1 naming style, Compose still needs to recreate service containers originally launched by V1 the first time up
is run by V2 to migrate the internal state.
Using Compose V2 with Docker-in-Docker
Compose V2 is now included in the Docker official image on Docker Hub.
Additionally, a new docker/compose-bin image on Docker Hub packages the latest version of Compose V2 for use in multi-stage builds.
Can I still use Compose V1 if I want to?
Yes. You can still download and install Compose V1 packages, but you won't get support from Docker if anything breaks.
Warning
The final Compose V1 release, version 1.29.2, was May 10, 2021. These packages haven't received any security updates since then. Use at your own risk.",,,
3dee0b58472396a5288e65d4eaa48c296c765b6ad03b45eea30308c6db56338c,"Known issues
- IPv6 is not yet supported on Docker Desktop.
The Mac Activity Monitor reports that Docker is using twice the amount of memory it's actually using. This is due to a bug in MacOS. We have written a detailed report on this.
Force-ejecting the
.dmg
after runningDocker.app
from it can cause the whale icon to become unresponsive, Docker tasks to show as not responding in the Activity Monitor, and for some processes to consume a large amount of CPU resources. Reboot and restart Docker to resolve these issues.Docker doesn't auto-start after sign in even when it's enabled in Settings. This is related to a set of issues with Docker helper, registration, and versioning.
Docker Desktop uses the
HyperKit
hypervisor ( https://github.com/docker/hyperkit) in macOS 10.10 Yosemite and higher. If you are developing with tools that have conflicts withHyperKit
, such as Intel Hardware Accelerated Execution Manager (HAXM), the current workaround is not to run them at the same time. You can pauseHyperKit
by quitting Docker Desktop temporarily while you work with HAXM. This allows you to continue work with the other tools and preventHyperKit
from interfering.If you are working with applications like Apache Maven that expect settings for
DOCKER_HOST
andDOCKER_CERT_PATH
environment variables, specify these to connect to Docker instances through Unix sockets. For example:$ export DOCKER_HOST=unix:///var/run/docker.sock
There are a number of issues with the performance of directories bind-mounted into containers. In particular, writes of small blocks, and traversals of large directories are currently slow. Additionally, containers that perform large numbers of directory operations, such as repeated scans of large directory trees, may suffer from poor performance. Applications that behave in this way include:
rake
ember build
- Symfony
- Magento
- Zend Framework
- PHP applications that use
Composer to install
dependencies in a
vendor
folder
As a workaround for this behavior, you can put vendor or third-party library directories in Docker volumes, perform temporary file system operations outside of bind mounts, and use third-party tools like Unison or
rsync
to synchronize between container directories and bind-mounted directories. We are actively working on performance improvements using a number of different techniques. To learn more, see the topic on our roadmap.
On Apple silicon in native
arm64
containers, older versions oflibssl
such asdebian:buster
,ubuntu:20.04
, andcentos:8
will segfault when connected to some TLS servers, for example,curl https://dl.yarnpkg.com
. The bug is fixed in newer versions oflibssl
indebian:bullseye
,ubuntu:21.04
, andfedora:35
.Some command line tools do not work when Rosetta 2 is not installed.
- The old version 1.x of
docker-compose
. Use Compose V2 instead - typedocker compose
. - The
docker-credential-ecr-login
credential helper.
- The old version 1.x of
Some images do not support the ARM64 architecture. You can add
--platform linux/amd64
to run (or build) an Intel image using emulation.However, attempts to run Intel-based containers on Apple silicon machines under emulation can crash as qemu sometimes fails to run the container. In addition, filesystem change notification APIs (
inotify
) do not work under qemu emulation. Even when the containers do run correctly under emulation, they will be slower and use more memory than the native equivalent.In summary, running Intel-based containers on Arm-based machines should be regarded as ""best effort"" only. We recommend running arm64 containers on Apple silicon machines whenever possible, and encouraging container authors to produce arm64, or multi-arch, versions of their containers. This issue should become less common over time, as more and more images are rebuilt supporting multiple architectures.
ping
from inside a container to the Internet does not work as expected. To test the network, usecurl
orwget
. See docker/for-mac#5322.Users may occasionally experience data drop when a TCP stream is half-closed.",,,
4e6a54b278fa708fbed3ec9634b701eb755b7660b153266c5058b00124252696,"Docker Engine 23.0 release notes
Note
From Docker Engine version 23.0.0, Buildx is distributed in a separate package:
docker-buildx-plugin
. In earlier versions, Buildx was included in thedocker-ce-cli
package. When you upgrade to this version of Docker Engine, make sure you update all packages. For example, on Ubuntu:$ sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
Refer to the Docker Engine installation instructions for your operating system for more details on upgrading Docker Engine.
This page describes the latest changes, additions, known issues, and fixes for Docker Engine version 23.0.
For more information about:
- Deprecated and removed features, see Deprecated Engine Features.
- Changes to the Engine API, see Engine API version history.
Starting with the 23.0.0 release, Docker Engine moves away from using CalVer versioning, and starts using the SemVer versioning format. Changing the version format is a stepping-stone towards Go module compatibility, but the repository doesn't yet use Go modules, and still requires using a ""+incompatible"" version. Work continues towards Go module compatibility in a future release.
23.0.6
2023-05-08For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- Fix vfs storage driver not working on NFS. moby/moby#45465
Packaging Updates
- Upgrade Go to
1.19.9
. docker/docker-ce-packaging#889, docker/cli#4254, moby/moby#45455 - Upgrade
containerd
to v1.6.21 - Upgrade
runc
to v1.1.7
23.0.5
2023-04-26For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- Add the
--all
/-a
option when pruning volumes. docker/cli#4229 - Add
--format=json
fordocker info
. docker/cli#4320 - Fix log loss with the AWSLogs log driver. moby/moby#45350
- Fix a regression introduced in v23.0.4 where dockerd would refuse to start if the fixed-cidr config parameter is provided but not bip. moby/moby#45403
- Fix a panic in libnetwork during daemon start moby/moby#45376
- Fix ""tag"" event not being sent when an image is built with
buildx
. moby/moby#45410
Packaging Updates
- Upgrade Compose to
2.17.3
. docker/docker-ce-packaging#883
23.0.4
2023-04-17For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- Fix a performance regression in Docker CLI 23.0.0 docker/cli#4141.
- Fix progress indicator on
docker cp
not functioning as intended docker/cli#4157. - Fix shell completion for
docker compose --file
docker/cli#4177. - Fix an error caused by incorrect handling of ""default-address-pools"" in
daemon.json
moby/moby#45246.
Packaging Updates
- Fix missing packages for CentOS 9 Stream.
- Upgrade Go to
1.19.8
. docker/docker-ce-packaging#878, docker/cli#4164, moby/moby#45277, which contains fixes for CVE-2023-24537, CVE-2023-24538, CVE-2023-24534, and CVE-2023-24536
23.0.3
2023-04-04Note
Due to an issue with CentOS 9 Stream's package repositories, packages for CentOS 9 are currently unavailable. Packages for CentOS 9 may be added later, or as part of the next (23.0.4) patch release.
Bug fixes and enhancements
- Fixed a number of issues that can cause Swarm encrypted overlay networks
to fail to uphold their guarantees, addressing
CVE-2023-28841,
CVE-2023-28840, and
CVE-2023-28842.
- A lack of kernel support for encrypted overlay networks now reports as an error.
- Encrypted overlay networks are eagerly set up, rather than waiting for multiple nodes to attach.
- Encrypted overlay networks are now usable on Red Hat Enterprise Linux 9
through the use of the
xt_bpf
kernel module. - Users of Swarm overlay networks should review GHSA-vwm3-crmr-xfxw to ensure that unintentional exposure has not occurred.
Packaging Updates
23.0.2
2023-03-28For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- Fully resolve missing checks for
apparmor_parser
when an AppArmor enabled kernel is detected. containerd/containerd#8087, moby/moby#45043 - Ensure that credentials are redacted from Git URLs when generating BuildKit buildinfo. Fixes CVE-2023-26054. moby/moby#45110
- Fix anonymous volumes created by a
VOLUME
line in a Dockerfile being excluded from volume prune. moby/moby#45159 - Fix a failure to properly propagate errors during removal of volumes on a Swarm node. moby/moby#45155
- Temporarily work around a bug in BuildKit
COPY --link
by disabling mergeop/diffop optimization. moby/moby#45112 - Properly clean up child tasks when a parent Swarm job is removed. moby/swarmkit#3112, moby/moby#45107
- Fix Swarm service creation logic so that both a GenericResource and a non-default network can be used together. moby/swarmkit#3082, moby/moby#45107
- Fix Swarm CSI support requiring the CSI plugin to offer staging endpoints in order to publish a volume. moby/swarmkit#3116, moby/moby#45107
- Fix a panic caused by log buffering in some configurations. containerd/fifo#47, moby/moby#45051
- Log errors in the REST to Swarm gRPC API translation layer at the debug level to reduce redundancy and noise. moby/moby#45016
- Fix a DNS resolution issue affecting containers created with
--dns-opt
or--dns-search
whensystemd-resolved
is used outside the container. moby/moby#45000 - Fix a panic when logging errors in handling DNS queries originating from inside a container. moby/moby#44980
- Improve the speed of
docker ps
by allowing users to opt out of size calculations with--size=false
. docker/cli#4107 - Extend support for Bash completion to all plugins. docker/cli#4092
- Fix
docker stack deploy
failing on Windows when special environment variables set bycmd.exe
are present. docker/cli#4083 - Add forward compatibility for future API versions by considering empty image tags to be the same as
<none>
. docker/cli#4065 - Atomically write context files to greatly reduce the probability of corruption, and improve the error message for a corrupt context. docker/cli#4063
Packaging
- Upgrade Go to
1.19.7
. docker/docker-ce-packaging#857, docker/cli#4086, moby/moby#45137 - Upgrade
containerd
tov1.6.19
. moby/moby#45084, moby/moby#45099 - Upgrade Buildx to
v0.10.4
. docker/docker-ce-packaging#855 - Upgrade Compose to
v2.17.2
. docker/docker-ce-packaging#867
23.0.1
2023-02-09For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- Fix containers not starting if the kernel has AppArmor enabled, but
apparmor_parser
is not available. moby/moby#44942 - Fix BuildKit-enabled builds with inline caching causing the daemon to crash. moby/moby#44944
- Fix BuildKit improperly loading cached layers created by previous versions. moby/moby#44959
- Fix an issue where
ipvlan
networks created prior to upgrading would prevent the daemon from starting. moby/moby#44937 - Fix the
overlay2
storage driver failing early inmetacopy
testing when initialized on an unsupported backing filesystem. moby/moby#44922 - Fix
exec
exit events being misinterpreted as container exits under some runtimes, such as Kata Containers. moby/moby#44892 - Improve the error message returned by the CLI when receiving a truncated JSON response caused by the API hanging up mid-request. docker/cli#4004
- Fix an incorrect CLI exit code when attempting to execute a directory with a
runc
compiled using Go 1.20. docker/cli#4004 - Fix mishandling the size argument to
--device-write-bps
as a path. docker/cli#4004
Packaging
- Add
/etc/docker
to RPM and DEB packaging. docker/docker-ce-packaging#842- Not all use cases will benefit; if you depend on this, you should explicitly
mkdir -p /etc/docker
.
- Not all use cases will benefit; if you depend on this, you should explicitly
- Upgrade Compose to
v2.16.0
. docker/docker-ce-packaging#844
23.0.0
2023-02-01For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
New
- Set Buildx and BuildKit as the default builder on Linux.
moby/moby#43992
- Alias
docker build
todocker buildx build
. docker/cli#3314 - The legacy builder can still be used by explicitly setting
DOCKER_BUILDKIT=0
. - There are differences in how BuildKit and the legacy builder handle multi-stage builds. For more information, see Multi-stage builds.
- Alias
- Add support for pulling
zstd
compressed layers. moby/moby#41759, moby/moby#42862 - Add support for alternate OCI runtimes on Linux, compatible with the containerd runtime v2 API. moby/moby#43887, moby/moby#43993
- Add support for the containerd
runhcs
shim on Windows (off by default). moby/moby#42089 - Add
dockerd --validate
to check the daemon JSON config and exit. moby/moby#42393 - Add the ability to configure the daemon's HTTP proxy via flags or JSON config. moby/moby#42835
- Add support for RFC 3021 point-to-point networks (IPv4 /31s) and single hosts (IPv4 /32s). For networks with two or fewer addresses, IPAM won't reserve a network and broadcast address. moby/moby#42626
- Add support for setting
ipvlan_flag
and using thel3s
ipvlan_mode
in theipvlan
network driver. moby/moby#42542 - Add support for displaying the value of the
metacopy
option for theoverlay2
storage driver. moby/moby#43557 - Add support for describing Windows devices using the syntax
IDType://ID
. moby/moby#43368 - Add
RootlessKit
,slirp4netns
, andVPNKit
version reporting. moby/moby#42330 - Add experimental support for SwarmKit cluster volumes (CSI).
moby/moby#41982
- CLI: Add cluster volume (CSI) options to
docker volume
. docker/cli#3606 - CLI: Add cluster volume (CSI) support to
docker stack
. docker/cli#3662
- CLI: Add cluster volume (CSI) options to
- Add support for SwarmKit jobs in
docker stack deploy
. docker/cli#2907 - Add the
docker stack config
command to output the merged and interpolated config files as utilized bystack deploy
. docker/cli#3544 - Add a new
docker context show
command that prints the name of the current context. docker/cli#3567 - Add the
--format=json
shorthand variant of--format=""{{ json . }}""
to all commands supporting the--format
flag. docker/cli#2936 - Add a
--quiet
option todocker create
anddocker run
commands to suppress output when pulling an image. docker/cli#3377 - Add a
--force
option todocker network rm
subcommand. Causes CLI to return a 0 exit code even if the network doesn't exist. Has no effect on the server-side procedure for removing a network. docker/cli#3547 - Add a
--signal
option todocker stop
anddocker restart
. docker/cli#3614 - Add a
-v/--version
flag todocker-proxy
. moby/moby#44703 - Plugins are now discovered in well-known user-level paths when the daemon is running in rootless mode. moby/moby#44778
- The daemon now handles common alternate JSON encodings in the JSON configuration file gracefully, and reports useful errors.
moby/moby#44777,
moby/moby#44832
- UTF-8 with a byte order mark is accepted.
- UTF-16 with a byte order mark is accepted.
- Invalid UTF-8 is reported early and with a comprehensible error message.
- Allow use of
STOPSIGNAL
viadocker commit
. moby/moby#43369 - Add a new option to the
awslogs
log driver to allow skipping log stream creation in CloudWatch. moby/moby#42132 - Add a new option to the
awslogs
log driver to specify the log format that's sent to CloudWatch. moby/moby#42838 - Add a new option to the
fluentd
log driver to set the reconnection interval. moby/moby#43100 - Add new options-setters to the Go API client:
WithTLSClientConfigFromEnv()
,WithHostFromEnv()
, andWithVersionFromEnv()
. moby/moby#42224 - Add generation of shell command completion through a
docker completion
subcommand. docker/cli#3429 - API: Add a
Swarm
header toGET /_ping
andHEAD /_ping
, allowing single-request detection of Swarm support. moby/moby#42064 - API: Add a
signal
parameter toPOST /containers/{id}/stop
andPOST /containers/{id}/restart
to set the signal used. moby/moby#43206 - API: Add a
CreateMountPoint
parameter toPOST /containers/create
. moby/moby#43484 - API: Add a
shared-size
parameter toGET /images/json
to enable shared-size computation of images. moby/moby#42531 - API: Add a
type
parameter toGET /system/df
, to control what object types to are considered when computing disk usage. moby/moby#42559 - systemd: Use a systemd-managed containerd instead of a daemon-managed containerd. moby/moby#42373
- systemd: Start
docker.service
aftertime-set.target
. moby/moby#43107
Removed
- Remove support for reading configuration from
~/.dockercfg
. docker/cli#2504- This location has been deprecated since 1.7.0.
- Deprecation notice
- Remove the
-g
and--graph
daemon options in favor of--data-root
. docker/cli#3739- These options have been hidden and deprecated since 17.05.
- Deprecation notice
- Remove client-side sorting of results, in favor of the order in which the search API returns. docker/cli#3470
- Remove warnings related to deprecated storage drivers from the CLI. Warnings are now handled by the daemon instead. docker/cli#3542
- Remove
Experimental
client field fromdocker version
. docker/cli#3543 - Require explicit opt-in to use deprecated storage drivers, and don't automatically select them when upgrading. moby/moby#43378
- Remove deprecated support for
overlay
andoverlay2
storage drivers on backing filesystems withoutd_type
support. moby/moby#43472 - Remove the deprecated
overrideKernelCheck
option from theoverlay2
storage driver. moby/moby#44279 Deprecation notice - Remove support for the deprecated
io.containerd.runtime.v1.linux
OCI runtime. moby/moby#43695 - Remove LCOW (Linux Containers on Windows).
moby/moby#42451,
moby/moby#42499,
moby/moby#42506,
moby/moby#42511,
moby/moby#42520,
moby/moby#42683,
moby/moby#42684,
moby/moby#42685,
moby/moby#43187
- LCOW was introduced as a technical preview in 17.09 and deprecated in 20.10.
- Deprecation notice
- Remove daemon options related to legacy overlay networks used with standalone Swarm.
- Remove
--cluster-xx
options fromdockerd
. moby/moby#40383 - Remove
host-discovery
and overlay networks with external k/v stores. moby/moby#42247 - Deprecation notice
- Remove
- Remove a deprecated
arm
platform fallback.--platform linux/arm/vY
will now return a error whenarm/vY
isn't available instead of pulling the wrong image. moby/moby#44414 - Remove the deprecated
SetCustomHTTPHeaders()
,CustomHTTPHeaders()
options-setters from the Go client API. moby/moby#42694 - Remove the deprecated
WithDialer()
option-setter from the Go client API. moby/moby#44022- Use
WithDialContext()
instead.
- Use
- Remove the daemon implementation of
opts.QuotedString
. The implementation has moved to the CLI. moby/moby#43250 - Remove separate daemon ID from trust-key in the daemon, and disable generating the trust-key. moby/moby#43555
- API: Remove the deprecated
KernelMemory
option fromPOST /containers/create
on API version >= 1.42. moby/moby#43214
Deprecated
- Require Windows Server RS5 / LTSC 2019 (build 17763) as the minimum to run the daemon. moby/moby#43254
- Deprecate
BuilderSize
on API version >= 1.42. moby/moby#42608 - Deprecate
BuildCache.Parent
in favor of the newly introducedBuildCache.Parents
on API version >= 1.42. moby/moby#43908 - Deprecate
pkg/urlutil
, moving the implementation tobuilder/remotecontext/urlutil
. moby/moby#43477
Upgrades
- Upgrade Go to
1.19.5
. docker/cli#3958, moby/moby#44794 - Upgrade
rootlesskit
tov0.14.4
. moby/moby#42708 - Upgrade
buildkit
tov0.10.6
. moby/moby#43239 - Upgrade
buildx
tov0.10.2
. docker/docker-ce-packaging#840 - Upgrade
swarmkit
tov2.0.0-20230119195359-904c221ac281
. moby/moby#44858 - Upgrade
containerd
tov1.6.16
. moby/moby#44766, moby/moby#44769, moby/moby#44881 - Upgrade
runc
tov1.1.4
. moby/moby#44039 - Upgrade
hcsshim
v0.9.6
. moby/moby#44658 - The
btrfs
storage driver now depends on Linux kernel headers (>= 4.12) instead of headers from btrfs-progs. moby/moby#44776
Security
- Change permissions on container
hostconfig.json
files to0600
(was0644
). moby/moby#41620 - Fix
--seccomp-profile
not acceptingunconfined
and renamed the default seccomp profile tobuiltin
. moby/moby#42481 - Always build with seccomp support, and remove the
seccomp
build tag. moby/moby#42501 - Add seccomp support on
riscv64
. moby/moby#43553 - Add support for setting flags passed to
seccomp(2)
in seccomp profiles. moby/moby#42648 - Refactor seccomp types to reuse runtime-spec, and add support for
ErrnoRet
. moby/moby#42005 - Add support for
DefaultErrnoRet
inseccomp
profiles. moby/moby#42604 - Add an explicit
DefaultErrnoRet
field to the default seccomp profile, with no behavior change. moby/moby#42649 - Block
socket
withAF_VSOCK
in the default seccomp profile. moby/moby#44563 - Re-enable
process_vm_readv
andprocess_vm_writev
in the default seccomp profile. moby/moby#42083 - Add syscalls related to PKU to the default seccomp profile. moby/moby#43812
- Allow
clock_settime64
withCAP_SYS_TIME
. moby/moby#43775 - Allow
bpf
withCAP_BPF
andperf_event_open
withCAP_PERFMON
. moby/moby#43988 - Explicitly set the
clone3
syscall to returnENOSYS
in the default seccomp profile, in order to ensureglibc
will correctly fallback to usingclone
. moby/moby#42681
Bug fixes and enhancements
- Promote
overlay2
to be the default storage driver (btrfs
andzfs
are now opt-in). moby/moby#42661 - Add a loading spinner to the
docker cp
command. docker/cli#2708 - Deprecate the
ElectAuthServer
function, and made it return the default registry without calling theGET /info
API endpoint. docker/cli#2819 - Progress bars are no longer reversed when rolling back Swarm services. docker/cli#2940
- Use
net.JoinHostPort()
to fix formatting with IPv6 addresses. docker/cli#2972 - CLI error messages are now printed to
stderr
. docker/cli#3044 - Improve performance of
docker info
if a custom--format
is used that only uses local information. With this change, the CLI only uses the daemon API if it detects that information from the daemon is needed. docker/cli#3179 - Remove the default value from the
--stop-signal
flag, as it may not reflect the actual default used by the daemon. docker/cli#3245 - Add Compose schema
3.10
todocker stack
; allow omitting theversion
field (resulting inlatest
). docker/cli#3257 - Compose version
3
is now equivalent to3.x
(latest) indocker stack
. docker/cli#3445 - Fix
<Ctrl-c>
hanging on Windows to exit after running a container in non-interactive mode. docker/cli#3302 - Add relative source paths to the
run
command in the-v
/--volume
and-m
/--mount
flags. docker/cli#3469 docker exec -t
now sets the console size for the executed process immediately when it's created. docker/cli#3627- Update the pretty-print format of
docker info
to provide more details on installed plugins. docker/cli#3645 - Print warning messages for the
docker context list
anddocker context use
commands when the context is overridden by the environment. docker/cli#3668 - Add a custom
aliases
annotation that can be used to print all available aliases for a command. docker/cli#3694 - The CLI no longer creates or updates the CLI configuration file when running
docker context use
and selecting the current context. docker/cli#3721 - Non-existing contexts are now ignored when running
docker context rm --force
. docker/cli#3791 - Add the ability to override integers to
0
in Compose files. docker/cli#3812 - SIGINT (
<Ctrl-c>
) now passes through to running containers instead of causing the CLI to exit. docker/cli#3849 - Improve
docker port CONTAINER
UX by sorting ports before printing. docker/cli#3892 - API:
GET /containers/{id}/logs
andPOST /containers/{id}/attach
now report which raw-stream format is in use using theContent-type
response header on API version >= 1.42. moby/moby#39812 - Set default sandbox size for Windows layers to 127GB, and ensure that the
--storage-opts
flag applies to all storage on Windows. moby/moby#41636 - Remove the plugin section from the containerd configuration file (
/var/run/docker/containerd/containerd.toml
). moby/moby#41675 - Reject
null
manifests during tar import. moby/moby#41842 - Add shim config for custom runtimes for plugins. moby/moby#41854
- Container health checks now resume when the daemon is restarted. moby/moby#41935
- Quota is no longer disabled on cleanup of the
btrfs
driver. moby/moby#42273 - Host devices that are accessible can now be mounted in
--privileged
rootless containers. moby/moby#42638 - Fix incorrect handling of
**/foo
recursive wildcard directory patterns in.dockerignore
. moby/moby#42676 - Extend
docker import --platform
to allow marking an imported image as a foreign architecture. moby/moby#43103 - Validation of CPU real-time options is now performed when the daemon starts instead of performing validations for each individual container, allowing startup to fail early. moby/moby#43131
- Freeze the
namesgenerator
package against new additions. Users will have to be satisfied with the existing 25359 adjective-name combinations. moby/moby#43210 - API:
containers/{id}/attach/ws
only to streams according bystdin
,stdout
andstderr
parameters on API version >= 1.42. moby/moby#43322 - Fix UDP traffic in containers not working after the container is restarted under sustained traffic. moby/moby#43409
- Add support for pulling images with custom amd64 micro-architecture feature levels as supported by the latest versions of Go, GCC, LLVM, and other compiler tools. moby/moby#43434
- Improve validation of invalid JSON requests in the API. moby/moby#43463
- Mitigate the impact of slow
exec
starts on health checks. Check timeout now only applies to the duration that the health check command is running. The time it takes to start the command no longer counts against the timeout. moby/moby#43480 - Console
tty
size is set immediately on creation. moby/moby#43593, moby/moby#43622 - Fix
overlay2
mounts not being cleaned up after failed container starts, or daemon shutdown. moby/moby#43659 - Match manifest list resolution with
containerd
. moby/moby#43675 - Skip use of
firewalld
for networking when the daemon is running in rootless mode. moby/moby#43813 - Custom NAT networks are now re-created after daemon restart if missing on Windows. moby/moby#43858
- Fix terminating the container health-check process when it times out. moby/moby#43994
- Fix
live-restore
with restart policies and volume refs. moby/moby#44237 - API: Only anonymous volumes now pruned by default on API version >= v1.42. Pass the filter
all=true
to prune named volumes in addition to anonymous. moby/moby#44259 - API: Support concurrent calls on the
GET /system/df
endpoint. moby/moby#42715 - Improve the reliability of the daemon dumping the stack and exits with code 2 when sent a SIGQUIT. moby/moby#44831
- Improve the reliability of
docker logs -f
on Windows, and prevent newlines from being dropped in thelocal
log driver. moby/moby#43294 - Fix a rare deadlock in the daemon caused by buffering of container logs. moby/moby#44856
- Improve error handling in misc filesystem operations so that the daemon can start on a overlayfs backing filesystem. moby/moby#44834
- Fix an issue where
--ipc=host
wasn't handled correctly when the daemon is running in rootless mode. moby/moby#44863 - Fix a long-standing set of issues where stale conntrack entries caused incorrect routing of UDP traffic for containers. moby/moby#44752
- Fix half-registered containers being listed in the API, as well as a nil pointer de-reference and panic caused by using a partially registered container in API calls. moby/moby#44633
- Fix a failure to create the
DOCKER-USER
ip6tables chain. moby/moby#44845 - Fix a failure to clean up iptables rules when the
ip6tables
command isn't available. moby/moby#44727 - Fix an issue where some iptables NAT rules weren't cleaned up after enabling the userland proxy. moby/moby#44811
- Fix a potentially leaked process in rare situations where cleaning up a failed attempt to start a container was mishandled. moby/moby#44400
- Fix the
CreatedAt
time of a volume reflecting initialization and not creation. moby/moby#44725 - Fix an issue where the CLI incorrectly reported an incompatible server instead of an unreachable server in some commands. docker/cli#3901, docker/cli#3904
- Fix broken completion of volumes in Zsh. docker/cli#2998
- Improve output of
docker context
when an invalid context is present. docker/cli#3847 - Remove ANSI decoration of CLI help annotations when the output isn't a TTY, and added a newline for readability. docker/cli#3973
- Add
docker container remove
as an alias fordocker container rm
. docker/cli#3986
Known issues
apparmor_parser ( tracking issue)
Some Debian users have reported issues with containers failing to start after upgrading to the 23.0 branch.
The error message indicates that the issue is due to a missing apparmor_parser
binary:
Error response from daemon: AppArmor enabled on system but the docker-default profile could not be loaded: running `apparmor_parser apparmor_parser --version` failed with output:
error: exec: ""apparmor_parser"": executable file not found in $PATH
Error: failed to start containers: somecontainer
The workaround to this issue is to install the apparmor
package manually:
apt-get install apparmor
BuildKit inline cache ( tracking issue)
Attempting to build an image with BuildKit's inline cache feature (e.g. docker build --build-arg BUILDKIT_INLINE_CACHE=1 .
, docker buildx build --cache-to type=inline .
) will result in the daemon unexpectedly exiting:
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x18 pc=0x147ff00]
goroutine 693 [running]:
github.com/docker/docker/vendor/github.com/moby/buildkit/cache.computeBlobChain.func4.1({0x245cca8, 0x4001394960})
/go/src/github.com/docker/docker/vendor/github.com/moby/buildkit/cache/blobs.go:206 +0xc90
github.com/docker/docker/vendor/github.com/moby/buildkit/util/flightcontrol.(*call).run(0x40013c2240)
/go/src/github.com/docker/docker/vendor/github.com/moby/buildkit/util/flightcontrol/flightcontrol.go:121 +0x64
sync.(*Once).doSlow(0x0?, 0x4001328240?)
/usr/local/go/src/sync/once.go:74 +0x100
sync.(*Once).Do(0x4001328240?, 0x0?)
/usr/local/go/src/sync/once.go:65 +0x24
created by github.com/docker/docker/vendor/github.com/moby/buildkit/util/flightcontrol.(*call).wait
The daemon will restart if configured to do so (e.g. via systemd) after such a crash. The only available mitigation in this release is to avoid performing builds with the inline cache feature enabled.
BuildKit with warm cache ( tracking issue)
If an image was built with BuildKit on a previous version of the daemon, and is built with a 23.0 daemon, previously cached layers will not be restored correctly. The image may appear to build correctly if no lines are changed in the Dockerfile; however, if partial cache invalidation occurs due to changing some lines in the Dockerfile, the still valid and previously cached layers will not be loaded correctly.
This most often presents as files that should be present in the image not being present in a RUN
stage, or any other stage that references files, after changing some lines in the Dockerfile:
[+] Building 0.4s (6/6) FINISHED
=> [internal] load build definition from Dockerfile
=> => transferring dockerfile: 102B
=> [internal] load .dockerignore
=> => transferring context: 2B
=> [internal] load metadata for docker.io/library/node:18-alpine
=> [base 1/2] FROM docker.io/library/node:18-alpine@sha256:bc329c7332cffc30c2d4801e38df03cbfa8dcbae2a7a52a449db104794f168a3
=> CACHED [base 2/2] WORKDIR /app
=> ERROR [stage-1 1/1] RUN uname -a
------
> [stage-1 1/1] RUN uname -a:
#0 0.138 runc run failed: unable to start container process: exec: ""/bin/sh"": stat /bin/sh: no such file or directory
------
Dockerfile:5
--------------------
3 |
4 | FROM base
5 | >>> RUN uname -a
6 |
--------------------
ERROR: failed to solve: process ""/bin/sh -c uname -a"" did not complete successfully: exit code: 1
To mitigate this, the previous build cache must be discarded. docker builder prune -a
will completely empty the build cache, and allow the affected builds to proceed again by removing the mishandled cache layers.
ipvlan networks ( tracking issue)
When upgrading to the 23.0 branch, the existence of any ipvlan networks will prevent the daemon from starting:
panic: interface conversion: interface {} is nil, not string
goroutine 1 [running]:
github.com/docker/docker/libnetwork/drivers/ipvlan.(*configuration).UnmarshalJSON(0x40011533b0, {0x400069c2d0, 0xef, 0xef})
/go/src/github.com/docker/docker/libnetwork/drivers/ipvlan/ipvlan_store.go:196 +0x414
encoding/json.(*decodeState).object(0x4001153440, {0x5597157640?, 0x40011533b0?, 0x559524115c?})
/usr/local/go/src/encoding/json/decode.go:613 +0x650
encoding/json.(*decodeState).value(0x4001153440, {0x5597157640?, 0x40011533b0?, 0x559524005c?})
/usr/local/go/src/encoding/json/decode.go:374 +0x40
encoding/json.(*decodeState).unmarshal(0x4001153440, {0x5597157640?, 0x40011533b0?})
/usr/local/go/src/encoding/json/decode.go:181 +0x204
encoding/json.Unmarshal({0x400069c2d0, 0xef, 0xef}, {0x5597157640, 0x40011533b0})
/usr/local/go/src/encoding/json/decode.go:108 +0xf4
github.com/docker/docker/libnetwork/drivers/ipvlan.(*configuration).SetValue(0x4000d18050?, {0x400069c2d0?, 0x23?, 0x23?})
/go/src/github.com/docker/docker/libnetwork/drivers/ipvlan/ipvlan_store.go:230 +0x38
To mitigate this, affected users can downgrade and remove the network, then upgrade again.
Alternatively, the entire network store can be removed, and networks can be recreated after the upgrade. The network store is located at /var/lib/docker/network/files/local-kv.db
. If the daemon is using an alternate --data-root
, substitute /var/lib/docker
for the alternate path.
Kata Containers ( tracking issue)
The 23.0 branch brings support for alternate containerd shims, such as io.containerd.runsc.v1
(gVisor) and io.containerd.kata.v2
(Kata Containers).
When using the Kata Containers runtime, exiting an exec
session stops the running container, and hangs the connected CLI if a TTY was opened. There is no mitigation at this time beyond avoiding execing into containers running on the Kata runtime.
The root cause of this issue is a long-standing bug in Moby. This will be resolved in a future release. Be advised that support for alternate OCI runtimes is a new feature and that similar issues may be discovered as more users start exercising this functionality.",,,
02745c8101c695b42acec418a91f922c2309304bbccb99c844d86a3ab8403b71,"Control startup and shutdown order in Compose
You can control the order of service startup and shutdown with the
depends_on attribute. Compose always starts and stops
containers in dependency order, where dependencies are determined by
depends_on
, links
, volumes_from
, and network_mode: ""service:...""
.
A good example of when you might use this is an application which needs to access a database. If both services are started with docker compose up
, there is a chance this will fail since the application service might start before the database service and won't find a database able to handle its SQL statements.
Control startup
On startup, Compose does not wait until a container is ""ready"", only until it's running. This can cause issues if, for example, you have a relational database system that needs to start its own services before being able to handle incoming connections.
The solution for detecting the ready state of a service is to use the condition
attribute with one of the following options:
service_started
service_healthy
. This specifies that a dependency is expected to be “healthy”, which is defined withhealthcheck
, before starting a dependent service.service_completed_successfully
. This specifies that a dependency is expected to run to successful completion before starting a dependent service.
Example
services:
web:
build: .
depends_on:
db:
condition: service_healthy
restart: true
redis:
condition: service_started
redis:
image: redis
db:
image: postgres
healthcheck:
test: [""CMD-SHELL"", ""pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}""]
interval: 10s
retries: 5
start_period: 30s
timeout: 10s
Compose creates services in dependency order. db
and redis
are created before web
.
Compose waits for healthchecks to pass on dependencies marked with service_healthy
. db
is expected to be ""healthy"" (as indicated by healthcheck
) before web
is created.
restart: true
ensures that if db
is updated or restarted due to an explicit Compose operation, for example docker compose restart
, the web
service is also restarted automatically, ensuring it re-establishes connections or dependencies correctly.
The healthcheck for the db
service uses the pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}'
command to check if the PostgreSQL database is ready. The service is retried every 10 seconds, up to 5 times.
Compose also removes services in dependency order. web
is removed before db
and redis
.",,,
941e42a28eb1a49f2be04f7666aec227a9f42fcc834a447abc4169677050184e,"3D Secure authentication
Note
Docker plan payments support 3D secure authentication.
3D Secure (3DS) authentication incorporates an additional security layer for credit card transactions. If you’re making payments for your Docker billing in a region that requires 3DS, or using a payment method that requires 3DS, you’ll need to verify your identity to complete any transactions. The method used to verify your identity varies depending on your banking institution.
The following transactions will use 3DS authentication if your payment method requires it.
- Starting a new paid subscription
- Changing your billing cycle from monthly to annual
- Upgrading your subscription
- Adding seats to an existing subscription
Troubleshooting
If you encounter errors completing payments due to 3DS, you can troubleshoot in the following ways.
- Retry your transaction and verification of your identity.
- Contact your bank to determine any errors on their end.
- Try a different payment method that doesn’t require 3DS.
Tip
Make sure you allow third-party scripts in your browser and that any ad blocker you may use is disabled when attempting to complete payments.",,,
f323ff19db4a11a5a753d268963deaba3e89849be20968de983a664f1592b4a7,"Manage keys for content trust
Trust for an image tag is managed through the use of keys. Docker's content trust makes use of five different types of keys:
| Key | Description |
|---|---|
| root key | Root of content trust for an image tag. When content trust is enabled, you create the root key once. Also known as the offline key, because it should be kept offline. |
| targets | This key allows you to sign image tags, to manage delegations including delegated keys or permitted delegation paths. Also known as the repository key, since this key determines what tags can be signed into an image repository. |
| snapshot | This key signs the current collection of image tags, preventing mix and match attacks. |
| timestamp | This key allows Docker image repositories to have freshness security guarantees without requiring periodic content refreshes on the client's side. |
| delegation | Delegation keys are optional tagging keys and allow you to delegate signing image tags to other publishers without having to share your targets key. |
When doing a docker push
with Content Trust enabled for the first time, the
root, targets, snapshot, and timestamp keys are generated automatically for
the image repository:
The root and targets key are generated and stored locally client-side.
The timestamp and snapshot keys are safely generated and stored in a signing server that is deployed alongside the Docker registry. These keys are generated in a backend service that isn't directly exposed to the internet and are encrypted at rest. Use the Notary CLI to manage your snapshot key locally.
Delegation keys are optional, and not generated as part of the normal docker
workflow. They need to be
manually generated and added to the repository.
Choose a passphrase
The passphrases you chose for both the root key and your repository key should be randomly generated and stored in a password manager. Having the repository key allows users to sign image tags on a repository. Passphrases are used to encrypt your keys at rest and ensure that a lost laptop or an unintended backup doesn't put the private key material at risk.
Back up your keys
All the Docker trust keys are stored encrypted using the passphrase you provide on creation. Even so, you should still take care of the location where you back them up. Good practice is to create two encrypted USB keys.
Warning
It is very important that you back up your keys to a safe, secure location. The loss of the repository key is recoverable, but the loss of the root key is not.
The Docker client stores the keys in the ~/.docker/trust/private
directory.
Before backing them up, you should tar
them into an archive:
$ umask 077; tar -zcvf private_keys_backup.tar.gz ~/.docker/trust/private; umask 022
Hardware storage and signing
Docker Content Trust can store and sign with root keys from a Yubikey 4. The Yubikey is prioritized over keys stored in the filesystem. When you initialize a new repository with content trust, Docker Engine looks for a root key locally. If a key is not found and the Yubikey 4 exists, Docker Engine creates a root key in the Yubikey 4. Consult the Notary documentation for more details.
Prior to Docker Engine 1.11, this feature was only in the experimental branch.
Key loss
Warning
If a publisher loses keys it means losing the ability to sign images for the repositories in question. If you lose a key, send an email to Docker Hub Support. As a reminder, the loss of a root key is not recoverable.
This loss also requires manual intervention from every consumer that used a signed
tag from this repository prior to the loss.
Image consumers get the following error for content previously downloaded from the affected repo(s):
Warning: potential malicious behavior - trust data has insufficient signatures for remote repository docker.io/my/image: valid signatures did not meet threshold
To correct this, they need to download a new image tag that is signed with the new key.",,,
47f1e6b5f3584e04e7f110bd02784345e4760a71abf84d92967ec0653a981baf,"Docker Engine 18.06 release notes
Table of contents
18.06.3-ce
2019-02-19
Security fixes for Docker Engine
- Change how the
runc
critical vulnerability patch is applied to include the fix in RPM packages. docker/engine#156
18.06.2
2019-02-11
Security fixes for Docker Engine
- Update
runc
to address a critical vulnerability that allows specially-crafted containers to gain administrative privileges on the host. CVE-2019-5736 - Ubuntu 14.04 customers using a 3.13 kernel will need to upgrade to a supported Ubuntu 4.x kernel
18.06.1-ce
2018-08-21
Builder
- Fix no error if build args are missing during docker build. docker/engine#25
- Set BuildKit's ExportedProduct variable to show useful errors. docker/engine#21
Client
- Various shell completion script updates: docker/cli#1229, docker/cli#1268, and docker/cli#1272
- Fix
DOCKER_CONFIG
warning message and fallback search. docker/cli#1241 - Fix help message flags on
docker stack
commands and sub-commands. docker/cli#1267
Runtime
- Disable CRI plugin listening on port 10010 by default. docker/engine#29
- Update containerd to v1.1.2. docker/engine#33
- Windows: Do not invoke HCS shutdown if terminate called. docker/engine#31
- Windows: Select polling-based watcher for Windows log watcher. docker/engine#34
Swarm Mode
- Fix the condition used for skipping over running tasks. docker/swarmkit#2677
- Fix task sorting. docker/swarmkit#2712
18.06.0-ce
2018-07-18
Important notes about this release
- Docker 18.06 CE will be the last release with a 4-month maintenance lifecycle. The planned Docker 18.09 CE release will be supported for 7 months with Docker 19.03 CE being the next release in line. More details about the release process can be found here.
Builder
- Builder: fix layer leak on multi-stage wildcard copy. moby/moby#37178
- Fix parsing of invalid environment variable substitution . moby/moby#37134
- Builder: use the arch info from base image. moby/moby#36816 moby/moby#37197
- New experimental builder backend based on
BuildKit. To enable, run daemon in experimental mode and set
DOCKER_BUILDKIT=1
environment variable on the docker CLI. moby/moby#37151 docker/cli#1111
- Fix handling uppercase targets names in multi-stage builds. moby/moby#36960
Client
- Bump spf13/cobra to v0.0.3, pflag to v1.0.1. moby/moby#37106
- Add support for the new Stack API for Kubernetes v1beta2. docker/cli#899
- K8s: more robust stack error detection on deploy. docker/cli#948
- Support for rollback config in compose 3.7. docker/cli#409
- Update Cobra and pflag, and use built-in --version feature. docker/cli#1069
- Fix
docker stack deploy --prune
with empty name removing all services. docker/cli#1088 - [Kubernetes] stack services filters. docker/cli#1023
- Only show orchestrator flag in root, stack and version commands in help. docker/cli#1106
- Add an
Extras
field on the compose config types. docker/cli#1126 - Add options to the compose loader. docker/cli#1128
- Fix always listing nodes in docker stack ps command on Kubernetes. docker/cli#1093
- Fix output being shown twice on stack rm error message. docker/cli#1093
- Extend client API with custom HTTP requests. moby/moby#37071
- Changed error message for unreadable files to clarify possibility of a .Dockerignore entry. docker/cli#1053
- Restrict kubernetes.allNamespaces value to 'enabled' or 'disabled' in configuration file. docker/cli#1087
- Check errors when initializing the docker client in the help command. docker/cli#1119
- Better namespace experience with Kubernetes. Fix using namespace defined in ~/.kube/config for stack commands. Add a NAMESPACE column for docker stack ls command. Add a --all-namespaces flag for docker stack ls command. docker/cli#991
- Export Push and Save. docker/cli#1123
- Export pull as a public function. docker/cli#1026
- Remove Kubernetes commands from experimental. docker/cli#1068
- Adding configs/secrets to service inspect pretty. docker/cli#1006
- Fix service filtering by name on Kubernetes. docker/cli#1101
- Fix component information alignment in
docker version
. docker/cli#1065 - Fix cpu/memory limits and reservations being reset on service update. docker/cli#1079
- Manifest list: request specific permissions. docker/cli#1024
- Setting --orchestrator=all also sets --all-namespaces unless specific --namespace are set. docker/cli#1059
- Fix panics when --compress and --stream are used together. docker/cli#1105
- Switch from x/net/context to context. docker/cli#1038
- Add --init option to
docker service create
. docker/cli#479 - Fixed bug displaying garbage output for build command when --stream and --quiet flags combined. docker/cli#1090
- Add
init
support in 3.7 schema. docker/cli#1129
- Fix docker trust signer removal. docker/cli#1112
- Fix error message from docker inspect. docker/cli#1071
- Allow
x-*
extension on 3rd level objects. docker/cli#1097 - An invalid orchestrator now generates an error instead of being silently ignored. docker/cli#1055
- Added ORCHESTRATOR column to docker stack ls command. docker/cli#973
- Warn when using host-ip for published ports for services. docker/cli#1017
- Added the option to enable experimental cli features through the
DOCKER_CLI_EXPERIMENTAL
environment variable. docker/cli#1138 - Add exec_die to the list of known container events. docker/cli#1028
- [K8s] Do env-variable expansion on the uninterpreted Config files. docker/cli#974
- Print warnings on stderr for each unsupported features while parsing a compose file for deployment on Kubernetes. docker/cli#903
- Added description about pids count. docker/cli#1045
- Warn user of filter when pruning. docker/cli#1043
- Fix
--rollback-*
options overwriting--update-*
options. docker/cli#1052
- Update Attach, Build, Commit, Cp, Create subcommand fish completions. docker/cli#1005
- Add bash completion for
dockerd --default-address-pool
. docker/cli#1173 - Add bash completion for
exec_die
event. docker/cli#1173
- Update docker-credential-helper so
pass
is not called on every docker command. docker/cli#1184 - Fix for rotating swarm external CA. docker/cli#1199
- Improve version output alignment. docker/cli#1207
- Add bash completion for
service create|update --init
. docker/cli#1210
Deprecation
- Document reserved namespaces deprecation. docker/cli#1040
Logging
- Allow awslogs to use non-blocking mode. moby/moby#36522
- Improve logging of long log lines on fluentd log driver.. moby/moby#36159
- Re-order CHANGELOG.md to pass
make validate
test. moby/moby#37047 - Update Events, Exec, Export, History, Images, Import, Inspect, Load, and Login subcommand fish completions. docker/cli#1061
- Update documentation for RingLogger's ring buffer. moby/moby#37084
- Add metrics for log failures/partials. moby/moby#37034
- Fix logging plugin crash unrecoverable. moby/moby#37028
- Fix logging test type. moby/moby#37070
- Fix race conditions in logs API. moby/moby#37062
- Fix some issues in logfile reader and rotation. moby/moby#37063
Networking
- Allow user to specify default address pools for docker networks. moby/moby#36396 docker/cli#818
- Adding logs for ipam state docker/libnetwork#2417
- Fix race conditions in the overlay network driver docker/libnetwork#2143
- Add wait time into xtables lock warning docker/libnetwork#2142
- filter xtables lock warnings when firewalld is active docker/libnetwork#2135
- Switch from x/net/context to context docker/libnetwork#2140
- Adding a recovery mechanism for a split gossip cluster docker/libnetwork#2134
- Running docker inspect on network attachment tasks now returns a full task object. moby/moby#35246
- Some container/network cleanups. moby/moby#37033
- Fix network inspect for overlay network. moby/moby#37045
- Improve Scalability of the Linux load balancing. docker/engine#16
- Change log level from error to warning. docker/engine#19
Runtime
- Aufs: log why aufs is not supported. moby/moby#36995
- Hide experimental checkpoint features on Windows. docker/cli#1094
- Lcow: Allow the client to customize capabilities and device cgroup rules for LCOW containers. moby/moby#37294
- Changed path given for executable output in windows to actual location of executable output. moby/moby#37295
- Add windows recycle bin test and update hcsshim to v0.6.11. moby/moby#36994
- Allow to add any args when doing a make run. moby/moby#37190
- Optimize ContainerTop() aka docker top. moby/moby#37131
- Fix compilation on 32bit machines. moby/moby#37292
- Update API version to v1 38. moby/moby#37141
- Fix
docker service update --host-add
does not update existing host entry. docker/cli#1054 - Fix swagger file type for ExecIds. moby/moby#36962
- Fix swagger volume type generation. moby/moby#37060
- Fix wrong assertion in volume/service package. moby/moby#37211
- Fix daemon panic on restart when a plugin is running. moby/moby#37234
- Construct and add 'LABEL' command from 'label' option to last stage. moby/moby#37011
- Fix race condition between exec start and resize.. moby/moby#37172
- Alternative failure mitigation of
TestExecInteractiveStdinClose
. moby/moby#37143 - RawAccess allows a set of paths to be not set as masked or readonly. moby/moby#36644
- Be explicit about github.com prefix being a legacy feature. moby/moby#37174
- Bump Golang to 1.10.3. docker/cli#1122
- Close ReadClosers to prevent xz zombies. moby/moby#34218
- Daemon.ContainerStop(): fix for a negative timeout. moby/moby#36874
- Daemon.setMounts(): copy slice in place. moby/moby#36991
- Describe IP field of swagger Port definition. moby/moby#36971
- Extract volume interaction to a volumes service. moby/moby#36688
- Fixed markdown formatting in docker image v1, v1.1, and v1.2 spec. moby/moby#37051
- Improve GetTimestamp parsing. moby/moby#35402
- Jsonmessage: pass message to aux callback. moby/moby#37064
- Overlay2: remove unused cdMountFrom() helper function. moby/moby#37041
- Overlay: Fix overlay storage-driver silently ignoring unknown storage-driver options. moby/moby#37040
- Remove some unused contrib items. moby/moby#36977
- Restartmanager: do not apply restart policy on created containers. moby/moby#36924
- Set item-type for ExecIDs. moby/moby#37121
- Use go-systemd const instead of magic string in Linux version of dockerd. moby/moby#37136
- Use stdlib TLS dialer. moby/moby#36687
- Warn when an engine label using a reserved namespace (com.docker.*, io.docker.*, or org.dockerproject.*) is configured, as per Docker object labels. moby/moby#36921
- Fix missing plugin name in message. moby/moby#37052
- Fix link anchors in CONTRIBUTING.md. moby/moby#37276
- Fix link to Docker Toolbox. moby/moby#37240
- Fix mis-used skip condition. moby/moby#37179
- Fix bind mounts not working in some cases. moby/moby#37031
- Fix fd leak on attach. moby/moby#37184
- Fix fluentd partial detection. moby/moby#37029
- Fix incorrect link in version-history.md. moby/moby#37049
- Allow vim to be case insensitive for D in dockerfile. moby/moby#37235
- Add
t.Name()
to tests so that service names are unique. moby/moby#37166 - Add additional message when backendfs is extfs without d_type support. moby/moby#37022
- Add api version checking for tests from new feature. moby/moby#37169
- Add image metrics for push and pull. moby/moby#37233
- Add support for
init
on services. moby/moby#37183 - Add verification of escapeKeys array length in pkg/term/proxy.go. moby/moby#36918
- When link id is empty for overlay2, do not remove this link.. moby/moby#36161
- Fix build on OpenBSD by defining Self(). moby/moby#37301
- Windows: Fix named pipe support for hyper-v isolated containers. docker/engine#2 docker/cli#1165
- Fix manifest lists to always use correct size. docker/cli#1183
- Register OCI media types. docker/engine#4
- Update containerd to v1.1.1 docker/engine#17
- LCOW: Prefer Windows over Linux in a manifest list. docker/engine#3
- Add updated
MaskPaths
that are used in code paths directly using containerd to address CVE-2018-10892. docker/engine#15 - Add
/proc/acpi
to masked paths to address CVE-2018-10892. docker/engine#14
- Fix bindmount autocreate race. docker/engine#11
Swarm Mode
- List stacks for both Swarm and Kubernetes with --orchestrator=all in docker stack ls. Allow several occurrences of --namespace for Kubernetes with docker stack ls. docker/cli#1031
- Bump SwarmKit to remove deprecated grpc metadata wrappers. moby/moby#36905
- Issue an error for --orchestrator=all when working on mismatched Swarm and Kubernetes hosts. docker/cli#1035
- Fix broken swarm commands with Kubernetes defined as orchestrator. ""--orchestrator"" flag is no longer global but local to stack commands and subcommands docker/cli#1137 docker/cli#1139
- Bump swarmkit to include task reaper fixes and more metrics. docker/engine#13
- Avoid a leak when a service with unassigned tasks is deleted. docker/engine#27
- Fix racy batching on the dispatcher. docker/engine#27",,,
754862df38e94a938678bbfe3769909034e14d0acf6bea834d38e1e4a7eb169e,"Install the Docker Compose standalone
This page contains instructions on how to install Docker Compose standalone on Linux or Windows Server, from the command line.
Warning
The Docker Compose standalone uses the
-compose
syntax instead of the current standard syntaxcompose
.
For example, you must typedocker-compose up
when using Docker Compose standalone, instead ofdocker compose up
.
On Linux
To download and install the Docker Compose standalone, run:
$ curl -SL https://github.com/docker/compose/releases/download/v2.33.1/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose
Apply executable permissions to the standalone binary in the target path for the installation.
$ chmod +x /usr/local/bin/docker-compose
Test and execute Docker Compose commands using
docker-compose
.
Tip
If the command
docker-compose
fails after installation, check your path. You can also create a symbolic link to/usr/bin
or any other directory in your path. For example:$ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
On Windows Server
Follow these instructions if you are running the Docker daemon directly on Microsoft Windows Server and want to install Docker Compose.
Run PowerShell as an administrator. In order to proceed with the installation, select Yes when asked if you want this app to make changes to your device.
Optional. Ensure TLS1.2 is enabled. GitHub requires TLS1.2 for secure connections. If you’re using an older version of Windows Server, for example 2016, or suspect that TLS1.2 is not enabled, run the following command in PowerShell:
[Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12
Download the latest release of Docker Compose (v2.33.1). Run the following command:
Start-BitsTransfer -Source ""https://github.com/docker/compose/releases/download/v2.33.1/docker-compose-windows-x86_64.exe"" -Destination $Env:ProgramFiles\Docker\docker-compose.exe
To install a different version of Docker Compose, substitute
v2.33.1
with the version of Compose you want to use.Note
On Windows Server 2019 you can add the Compose executable to
$Env:ProgramFiles\Docker
. Because this directory is registered in the systemPATH
, you can run thedocker-compose --version
command on the subsequent step with no additional configuration.Test the installation.
$ docker-compose.exe version Docker Compose version v2.33.1",,,
4a9350badff58ff176a07800387ddd5db90054437d74ff29e5e5b23d18925886,"Validate your extension
Validate your extension before you share or publish it. Validating the extension ensures that the extension:
- Is built with the image labels it requires to display correctly in the marketplace
- Installs and runs without problems
The Extensions CLI lets you validate your extension before installing and running it locally.
The validation checks if the extension’s Dockerfile
specifies all the required labels and if the metadata file is valid against the JSON schema file.
To validate, run:
$ docker extension validate <name-of-your-extension>
If your extension is valid, the following message displays:
The extension image ""name-of-your-extension"" is valid
Before the image is built, it's also possible to validate only the metadata.json
file:
$ docker extension validate /path/to/metadata.json
The JSON schema used to validate the metadata.json
file against can be found under the
releases page.",,,
419c45392cb2fb71ccfc9db57f8074ba76d9437362881e5d348120f9f1d66872,"Docker Engine 18.09 release notes
Note:
With this release, the daemon, client and container runtime are now all shipped in separate packages. When updating, you need to update all packages at the same time to get the latest patch releases for each. For example, on Ubuntu:
$ sudo apt-get install docker-ce docker-ce-cli containerd.io
See the installation instructions for the corresponding Linux distribution for details.
18.09.9
2019-09-03
Client
- Fix Windows absolute path detection on non-Windows. docker/cli#1990
- Fix Docker refusing to load key from delegation.key on Windows. docker/cli#1968
- Completion scripts updates for bash and zsh.
Logging
- Fix for reading journald logs. moby/moby#37819 moby/moby#38859
Networking
- Prevent panic on network attached to a container with disabled networking. moby/moby#39589
- Fix service port for an application becomes unavailable randomly. docker/libnetwork#2069
- Fix cleaning up
--config-only
networks--config-from
networkshave ungracefully exited. docker/libnetwork#2373
Runtime
- Update to Go 1.11.13.
- Fix a potential engine panic when using XFS disk quota for containers. moby/moby#39644
Swarm
- Fix ""grpc: received message larger than max"" errors. moby/moby#39306
- Fix an issue where nodes several tasks could not be removed. docker/swarmkit#2867
18.09.8
2019-07-17
Runtime
- Masked the secrets updated to the log files when running Docker Engine in debug mode.
CVE-2019-13509: If a Docker engine is running in debug mode, and
docker stack deploy
is used to redeploy a stack which includes non-external secrets, the logs will contain the secret.
Client
- Fixed rollback config type interpolation for
parallelism
andmax_failure_ratio
fields.
Known Issue
- There are important changes to the upgrade process that, if not correctly followed, can have an impact on the availability of applications running on the Swarm during upgrades. These constraints impact any upgrades coming from any version before 18.09 to version 18.09 or later.
18.09.7
2019-06-27
Builder
- Fixed a panic error when building dockerfiles that contain only comments. moby/moby#38487
- Added a workaround for GCR authentication issue. moby/moby#38246
- Builder-next: Fixed a bug in the GCR token cache implementation workaround. moby/moby#39183
Networking
- Fixed an error where
--network-rm
would fail to remove a network. moby/moby#39174
Runtime
- Added performance optimizations in aufs and layer store that helps in massively parallel container creation and removal. moby/moby#39107, moby/moby#39135
- Updated containerd to version 1.2.6. moby/moby#39016
- Fixed CVE-2018-15664 symlink-exchange attack with directory traversal. moby/moby#39357
- Windows: fixed support for
docker service create --limit-cpu
. moby/moby#39190 - daemon: fixed a mirrors validation issue. moby/moby#38991
- Docker no longer supports sorting UID and GID ranges in ID maps. moby/moby#39288
Logging
- Added a fix that now allows large log lines for logger plugins. moby/moby#39038
Known Issue
- There are important changes to the upgrade process that, if not correctly followed, can have an impact on the availability of applications running on the Swarm during upgrades. These constraints impact any upgrades coming from any version before 18.09 to version 18.09 or later.
18.09.6
2019-05-06
Builder
- Fixed
COPY
andADD
with multiple<src>
to not invalidate cache ifDOCKER_BUILDKIT=1
. moby/moby#38964
Networking
- Cleaned up the cluster provider when the agent is closed. docker/libnetwork#2354
- Windows: Now selects a random host port if the user does not specify a host port. docker/libnetwork#2369
Known Issues
- There are important changes to the upgrade process that, if not correctly followed, can have an impact on the availability of applications running on the Swarm during upgrades. These constraints impact any upgrades coming from any version before 18.09 to version 18.09 or later.
18.09.5
2019-04-11
Builder
- Fixed
DOCKER_BUILDKIT=1 docker build --squash ..
docker/engine#176
Client
- Fixed tty initial size error. docker/cli#1775
- Fixed dial-stdio goroutine leakage. docker/cli#1795
- Fixed the stack informer's selector used to track deployment. docker/cli#1794
Networking
- Fixed
network=host
using wrongresolv.conf
withsystemd-resolved
. docker/engine#180 - Fixed Windows ARP entries getting corrupted randomly under load. docker/engine#192
Runtime
- Now showing stopped containers with restart policy as
Restarting
. docker/engine#181 - Now using original process spec for execs. docker/engine#178
Swarm Mode
- Fixed leaking task resources when nodes are deleted. docker/engine#185
Known Issues
- There are important changes to the upgrade process that, if not correctly followed, can have an impact on the availability of applications running on the Swarm during upgrades. These constraints impact any upgrades coming from any version before 18.09 to version 18.09 or later.
18.09.4
2019-03-28
Builder
- Fixed
CVE-2019-13139 by adding validation for
git ref
to avoid misinterpretation as a flag. moby/moby#38944
Runtime
- Fixed
docker cp
error for filenames greater than 100 characters. moby/moby#38634 - Fixed
layer/layer_store
to ensureNewInputTarStream
resources are released. moby/moby#38413 - Increased GRPC limit for
GetConfigs
. moby/moby#38800 - Updated
containerd
1.2.5. docker/engine#173
Swarm Mode
- Fixed nil pointer exception when joining node to swarm. moby/moby#38618
- Fixed issue for swarm nodes not being able to join as masters if http proxy is set. [moby/moby#36951]
Known Issues
- There are important changes to the upgrade process that, if not correctly followed, can have impact on the availability of applications running on the Swarm during upgrades. These constraints impact any upgrades coming from any version before 18.09 to version 18.09 or later.
18.09.3
2019-02-28
Networking fixes
- Windows: now avoids regeneration of network IDs to prevent broken references to networks. docker/engine#149
- Windows: Fixed an issue to address
- restart always
flag on standalone containers not working when specifying a network. (docker/escalation#1037) - Fixed an issue to address the IPAM state from networkdb if the manager is not attached to the overlay network. (docker/escalation#1049)
Runtime fixes and updates
- Updated to Go version 1.10.8.
- Modified names in the container name generator. docker/engine#159
- When copying an existing folder, xattr set errors when the target filesystem doesn't support xattr are now ignored. docker/engine#135
- Graphdriver: fixed ""device"" mode not being detected if ""character-device"" bit is set. docker/engine#160
- Fixed nil pointer dereference on failure to connect to containerd. docker/engine#162
- Deleted stale containerd object on start failure. docker/engine#154
Known Issues
- There are important changes to the upgrade process that, if not correctly followed, can have impact on the availability of applications running on the Swarm during upgrades. These constraints impact any upgrades coming from any version before 18.09 to version 18.09 or greater.
18.09.2
2019-02-11
Security fixes
- Update
runc
to address a critical vulnerability that allows specially-crafted containers to gain administrative privileges on the host. CVE-2019-5736 - Ubuntu 14.04 customers using a 3.13 kernel will need to upgrade to a supported Ubuntu 4.x kernel
For additional information, refer to the Docker blog post.
Known Issues
- There are important changes to the upgrade process that, if not correctly followed, can have impact on the availability of applications running on the Swarm during upgrades. These constraints impact any upgrades coming from any version before 18.09 to version 18.09 or greater.
18.09.1
2019-01-09
Important notes about this release
In Docker versions prior to 18.09, containerd was managed by the Docker engine daemon. In Docker Engine 18.09, containerd is managed by systemd. Since containerd is managed by systemd, any custom configuration to the docker.service
systemd configuration which changes mount settings (for example, MountFlags=slave
) breaks interactions between the Docker Engine daemon and containerd, and you will not be able to start containers.
Run the following command to get the current value of the MountFlags
property for the docker.service
:
$ sudo systemctl show --property=MountFlags docker.service
MountFlags=
Update your configuration if this command prints a non-empty value for MountFlags
, and restart the docker service.
Security fixes
- Upgraded Go language to 1.10.6 to resolve CVE-2018-16873, CVE-2018-16874, and CVE-2018-16875.
- Fixed authz plugin for 0-length content and path validation.
- Added
/proc/asound
to masked paths docker/engine#126
Improvements
- Updated to BuildKit 0.3.3 docker/engine#122
- Updated to containerd 1.2.2 docker/engine#144
- Provided additional warnings for use of deprecated legacy overlay and devicemapper storage drivers docker/engine#85
- prune: perform image pruning before build cache pruning docker/cli#1532
- Added bash completion for experimental CLI commands (manifest) docker/cli#1542
- Windows: allow process isolation on Windows 10 docker/engine#81
Fixes
- Disable kmem accounting in runc on RHEL/CentOS (docker/escalation#614, docker/escalation#692) docker/engine#121
- Fixed inefficient networking configuration docker/engine#123
- Fixed docker system prune doesn't accept until filter docker/engine#122
- Avoid unset credentials in
containerd
docker/engine#122 - Fixed iptables compatibility on Debian docker/engine#107
- Fixed setting default schema to tcp for docker host docker/cli#1454
- Fixed bash completion for
service update --force
docker/cli#1526 - Windows: DetachVhd attempt in cleanup docker/engine#113
- API: properly handle invalid JSON to return a 400 status docker/engine#110
- API: ignore default address-pools on API < 1.39 docker/engine#118
- API: add missing default address pool fields to swagger docker/engine#119
- awslogs: account for UTF-8 normalization in limits docker/engine#112
- Prohibit reading more than 1MB in HTTP error responses docker/engine#114
- apparmor: allow receiving of signals from
docker kill
docker/engine#116 - overlay2: use index=off if possible (fix EBUSY on mount) docker/engine#84
Packaging
- Add docker.socket requirement for docker.service. docker/docker-ce-packaging#276
- Add socket activation for RHEL-based distributions. docker/docker-ce-packaging#274
- Add libseccomp requirement for RPM packages. docker/docker-ce-packaging#266
Known Issues
- When upgrading from 18.09.0 to 18.09.1,
containerd
is not upgraded to the correct version on Ubuntu. - There are important changes to the upgrade process that, if not correctly followed, can have impact on the availability of applications running on the Swarm during upgrades. These constraints impact any upgrades coming from any version before 18.09 to version 18.09 or greater.
18.09.0
2018-11-08
Important notes about this release
In Docker versions prior to 18.09, containerd was managed by the Docker engine daemon. In Docker Engine 18.09, containerd is managed by systemd. Since containerd is managed by systemd, any custom configuration to the docker.service
systemd
configuration which changes mount settings (for example, MountFlags=slave
) breaks interactions between the Docker Engine daemon and containerd, and you will not be able to start containers.
Run the following command to get the current value of the MountFlags
property for the docker.service
:
$ sudo systemctl show --property=MountFlags docker.service
MountFlags=
Update your configuration if this command prints a non-empty value for MountFlags
, and restart the docker service.
New features
- Updated API version to 1.39 moby/moby#37640
- Added support for remote connections using SSH docker/cli#1014
- Builder: added prune options to the API moby/moby#37651
- Added ""Warnings"" to
/info
endpoint, and move detection to the daemon moby/moby#37502 - Allows BuildKit builds to run without experimental mode enabled. Buildkit can now be configured with an option in daemon.json moby/moby#37593 moby/moby#37686 moby/moby#37692 docker/cli#1303 docker/cli#1275
- Added support for build-time secrets using a
--secret
flag when using BuildKit docker/cli#1288 - Added SSH agent socket forwarder (
docker build --ssh $SSHMOUNTID=$SSH_AUTH_SOCK
) when using BuildKit docker/cli#1438 / docker/cli#1419 - Added
--chown
flag support forADD
andCOPY
commands on Windows moby/moby#35521 - Added
builder prune
subcommand to prune BuildKit build cache docker/cli#1295 docker/cli#1334 - BuildKit: Adds configurable garbage collection policy for the BuildKit build cache docker/engine#59 / moby/moby#37846
- BuildKit: Adds support for
docker build --pull ...
when using BuildKit moby/moby#37613 - BuildKit: Adds support or ""registry-mirrors"" and ""insecure-registries"" when using BuildKit docker/engine#59 / moby/moby#37852
- BuildKit: Enables net modes and bridge. moby/moby#37620
- Added
docker engine
subcommand to manage the lifecycle of a Docker Engine running as a privileged container on top of containerd, and to allow upgrades to Docker Engine Enterprise docker/cli#1260 - Exposed product license in
docker info
output docker/cli#1313 - Showed warnings produced by daemon in
docker info
output docker/cli#1225 - Added ""local"" log driver moby/moby#37092
- Amazon CloudWatch: adds
awslogs-endpoint
logging option moby/moby#37374 - Added support for global default address pools moby/moby#37558 docker/cli#1233
- Configured containerd log-level to be the same as dockerd moby/moby#37419
- Added configuration option for cri-containerd moby/moby#37519
- Updates containerd client to v1.2.0-rc.1 moby/moby#37664, docker/engine#75 / moby/moby#37710
- Added support for global default address pools moby/moby#37558 docker/cli#1233
- Moved the
POST /session
endpoint out of experimental. moby/moby#40028
Improvements
- Does not return ""
<unknown>
"" in /info response moby/moby#37472 - BuildKit: Changes
--console=[auto,false,true]
to--progress=[auto,plain,tty]
docker/cli#1276 - BuildKit: Sets BuildKit's ExportedProduct variable to show useful errors in the future. moby/moby#37439
- Hides
--data-path-addr
flags when connected to a daemon that doesn't support this option docker/docker/cli#1240 - Only shows buildkit-specific flags if BuildKit is enabled docker/cli#1438 / docker/cli#1427
- Improves version output alignment docker/cli#1204
- Sorts plugin names and networks in a natural order docker/cli#1166, docker/cli#1266
- Updates bash and zsh completion scripts
- Passes log-level to containerd. moby/moby#37419
- Uses direct server return (DSR) in east-west overlay load balancing docker/engine#93 / docker/libnetwork#2270
- Builder: temporarily disables bridge networking when using buildkit. moby/moby#37691
- Blocks task starting until node attachments are ready moby/moby#37604
- Propagates the provided external CA certificate to the external CA object in swarm. docker/cli#1178
- Removes Ubuntu 14.04 ""Trusty Tahr"" as a supported platform docker-ce-packaging#255 / docker-ce-packaging#254
- Removes Debian 8 ""Jessie"" as a supported platform docker-ce-packaging#255 / docker-ce-packaging#254
- Removes 'docker-' prefix for containerd and runc binaries docker/engine#61 / moby/moby#37907, docker-ce-packaging#241
- Splits ""engine"", ""cli"", and ""containerd"" to separate packages, and run containerd as a separate systemd service docker-ce-packaging#131, docker-ce-packaging#158
- Builds binaries with Go 1.10.4 docker-ce-packaging#181
- Removes
-ce
suffix from version string docker-ce-packaging#206
Fixes
- BuildKit: Do not cancel buildkit status request. moby/moby#37597
- Fixes no error is shown if build args are missing during docker build moby/moby#37396
- Fixes error ""unexpected EOF"" when adding an 8GB file moby/moby#37771
- LCOW: Ensures platform is populated on
COPY
/ADD
. moby/moby#37563 - Fixes mapping a range of host ports to a single container port docker/cli#1102
- Fixes
trust inspect
typo: ""AdminstrativeKeys
"" docker/cli#1300 - Fixes environment file parsing for imports of absent variables and those with no name. docker/cli#1019
- Fixes a potential ""out of memory exception"" when running
docker image prune
with a large list of dangling images docker/cli#1432 / docker/cli#1423 - Fixes pipe handling in ConEmu and ConsoleZ on Windows moby/moby#37600
- Fixes long startup on windows, with non-hns governed Hyper-V networks docker/engine#67 / moby/moby#37774
- Fixes daemon won't start when ""runtimes"" option is defined both in config file and cli docker/engine#57 / moby/moby#37871
- Loosens permissions on
/etc/docker
directory to prevent ""permission denied"" errors when usingdocker manifest inspect
docker/engine#56 / moby/moby#37847 - Fixes denial of service with large numbers in
cpuset-cpus
andcpuset-mems
docker/engine#70 / moby/moby#37967 - LCOW: Add
--platform
todocker import
docker/cli#1375 / docker/cli#1371 - LCOW: Add LinuxMetadata support by default on Windows moby/moby#37514
- LCOW: Mount to short container paths to avoid command-line length limit moby/moby#37659
- LCOW: Fix builder using wrong cache layer moby/moby#37356
- Fixes json-log file descriptors leaking when using
--follow
docker/engine#48 moby/moby#37576 moby/moby#37734 - Fixes a possible deadlock on closing the watcher on kqueue moby/moby#37392
- Uses poller based watcher to work around the file caching issue in Windows moby/moby#37412
- Handles systemd-resolved case by providing appropriate resolv.conf to networking layer moby/moby#37485
- Removes support for TLS < 1.2 moby/moby#37660
- Seccomp: Whitelist syscalls linked to
CAP_SYS_NICE
in default seccomp profile moby/moby#37242 - Seccomp: move the syslog syscall to be gated by
CAP_SYS_ADMIN
orCAP_SYSLOG
docker/engine#64 / moby/moby#37929 - SELinux: Fix relabeling of local volumes specified via Mounts API on selinux-enabled systems moby/moby#37739
- Adds warning if REST API is accessible through an insecure connection moby/moby#37684
- Masks proxy credentials from URL when displayed in system info docker/engine#72 / moby/moby#37934
- Fixes mount propagation for btrfs docker/engine#86 / moby/moby#38026
- Fixes nil pointer dereference in node allocation docker/engine#94 / docker/swarmkit#2764
Known Issues
There are important changes to the upgrade process that, if not correctly followed, can have impact on the availability of applications running on the Swarm during upgrades. These constraints impact any upgrades coming from any version before 18.09 to version 18.09 or greater.
With https://github.com/boot2docker/boot2docker/releases/download/v18.09.0/boot2docker.iso, connection is being refused from a node on the virtual machine. Any publishing of swarm ports in virtualbox-created docker-machine VM's will not respond. This is occurring on macOS and Windows 10, using docker-machine version 0.15 and 0.16.
The following
docker run
command works, allowing access from host browser:docker run -d -p 4000:80 nginx
However, the following
docker service
command fails, resulting in curl/chrome unable to connect (connection refused):docker service create -p 5000:80 nginx
This issue is not apparent when provisioning 18.09.0 cloud VM's using docker-machine.
Workarounds:
- Use cloud VM's that don't rely on boot2docker.
docker run
is unaffected.- For Swarm, set VIRTUALBOX_BOOT2DOCKER_URL=https://github.com/boot2docker/boot2docker/releases/download/v18.06.1-ce/boot2docker.iso.
This issue is resolved in 18.09.1.
Deprecation Notices
Docker has deprecated support for Device Mapper as a storage driver. It will continue to be supported at this time, but support will be removed in a future release.
The Overlay2 storage driver is now the default for Docker Engine implementations.
For more information on the list of deprecated flags and APIs, have a look at the deprecation information where you can find the target removal dates.
End of Life Notification
In this release, Docker has also removed support for TLS < 1.2 moby/moby#37660, Ubuntu 14.04 ""Trusty Tahr"" docker-ce-packaging#255 / docker-ce-packaging#254, and Debian 8 ""Jessie"" docker-ce-packaging#255 / docker-ce-packaging#254.",,,
2d7c99800a3b548fa57ea55f8c0c3fc1abaf0a0156538bc615d0010b437fc032,"Integrate Docker Scout with GitHub
The GitHub app integration for Docker Scout grants Docker Scout access to your source code repository on GitHub. This improved visibility into how your image gets created means Docker Scout can give you automated and contextual remediation advice.
How it works
When you enable the GitHub integration, Docker Scout can make a direct link between the image analysis results and the source.
When analyzing your image, Docker Scout checks for provenance attestations to detect the location of the source code repository for the image. If the source location is found, and you've enabled the GitHub app, Docker Scout parses the Dockerfile used to create the image.
Parsing the Dockerfile reveals the base image tag used to build the image. By
knowing the base image tags used, Docker Scout can detect whether the tag is
outdated, meaning it's been changed to a different image digest. For example,
say you're using alpine:3.18
as your base image, and at a later point in
time, the image maintainers release a patch version for version 3.18
,
containing security fixes. The alpine:3.18
tag you've been using becomes
out-of-date; the alpine:3.18
you're using is no longer the latest.
When this happens, Docker Scout detects the discrepancy and surfaces it through the Up-to-Date Base Images policy. When the GitHub integration's enabled, you'll also get automated suggestions on how to update your base image. For more information about how Docker Scout can help you automatically improve your supply chain conduct and security posture, see Remediation.
Setup
To integrate Docker Scout with your GitHub organization:
Go to GitHub integration on the Docker Scout Dashboard.
Select the Integrate GitHub app button to open GitHub.
Select the organization that you want to integrate.
Select whether you want to integrate all repositories in the GitHub organization or a manual selection of repositories.
Select Install & Authorize to add the Docker Scout app to the organization.
This redirects you back to the Docker Scout Dashboard, which lists your active GitHub integrations.
The GitHub integration is now active.",,,
c7a2848d14635f917adfbc10e704d1a36ed94b897d187b5f6cf01342c0834438,"Manage builders
You can create, inspect, and manage builders using docker buildx
commands,
or
using Docker Desktop.
Create a new builder
The default builder uses the
docker
driver.
You can't manually create new docker
builders, but you can create builders
that use other drivers, such as the
docker-container
driver,
which runs the BuildKit daemon in a container.
Use the
docker buildx create
command to create a builder.
$ docker buildx create --name=<builder-name>
Buildx uses the docker-container
driver by default if you omit the --driver
flag. For more information about available drivers, see
Build drivers.
List available builders
Use docker buildx ls
to see builder instances available on your system, and
the drivers they're using.
$ docker buildx ls
NAME/NODE DRIVER/ENDPOINT STATUS BUILDKIT PLATFORMS
default * docker
default default running v0.11.6 linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386
my_builder docker-container
my_builder0 default running v0.11.6 linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386
The asterisk (*
) next to the builder name indicates the
selected builder.
Inspect a builder
To inspect a builder with the CLI, use docker buildx inspect <name>
.
You can only inspect a builder if the builder is active.
You can add the --bootstrap
flag to the command to start the builder.
$ docker buildx inspect --bootstrap my_builder
[+] Building 1.7s (1/1) FINISHED
=> [internal] booting buildkit 1.7s
=> => pulling image moby/buildkit:buildx-stable-1 1.3s
=> => creating container buildx_buildkit_my_builder0 0.4s
Name: my_builder
Driver: docker-container
Last Activity: 2023-06-21 18:28:37 +0000 UTC
Nodes:
Name: my_builder0
Endpoint: unix:///var/run/docker.sock
Status: running
Buildkit: v0.11.6
Platforms: linux/arm64, linux/amd64, linux/amd64/v2, linux/riscv64, linux/ppc64le, linux/s390x, linux/386, linux/mips64le, linux/mips64, linux/arm/v7, linux/arm/v6
If you want to see how much disk space a builder is using, use the
docker buildx du
command. By default, this command shows the total disk usage
for all available builders. To see usage for a specific builder, use the
--builder
flag.
$ docker buildx du --builder my_builder
ID RECLAIMABLE SIZE LAST ACCESSED
olkri5gq6zsh8q2819i69aq6l true 797.2MB 37 seconds ago
6km4kasxgsywxkm6cxybdumbb* true 438.5MB 36 seconds ago
qh3wwwda7gx2s5u4hsk0kp4w7 true 213.8MB 37 seconds ago
54qq1egqem8max3lxq6180cj8 true 200.2MB 37 seconds ago
ndlp969ku0950bmrw9muolw0c* true 116.7MB 37 seconds ago
u52rcsnfd1brwc0chwsesb3io* true 116.7MB 37 seconds ago
rzoeay0s4nmss8ub59z6lwj7d true 46.25MB 4 minutes ago
itk1iibhmv7awmidiwbef633q true 33.33MB 37 seconds ago
4p78yqnbmgt6xhcxqitdieeln true 19.46MB 4 minutes ago
dgkjvv4ay0szmr9bl7ynla7fy* true 19.24MB 36 seconds ago
tuep198kmcw299qc9e4d1a8q2 true 8.663MB 4 minutes ago
n1wzhauk9rpmt6ib1es7dktvj true 20.7kB 4 minutes ago
0a2xfhinvndki99y69157udlm true 16.56kB 37 seconds ago
gf0z1ypz54npfererqfeyhinn true 16.38kB 37 seconds ago
nz505f12cnsu739dw2pw0q78c true 8.192kB 37 seconds ago
hwpcyq5hdfvioltmkxu7fzwhb* true 8.192kB 37 seconds ago
acekq89snc7j6im1rjdizvsg1* true 8.192kB 37 seconds ago
Reclaimable: 2.01GB
Total: 2.01GB
Remove a builder
Use the
docker buildx remove
command to remove a builder.
$ docker buildx rm <builder-name>
If you remove your currently selected builder,
the default docker
builder is automatically selected.
You can't remove the default builder.
Local build cache for the builder is also removed.
Removing remote builders
Removing a remote builder doesn't affect the remote build cache. It also doesn't stop the remote BuildKit daemon. It only removes your connection to the builder.
Manage builders with Docker Desktop
If you have turned on the Docker Desktop Builds view, you can inspect builders in Docker Desktop settings.",,,
5bc306086a6b242dd6d069a63daf52bd9e85fcb718a1737df43cd2774eed60fe,"Docker design principles
Provide actionable guidance
We anticipate needs and provide simple explanations with clear actions so people are never lost and always know what to do next. Recommendations lead users to functionality that enhances the experience and extends their knowledge.
Create value through confidence
People from all levels of experience should feel they know how to use our product. Experiences are familiar, unified, and easy to use so all users feel like experts.
Infuse productivity with delight
We seek out moments of purposeful delight that elevate rather than distract, making work easier and more gratifying. Simple tasks are automated and users are left with more time for innovation.
Build trust through transparency
We always provide clarity on what is happening and why. No amount of detail is withheld; the right information is shown at the right time and is always accessible.
Scale with intention
Our products focus on inclusive growth and are continuously useful and adapt to match changing individual needs. We support all levels of expertise by meeting users where they are with conscious personalization.
What's next?
- Take a look at our UI styling guidelines.
- Learn how to publish your extension.",,,
d9b6a69d2213e35c72c008389750800ebf946e5cbf0fd01fee1031df98af9626,"How Compose works
With Docker Compose you use a YAML configuration file, known as the Compose file, to configure your application’s services, and then you create and start all the services from your configuration with the Compose CLI.
The Compose file, or compose.yaml
file, follows the rules provided by the
Compose Specification in how to define multi-container applications. This is the Docker Compose implementation of the formal
Compose Specification.
Computing components of an application are defined as services. A service is an abstract concept implemented on platforms by running the same container image, and configuration, one or more times.
Services communicate with each other through networks. In the Compose Specification, a network is a platform capability abstraction to establish an IP route between containers within services connected together.
Services store and share persistent data into volumes. The Specification describes such a persistent data as a high-level filesystem mount with global options.
Some services require configuration data that is dependent on the runtime or platform. For this, the Specification defines a dedicated configs concept. From a service container point of view, configs are comparable to volumes, in that they are files mounted into the container. But the actual definition involves distinct platform resources and services, which are abstracted by this type.
A secret is a specific flavor of configuration data for sensitive data that should not be exposed without security considerations. Secrets are made available to services as files mounted into their containers, but the platform-specific resources to provide sensitive data are specific enough to deserve a distinct concept and definition within the Compose specification.
Note
With volumes, configs and secrets you can have a simple declaration at the top-level and then add more platform-specific information at the service level.
A project is an individual deployment of an application specification on a platform. A project's name, set with the top-level
name
attribute, is used to group
resources together and isolate them from other applications or other installation of the same Compose-specified application with distinct parameters. If you are creating resources on a platform, you must prefix resource names by project and
set the label com.docker.compose.project
.
Compose offers a way for you to set a custom project name and override this name, so that the same compose.yaml
file can be deployed twice on the same infrastructure, without changes, by just passing a distinct name.
The Compose file
The default path for a Compose file is compose.yaml
(preferred) or compose.yml
that is placed in the working directory.
Compose also supports docker-compose.yaml
and docker-compose.yml
for backwards compatibility of earlier versions.
If both files exist, Compose prefers the canonical compose.yaml
.
You can use fragments and extensions to keep your Compose file efficient and easy to maintain.
Multiple Compose files can be merged together to define the application model. The combination of YAML files is implemented by appending or overriding YAML elements based on the Compose file order you set. Simple attributes and maps get overridden by the highest order Compose file, lists get merged by appending. Relative paths are resolved based on the first Compose file's parent folder, whenever complimentary files being merged are hosted in other folders. As some Compose file elements can both be expressed as single strings or complex objects, merges apply to the expanded form. For more information, see Working with multiple Compose files.
If you want to reuse other Compose files, or factor out parts of your application model into separate Compose files, you can also use
include
. This is useful if your Compose application is dependent on another application which is managed by a different team, or needs to be shared with others.
CLI
The Docker CLI lets you interact with your Docker Compose applications through the docker compose
command, and its subcommands. Using the CLI, you can manage the lifecycle of your multi-container applications defined in the compose.yaml
file. The CLI commands enable you to start, stop, and configure your applications effortlessly.
Key commands
To start all the services defined in your compose.yaml
file:
$ docker compose up
To stop and remove the running services:
$ docker compose down
If you want to monitor the output of your running containers and debug issues, you can view the logs with:
$ docker compose logs
To lists all the services along with their current status:
$ docker compose ps
For a full list of all the Compose CLI commands, see the reference documentation.
Illustrative example
The following example illustrates the Compose concepts outlined above. The example is non-normative.
Consider an application split into a frontend web application and a backend service.
The frontend is configured at runtime with an HTTP configuration file managed by infrastructure, providing an external domain name, and an HTTPS server certificate injected by the platform's secured secret store.
The backend stores data in a persistent volume.
Both services communicate with each other on an isolated back-tier network, while the frontend is also connected to a front-tier network and exposes port 443 for external usage.
The example application is composed of the following parts:
- 2 services, backed by Docker images:
webapp
anddatabase
- 1 secret (HTTPS certificate), injected into the frontend
- 1 configuration (HTTP), injected into the frontend
- 1 persistent volume, attached to the backend
- 2 networks
services:
frontend:
image: example/webapp
ports:
- ""443:8043""
networks:
- front-tier
- back-tier
configs:
- httpd-config
secrets:
- server-certificate
backend:
image: example/database
volumes:
- db-data:/etc/data
networks:
- back-tier
volumes:
db-data:
driver: flocker
driver_opts:
size: ""10GiB""
configs:
httpd-config:
external: true
secrets:
server-certificate:
external: true
networks:
# The presence of these objects is sufficient to define them
front-tier: {}
back-tier: {}
The docker compose up
command starts the frontend
and backend
services, create the necessary networks and volumes, and injects the configuration and secret into the frontend service.
docker compose ps
provides a snapshot of the current state of your services, making it easy to see which containers are running, their status, and the ports they are using:
$ docker compose ps
NAME IMAGE COMMAND SERVICE CREATED STATUS PORTS
example-frontend-1 example/webapp ""nginx -g 'daemon of…"" frontend 2 minutes ago Up 2 minutes 0.0.0.0:443->8043/tcp
example-backend-1 example/database ""docker-entrypoint.s…"" backend 2 minutes ago Up 2 minutes",,,
57fd996badb83da6554e4d1cce99b1c6b9167b9d1f15754e1bb3dd13a15d7d5e,"Remediation with Docker Scout
Docker Scout helps you remediate supply chain or security issues by providing recommendations based on policy evaluation results. Recommendations are suggested actions you can take that improve policy compliance, or that add metadata to images which enables Docker Scout to provide better evaluation results and recommendations.
Docker Scout provides remediation advice for the default policies of the following policy types:
Note
Guided remediation is not supported for custom policies.
For images that violate policy, the recommendations focus on addressing compliance issues and fixing violations. For images where Docker Scout is unable to determine compliance, recommendations show you how to meet the prerequisites that ensure Docker Scout can successfully evaluate the policy.
View recommendations
Recommendations appear on the policy details pages of the Docker Scout Dashboard. To get to this page:
- Go to the Policies page in the Docker Scout Dashboard.
- Select a policy in the list.
The policy details page groups evaluation results into two different tabs depending on the policy status:
- Violations
- Compliance unknown
The Violations tab lists images that don't comply with the selected policy. The Compliance unknown tab lists images that Docker Scout is unable to determine the compliance status for. When compliance is unknown, Docker Scout needs more information about the image.
To view recommended actions for an image, hover over one of the images in the list to reveal a View fixes button.
Select the View fixes button to opens the remediation side panel containing recommended actions for your image.
If there are more than one recommendations available, the primary recommendation displays as the Recommended fix. Additional recommendations are listed as Quick fixes. Quick fixes are usually actions that provide a temporary solution.
The side panel may also contain one or more help sections related to the available recommendations.
Up-to-Date Base Images remediation
The Up-to-Date Base Images policy checks whether the base image you use is up-to-date. The recommended actions displayed in the remediation side panel depend on how much information Docker Scout has about your image. The more information that's available, the better the recommendations.
The following scenarios outline the different recommendations depending on the information available about the image.
No provenance attestations
For Docker Scout to be able to evaluate this policy, you must add provenance attestations to your image. If your image doesn't have provenance attestations, compliance is undeterminable.
Provenance attestations available
With provenance attestations added, Docker Scout can correctly detect the base image version that you're using. The version found in the attestations is cross-referenced against the current version of the corresponding tag to determine if it's up-to-date.
If there's a policy violation, the recommended actions show how to update your base image version to the latest version, while also pinning the base image version to a specific digest. For more information, see Pin base image versions.
GitHub integration enabled
If you're hosting the source code for your image on GitHub, you can enable the GitHub integration. This integration enables Docker Scout to provide even more useful remediation advice, and lets you initiate remediation for violations directly from the Docker Scout Dashboard.
With the GitHub integration enabled, you can use the remediation side panel to raise a pull request on the GitHub repository of the image. The pull request automatically updates the base image version in your Dockerfile to the up-to-date version.
This automated remediation pins your base image to a specific digest, while helping you stay up-to-date as new versions become available. Pinning the base image to a digest is important for reproducibility, and helps avoid unwanted changes from making their way into your supply chain.
For more information about base image pinning, see Pin base image versions.
Supply Chain Attestations remediation
The default Supply Chain Attestations policy requires full provenance and SBOM attestations on images. If your image is missing an attestation, or if an attestation doesn't contain enough information, the policy is violated.
The recommendations available in the remediation side panel helps guide you to
what action you need to take to address the issues. For example, if your image
has a provenance attestation, but the attestation doesn't contain enough
information, you're recommended to re-build your image with
mode=max
provenance.",,,
1cea1e920015dba558f4f6c845ef4742558d4f8e4d4a1962b59d6fa3477bb961,"Add nodes to the swarm
Once you've created a swarm with a manager node, you're ready to add worker nodes.
Open a terminal and ssh into the machine where you want to run a worker node. This tutorial uses the name
worker1
.Run the command produced by the
docker swarm init
output from the Create a swarm tutorial step to create a worker node joined to the existing swarm:$ docker swarm join \ --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \ 192.168.99.100:2377 This node joined a swarm as a worker.
If you don't have the command available, you can run the following command on a manager node to retrieve the join command for a worker:
$ docker swarm join-token worker To add a worker to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \ 192.168.99.100:2377
Open a terminal and ssh into the machine where you want to run a second worker node. This tutorial uses the name
worker2
.Run the command produced by the
docker swarm init
output from the Create a swarm tutorial step to create a second worker node joined to the existing swarm:$ docker swarm join \ --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \ 192.168.99.100:2377 This node joined a swarm as a worker.
Open a terminal and ssh into the machine where the manager node runs and run the
docker node ls
command to see the worker nodes:$ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS 03g1y59jwfg7cf99w4lt0f662 worker2 Ready Active 9j68exjopxe7wfl6yuxml7a7j worker1 Ready Active dxn1zf6l61qsb1josjja83ngz * manager1 Ready Active Leader
The
MANAGER
column identifies the manager nodes in the swarm. The empty status in this column forworker1
andworker2
identifies them as worker nodes.Swarm management commands like
docker node ls
only work on manager nodes.
What's next?
Now your swarm consists of a manager and two worker nodes. Next, you'll deploy a service.",,,
ee67ba362bec65459ee10ee9b568064ff686862864c005ad5e74c64b47271ae0,"Sample apps with Compose
The following samples show the various aspects of how to work with Docker Compose. As a prerequisite, be sure to install Docker Compose if you have not already done so.
Key concepts these samples cover
The samples should help you to:
- Define services based on Docker images using
Compose files:
compose.yaml
anddocker-stack.yml
- Understand the relationship between
compose.yaml
and Dockerfiles - Learn how to make calls to your application services from Compose files
- Learn how to deploy applications and services to a swarm
Samples tailored to demo Compose
These samples focus specifically on Docker Compose:
Quickstart: Compose and ELK - Shows how to use Docker Compose to set up and run ELK - Elasticsearch-Logstash-Kibana.
Quickstart: Compose and Django - Shows how to use Docker Compose to set up and run a simple Django/PostgreSQL app.
Quickstart: Compose and Rails - Shows how to use Docker Compose to set up and run a Rails/PostgreSQL app.
Quickstart: Compose and WordPress - Shows how to use Docker Compose to set up and run WordPress in an isolated environment with Docker containers.
Awesome Compose samples
The Awesome Compose samples provide a starting point on how to integrate different frameworks and technologies using Docker Compose. All samples are available in the
Awesome-compose GitHub repo and are ready to run with docker compose up
.",,,
9cc9f8d0a26954b05dca1eff0d6a836221d56c49e6595bc26ff5283457736a40,"Docker Engine 18.03 release notes
Table of contents
18.03.1-ce
2018-04-26
Client
- Fix error with merge compose file with networks docker/cli#983
- Fix docker stack deploy re-deploying services after the service was updated with
--force
docker/cli#963 - Fix docker version output alignment docker/cli#965
Runtime
- Fix AppArmor profiles not being applied to
docker exec
processes moby/moby#36466 - Don't sort plugin mount slice moby/moby#36711
- Daemon/cluster: handle partial attachment entries during configure moby/moby#36769
- Bump Golang to 1.9.5 moby/moby#36779 docker/cli#986
- Daemon/stats: more resilient cpu sampling moby/moby#36519
- Containerd: update to 1.0.3 release moby/moby#36749
- Fix Windows layer leak when write fails moby/moby#36728
- Don't make container mount unbindable moby/moby#36768
- Fix Daemon panics on container export after a daemon restart moby/moby/36586
- Fix digest cache being removed on autherrors moby/moby#36509
- Make sure plugin container is removed on failure moby/moby#36715
- Copy: avoid using all system memory with authz plugins moby/moby#36595
- Relax some libcontainerd client locking moby/moby#36848
- Update
hcsshim
to v0.6.10 to address CVE-2018-8115
Swarm Mode
- Increase raft Election tick to 10 times Heartbeat tick moby/moby#36672
Networking
- Gracefully remove LB endpoints from services docker/libnetwork#2112
- Retry other external DNS servers on ServFail docker/libnetwork#2121
- Improve scalability of bridge network isolation rules docker/libnetwork#2117
- Allow for larger preset property values, do not override docker/libnetwork#2124
- Prevent panics on concurrent reads/writes when calling
changeNodeState
docker/libnetwork#2136
18.03.0-ce
2018-03-21
Builder
- Switch to -buildmode=pie moby/moby#34369
- Allow Dockerfile to be outside of build-context docker/cli#886
- Builder: fix wrong cache hits building from tars moby/moby#36329
- Fixes files leaking to other images in a multi-stage build moby/moby#36338
Client
- Simplify the marshaling of compose types.Config docker/cli#895
- Add support for multiple composefile when deploying docker/cli#569
- Fix broken Kubernetes stack flags docker/cli#831
- Fix stack marshaling for Kubernetes docker/cli#890
- Fix and simplify bash completion for service env, mounts and labels docker/cli#682
- Fix
before
andsince
filter fordocker ps
moby/moby#35938 - Fix
--label-file
weird behavior docker/cli#838 - Fix compilation of defaultCredentialStore() on unsupported platforms docker/cli#872
- Improve and fix bash completion for images docker/cli#717
- Added check for empty source in bind mount docker/cli#824
- Fix TLS from environment variables in client moby/moby#36270
- docker build now runs faster when registry-specific credential helper(s) are configured docker/cli#840
- Update event filter zsh completion with
disable
,enable
,install
andremove
docker/cli#372 - Produce errors when empty ids are passed into inspect calls moby/moby#36144
- Marshall version for the k8s controller docker/cli#891
- Set a non-zero timeout for HTTP client communication with plugin backend docker/cli#883
- Add DOCKER_TLS environment variable for --tls option docker/cli#863
- Add --template-driver option for secrets/configs docker/cli#896
- Move
docker trust
commands out of experimental docker/cli#934 docker/cli#935 docker/cli#944
Logging
- AWS logs - don't add new lines to maximum sized events moby/moby#36078
- Move log validator logic after plugins are loaded moby/moby#36306
- Support a proxy in Splunk log driver moby/moby#36220
- Fix log tail with empty logs moby/moby#36305
Networking
- Libnetwork revendoring moby/moby#36137
- Fix for deadlock on exit with Memberlist revendor docker/libnetwork#2040
- Fix user specified ndots option docker/libnetwork#2065
- Fix to use ContainerID for Windows instead of SandboxID docker/libnetwork#2010
- Verify NetworkingConfig to make sure EndpointSettings is not nil moby/moby#36077
- Fix
DockerNetworkInternalMode
issue moby/moby#36298 - Fix race in attachable network attachment moby/moby#36191
- Fix timeout issue of
InspectNetwork
on AArch64 moby/moby#36257
- Verbose info is missing for partial overlay ID moby/moby#35989
- Update
FindNetwork
to address network name duplications moby/moby#30897 - Disallow attaching ingress network docker/swarmkit#2523
- Prevent implicit removal of the ingress network moby/moby#36538
- Fix stale HNS endpoints on Windows moby/moby#36603
- IPAM fixes for duplicate IP addresses docker/libnetwork#2104 docker/libnetwork#2105
Runtime
- Enable HotAdd for Windows moby/moby#35414
- LCOW: Graphdriver fix deadlock in hotRemoveVHDs moby/moby#36114
- LCOW: Regular mount if only one layer moby/moby#36052
- Remove interim env var LCOW_API_PLATFORM_IF_OMITTED moby/moby#36269
- Revendor Microsoft/opengcs @ v0.3.6 moby/moby#36108
- Fix issue of ExitCode and PID not show up in Task.Status.ContainerStatus moby/moby#36150
- Fix issue with plugin scanner going too deep moby/moby#36119
- Do not make graphdriver homes private mounts moby/moby#36047
- Do not recursive unmount on cleanup of zfs/btrfs moby/moby#36237
- Don't restore image if layer does not exist moby/moby#36304
- Adjust minimum API version for templated configs/secrets moby/moby#36366
- Bump containerd to 1.0.2 (cfd04396dc68220d1cecbe686a6cc3aa5ce3667c) moby/moby#36308
- Bump Golang to 1.9.4 moby/moby#36243
- Ensure daemon root is unmounted on shutdown moby/moby#36107
- Update runc to 6c55f98695e902427906eed2c799e566e3d3dfb5 moby/moby#36222
- Fix container cleanup on daemon restart moby/moby#36249
- Support SCTP port mapping (bump up API to v1.37) moby/moby#33922
- Support SCTP port mapping docker/cli#278
- Fix Volumes property definition in ContainerConfig moby/moby#35946
- Bump moby and dependencies docker/cli#829
- C.RWLayer: check for nil before use moby/moby#36242
- Add
REMOVE
andORPHANED
to TaskState moby/moby#36146
- Fixed error detection using
IsErrNotFound
andIsErrNotImplemented
forContainerStatPath
,CopyFromContainer
, andCopyToContainer
methods moby/moby#35979
- Add an integration/internal/container helper package moby/moby#36266
- Add canonical import path moby/moby#36194
- Add/use container.Exec() to integration moby/moby#36326
- Fix ""--node-generic-resource"" singular/plural moby/moby#36125
- Daemon.cleanupContainer: nullify container RWLayer upon release moby/moby#36160
- Daemon: passdown the
--oom-kill-disable
option to containerd moby/moby#36201 - Display a warn message when there is binding ports and net mode is host moby/moby#35510
- Refresh containerd remotes on containerd restarted moby/moby#36173
- Set daemon root to use shared propagation moby/moby#36096
- Optimizations for recursive unmount moby/moby#34379
- Perform plugin mounts in the runtime moby/moby#35829
- Graphdriver: Fix RefCounter memory leak moby/moby#36256
- Use continuity fs package for volume copy moby/moby#36290
- Use proc/exe for reexec moby/moby#36124
- Add API support for templated secrets and configs moby/moby#33702 and moby/moby#36366
- Use rslave propagation for mounts from daemon root moby/moby#36055
- Add /proc/keys to masked paths moby/moby#36368
- Bump Runc to 1.0.0-rc5 moby/moby#36449
- Fixes
runc exec
on big-endian architectures moby/moby#36449
- Use chroot when mount namespaces aren't provided moby/moby#36449
- Fix systemd slice expansion so that it could be consumed by cAdvisor moby/moby#36449
- Fix devices mounted with wrong uid/gid moby/moby#36449
- Fix read-only containers with IPC private mounts
/dev/shm
read-only moby/moby#36526
Swarm Mode
- Replace EC Private Key with PKCS#8 PEMs docker/swarmkit#2246
- Fix IP overlap with empty EndpointSpec docker/swarmkit #2505
- Add support for Support SCTP port mapping docker/swarmkit#2298
- Do not reschedule tasks if only placement constraints change and are satisfied by the assigned node docker/swarmkit#2496
- Ensure task reaper stopChan is closed no more than once docker/swarmkit #2491
- Synchronization fixes docker/swarmkit#2495
- Add log message to indicate message send retry if streaming unimplemented docker/swarmkit#2483
- Debug logs for session, node events on dispatcher, heartbeats docker/swarmkit#2486
- Add swarm types to bash completion event type filter docker/cli#888
- Fix issue where network inspect does not show Created time for networks in swarm scope moby/moby#36095",,,
8ce3133e20891a901d1b271bf53950fed72fbb8db1584072023fa61a9eb43685,"Create a company
Table of contents
Subscription:
Business
For:
Administrators
You can create a new company in the Docker Admin Console. Before you begin, you must:
- Be the owner of the organization you want to add to your company
- Have a Docker Business subscription
Create a company
To create a new company:
Sign in to the Admin Console.
Select your organization you want to add to your company from the Choose profile page.
Under Organization settings, select Company management.
Select Create a company.
Enter a unique name for your company, then select Continue.
Tip
The name for your company can't be the same as an existing user, organization, or company namespace.
Review the company migration details and then select Create company.
For more information on how you can add organizations to your company, see Add organizations to a company.",,,
285f2b212921bbf461ef5525873a1840e494c06141524593dc901b92139872ef,"Docker Engine 18.05 release notes
Table of contents
18.05.0-ce
2018-05-09
Builder
- Adding
netbsd
compatibility to the packagepkg/term
. moby/moby#36887 - Standardizes output path for artifacts of intermediate builds to
/build/
. moby/moby#36858
Client
- Fix
docker stack deploy
reference flag. docker/cli#981 - Fix docker stack deploy re-deploying services after the service was updated with
--force
. docker/cli#963
- Add bash completion for
secret|config create --template-driver
. docker/cli#1004 - Add fish completions for docker trust subcommand. docker/cli#984
- Fix --format example for docker history. docker/cli#980
- Fix error with merge composefile with networks. docker/cli#983
Logging
- Standardized the properties of storage-driver log messages. moby/moby#36492
- Improve partial message support in logger. moby/moby#35831
Networking
- Allow for larger preset property values, do not override. docker/libnetwork#2124
- networkdb: User write lock in handleNodeEvent. docker/libnetwork#2136
- Import libnetwork fix for rolling updates. moby/moby#36638
- Update libnetwork to improve scalability of bridge network isolation rules. moby/moby#36774
- Fix a misused network object name. moby/moby#36745
Runtime
- LCOW: Implement
docker save
. moby/moby#36599 - Pkg: devmapper: dynamically load dm_task_deferred_remove. moby/moby#35518
- Windows: Add GetLayerPath implementation in graphdriver. moby/moby#36738
- Fix Windows layer leak when write fails. moby/moby#36728
- Fix FIFO, sockets and device files when run in user NS. moby/moby#36756
- Fix docker version output alignment. docker/cli#965
- Always make sysfs read-write with privileged. moby/moby#36808
- Bump Golang to 1.10.1. moby/moby#35739
- Bump containerd client. moby/moby#36684
- Bump golang.org/x/net to go1.10 release commit. moby/moby#36894
- Context.WithTimeout: do call the cancel func. moby/moby#36920
- Copy: avoid using all system memory with authz plugins. moby/moby#36595
- Daemon/cluster: handle partial attachment entries during configure. moby/moby#36769
- Don't make container mount unbindable. moby/moby#36768
- Extra check before unmounting on shutdown. moby/moby#36879
- Move mount parsing to separate package. moby/moby#36896
- No global volume driver store. moby/moby#36637
- Pkg/mount improvements. moby/moby#36091
- Relax some libcontainerd client locking. moby/moby#36848
- Remove daemon dependency on api packages. moby/moby#36912
- Remove the retries for service update. moby/moby#36827
- Revert unencryted storage warning prompt. docker/cli#1008
- Support cancellation in
directory.Size()
. moby/moby#36734 - Switch from x/net/context -> context. moby/moby#36904
- Fixed a function to check Content-type is
application/json
or not. moby/moby#36778
- Add default pollSettings config functions. moby/moby#36706
- Add if judgment before receiving operations on daemonWaitCh. moby/moby#36651
- Fix issues with running volume tests as non-root.. moby/moby#36935
Swarm Mode
- RoleManager will remove detected nodes from the cluster membership docker/swarmkit#2548
- Scheduler/TaskReaper: handle unassigned tasks marked for shutdown docker/swarmkit#2574
- Avoid predefined error log. docker/swarmkit#2561
- Task reaper should delete tasks with removed slots that were not yet assigned. docker/swarmkit#2557
- Agent reports FIPS status. docker/swarmkit#2587
- Fix: timeMutex critical operation outside of critical section. docker/swarmkit#2603
- Expose swarmkit's Raft tuning parameters in engine config. moby/moby#36726
- Make internal/test/daemon.Daemon swarm aware. moby/moby#36826",,,
f6a7cec9bddd765e3109f3c1896cae47f431d79372924adb8c95ff8ce90af9a3,"FAQs for Docker Desktop for Windows
Can I use VirtualBox alongside Docker Desktop?
Yes, you can run VirtualBox along with Docker Desktop if you have enabled the Windows Hypervisor Platform feature on your machine.
Why is Windows 10 or Windows 11 required?
Docker Desktop uses the Windows Hyper-V features. While older Windows versions have Hyper-V, their Hyper-V implementations lack features critical for Docker Desktop to work.
Can I run Docker Desktop on Windows Server?
No, running Docker Desktop on Windows Server is not supported.
Can I change permissions on shared volumes for container-specific deployment requirements?
Docker Desktop does not enable you to control (chmod
)
the Unix-style permissions on
shared volumes for
deployed containers, but rather sets permissions to a default value of
0777
(read
, write
, execute
permissions for user
and for
group
) which is not configurable.
For workarounds and to learn more, see Permissions errors on data directories for shared volumes.
How do symlinks work on Windows?
Docker Desktop supports two types of symlinks: Windows native symlinks and symlinks created inside a container.
The Windows native symlinks are visible within the containers as symlinks, whereas symlinks created inside a container are represented as mfsymlinks. These are regular Windows files with a special metadata. Therefore the symlinks created inside a container appear as symlinks inside the container, but not on the host.
File sharing with Kubernetes and WSL 2
Docker Desktop mounts the Windows host filesystem under /run/desktop
inside the container running Kubernetes.
See the
Stack Overflow post for an example of how to configure a Kubernetes Persistent Volume to represent directories on the host.
How do I add custom CA certificates?
You can add trusted Certificate Authorities (CAs) to your Docker daemon to verify registry server certificates, and client certificates, to authenticate to registries.
Docker Desktop supports all trusted Certificate Authorities (CAs) (root or intermediate). Docker recognizes certs stored under Trust Root Certification Authorities or Intermediate Certification Authorities.
Docker Desktop creates a certificate bundle of all user-trusted CAs based on the Windows certificate store, and appends it to Moby trusted certificates. Therefore, if an enterprise SSL certificate is trusted by the user on the host, it is trusted by Docker Desktop.
To learn more about how to install a CA root certificate for the registry, see Verify repository client with certificates in the Docker Engine topics.
How do I add client certificates?
You can add your client certificates
in ~/.docker/certs.d/<MyRegistry><Port>/client.cert
and
~/.docker/certs.d/<MyRegistry><Port>/client.key
. You do not need to push your certificates with git
commands.
When the Docker Desktop application starts, it copies the
~/.docker/certs.d
folder on your Windows system to the /etc/docker/certs.d
directory on Moby (the Docker Desktop virtual machine running on Hyper-V).
You need to restart Docker Desktop after making any changes to the keychain
or to the ~/.docker/certs.d
directory in order for the changes to take effect.
The registry cannot be listed as an insecure registry (see
Docker Daemon). Docker Desktop ignores
certificates listed under insecure registries, and does not send client
certificates. Commands like docker run
that attempt to pull from the registry
produce error messages on the command line, as well as on the registry.
To learn more about how to set the client TLS certificate for verification, see Verify repository client with certificates in the Docker Engine topics.",,,
e6fabb08b337e92bbd6b50d1437fe81fdebf43c26e7af87e1cdad2eff872e784,"Docker Engine version 27 release notes
This page describes the latest changes, additions, known issues, and fixes for Docker Engine version 27.
For more information about:
- Deprecated and removed features, see Deprecated Engine Features.
- Changes to the Engine API, see Engine API version history.
27.5
Release notes for Docker Engine version 27.5 releases.
27.5.1
2025-01-22For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- Fix an issue that could persistently prevent daemon startup after failure to initialize the default bridge. moby/moby#49307
- Add a
DOCKER_IGNORE_BR_NETFILTER_ERROR
environment variable. Setting it to1
allows running on hosts that cannot loadbr_netfilter
. Some things won't work, including disabling inter-container communication in a bridge network. With the userland proxy disabled, it won't be possible to access one container's published ports from another container on the same network. moby/moby#49306
Packaging updates
- Update Go runtime to 1.22.11 (fix CVE-2024-45341, CVE-2024-45336). moby/moby#49312, docker/docker-ce-packaging#1147, docker/cli#5762
- Update RootlessKit to v2.3.2 to support
passt
>= 2024_10_30.ee7d0b6. moby/moby#49304 - Update Buildx to v0.20.0. docker/docker-ce-packaging#1149
27.5.0
2025-01-13For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bugfixes and enhancements
- containerd image store: Fix passing a build context via tarball to the
/build
endpoint. moby/moby#49194 - Builder garbage collection policies without a
keepStorage
value now inherit thedefaultKeepStorage
limit as intended. moby/moby#49137 - Preserve network labels during daemon startup. moby/moby#49200
- Fix a potential race condition error when deleting a container. moby/moby#49239
Go SDK
pkg/sysinfo
: deprecateNumCPU
. This utility has the same behavior asruntime.NumCPU
. moby/moby#49247pkg/fileutils
: deprecateGetTotalUsedFds
: this function is only used internally and will be removed in the next release. moby/moby#49209pkg/ioutils
: deprecateBytesPipe
,NewBytesPipe
,ErrClosed
,WriteCounter
,NewWriteCounter
,NewReaderErrWrapper
,NopFlusher
,NopWriter
,NopWriteCloser
. They were only used internally and will be removed in the next release. moby/moby#49246, moby/moby#49255pkg/reexec
: This package is deprecated and moved to a separate module. Usegithub.com/moby/sys/reexec
instead. moby/moby#49135
Packaging updates
- Update containerd to v1.7.25 moby/moby#49253
- Update
runc
to v1.2.4 moby/moby#49243 - Update BuildKit to v0.18.2 moby/moby#48949
- Update Compose to v2.32.2 docker/docker-ce-packaging#1140
27.4
Release notes for Docker Engine version 27.4 releases.
27.4.1
2024-12-18For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- Fix excessive memory allocations when OTel is not configured. moby/moby#49079
- The
docker info
command and the correspondingGET /info
API endpoint no longer include warnings whenbridge-nf-call-iptables
orbridge-nf-call-ip6tables
are disabled at the daemon is started. Thebr_netfilter
kernel module is now attempted to be loaded when needed, which made those warnings inaccurate. moby/moby#49090 - Attempt to load kernel modules, including
ip6_tables
andbr_netfilter
when required, using a method that is likely to succeed inside a Docker-in-Docker container. moby/moby#49043 - Fix a bug that could result in an iptables
DOCKER FILTER
chain not being cleaned up on failure. moby/moby#49110
Packaging updates
- Update Compose to v2.32.1. docker/docker-ce-packaging#1130
- Update Buildx to v0.19.3. docker/docker-ce-packaging#1132
- Update runc (static binaries only) to v1.2.3 moby/moby#49085
27.4.0
2024-12-09For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
API
GET /images/json
with themanifests
option enabled now preserves the original order in which manifests appeared in the manifest-index. moby/moby#48712
Bug fixes and enhancements
- When reading logs with the
jsonfile
orlocal
log drivers, any errors while trying to read or parse underlying log files will cause the rest of the file to be skipped and move to the next log file (if one exists) rather than returning an error to the client and closing the stream. The errors are viewable in the Docker Daemon logs and exported to traces when tracing is configured. moby/moby#48842 - When reading log files, compressed log files are now only decompressed when needed rather than decompressing all files before starting the log stream. moby/moby#48842
- Fix an issue that meant published ports from one container on a bridge network were not accessible from another container on the same network with
userland-proxy
disabled, if the kernel'sbr_netfilter
module was not loaded and enabled. The daemon will now attempt to load the module and enablebridge-nf-call-iptables
orbridge-nf-call-ip6tables
when creating a network with the userland proxy disabled. moby/moby#48685 - Fix loading of
bridge
andbr_netfilter
kernel modules. moby/moby#48966 - containerd image store: Fix Docker daemon failing to fully start with a ""context deadline exceeded error"" with containerd snapshotter and many builds/images. moby/moby#48954
- containerd image store: Fix partially pulled images not being garbage-collected. moby#48910, moby/moby#48957
- containerd image store: Fix
docker image inspect
outputting duplicate references inRepoDigests
. moby/moby#48785 - containerd image store: Fix not being able to connect to some insecure registries in cases where the HTTPS request failed due to a non-TLS related error. moby/moby#48758
- containerd image store: Remove a confusing warning log when tagging a non-dangling image. moby/moby#49010
- containerd image store: Do not underline names in
docker image ls --tree
. docker/cli#5519 - containerd image store: Change name of
USED
column indocker image ls --tree
toIN USE
. docker/cli#5518 dockerd-rootless-setuptool.sh install --force
now ignores RootlessKit errors moby/moby#48695- Disable IPv6 Duplicate Address Detection (DAD) for addresses assigned to the bridges belonging to bridge networks. moby/moby#48684
- Remove BuildKit init timeout. moby/moby#48963
- Ignore ""dataset does not exist"" error when removing dataset on ZFS. moby/moby#48968
- Client: Prevent idle connections leaking FDs. moby/moby#48764
- Fix anonymous volumes being created through the
--mount
option not being marked as anonymous. moby/moby#48755 - After a daemon restart with live-restore, ensure an iptables jump to the
DOCKER-USER
chain is placed before other rules. moby/moby#48714 - Fix a possible memory leak caused by OTel meters. moby/moby#48693
- Create distinct build history db for each image store. moby/moby#48688
- Fix an issue that caused excessive memory usage when DNS resolution was made in a tight loop. moby/moby#48840
- Fix a bug preventing image pulls from being cancelled during
docker run
. docker/cli#5654 - The
docker login
anddocker logout
command no longer update the configuration file if the credentials didn't change. docker/cli#5569 - Optimize
docker stats
to reduce flickering issues. docker/cli#5588, docker/cli#5635 - Fix inaccessible plugins paths preventing plugins from being detected. docker/cli#5652
- Add support for
events --filter
in cobra generated shell completions. docker/cli#5614 - Fix bash completion for
events --filter daemon=
. docker/cli#5563 - Improve shell completion of containers for
docker rm
. docker/cli#5540 - Add shell completion for
--platform
flags. docker/cli#5540 - rootless: Make
/etc/cdi
and/var/run/cdi
accessible by the Container Device Interface (CDI) integration. moby/moby#49027
Removed
- Deprecate
Daemon.Exists()
andDaemon.IsPaused()
. These functions are no longer used and will be removed in the next release. moby/moby#48719 - Deprecate
container.ErrNameReserved
andcontainer.ErrNameNotReserved
. moby/moby#48697 - Deprecate
pkg/platform
- this package is only used internally, and will be removed in the next release. moby/moby#48863 - Deprecate
RepositoryInfo.Class
. This field is no longer used, and will be removed in the next release. moby/moby#49013 - Go SDK: Fix deprecation of
cli/command.ConfigureAuth()
, which was deprecated since v27.2.1. docker/cli#5552 - Go SDK: Deprecate
cli.Errors
type in favour of Go'serrors.Join
docker/cli#5548
Packaging updates
- Update Go runtime to 1.22.10. moby/moby#49026, docker/cli#5669, docker/docker-ce-packaging#1120.
- Update Compose to v2.31.0. docker/docker-ce-packaging#1100
- Update BuildKit to v0.17.3. moby/moby#49024
- Update Buildx to v0.19.1. docker/docker-ce-packaging#1115
- Update containerd to v1.7.24. moby/moby#48934
- Update containerd (static binaries only) to v1.7.24. moby/moby#48919
- Update runc to v1.2.2. moby/moby#48919
27.3
Release notes for Docker Engine version 27.3 releases.
27.3.1
2024-09-20For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- CLI: Fix issue with command execution metrics not being exported correctly. docker/cli#5457
Packaging updates
- Update Compose to v2.29.7
27.3.0
2024-09-19For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- containerd image store: Fix
docker image prune -a
untagging images used by containers started from images referenced by a digested reference. moby/moby#48488 - Add a
--feature
flag to the daemon options. moby/moby#48487 - Updated the handling of the
--gpus=0
flag to be consistent with the NVIDIA Container Runtime. moby/moby#48483 - Support WSL2 mirrored-mode networking's use of interface
loopback0
for packets from the Windows host. moby/moby#48514 - Fix an issue that prevented communication between containers on an IPv4 bridge network when running with
--iptables=false
,--ip6tables=true
(the default), a firewall with a DROP rule for forwarded packets on hosts where thebr_netfilter
kernel module was not normally loaded. moby/moby#48511 - CLI: Fix issue where
docker volume update
command would cause the CLI to panic if no argument/volume was passed. docker/cli#5426 - CLI: Properly report metrics when run in WSL environment on Windows. docker/cli#5432
Packaging updates
- Update containerd (static binaries only) to v1.7.22 moby/moby#48468
- Updated Buildkit to v0.16.0
- Update Compose to v2.29.6
- Update Buildx to v0.17.1
27.2
Release notes for Docker Engine version 27.2 releases.
27.2.1
2024-09-09Bug fixes and enhancements
- containerd image store: Fix non-container images being hidden in the
docker image ls
output. moby/moby#48402 - containerd image store: Improve
docker pull
error message when the image platform doesn't match. moby/moby#48415 - CLI: Fix issue causing
docker login
to not remove repository names from passed in registry addresses, resulting in credentials being stored under the wrong key. docker/cli#5385 - CLI: Fix issue that will sometimes cause the browser-login flow to fail if the CLI process is suspended and then resumed while waiting for the user to authenticate. docker/cli#5376
- CLI:
docker login
now returns an error instead of hanging if called non-interactively with--password
or--password-stdin
but without--user
. docker/cli#5402
Packaging updates
- Update runc to v1.1.14, which contains a fix for CVE-2024-45310. moby/moby#48426
- Update Go runtime to 1.22.7. moby/moby#48433, docker/cli#5411, docker/docker-ce-packaging#1068
27.2.0
2024-08-27For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 27.2.0 milestone
- moby/moby, 27.2.0 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
New
The new features in this release are:
Device code login
This release adds support for using device code login when authenticating to Docker Hub.
You can still use the old method of logging in with a username and password or access token, but device code login is more secure and doesn't require you to enter your password in the CLI.
To use the old method, use docker login -u <username>
.
Multi-platform support for docker image ls
Experimental
This is experimental and may change at any time without any backward compatibility.With the containerd image store enabled, the docker image ls
command (or
docker images
shorthand) now supports a --tree
flag that now shows
if an image is a multi-platform image.
API
GET /images/json
response now includesManifests
field, which contains information about the sub-manifests included in the image index. This includes things like platform-specific manifests and build attestations.The new field will only be populated if the request also sets the
manifests
query parameter totrue
.Experimental
This is experimental and may change at any time without any backward compatibility.
Bug fixes and enhancements
- CLI: Fix issue with remote contexts over SSH where the CLI would allocate a pseudo-TTY when connecting to the remote host, which causes issues in rare situations. docker/cli#5351
- Fix an issue that prevented network creation with a
--ip-range
ending on a 64-bit boundary. moby/moby#48326 - CLI: IPv6 addresses shown by
docker ps
in port bindings are now bracketed. docker/cli#5365 - containerd image store: Fix early error exit from
docker load
in cases where unpacking the image would fail. moby/moby#48376 - containerd image store: Fix the previous image not being persisted as dangling after
docker pull
. moby/moby#48380
Packaging updates
- Update BuildKit to v0.15.2. moby/moby#48341
- Update Compose to v2.29.2. docker/docker-ce-packaging#1050
- Update containerd to v1.7.21. moby/moby#48383, docker/containerd-packaging#389
Known Issues
- There is a known issue when authenticating against a registry in the Docker CLI (
docker login [registry address]
) where, if the provided registry address includes a repository/image name (such asdocker login index.docker.io/docker/welcome-to-docker
), the repository part (docker/welcome-to-docker
) is not normalized and results in credentials being stored incorrectly, which causes subsequent pulls from the registry (docker pull index.docker.io/docker/welcome-to-docker
) to not be authenticated. To prevent this, don't include any extraneous suffix in the registry address when runningdocker login
.Note
Using
docker login
with an address that includes URL path segments is not a documented use case and is considered unsupported. The recommended usage is to specify only a registry hostname, and optionally a port, as the address fordocker login
.
27.1
Release notes for Docker Engine version 27.1 releases.
27.1.2
2024-08-13For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 27.1.2 milestone
- moby/moby, 27.1.2 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
Bug fixes and enhancements
- Fix a regression that could result in a
ResourceExhausted desc = grpc: received message larger than max
error when building from a large Dockerfile. moby/moby#48245 - CLI: Fix
docker attach
printing a spuriouscontext cancelled
error message. docker/cli#5296 - CLI: Fix
docker attach
exiting onSIGINT
instead of forwarding the signal to the container and waiting for it to exit. docker/cli#5302 - CLI: Fix
--device-read-bps
and--device-write-bps
options not taking effect. docker/cli#5339 - CLI: Fix a panic happening in some cases while running a plugin. docker/cli#5337
Packaging updates
- Update BuildKit to v0.15.1. moby/moby#48246
- Update Buildx to v0.16.2. docker/docker-ce-packaging#1043
- Update Go runtime to 1.21.13. moby/moby#48301, docker/cli#5325, docker/docker-ce-packaging#1046
- Remove unused
docker-proxy.exe
binary from Windows packages. docker/docker-ce-packaging#1045
27.1.1
2024-07-23Security
This release contains a fix for CVE-2024-41110 / GHSA-v23v-6jw2-98fq that impacted setups using authorization plugins (AuthZ) for access control. No other changes are included in this release, and this release is otherwise identical for users not using AuthZ plugins.
Packaging updates
- Update Compose to v2.29.1. moby/docker-ce-packaging#1041
27.1.0
2024-07-22For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 27.1.0 milestone
- moby/moby, 27.1.0 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
Bug fixes and enhancements
- rootless: add
Requires=dbus.socket
to prevent errors when starting the daemon on a cgroup v2 host with systemd moby/moby#48141 - containerd integration:
image tag
event is now properly emitted when building images with BuildKit moby/moby#48182 - CLI: enable shell completion for
docker image rm
,docker image history
, anddocker image inspect
moby/moby#5261 - CLI: add and improve shell completions for various flags moby/moby#5261
- CLI: add OOMScoreAdj to
docker service create
anddocker stack
docker/cli#5274 - CLI: add support for
DOCKER_CUSTOM_HEADERS
environment variable (experimental) docker/cli#5271 - CLI: containerd-integration: Fix
docker push
defaulting the--platform
flag to a value ofDOCKER_DEFAULT_PLATFORM
environment variable on unsupported API versions docker/cli#5248 - CLI: fix: context cancellation on
login
prompt docker/cli#5260 - CLI: fix: wait for the container to exit before closing the stream when sending a termination request to the CLI while attached to a container docker/cli#5250
Deprecated
- The
pkg/rootless/specconv
package is deprecated, and will be removed in the next release moby/moby#48185 - The
pkg/containerfs
package is deprecated, and will be removed in the next release moby/moby#48185 - The
pkg/directory
package is deprecated, and will be removed in the next release moby/moby#48185 api/types/system
: remove deprecatedInfo.ExecutionDriver
moby/moby#48184
Packaging updates
- Update Buildx to v0.16.1. moby/docker-ce-packaging#1039
- Update Compose to v2.29.0. moby/docker-ce-packaging#1038
- Update Containerd (static binaries only) to v1.7.20. moby/moby#48191
- Update BuildKit to v0.15.0. moby/moby#48175
- Update Go runtime to 1.21.12, which contains security fixes for CVE-2024-24791 moby/moby#48120
27.0
Release notes for Docker Engine 27.0.
27.0.3
2024-07-01For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 27.0.3 milestone
- moby/moby, 27.0.3 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
Bug fixes and enhancements
- Fix a regression that incorrectly reported a port mapping from a host IPv6 address to an IPv4-only container as an error. moby/moby#48090
- Fix a regression that caused duplicate subnet allocations when creating networks. moby/moby#48089
- Fix a regression resulting in
fail to register layer: failed to Lchown
errors when trying to pull an image with rootless enabled on a system that supports native overlay with user-namespaces. moby/moby#48086
27.0.2
2024-06-27For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 27.0.2 milestone
- moby/moby, 27.0.2 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
Bug fixes and enhancements
- Fix a regression that caused port numbers to be ignored when parsing a Docker registry URL. docker/cli#5197, docker/cli#5198
Removed
- api/types: deprecate
ContainerJSONBase.Node
field andContainerNode
type. These definitions were used by the standalone (""classic"") Swarm API, but never implemented in the Docker Engine itself. moby/moby#48055
27.0.1
2024-06-24For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
- docker/cli, 27.0.0 milestone
- moby/moby, 27.0.0 milestone
- Deprecated and removed features, see Deprecated Features.
- Changes to the Engine API, see API version history.
New
- containerd image store: Add
--platform
flag todocker image push
and improve the default behavior when not all platforms of the multi-platform image are available locally. docker/cli#4984, moby/moby#47679 - Add support to
docker stack deploy
fordriver_opts
in a service's networks. docker/cli#5125 - Consider additional
/usr/local/libexec
and/usr/libexec
paths when looking up the userland proxy binaries by a name with adocker-
prefix. moby/moby#47804
Bug fixes and enhancements
*client.Client
instances are now always safe for concurrent use by multiple goroutines. Previously, this could lead to data races when theWithAPIVersionNegotiation()
option is used. moby/moby#47961- Fix a bug causing the Docker CLI to leak Unix sockets in
$TMPDIR
in some cases. docker/cli#5146 - Don't ignore a custom seccomp profile when used in conjunction with
--privileged
. moby/moby#47500 - rootless: overlay2: support native overlay diff when using rootless-mode with Linux kernel version 5.11 and later. moby/moby#47605
- Fix the
StartInterval
default value of healthcheck to reflect the documented value of 5s. moby/moby#47799 - Fix
docker save
anddocker load
not ending on the daemon side when the operation was cancelled by the user, for example with Ctrl+C. moby/moby#47629 - The
StartedAt
property of containers is now recorded before container startup, guaranteeing that theStartedAt
is always beforeFinishedAt
. moby/moby#47003 - The internal DNS resolver used by Windows containers on Windows now forwards requests to external DNS servers by default. This enables
nslookup
to resolve external hostnames. This behaviour can be disabled viadaemon.json
, using""features"": { ""windows-dns-proxy"": false }
. The configuration option will be removed in a future release. moby/moby#47826 - Print a warning when the CLI does not have permissions to read the configuration file. docker/cli#5077
- Fix a goroutine and file-descriptor leak on container attach. moby/moby#45052
- Clear the networking state of all stopped or dead containers during daemon start-up. moby/moby#47984
- Write volume options JSON atomically to avoid ""invalid JSON"" errors after system crash. moby/moby#48034
- Allow multiple macvlan networks with the same parent. moby/moby#47318
- Allow BuildKit to be used on Windows daemons that advertise it. docker/cli#5178
Networking
- Allow sysctls to be set per-interface during container creation and network connection.
moby/moby#47686
- In a future release, this will be the only way to set per-interface sysctl options.
For example, on the command line in a
docker run
command,--network mynet --sysctl net.ipv4.conf.eth0.log_martians=1
will be rejected. Instead, you must use--network name=mynet,driver-opt=com.docker.network.endpoint.sysctls=net.ipv4.conf.IFNAME.log_martians=1
.
- In a future release, this will be the only way to set per-interface sysctl options.
For example, on the command line in a
IPv6
ip6tables
is no longer experimental. You may remove theexperimental
configuration option and continue to use IPv6, if it is not required by any other features.ip6tables
is now enabled for Linux bridge networks by default. moby/moby#47747- This makes IPv4 and IPv6 behaviors consistent with each other, and reduces the risk that IPv6-enabled containers are inadvertently exposed to the network.
- There is no impact if you are running Docker Engine with
ip6tables
enabled (new default). - If you are using an IPv6-enabled bridge network without
ip6tables
, this is likely a breaking change. Only published container ports (-p
or--publish
) are accessible from outside the Docker bridge network, and outgoing connections masquerade as the host. - To restore the behavior of earlier releases, no
ip6tables
at all, set""ip6tables"": false
indaemon.json
, or use the CLI option--ip6tables=false
. Alternatively, leaveip6tables
enabled, publish ports, and enable direct routing. - With
ip6tables
enabled, ifip6tables
is not functional on your host, Docker Engine will start but it will not be possible to create an IPv6-enabled network.
IPv6 network configuration improvements
- A Unique Local Address (ULA) base prefix is automatically added to
default-address-pools
if this parameter wasn't manually configured, or if it contains no IPv6 prefixes. moby/moby#47853- Prior to this release, to create an IPv6-enabled network it was necessary to use the
--subnet
option to specify an IPv6 subnet, or add IPv6 ranges todefault-address-pools
indaemon.json
. - Starting in this release, when a bridge network is created with
--ipv6
and no IPv6 subnet is defined by those options, an IPv6 Unique Local Address (ULA) base prefix is used. - The ULA prefix is derived from the Engine host ID such that it's unique across hosts and over time.
- Prior to this release, to create an IPv6-enabled network it was necessary to use the
- IPv6 address pools of any size can now be added to
default-address-pools
. moby/moby#47768 - IPv6 can now be enabled by default on all custom bridge networks using
""default-network-opts"": { ""bridge"": {""com.docker.network.enable_ipv6"": ""true""}}
indaemon.json
, ordockerd --default-network-opt=bridge=com.docker.network.enable_ipv6=true
on the command line. moby/moby#47867 - Direct routing for IPv6 networks, with
ip6tables
enabled. moby/moby#47871- Added bridge driver option
com.docker.network.bridge.gateway_mode_ipv6=<nat|routed>
. - The default behavior,
nat
, is unchanged from previous releases running withip6tables
enabled. NAT and masquerading rules are set up for each published container port. - When set to
routed
, no NAT or masquerading rules are configured for published ports. This enables direct IPv6 access to the container, if the host's network can route packets for the container's address to the host. Published ports will be opened in the container's firewall. - When a port mapping only applies to
routed
mode, only addresses0.0.0.0
or::
are allowed and a host port must not be given. - Note that published container ports, in
nat
orrouted
mode, are accessible from any remote address if routing is set up in the network, unless the Docker host's firewall has additional restrictions. For example:docker network create --ipv6 -o com.docker.network.bridge.gateway_mode_ipv6=routed mynet
. - The option
com.docker.network.bridge.gateway_mode_ipv4=<nat|routed>
is also available, with the same behavior but for IPv4.
- Added bridge driver option
- If firewalld is running on the host, Docker creates policy
docker-forwarding
to allow forwarding from any zone to thedocker
zone. This makes it possible to configure a bridge network with a routable IPv6 address, and no NAT or masquerading. moby/moby#47745 - When a port is published with no host port specified, or a host port range is given, the same port will be allocated for IPv4 and IPv6.
moby/moby#47871
- For example
-p 80
will result in the same ephemeral port being allocated for0.0.0.0
and::
, and-p 8080-8083:80
will pick the same port from the range for both address families. - Similarly, ports published to specific addresses will be allocated the same port. For example,
-p 127.0.0.1::80 -p '[::1]::80'
. - If no port is available on all required addresses, container creation will fail.
- For example
- Environment variable
DOCKER_ALLOW_IPV6_ON_IPV4_INTERFACE
, introduced in release 26.1.1, no longer has any effect. moby/moby#47963- If IPv6 could not be disabled on an interface because of a read-only
/proc/sys/net
, the environment variable allowed the container to start anyway. - In this release, if IPv4 cannot be disabled for an interface, IPv6 can be explicitly enabled for the network simply by using
--ipv6
when creating it. Other workarounds are to configure the OS to disable IPv6 by default on new interfaces, mount/proc/sys/net
read-write, or use a kernel with no IPv6 support.
- If IPv6 could not be disabled on an interface because of a read-only
- For IPv6-enabled bridge networks, do not attempt to replace the bridge's kernel-assigned link local address with
fe80::1
. moby/moby#47787
Removed
- Deprecate experimental GraphDriver plugins. moby/moby#48050, docker/cli#5172
- pkg/archive: deprecate
NewTempArchive
andTempArchive
. These types were only used in tests and will be removed in the next release. moby/moby#48002 - pkg/archive: deprecate
CanonicalTarNameForPath
moby/moby#48001 - Deprecate pkg/dmesg. This package was no longer used, and will be removed in the next release. moby/moby#47999
- Deprecate
pkg/stringid.ValidateID
andpkg/stringid.IsShortID
moby/moby#47995 - runconfig: deprecate
SetDefaultNetModeIfBlank
and moveContainerConfigWrapper
toapi/types/container
moby/moby#48007 - runconfig: deprecate
DefaultDaemonNetworkMode
and move todaemon/network
moby/moby#48008 - runconfig: deprecate
opts.ConvertKVStringsToMap
. This utility is no longer used, and will be removed in the next release. moby/moby#48016 - runconfig: deprecate
IsPreDefinedNetwork
. moby/moby#48011
API
- containerd image store:
POST /images/{name}/push
now supports aplatform
parameter (JSON encoded OCI Platform type) that allows selecting a specific platform-manifest from the multi-platform image. This is experimental and may change in future API versions. moby/moby#47679 POST /services/create
andPOST /services/{id}/update
now supportOomScoreAdj
. moby/moby#47950ContainerList
api returns container annotations. moby/moby#47866POST /containers/create
andPOST /services/create
now takeOptions
as part ofHostConfig.Mounts.TmpfsOptions
allowing to set options for tmpfs mounts. moby/moby#46809- The
Healthcheck.StartInterval
property is now correctly ignored when updating a Swarm service using API versions less than v1.44. moby/moby#47991 GET /events
now supports imagecreate
event that is emitted when a new image is built regardless if it was tagged or not. moby/moby#47929GET /info
now includes aContainerd
field containing information about the location of the containerd API socket and containerd namespaces used by the daemon to run containers and plugins. moby/moby#47239- Deprecate non-standard (config) fields in image inspect output. The
Config
field returned by this endpoint (used fordocker image inspect
) returned additional fields that are not part of the image's configuration and not part of the Docker Image Spec and the OCI Image Spec. These fields are never set (and always return the default value for the type), but are not omitted in the response when left empty. As these fields were not intended to be part of the image configuration response, they are deprecated, and will be removed in the future API versions. - Deprecate the daemon flag
--api-cors-header
and the correspondingdaemon.json
configuration option. These will be removed in the next major release. moby/moby#45313
The following deprecated fields are currently included in the API response, but are not part of the underlying image's Config
:
moby/moby#47941
Hostname
Domainname
AttachStdin
AttachStdout
AttachStderr
Tty
OpenStdin
StdinOnce
Image
NetworkDisabled
(already omitted unless set)MacAddress
(already omitted unless set)StopTimeout
(already omitted unless set)
Go SDK changes
- Client API callback for the following functions now require a context parameter.
moby/moby#47536
client.RequestPrivilegeFunc
client.ImageSearchOptions.AcceptPermissionsFunc
image.ImportOptions.PrivilegeFunc
- Remove deprecated aliases for Image types.
moby/moby#47900
ImageImportOptions
ImageCreateOptions
ImagePullOptions
ImagePushOptions
ImageListOptions
ImageRemoveOptions
- Introduce
Ulimit
type alias forgithub.com/docker/go-units.Ulimit
. TheUlimit
type as used in the API is defined in a Go module that will transition to a new location in future. A type alias is added to reduce the friction that comes with moving the type to a new location. The alias makes sure that existing code continues to work, but its definition may change in future. Users are recommended to use this alias instead of theunits.Ulimit
directly. moby/moby#48023
Move and rename types, changing their import paths and exported names. moby/moby#47936, moby/moby#47873, moby/moby#47887, moby/moby#47882, moby/moby#47921, moby/moby#48040
- Move the following types to
api/types/container
:BlkioStatEntry
BlkioStats
CPUStats
CPUUsage
ContainerExecInspect
ContainerPathStat
ContainerStats
ContainersPruneReport
CopyToContainerOptions
ExecConfig
ExecStartCheck
MemoryStats
NetworkStats
PidsStats
StatsJSON
Stats
StorageStats
ThrottlingData
- Move the following types to
api/types/image
:ImagesPruneReport
ImageImportSource
ImageLoadResponse
- Move the
ExecStartOptions
type toapi/types/backend
. - Move the
VolumesPruneReport
type toapi/types/volume
. - Move the
EventsOptions
type toapi/types/events
. - Move the
ImageSearchOptions
type toapi/types/registry
. - Drop
Network
prefix and move the following types toapi/types/network
:NetworkCreateResponse
NetworkConnect
NetworkDisconnect
NetworkInspectOptions
EndpointResource
NetworkListOptions
NetworkCreateOptions
NetworkCreateRequest
NetworksPruneReport
- Move
NetworkResource
toapi/types/network
.
Packaging updates
- Update Buildx to v0.15.1. docker/docker-ce-packaging#1029
- Update BuildKit to v0.14.1. moby/moby#48028
- Update runc to v1.1.13 moby/moby#47976
- Update Compose to v2.28.1. moby/docker-ce-packaging#1032
27.0.0
There's no 27.0.0 release due to a mistake during the pre-release of 27.0.0-rc.1 on GitHub which resulted in the v27.0.0 tag being created. Unfortunately the tag was already picked up by the Go Module Mirror so it's not possible to cleanly change the v27.0.0. To workaround this, the 27.0.1 will be the first release of the 27.0.",,,
a243a8389988b4d9613cacbf84fe6d93fccc75c5c50bcb778539cfeb87105976,"How services work
To deploy an application image when Docker Engine is in Swarm mode, you create a service. Frequently a service is the image for a microservice within the context of some larger application. Examples of services might include an HTTP server, a database, or any other type of executable program that you wish to run in a distributed environment.
When you create a service, you specify which container image to use and which commands to execute inside running containers. You also define options for the service including:
- The port where the swarm makes the service available outside the swarm
- An overlay network for the service to connect to other services in the swarm
- CPU and memory limits and reservations
- A rolling update policy
- The number of replicas of the image to run in the swarm
Services, tasks, and containers
When you deploy the service to the swarm, the swarm manager accepts your service definition as the desired state for the service. Then it schedules the service on nodes in the swarm as one or more replica tasks. The tasks run independently of each other on nodes in the swarm.
For example, imagine you want to load balance between three instances of an HTTP listener. The diagram below shows an HTTP listener service with three replicas. Each of the three instances of the listener is a task in the swarm.
A container is an isolated process. In the Swarm mode model, each task invokes exactly one container. A task is analogous to a “slot” where the scheduler places a container. Once the container is live, the scheduler recognizes that the task is in a running state. If the container fails health checks or terminates, the task terminates.
Tasks and scheduling
A task is the atomic unit of scheduling within a swarm. When you declare a desired service state by creating or updating a service, the orchestrator realizes the desired state by scheduling tasks. For instance, you define a service that instructs the orchestrator to keep three instances of an HTTP listener running at all times. The orchestrator responds by creating three tasks. Each task is a slot that the scheduler fills by spawning a container. The container is the instantiation of the task. If an HTTP listener task subsequently fails its health check or crashes, the orchestrator creates a new replica task that spawns a new container.
A task is a one-directional mechanism. It progresses monotonically through a series of states: assigned, prepared, running, etc. If the task fails, the orchestrator removes the task and its container and then creates a new task to replace it according to the desired state specified by the service.
The underlying logic of Docker's Swarm mode is a general purpose scheduler and orchestrator. The service and task abstractions themselves are unaware of the containers they implement. Hypothetically, you could implement other types of tasks such as virtual machine tasks or non-containerized process tasks. The scheduler and orchestrator are agnostic about the type of the task. However, the current version of Docker only supports container tasks.
The diagram below shows how Swarm mode accepts service create requests and schedules tasks to worker nodes.
Pending services
A service may be configured in such a way that no node currently in the
swarm can run its tasks. In this case, the service remains in state pending
.
Here are a few examples of when a service might remain in state pending
.
Tip
If your only intention is to prevent a service from being deployed, scale the service to 0 instead of trying to configure it in such a way that it remains in
pending
.
If all nodes are paused or drained, and you create a service, it is pending until a node becomes available. In reality, the first node to become available gets all of the tasks, so this is not a good thing to do in a production environment.
You can reserve a specific amount of memory for a service. If no node in the swarm has the required amount of memory, the service remains in a pending state until a node is available which can run its tasks. If you specify a very large value, such as 500 GB, the task stays pending forever, unless you really have a node which can satisfy it.
You can impose placement constraints on the service, and the constraints may not be able to be honored at a given time.
This behavior illustrates that the requirements and configuration of your tasks are not tightly tied to the current state of the swarm. As the administrator of a swarm, you declare the desired state of your swarm, and the manager works with the nodes in the swarm to create that state. You do not need to micro-manage the tasks on the swarm.
Replicated and global services
There are two types of service deployments, replicated and global.
For a replicated service, you specify the number of identical tasks you want to run. For example, you decide to deploy an HTTP service with three replicas, each serving the same content.
A global service is a service that runs one task on every node. There is no pre-specified number of tasks. Each time you add a node to the swarm, the orchestrator creates a task and the scheduler assigns the task to the new node. Good candidates for global services are monitoring agents, anti-virus scanners or other types of containers that you want to run on every node in the swarm.
The diagram below shows a three-service replica in gray and a global service in black.",,,
8bfa9a475a3bf76f3b33a5145c362538b4020d5735850e3a0e7613792fc33c93,"Set up your subscription
Docker subscriptions offer features and benefits to support both new and professional developers, as well as plans for individuals, teams, and enterprise businesses. To learn more about what's included with each tier, see Docker subscriptions and features and Docker Pricing.
In this section, learn how to get started with a Docker subscription for individuals or for organizations. Before you begin, make sure you have a Docker ID.
Important
Starting July 1, 2024, Docker will begin collecting sales tax on subscription fees in compliance with state regulations for customers in the United States. For our global customers subject to VAT, the implementation will start rolling out on July 1, 2024. Note that while the roll out begins on this date, VAT charges may not apply to all applicable subscriptions immediately.
To ensure that tax assessments are correct, make sure that your billing information and VAT/Tax ID, if applicable, are updated. If you're exempt from sales tax, see Register a tax certificate.
Set up a Docker subscription for a personal account
After you create your Docker ID, you have a Docker Personal subscription. To continue using this plan, no further action is necessary. For additional features, you can upgrade to a Docker Pro plan.
To upgrade from Docker Personal to Docker Pro, see Upgrade your subscription.
Set up a Docker subscription for an organization
You can subscribe a new or existing organization to a Docker plan. Only organization owners can manage billing for the organization.
After you create your Docker ID, you have a Docker Personal plan. You must then create an organization and choose a subscription for it. For more details, see Create your organization.
To learn how to upgrade a Docker subscription for an existing organization, see Upgrade your subscription.",,,
ff85e626031797c5319d6d720d2e458d1d11f279721abad46e9030721e261493,"Use docker logs with remote logging drivers
Overview
You can use the docker logs
command to read container logs regardless of the
configured logging driver or plugin. Docker Engine uses the
local
logging driver to act as cache for reading the latest logs of your containers.
This is called dual logging. By default, the cache has log-file rotation
enabled, and is limited to a maximum of 5 files of 20 MB each (before
compression) per container.
Refer to the configuration options section to customize these defaults, or to the disable dual logging section to disable this feature.
Prerequisites
Docker Engine automatically enables dual logging if the configured logging driver doesn't support reading logs.
The following examples show the result of running a docker logs
command with
and without dual logging availability:
Without dual logging capability
When a container is configured with a remote logging driver such as splunk
, and
dual logging is disabled, an error is displayed when attempting to read container
logs locally:
Step 1: Configure Docker daemon
$ cat /etc/docker/daemon.json { ""log-driver"": ""splunk"", ""log-opts"": { ""cache-disabled"": ""true"", ... (options for ""splunk"" logging driver) } }
Step 2: Start the container
$ docker run -d busybox --name testlog top
Step 3: Read the container logs
$ docker logs 7d6ac83a89a0 Error response from daemon: configured logging driver does not support reading
With dual logging capability
With the dual logging cache enabled, the docker logs
command can be used to
read logs, even if the logging driver doesn't support reading logs. The following
example shows a daemon configuration that uses the splunk
remote logging driver
as a default, with dual logging caching enabled:
Step 1: Configure Docker daemon
$ cat /etc/docker/daemon.json { ""log-driver"": ""splunk"", ""log-opts"": { ... (options for ""splunk"" logging driver) } }
Step 2: Start the container
$ docker run -d busybox --name testlog top
Step 3: Read the container logs
$ docker logs 7d6ac83a89a0 2019-02-04T19:48:15.423Z [INFO] core: marked as sealed 2019-02-04T19:48:15.423Z [INFO] core: pre-seal teardown starting 2019-02-04T19:48:15.423Z [INFO] core: stopping cluster listeners 2019-02-04T19:48:15.423Z [INFO] core: shutting down forwarding rpc listeners 2019-02-04T19:48:15.423Z [INFO] core: forwarding rpc listeners stopped 2019-02-04T19:48:15.599Z [INFO] core: rpc listeners successfully shut down 2019-02-04T19:48:15.599Z [INFO] core: cluster listeners successfully shut down
Note
For logging drivers that support reading logs, such as the
local
,json-file
andjournald
drivers, there is no difference in functionality before or after the dual logging capability became available. For these drivers, Logs can be read usingdocker logs
in both scenarios.
Configuration options
The dual logging cache accepts the same configuration options as the
local
logging driver, but with a cache-
prefix. These options
can be specified per container, and defaults for new containers can be set using
the
daemon configuration file.
By default, the cache has log-file rotation enabled, and is limited to a maximum of 5 files of 20MB each (before compression) per container. Use the configuration options described below to customize these defaults.
| Option | Default | Description |
|---|---|---|
cache-disabled | ""false"" | Disable local caching. Boolean value passed as a string (true , 1 , 0 , or false ). |
cache-max-size | ""20m"" | The maximum size of the cache before it is rotated. A positive integer plus a modifier representing the unit of measure (k , m , or g ). |
cache-max-file | ""5"" | The maximum number of cache files that can be present. If rotating the logs creates excess files, the oldest file is removed. A positive integer. |
cache-compress | ""true"" | Enable or disable compression of rotated log files. Boolean value passed as a string (true , 1 , 0 , or false ). |
Disable the dual logging cache
Use the cache-disabled
option to disable the dual logging cache. Disabling the
cache can be useful to save storage space in situations where logs are only read
through a remote logging system, and if there is no need to read logs through
docker logs
for debugging purposes.
Caching can be disabled for individual containers or by default for new containers, when using the daemon configuration file.
The following example uses the daemon configuration file to use the
splunk
logging driver as a default, with caching disabled:
$ cat /etc/docker/daemon.json
{
""log-driver"": ""splunk"",
""log-opts"": {
""cache-disabled"": ""true"",
... (options for ""splunk"" logging driver)
}
}
Note
For logging drivers that support reading logs, such as the
local
,json-file
andjournald
drivers, dual logging isn't used, and disabling the option has no effect.
Limitations
- If a container using a logging driver or plugin that sends logs remotely
has a network issue, no
write
to the local cache occurs. - If a write to
logdriver
fails for any reason (file system full, write permissions removed), the cache write fails and is logged in the daemon log. The log entry to the cache isn't retried. - Some logs might be lost from the cache in the default configuration because a ring buffer is used to prevent blocking the stdio of the container in case of slow file writes. An admin must repair these while the daemon is shut down.",,,
9466ddd34a8bb1dbf808ab0528a3637ab0e336cc46abec18b324e6a1dc8438b8,"Image and registry exporters
Table of contents
The image
exporter outputs the build result into a container image format. The
registry
exporter is identical, but it automatically pushes the result by
setting push=true
.
Synopsis
Build a container image using the image
and registry
exporters:
$ docker buildx build --output type=image[,parameters] .
$ docker buildx build --output type=registry[,parameters] .
The following table describes the available parameters that you can pass to
--output
for type=image
:
| Parameter | Type | Default | Description |
|---|---|---|---|
name | String | Specify image name(s) | |
push | true ,false | false | Push after creating the image. |
push-by-digest | true ,false | false | Push image without name. |
registry.insecure | true ,false | false | Allow pushing to insecure registry. |
dangling-name-prefix | <value> | Name image with prefix@<digest> , used for anonymous images | |
name-canonical | true ,false | Add additional canonical name name@<digest> | |
compression | uncompressed ,gzip ,estargz ,zstd | gzip | Compression type, see compression |
compression-level | 0..22 | Compression level, see compression | |
force-compression | true ,false | false | Forcefully apply compression, see compression |
rewrite-timestamp | true ,false | false | Rewrite the file timestamps to the SOURCE_DATE_EPOCH value. See
build reproducibility for how to specify the SOURCE_DATE_EPOCH value. |
oci-mediatypes | true ,false | false | Use OCI media types in exporter manifests, see OCI Media types |
oci-artifact | true ,false | false | Attestations are formatted as OCI artifacts, see OCI Media types |
unpack | true ,false | false | Unpack image after creation (for use with containerd) |
store | true ,false | true | Store the result images to the worker's (for example, containerd) image store, and ensures that the image has all blobs in the content store. Ignored if the worker doesn't have image store (when using OCI workers, for example). |
annotation.<key> | String | Attach an annotation with the respective key and value to the built image,see
annotations |
Annotations
These exporters support adding OCI annotation using annotation
parameter,
followed by the annotation name using dot notation. The following example sets
the org.opencontainers.image.title
annotation:
$ docker buildx build \
--output ""type=<type>,name=<registry>/<image>,annotation.org.opencontainers.image.title=<title>"" .
For more information about annotations, see BuildKit documentation.
Further reading
For more information on the image
or registry
exporters, see the
BuildKit README.",,,
b684544db2ee1713858f0b8a27f1fb675e0a94626c86f55bcd3f8e861a16e65a,"Uninstall Docker Desktop
Warning
Uninstalling Docker Desktop destroys Docker containers, images, volumes, and other Docker-related data local to the machine, and removes the files generated by the application. To learn how to preserve important data before uninstalling, refer to the back up and restore data section.
To uninstall Docker Desktop from your Windows machine:
- From the Windows Start menu, select Settings > Apps > Apps & features.
- Select Docker Desktop from the Apps & features list and then select Uninstall.
- Select Uninstall to confirm your selection.
You can also uninstall Docker Desktop from the CLI:
- Locate the installer:
$ C:\Program Files\Docker\Docker\Docker Desktop Installer.exe
- Uninstall Docker Desktop.
- In PowerShell, run:
$ Start-Process 'Docker Desktop Installer.exe' -Wait uninstall
- In the Command Prompt, run:
$ start /w """" ""Docker Desktop Installer.exe"" uninstall
After uninstalling Docker Desktop, there may be some residual files left behind which you can remove manually. These are:
C:\ProgramData\Docker
C:\ProgramData\DockerDesktop
C:\Program Files\Docker
C:\Users\<your user name>\AppData\Local\Docker
C:\Users\<your user name>\AppData\Roaming\Docker
C:\Users\<your user name>\AppData\Roaming\Docker Desktop
C:\Users\<your user name>\.docker
To uninstall Docker Desktop from your Mac:
- From the Docker menu, select the Troubleshoot icon in the top-right corner of the Docker Desktop Dashboard and then select Uninstall.
- Select Uninstall to confirm your selection.
You can also uninstall Docker Desktop from the CLI. Run:
$ /Applications/Docker.app/Contents/MacOS/uninstall
You may encounter the following error when uninstalling Docker Desktop using the uninstall command.
$ /Applications/Docker.app/Contents/MacOS/uninstall
Password:
Uninstalling Docker Desktop...
Error: unlinkat /Users/<USER_HOME>/Library/Containers/com.docker.docker/.com.apple.containermanagerd.metadata.plist: operation not permitted
The operation not permitted error is reported either on the file .com.apple.containermanagerd.metadata.plist
or on the parent directory /Users/<USER_HOME>/Library/Containers/com.docker.docker/
. This error can be ignored as you have successfully uninstalled Docker Desktop.
You can remove the directory /Users/<USER_HOME>/Library/Containers/com.docker.docker/
later by allowing Full Disk Access to the terminal application you are using (System Settings > Privacy & Security > Full Disk Access).
After uninstalling Docker Desktop, there may be some residual files left behind which you can remove:
$ rm -rf ~/Library/Group\ Containers/group.com.docker
$ rm -rf ~/.docker
With Docker Desktop version 4.36 and earlier, the following files can also be left on the file system. You can remove these with administrative privileges:
/Library/PrivilegedHelperTools/com.docker.vmnetd
/Library/PrivilegedHelperTools/com.docker.socket
You can also move the Docker application to the trash.
Docker Desktop is removed from a Linux host using the package manager.
Once Docker Desktop is removed, users must delete the credsStore
and currentContext
properties from the ~/.docker/config.json
.
To remove Docker Desktop for Ubuntu, run:
$ sudo apt remove docker-desktop
For a complete cleanup, remove configuration and data files at $HOME/.docker/desktop
, the symlink at /usr/local/bin/com.docker.cli
, and purge
the remaining systemd service files.
$ rm -r $HOME/.docker/desktop
$ sudo rm /usr/local/bin/com.docker.cli
$ sudo apt purge docker-desktop
Remove the credsStore
and currentContext
properties from $HOME/.docker/config.json
. Additionally, you must delete any edited configuration files manually.
To remove Docker Desktop for Debian, run:
$ sudo apt remove docker-desktop
For a complete cleanup, remove configuration and data files at $HOME/.docker/desktop
, the symlink at /usr/local/bin/com.docker.cli
, and purge
the remaining systemd service files.
$ rm -r $HOME/.docker/desktop
$ sudo rm /usr/local/bin/com.docker.cli
$ sudo apt purge docker-desktop
Remove the credsStore
and currentContext
properties from $HOME/.docker/config.json
. Additionally, you must delete any edited configuration files manually.
To remove Docker Desktop for Fedora, run:
$ sudo dnf remove docker-desktop
For a complete cleanup, remove configuration and data files at $HOME/.docker/desktop
, the symlink at /usr/local/bin/com.docker.cli
, and purge
the remaining systemd service files.
$ rm -r $HOME/.docker/desktop
$ sudo rm /usr/local/bin/com.docker.cli
Remove the credsStore
and currentContext
properties from $HOME/.docker/config.json
. Additionally, you must delete any edited configuration files manually.
To remove Docker Desktop for Arch, run:
$ sudo pacman -R docker-desktop
For a complete cleanup, remove configuration and data files at $HOME/.docker/desktop
, the symlink at /usr/local/bin/com.docker.cli
, and purge
the remaining systemd service files.
$ rm -r $HOME/.docker/desktop
$ sudo rm /usr/local/bin/com.docker.cli
$ sudo pacman -Rns docker-desktop
Remove the credsStore
and currentContext
properties from $HOME/.docker/config.json
. Additionally, you must delete any edited configuration files manually.",,,
04f0e799f430c8c363f41e49fa498a5c5c7dd0a243037ab453e27def40456b37,"Best practices for working with environment variables in Docker Compose
Table of contents
Handle sensitive information securely
Be cautious about including sensitive data in environment variables. Consider using Secrets for managing sensitive information.
Understand environment variable precedence
Be aware of how Docker Compose handles the
precedence of environment variables from different sources (.env
files, shell variables, Dockerfiles).
Use specific environment files
Consider how your application adapts to different environments. For example development, testing, production, and use different .env
files as needed.
Know interpolation
Understand how interpolation works within compose files for dynamic configurations.
Command line overrides
Be aware that you can override environment variables from the command line when starting containers. This is useful for testing or when you have temporary changes.",,,
a4b77400588aa012c54f4c097489b2ef8a668f758f1e417d619aab764bd3b4f7,"Access management
In this topic learn about the features available to manage access to your repositories. This includes visibility, collaborators, roles, teams, and organization access tokens.
Repository visibility
The most basic repository access is controlled via the visibility. A repository's visibility can be public or private.
With public visibility, the repository appears in Docker Hub search results and can be pulled by everyone. To manage push access to public personal repositories, you can use collaborators. To manage push access to public organization repositories, you can use roles, teams, or organization access tokens.
With private visibility, the repository doesn't appear in Docker Hub search results and is only accessible to those with granted permission. To manage push and pull access to private personal repositories, you can use collaborators. To manage push and pull access to private organization repositories, you can use roles, teams, or organization access tokens.
Change repository visibility
When creating a repository in Docker Hub, you can set the repository visibility. In addition, you can set the default repository visibility when a repository is created in your personal repository settings. The following describes how to change the visibility after the repository has been created.
To change repository visibility:
Sign in to Docker Hub.
Select Repositories.
Select a repository.
The General page for the repository appears.
Select the Settings tab.
Under Visibility settings, select one of the following:
- Make public: The repository appears in Docker Hub search results and can be pulled by everyone.
- Make private: The repository doesn't appear in Docker Hub search results and is only accessible to you and collaborators. In addition, if the repository is in an organization's namespace, then the repository is accessible to those with applicable roles or permissions.
Type the repository's name to verify the change.
Select Make public or Make private.
Collaborators
A collaborator is someone you want to give push
and pull
access to a
personal repository. Collaborators aren't able to perform any administrative
tasks such as deleting the repository or changing its visibility from private to
public. In addition, collaborators can't add other collaborators.
Only personal repositories can use collaborators. You can add unlimited collaborators to public repositories, and Docker Pro accounts can add up to 1 collaborator on private repositories.
Organization repositories can't use collaborators, but can use member roles, teams, or organization access tokens to manage access.
Manage collaborators
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears.
Select the Collaborators tab.
Add or remove collaborators based on their Docker username.
You can choose collaborators and manage their access to a private repository from that repository's Settings page.
Organization roles
Organizations can use roles for individuals, giving them different permissions in the organization. For more details, see Roles and permissions.
Organization teams
Organizations can use teams. A team can be assigned fine-grained repository access.
Configure team repository permissions
You must create a team before you are able to configure repository permissions. For more details, see Create and manage a team.
To configure team repository permissions:
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears.
Select the Permissions tab.
Add, modify, or remove a team's repository permissions.
- Add: Specify the Team, select the Permission, and then select Add.
- Modify: Specify the new permission next to the team.
- Remove: Select the Remove permission icon next to the team.
Organization access tokens (OATs)
Organizations can use OATs. OATs let you assign fine-grained repository access permissions to tokens. For more details, see Organization access tokens.",,,
d19f175f3d34caa63e68ebffee24bbeda1d2c4adb91f4dd277e22c74f2b62684,"Docker accounts
You can create a Docker account to secure a Docker ID, which is a username for your account that lets you access Docker products. You can use your Docker account to sign in to Docker products like Docker Hub, Docker Desktop, or Docker Scout. You can centrally manage your Docker account settings, as well as account security features, in Docker Home.
In this section, explore how you can create, manage, or update your account.",,,
5d35e0975a0356f203c693356c331c6e28c707bc3ad0ef6f55fe7a3787520183,"Builders
A builder is a BuildKit daemon that you can use to run your builds. BuildKit is the build engine that solves the build steps in a Dockerfile to produce a container image or other artifacts.
You can create and manage builders, inspect them, and even connect to builders running remotely. You interact with builders using the Docker CLI.
Default builder
Docker Engine automatically creates a builder that becomes the default backend for your builds. This builder uses the BuildKit library bundled with the daemon. This builder requires no configuration.
The default builder is directly bound to the Docker daemon and its
context. If you change the
Docker context, your default
builder refers to the new Docker context.
Build drivers
Buildx implements a concept of
build drivers to refer to
different builder configurations. The default builder created by the daemon
uses the
docker
driver.
Buildx supports the following build drivers:
docker
: uses the BuildKit library bundled into the Docker daemon.docker-container
: creates a dedicated BuildKit container using Docker.kubernetes
: creates BuildKit pods in a Kubernetes cluster.remote
: connects directly to a manually managed BuildKit daemon.
Selected builder
Selected builder refers to the builder that's used by default when you run build commands.
When you run a build, or interact with builders in some way using the CLI,
you can use the optional --builder
flag, or the BUILDX_BUILDER
environment variable,
to specify a builder by name. If you don't specify a builder,
the selected builder is used.
Use the docker buildx ls
command to see the available builder instances.
The asterisk (*
) next to a builder name indicates the selected builder.
$ docker buildx ls
NAME/NODE DRIVER/ENDPOINT STATUS BUILDKIT PLATFORMS
default * docker
default default running v0.11.6 linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386
my_builder docker-container
my_builder0 default running v0.11.6 linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386
Select a different builder
To switch between builders, use the docker buildx use <name>
command.
After running this command, the builder you specify is automatically selected when you invoke builds.
Difference between docker build
and docker buildx build
Even though docker build
is an alias for docker buildx build
, there are
subtle differences between the two commands. With Buildx, the build client and
the and daemon (BuildKit) are decoupled. This means you can use multiple
builders from a single client, even remote ones.
The docker build
command always defaults to using the default builder that
comes bundled with the Docker Engine, for ensuring backwards compatibility with
older versions of the Docker CLI. The docker buildx build
command, on the
other hand, checks whether you've set a different builder as the default
builder before it sends your build to BuildKit.
To use the docker build
command with a non-default builder, you must either:
Specify the builder explicitly, using the
--builder
flag or theBUILDX_BUILDER
environment variable:$ BUILDX_BUILDER=my_builder docker build . $ docker build --builder my_builder .
Configure Buildx as the default client by running the following command:
$ docker buildx install
This updates your Docker CLI configuration file to ensure all of your build-related commands are routed via Buildx.
Tip
To undo this change, run
docker buildx uninstall
.
In general, we recommend that you use the docker buildx build
command when
you want to use custom builders. This ensures that your
selected
builder configuration is interpreted correctly.
Additional information
- For information about how to interact with and manage builders, see Manage builders
- To learn about different types of builders, see Build drivers",,,
ae0fb8f923a488fb160b4c190f7b9b3ea7eb37d57ef47fa1fa20623a73d78362,"Enable two-factor authentication for your Docker account
Two-factor authentication adds an extra layer of security to your Docker account by requiring a unique security code when you sign in to your account. The security code is required in addition to your password.
When you enable two-factor authentication, you are also provided with a recovery code. Each recovery code is unique and specific to your account. You can use this code to recover your account in case you lose access to your authenticator app. See Recover your Docker account.
Prerequisites
You need a mobile phone with a time-based one-time password (TOTP) authenticator application installed. Common examples include Google Authenticator or Yubico Authenticator with a registered YubiKey.
Enable two-factor authentication
- Sign in to your Docker account.
- Select your avatar and then from the drop-down menu, select Account settings.
- Select 2FA.
- Enter your account password, then select Confirm.
- Save your recovery code and store it somewhere safe. You can use your recovery code to recover your account in the event you lose access to your authenticator app.
- Use a Time-based One-time password (TOTP) mobile app to scan the QR code or enter the text code.
- Once you've linked your authenticator app, enter the six-digit code in the text-field.
- Select Enable 2FA.
Two-factor authentication is now enabled. The next time you sign in to your Docker account, you will need to enter a security code.",,,
91214cc91b0bed9d18728dceb047fdcf3464e37c13978005e87aefe5ebfd6844,"Fluentd logging driver
The fluentd
logging driver sends container logs to the
Fluentd collector as structured log data. Then, users
can use any of the
various output plugins of
Fluentd to write these logs to various
destinations.
In addition to the log message itself, the fluentd
log
driver sends the following metadata in the structured log message:
| Field | Description |
|---|---|
container_id | The full 64-character container ID. |
container_name | The container name at the time it was started. If you use docker rename to rename a container, the new name isn't reflected in the journal entries. |
source | stdout or stderr |
log | The container log |
Usage
Some options are supported by specifying --log-opt
as many times as needed:
fluentd-address
: specify a socket address to connect to the Fluentd daemon, exfluentdhost:24224
orunix:///path/to/fluentd.sock
.tag
: specify a tag for Fluentd messages. Supports some Go template markup, ex{{.ID}}
,{{.FullID}}
or{{.Name}}
docker.{{.ID}}
.
To use the fluentd
driver as the default logging driver, set the log-driver
and log-opt
keys to appropriate values in the daemon.json
file, which is
located in /etc/docker/
on Linux hosts or
C:\ProgramData\docker\config\daemon.json
on Windows Server. For more about
configuring Docker using daemon.json
, see
daemon.json.
The following example sets the log driver to fluentd
and sets the
fluentd-address
option.
{
""log-driver"": ""fluentd"",
""log-opts"": {
""fluentd-address"": ""fluentdhost:24224""
}
}
Restart Docker for the changes to take effect.
Note
log-opts
configuration options in thedaemon.json
configuration file must be provided as strings. Boolean and numeric values (such as the value forfluentd-async
orfluentd-max-retries
) must therefore be enclosed in quotes (""
).
To set the logging driver for a specific container, pass the
--log-driver
option to docker run
:
$ docker run --log-driver=fluentd ...
Before using this logging driver, launch a Fluentd daemon. The logging driver
connects to this daemon through localhost:24224
by default. Use the
fluentd-address
option to connect to a different address.
$ docker run --log-driver=fluentd --log-opt fluentd-address=fluentdhost:24224
If container cannot connect to the Fluentd daemon, the container stops
immediately unless the fluentd-async
option is used.
Options
Users can use the --log-opt NAME=VALUE
flag to specify additional Fluentd logging driver options.
fluentd-address
By default, the logging driver connects to localhost:24224
. Supply the
fluentd-address
option to connect to a different address. tcp
(default) and unix
sockets are supported.
$ docker run --log-driver=fluentd --log-opt fluentd-address=fluentdhost:24224
$ docker run --log-driver=fluentd --log-opt fluentd-address=tcp://fluentdhost:24224
$ docker run --log-driver=fluentd --log-opt fluentd-address=unix:///path/to/fluentd.sock
Two of the above specify the same address, because tcp
is default.
tag
By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the log tag option documentation for customizing the log tag format.
labels, labels-regex, env, and env-regex
The labels
and env
options each take a comma-separated list of keys. If
there is collision between label
and env
keys, the value of the env
takes
precedence. Both options add additional fields to the extra attributes of a
logging message.
The env-regex
and labels-regex
options are similar to and compatible with
respectively env
and labels
. Their values are regular expressions to match
logging-related environment variables and labels. It is used for advanced
log tag options.
fluentd-async
Docker connects to Fluentd in the background. Messages are buffered until the
connection is established. Defaults to false
.
fluentd-async-reconnect-interval
When fluentd-async
is enabled, the fluentd-async-reconnect-interval
option
defines the interval, in milliseconds, at which the connection to
fluentd-address
is re-established. This option is useful if the address
resolves to one or more IP addresses, for example a Consul service address.
fluentd-buffer-limit
Sets the number of events buffered on the memory. Records will be stored in memory up to this number. If the buffer is full, the call to record logs will fail. The default is 1048576. ( https://github.com/fluent/fluent-logger-golang/tree/master#bufferlimit)
fluentd-retry-wait
How long to wait between retries. Defaults to 1 second.
fluentd-max-retries
The maximum number of retries. Defaults to 4294967295
(2**32 - 1).
fluentd-sub-second-precision
Generates event logs in nanosecond resolution. Defaults to false
.
Fluentd daemon management with Docker
About Fluentd
itself, see
the project webpage
and
its documents.
To use this logging driver, start the fluentd
daemon on a host. We recommend
that you use
the Fluentd docker
image. This image is
especially useful if you want to aggregate multiple container logs on each
host then, later, transfer the logs to another Fluentd node to create an
aggregate store.
Test container loggers
Write a configuration file (
test.conf
) to dump input logs:<source> @type forward </source> <match *> @type stdout </match>
Launch Fluentd container with this configuration file:
$ docker run -it -p 24224:24224 -v /path/to/conf/test.conf:/fluentd/etc/test.conf -e FLUENTD_CONF=test.conf fluent/fluentd:latest
Start one or more containers with the
fluentd
logging driver:$ docker run --log-driver=fluentd your/application",,,
732ea99728811a3c70d3da72c6949de60db6edc239744cd62cc3d2e229fb4b2a,"Merge Compose files
Docker Compose lets you merge and override a set of Compose files together to create a composite Compose file.
By default, Compose reads two files, a compose.yaml
and an optional
compose.override.yaml
file. By convention, the compose.yaml
contains your base configuration. The override file can
contain configuration overrides for existing services or entirely new
services.
If a service is defined in both files, Compose merges the configurations using the rules described below and in the Compose Specification.
How to merge multiple Compose files
To use multiple override files, or an override file with a different name, you
can either use the pre-defined
COMPOSE_FILE environment variable, or use the -f
option to specify the list of files.
Compose merges files in the order they're specified on the command line. Subsequent files may merge, override, or add to their predecessors.
For example:
$ docker compose -f compose.yaml -f compose.admin.yaml run backup_db
The compose.yaml
file might specify a webapp
service.
webapp:
image: examples/web
ports:
- ""8000:8000""
volumes:
- ""/data""
The compose.admin.yaml
may also specify this same service:
webapp:
environment:
- DEBUG=1
Any matching
fields override the previous file. New values, add to the webapp
service
configuration:
webapp:
image: examples/web
ports:
- ""8000:8000""
volumes:
- ""/data""
environment:
- DEBUG=1
Merging rules
Paths are evaluated relative to the base file. When you use multiple Compose files, you must make sure all paths in the files are relative to the base Compose file (the first Compose file specified with
-f
). This is required because override files need not be valid Compose files. Override files can contain small fragments of configuration. Tracking which fragment of a service is relative to which path is difficult and confusing, so to keep paths easier to understand, all paths must be defined relative to the base file.Tip
You can use
docker compose config
to review your merged configuration and avoid path-related issues.Compose copies configurations from the original service over to the local one. If a configuration option is defined in both the original service and the local service, the local value replaces or extends the original value.
For single-value options like
image
,command
ormem_limit
, the new value replaces the old value.original service:
services: myservice: # ... command: python app.py
local service:
services: myservice: # ... command: python otherapp.py
result:
services: myservice: # ... command: python otherapp.py
For the multi-value options
ports
,expose
,external_links
,dns
,dns_search
, andtmpfs
, Compose concatenates both sets of values:original service:
services: myservice: # ... expose: - ""3000""
local service:
services: myservice: # ... expose: - ""4000"" - ""5000""
result:
services: myservice: # ... expose: - ""3000"" - ""4000"" - ""5000""
In the case of
environment
,labels
,volumes
, anddevices
, Compose ""merges"" entries together with locally defined values taking precedence. Forenvironment
andlabels
, the environment variable or label name determines which value is used:original service:
services: myservice: # ... environment: - FOO=original - BAR=original
local service:
services: myservice: # ... environment: - BAR=local - BAZ=local
result:
services: myservice: # ... environment: - FOO=original - BAR=local - BAZ=local
Entries for
volumes
anddevices
are merged using the mount path in the container:original service:
services: myservice: # ... volumes: - ./original:/foo - ./original:/bar
local service:
services: myservice: # ... volumes: - ./local:/bar - ./local:/baz
result:
services: myservice: # ... volumes: - ./original:/foo - ./local:/bar - ./local:/baz
For more merging rules, see Merge and override in the Compose Specification.
Additional information
Using
-f
is optional. If not provided, Compose searches the working directory and its parent directories for acompose.yaml
and acompose.override.yaml
file. You must supply at least thecompose.yaml
file. If both files exist on the same directory level, Compose combines them into a single configuration.You can use a
-f
with-
(dash) as the filename to read the configuration fromstdin
. For example:$ docker compose -f - <<EOF webapp: image: examples/web ports: - ""8000:8000"" volumes: - ""/data"" environment: - DEBUG=1 EOF
When
stdin
is used, all paths in the configuration are relative to the current working directory.You can use the
-f
flag to specify a path to a Compose file that is not located in the current directory, either from the command line or by setting up a COMPOSE_FILE environment variable in your shell or in an environment file.For example, if you are running the Compose Rails sample, and have a
compose.yaml
file in a directory calledsandbox/rails
. You can use a command like docker compose pull to get the postgres image for thedb
service from anywhere by using the-f
flag as follows:docker compose -f ~/sandbox/rails/compose.yaml pull db
Here's the full example:
$ docker compose -f ~/sandbox/rails/compose.yaml pull db Pulling db (postgres:latest)... latest: Pulling from library/postgres ef0380f84d05: Pull complete 50cf91dc1db8: Pull complete d3add4cd115c: Pull complete 467830d8a616: Pull complete 089b9db7dc57: Pull complete 6fba0a36935c: Pull complete 81ef0e73c953: Pull complete 338a6c4894dc: Pull complete 15853f32f67c: Pull complete 044c83d92898: Pull complete 17301519f133: Pull complete dcca70822752: Pull complete cecf11b8ccf3: Pull complete Digest: sha256:1364924c753d5ff7e2260cd34dc4ba05ebd40ee8193391220be0f9901d4e1651 Status: Downloaded newer image for postgres:latest
Example
A common use case for multiple files is changing a development Compose app for a production-like environment (which may be production, staging or CI). To support these differences, you can split your Compose configuration into a few different files:
Start with a base file that defines the canonical configuration for the services.
compose.yaml
services:
web:
image: example/my_web_app:latest
depends_on:
- db
- cache
db:
image: postgres:latest
cache:
image: redis:latest
In this example the development configuration exposes some ports to the host, mounts our code as a volume, and builds the web image.
compose.override.yaml
services:
web:
build: .
volumes:
- '.:/code'
ports:
- 8883:80
environment:
DEBUG: 'true'
db:
command: '-d'
ports:
- 5432:5432
cache:
ports:
- 6379:6379
When you run docker compose up
it reads the overrides automatically.
To use this Compose app in a production environment, another override file is created, which might be stored in a different git repository or managed by a different team.
compose.prod.yaml
services:
web:
ports:
- 80:80
environment:
PRODUCTION: 'true'
cache:
environment:
TTL: '500'
To deploy with this production Compose file you can run
$ docker compose -f compose.yaml -f compose.prod.yaml up -d
This deploys all three services using the configuration in
compose.yaml
and compose.prod.yaml
but not the
dev configuration in compose.override.yaml
.
For more information, see Using Compose in production.
Limitations
Docker Compose supports relative paths for the many resources to be included in the application model: build context for service images, location of file defining environment variables, path to a local directory used in a bind-mounted volume. With such a constraint, code organization in a monorepo can become hard as a natural choice would be to have dedicated folders per team or component, but then the Compose files relative paths become irrelevant.",,,
8e6afaacce991944682346c3b952f45db4e954f0428c24a0cd782936babc3d5b,"Configure single sign-on
Get started creating a single sign-on (SSO) connection for your organization or company. This guide walks through the steps to add and verify the domains your members use to sign in to Docker.
Step one: Add your domain
- Sign in to the Admin Console.
- Select your organization or company from the Choose profile page. Note that when an organization is part of a company, you must select the company and configure the domain for the organization at the company level.
- Under Security and access, select Domain management.
- Select Add a domain.
- Enter your domain in the text box and select Add domain.
- The pop-up modal will prompt you with steps to verify your domain. Copy the TXT Record Value.
- Sign in to Docker Hub.
- Select Organizations and then your organization from the list.
- On your organization page, select Settings and then Security.
- Select Add a domain.
- Enter your domain in the text box and select Add domain.
- The pop-up modal will prompt you with steps to verify your domain. Copy the TXT Record Value.
Step two: Verify your domain
Verifying your domain ensures Docker knows you own it. Domain verification is done by adding your Docker TXT Record Value to your domain host. The TXT Record Value proves ownership, which signals the Domain Name System (DNS) to add this record. It can take up to 72 hours for DNS to recognize the change. When the change is reflected in DNS, Docker will automatically check the record to confirm your ownership.
- Navigate to your domain host, create a new TXT record, and paste the TXT Record Value from Docker.
- TXT record verification can take 72 hours. Once you have waited for TXT record verification, return to the Domain management page of the Admin Console and select Verify next to your domain name.
- Navigate to your domain host, create a new TXT record, and paste the TXT Record Value from Docker.
- TXT Record Verification can take 72 hours. Once you have waited for TXT record verification, return to the Security page of Docker Hub and select Verify next to your domain name.
Once you have added and verified your domain, you are ready to create an SSO connection between Docker and your identity provider (IdP).
More resources
The following videos walk through verifying your domain to create your SSO connection in Docker.",,,
069f5c232eafb09b4fb5cb2f9530af3de621b62ce46b70054aaef774af6a9ce2,"General security FAQs
How do I report a vulnerability?
If you’ve discovered a security vulnerability in Docker, we encourage you to report it responsibly. Report security issues to security@docker.com so that they can be quickly addressed by our team.
How are passwords managed when SSO isn't used?
Passwords are encrypted and salt-hashed. If you use application-level passwords instead of SSO, you are responsible for ensuring that your employees know how to pick strong passwords, don't share passwords, and don't reuse passwords across multiple systems.
Does Docker require password resets when SSO isn't used?
Passwords aren't required to be periodically reset. NIST no longer recommends password resets as part of best practice.
Does Docker lockout users after failed sign-ins?
Docker Hub’s global setting for system lockout is after 10 failed sign in attempts in a period of 5 minutes, and the lockout duration is 5 minutes. The same global policy applies to authenticated Docker Desktop users and Docker Scout, both of which use Docker Hub for authentication.
Do you support physical MFA with YubiKeys?
You can configure this through SSO using your IdP. Check with your IdP if they support physical MFA.
How are sessions managed and do they expire?
Docker Desktop uses tokens to manage sessions after a user signs in. Docker Desktop signs you out after 90 days, or 30 days of inactivity.
In Docker Hub, you need to re-authenticate after 24 hours. If users are authenticating using SSO, the default session timeout for the IdP is respected.
Custom settings per organization for sessions aren't supported.
How does Docker attribute downloads to us and what data is used to classify or verify the user is part of our organization?
Docker Desktop downloads are linked to a specific organization by the user's email containing the customer's domain. Additionally, we use IP addresses to correlate users with organizations.
How do you attribute that number of downloads to us from IP data if most of our engineers work from home and aren’t allowed to use VPNs?
We attribute users and their IP addresses to domains using 3rd party data enrichment software, where our provider analyzes activity from public and private data sources related to that specific IP address, then uses that activity to identify the domain and map it to the IP address.
Some users authenticate by signing in to Docker Desktop and joining their domain's Docker organization, which allows us to map them with a much higher degree of accuracy and report on direct feature usage for you. We highly encourage you to get your users authenticated so we can provide you with the most accurate data.
How does Docker distinguish between employee users and contractor users?
Organizations set up in Docker use verified domains and any team member with an email domain other than what's verified is noted as a ""Guest"" in that organization.
How long are Docker Hub logs available?
Docker provides various types of audit logs and log retention varies. For example, Docker Hub Activity logs are available for 90 days. You are responsible for exporting logs or setting up drivers to their own internal systems.
Can I export a list of all users with their assigned roles and privileges and if so, in what format?
Using the Export Members feature, you can export to CSV a list of your organization's users with role and team information.
How does Docker Desktop handle and store authentication information?
Docker Desktop utilizes the host operating system's secure key management for handling and storing authentication tokens necessary for authenticating with image registries. On macOS, this is Keychain; on Windows, this is Security and Identity API via Wincred; and on Linux, this is Pass.
How does Docker Hub secure passwords in storage and in transit?
This is applicable only when using Docker Hub's application-level password versus SSO/SAML. For users created through SSO Just-in-Time or SCIM provisioning, Docker Hub doesn't store passwords. For all other users, application-level passwords are salt-hashed in storage (SHA-256) and encrypted in transit (TLS).
How do we de-provision users who are not part of our IdP? We use SSO but not SCIM
If SCIM isn't enabled, you have to manually remove users from the organization in our system. Using SCIM automates this.
What metadata is collected from container images that Scout analyzes?
For information about the metadata stored by Docker Scout, see Data handling.
How are extensions within the Marketplace vetted for security prior to placement?
Security vetting for extensions is on our roadmap however this vetting isn't currently done.
Extensions are not covered as part of Docker’s Third-Party Risk Management Program.
Can I disable private repos in my organization via a setting to make sure nobody is pushing images into Docker Hub?
No. With Registry Access Management (RAM), administrators can ensure that their developers using Docker Desktop only access allowed registries. This is done through the Registry Access Management dashboard on Docker Hub.",,,
9cdaa989bae1a5f3780c398b8864fca5469317dbd31e7777f0ef751c2cbf6331,"Networking with standalone containers
This series of tutorials deals with networking for standalone Docker containers. For networking with swarm services, see Networking with swarm services. If you need to learn more about Docker networking in general, see the overview.
This topic includes two different tutorials. You can run each of them on Linux, Windows, or a Mac, but for the last one, you need a second Docker host running elsewhere.
Use the default bridge network demonstrates how to use the default
bridge
network that Docker sets up for you automatically. This network is not the best choice for production systems.Use user-defined bridge networks shows how to create and use your own custom bridge networks, to connect containers running on the same Docker host. This is recommended for standalone containers running in production.
Although overlay networks are generally used for swarm services, you can also use an overlay network for standalone containers. That's covered as part of the tutorial on using overlay networks.
Use the default bridge network
In this example, you start two different alpine
containers on the same Docker
host and do some tests to understand how they communicate with each other. You
need to have Docker installed and running.
Open a terminal window. List current networks before you do anything else. Here's what you should see if you've never added a network or initialized a swarm on this Docker daemon. You may see different networks, but you should at least see these (the network IDs will be different):
$ docker network ls NETWORK ID NAME DRIVER SCOPE 17e324f45964 bridge bridge local 6ed54d316334 host host local 7092879f2cc8 none null local
The default
bridge
network is listed, along withhost
andnone
. The latter two are not fully-fledged networks, but are used to start a container connected directly to the Docker daemon host's networking stack, or to start a container with no network devices. This tutorial will connect two containers to thebridge
network.Start two
alpine
containers runningash
, which is Alpine's default shell rather thanbash
. The-dit
flags mean to start the container detached (in the background), interactive (with the ability to type into it), and with a TTY (so you can see the input and output). Since you are starting it detached, you won't be connected to the container right away. Instead, the container's ID will be printed. Because you have not specified any--network
flags, the containers connect to the defaultbridge
network.$ docker run -dit --name alpine1 alpine ash $ docker run -dit --name alpine2 alpine ash
Check that both containers are actually started:
$ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 602dbf1edc81 alpine ""ash"" 4 seconds ago Up 3 seconds alpine2 da33b7aa74b0 alpine ""ash"" 17 seconds ago Up 16 seconds alpine1
Inspect the
bridge
network to see what containers are connected to it.$ docker network inspect bridge [ { ""Name"": ""bridge"", ""Id"": ""17e324f459648a9baaea32b248d3884da102dde19396c25b30ec800068ce6b10"", ""Created"": ""2017-06-22T20:27:43.826654485Z"", ""Scope"": ""local"", ""Driver"": ""bridge"", ""EnableIPv6"": false, ""IPAM"": { ""Driver"": ""default"", ""Options"": null, ""Config"": [ { ""Subnet"": ""172.17.0.0/16"", ""Gateway"": ""172.17.0.1"" } ] }, ""Internal"": false, ""Attachable"": false, ""Containers"": { ""602dbf1edc81813304b6cf0a647e65333dc6fe6ee6ed572dc0f686a3307c6a2c"": { ""Name"": ""alpine2"", ""EndpointID"": ""03b6aafb7ca4d7e531e292901b43719c0e34cc7eef565b38a6bf84acf50f38cd"", ""MacAddress"": ""02:42:ac:11:00:03"", ""IPv4Address"": ""172.17.0.3/16"", ""IPv6Address"": """" }, ""da33b7aa74b0bf3bda3ebd502d404320ca112a268aafe05b4851d1e3312ed168"": { ""Name"": ""alpine1"", ""EndpointID"": ""46c044a645d6afc42ddd7857d19e9dcfb89ad790afb5c239a35ac0af5e8a5bc5"", ""MacAddress"": ""02:42:ac:11:00:02"", ""IPv4Address"": ""172.17.0.2/16"", ""IPv6Address"": """" } }, ""Options"": { ""com.docker.network.bridge.default_bridge"": ""true"", ""com.docker.network.bridge.enable_icc"": ""true"", ""com.docker.network.bridge.enable_ip_masquerade"": ""true"", ""com.docker.network.bridge.host_binding_ipv4"": ""0.0.0.0"", ""com.docker.network.bridge.name"": ""docker0"", ""com.docker.network.driver.mtu"": ""1500"" }, ""Labels"": {} } ]
Near the top, information about the
bridge
network is listed, including the IP address of the gateway between the Docker host and thebridge
network (172.17.0.1
). Under theContainers
key, each connected container is listed, along with information about its IP address (172.17.0.2
foralpine1
and172.17.0.3
foralpine2
).The containers are running in the background. Use the
docker attach
command to connect toalpine1
.$ docker attach alpine1 / #
The prompt changes to
#
to indicate that you are theroot
user within the container. Use theip addr show
command to show the network interfaces foralpine1
as they look from within the container:# ip addr show 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 27: eth0@if28: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff inet 172.17.0.2/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe11:2/64 scope link valid_lft forever preferred_lft forever
The first interface is the loopback device. Ignore it for now. Notice that the second interface has the IP address
172.17.0.2
, which is the same address shown foralpine1
in the previous step.From within
alpine1
, make sure you can connect to the internet by pinginggoogle.com
. The-c 2
flag limits the command to twoping
attempts.# ping -c 2 google.com PING google.com (172.217.3.174): 56 data bytes 64 bytes from 172.217.3.174: seq=0 ttl=41 time=9.841 ms 64 bytes from 172.217.3.174: seq=1 ttl=41 time=9.897 ms --- google.com ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 9.841/9.869/9.897 ms
Now try to ping the second container. First, ping it by its IP address,
172.17.0.3
:# ping -c 2 172.17.0.3 PING 172.17.0.3 (172.17.0.3): 56 data bytes 64 bytes from 172.17.0.3: seq=0 ttl=64 time=0.086 ms 64 bytes from 172.17.0.3: seq=1 ttl=64 time=0.094 ms --- 172.17.0.3 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.086/0.090/0.094 ms
This succeeds. Next, try pinging the
alpine2
container by container name. This will fail.# ping -c 2 alpine2 ping: bad address 'alpine2'
Detach from
alpine1
without stopping it by using the detach sequence,CTRL
+p
CTRL
+q
(hold downCTRL
and typep
followed byq
). If you wish, attach toalpine2
and repeat steps 4, 5, and 6 there, substitutingalpine1
foralpine2
.Stop and remove both containers.
$ docker container stop alpine1 alpine2 $ docker container rm alpine1 alpine2
Remember, the default bridge
network is not recommended for production. To
learn about user-defined bridge networks, continue to the
next tutorial.
Use user-defined bridge networks
In this example, we again start two alpine
containers, but attach them to a
user-defined network called alpine-net
which we have already created. These
containers are not connected to the default bridge
network at all. We then
start a third alpine
container which is connected to the bridge
network but
not connected to alpine-net
, and a fourth alpine
container which is
connected to both networks.
Create the
alpine-net
network. You do not need the--driver bridge
flag since it's the default, but this example shows how to specify it.$ docker network create --driver bridge alpine-net
List Docker's networks:
$ docker network ls NETWORK ID NAME DRIVER SCOPE e9261a8c9a19 alpine-net bridge local 17e324f45964 bridge bridge local 6ed54d316334 host host local 7092879f2cc8 none null local
Inspect the
alpine-net
network. This shows you its IP address and the fact that no containers are connected to it:$ docker network inspect alpine-net [ { ""Name"": ""alpine-net"", ""Id"": ""e9261a8c9a19eabf2bf1488bf5f208b99b1608f330cff585c273d39481c9b0ec"", ""Created"": ""2017-09-25T21:38:12.620046142Z"", ""Scope"": ""local"", ""Driver"": ""bridge"", ""EnableIPv6"": false, ""IPAM"": { ""Driver"": ""default"", ""Options"": {}, ""Config"": [ { ""Subnet"": ""172.18.0.0/16"", ""Gateway"": ""172.18.0.1"" } ] }, ""Internal"": false, ""Attachable"": false, ""Containers"": {}, ""Options"": {}, ""Labels"": {} } ]
Notice that this network's gateway is
172.18.0.1
, as opposed to the default bridge network, whose gateway is172.17.0.1
. The exact IP address may be different on your system.Create your four containers. Notice the
--network
flags. You can only connect to one network during thedocker run
command, so you need to usedocker network connect
afterward to connectalpine4
to thebridge
network as well.$ docker run -dit --name alpine1 --network alpine-net alpine ash $ docker run -dit --name alpine2 --network alpine-net alpine ash $ docker run -dit --name alpine3 alpine ash $ docker run -dit --name alpine4 --network alpine-net alpine ash $ docker network connect bridge alpine4
Verify that all containers are running:
$ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 156849ccd902 alpine ""ash"" 41 seconds ago Up 41 seconds alpine4 fa1340b8d83e alpine ""ash"" 51 seconds ago Up 51 seconds alpine3 a535d969081e alpine ""ash"" About a minute ago Up About a minute alpine2 0a02c449a6e9 alpine ""ash"" About a minute ago Up About a minute alpine1
Inspect the
bridge
network and thealpine-net
network again:$ docker network inspect bridge [ { ""Name"": ""bridge"", ""Id"": ""17e324f459648a9baaea32b248d3884da102dde19396c25b30ec800068ce6b10"", ""Created"": ""2017-06-22T20:27:43.826654485Z"", ""Scope"": ""local"", ""Driver"": ""bridge"", ""EnableIPv6"": false, ""IPAM"": { ""Driver"": ""default"", ""Options"": null, ""Config"": [ { ""Subnet"": ""172.17.0.0/16"", ""Gateway"": ""172.17.0.1"" } ] }, ""Internal"": false, ""Attachable"": false, ""Containers"": { ""156849ccd902b812b7d17f05d2d81532ccebe5bf788c9a79de63e12bb92fc621"": { ""Name"": ""alpine4"", ""EndpointID"": ""7277c5183f0da5148b33d05f329371fce7befc5282d2619cfb23690b2adf467d"", ""MacAddress"": ""02:42:ac:11:00:03"", ""IPv4Address"": ""172.17.0.3/16"", ""IPv6Address"": """" }, ""fa1340b8d83eef5497166951184ad3691eb48678a3664608ec448a687b047c53"": { ""Name"": ""alpine3"", ""EndpointID"": ""5ae767367dcbebc712c02d49556285e888819d4da6b69d88cd1b0d52a83af95f"", ""MacAddress"": ""02:42:ac:11:00:02"", ""IPv4Address"": ""172.17.0.2/16"", ""IPv6Address"": """" } }, ""Options"": { ""com.docker.network.bridge.default_bridge"": ""true"", ""com.docker.network.bridge.enable_icc"": ""true"", ""com.docker.network.bridge.enable_ip_masquerade"": ""true"", ""com.docker.network.bridge.host_binding_ipv4"": ""0.0.0.0"", ""com.docker.network.bridge.name"": ""docker0"", ""com.docker.network.driver.mtu"": ""1500"" }, ""Labels"": {} } ]
Containers
alpine3
andalpine4
are connected to thebridge
network.$ docker network inspect alpine-net [ { ""Name"": ""alpine-net"", ""Id"": ""e9261a8c9a19eabf2bf1488bf5f208b99b1608f330cff585c273d39481c9b0ec"", ""Created"": ""2017-09-25T21:38:12.620046142Z"", ""Scope"": ""local"", ""Driver"": ""bridge"", ""EnableIPv6"": false, ""IPAM"": { ""Driver"": ""default"", ""Options"": {}, ""Config"": [ { ""Subnet"": ""172.18.0.0/16"", ""Gateway"": ""172.18.0.1"" } ] }, ""Internal"": false, ""Attachable"": false, ""Containers"": { ""0a02c449a6e9a15113c51ab2681d72749548fb9f78fae4493e3b2e4e74199c4a"": { ""Name"": ""alpine1"", ""EndpointID"": ""c83621678eff9628f4e2d52baf82c49f974c36c05cba152db4c131e8e7a64673"", ""MacAddress"": ""02:42:ac:12:00:02"", ""IPv4Address"": ""172.18.0.2/16"", ""IPv6Address"": """" }, ""156849ccd902b812b7d17f05d2d81532ccebe5bf788c9a79de63e12bb92fc621"": { ""Name"": ""alpine4"", ""EndpointID"": ""058bc6a5e9272b532ef9a6ea6d7f3db4c37527ae2625d1cd1421580fd0731954"", ""MacAddress"": ""02:42:ac:12:00:04"", ""IPv4Address"": ""172.18.0.4/16"", ""IPv6Address"": """" }, ""a535d969081e003a149be8917631215616d9401edcb4d35d53f00e75ea1db653"": { ""Name"": ""alpine2"", ""EndpointID"": ""198f3141ccf2e7dba67bce358d7b71a07c5488e3867d8b7ad55a4c695ebb8740"", ""MacAddress"": ""02:42:ac:12:00:03"", ""IPv4Address"": ""172.18.0.3/16"", ""IPv6Address"": """" } }, ""Options"": {}, ""Labels"": {} } ]
Containers
alpine1
,alpine2
, andalpine4
are connected to thealpine-net
network.On user-defined networks like
alpine-net
, containers can not only communicate by IP address, but can also resolve a container name to an IP address. This capability is called automatic service discovery. Let's connect toalpine1
and test this out.alpine1
should be able to resolvealpine2
andalpine4
(andalpine1
, itself) to IP addresses.Note
Automatic service discovery can only resolve custom container names, not default automatically generated container names,
$ docker container attach alpine1 # ping -c 2 alpine2 PING alpine2 (172.18.0.3): 56 data bytes 64 bytes from 172.18.0.3: seq=0 ttl=64 time=0.085 ms 64 bytes from 172.18.0.3: seq=1 ttl=64 time=0.090 ms --- alpine2 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.085/0.087/0.090 ms # ping -c 2 alpine4 PING alpine4 (172.18.0.4): 56 data bytes 64 bytes from 172.18.0.4: seq=0 ttl=64 time=0.076 ms 64 bytes from 172.18.0.4: seq=1 ttl=64 time=0.091 ms --- alpine4 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.076/0.083/0.091 ms # ping -c 2 alpine1 PING alpine1 (172.18.0.2): 56 data bytes 64 bytes from 172.18.0.2: seq=0 ttl=64 time=0.026 ms 64 bytes from 172.18.0.2: seq=1 ttl=64 time=0.054 ms --- alpine1 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.026/0.040/0.054 ms
From
alpine1
, you should not be able to connect toalpine3
at all, since it is not on thealpine-net
network.# ping -c 2 alpine3 ping: bad address 'alpine3'
Not only that, but you can't connect to
alpine3
fromalpine1
by its IP address either. Look back at thedocker network inspect
output for thebridge
network and findalpine3
's IP address:172.17.0.2
Try to ping it.# ping -c 2 172.17.0.2 PING 172.17.0.2 (172.17.0.2): 56 data bytes --- 172.17.0.2 ping statistics --- 2 packets transmitted, 0 packets received, 100% packet loss
Detach from
alpine1
using detach sequence,CTRL
+p
CTRL
+q
(hold downCTRL
and typep
followed byq
).Remember that
alpine4
is connected to both the defaultbridge
network andalpine-net
. It should be able to reach all of the other containers. However, you will need to addressalpine3
by its IP address. Attach to it and run the tests.$ docker container attach alpine4 # ping -c 2 alpine1 PING alpine1 (172.18.0.2): 56 data bytes 64 bytes from 172.18.0.2: seq=0 ttl=64 time=0.074 ms 64 bytes from 172.18.0.2: seq=1 ttl=64 time=0.082 ms --- alpine1 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.074/0.078/0.082 ms # ping -c 2 alpine2 PING alpine2 (172.18.0.3): 56 data bytes 64 bytes from 172.18.0.3: seq=0 ttl=64 time=0.075 ms 64 bytes from 172.18.0.3: seq=1 ttl=64 time=0.080 ms --- alpine2 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.075/0.077/0.080 ms # ping -c 2 alpine3 ping: bad address 'alpine3' # ping -c 2 172.17.0.2 PING 172.17.0.2 (172.17.0.2): 56 data bytes 64 bytes from 172.17.0.2: seq=0 ttl=64 time=0.089 ms 64 bytes from 172.17.0.2: seq=1 ttl=64 time=0.075 ms --- 172.17.0.2 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.075/0.082/0.089 ms # ping -c 2 alpine4 PING alpine4 (172.18.0.4): 56 data bytes 64 bytes from 172.18.0.4: seq=0 ttl=64 time=0.033 ms 64 bytes from 172.18.0.4: seq=1 ttl=64 time=0.064 ms --- alpine4 ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 0.033/0.048/0.064 ms
As a final test, make sure your containers can all connect to the internet by pinging
google.com
. You are already attached toalpine4
so start by trying from there. Next, detach fromalpine4
and connect toalpine3
(which is only attached to thebridge
network) and try again. Finally, connect toalpine1
(which is only connected to thealpine-net
network) and try again.# ping -c 2 google.com PING google.com (172.217.3.174): 56 data bytes 64 bytes from 172.217.3.174: seq=0 ttl=41 time=9.778 ms 64 bytes from 172.217.3.174: seq=1 ttl=41 time=9.634 ms --- google.com ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 9.634/9.706/9.778 ms CTRL+p CTRL+q $ docker container attach alpine3 # ping -c 2 google.com PING google.com (172.217.3.174): 56 data bytes 64 bytes from 172.217.3.174: seq=0 ttl=41 time=9.706 ms 64 bytes from 172.217.3.174: seq=1 ttl=41 time=9.851 ms --- google.com ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 9.706/9.778/9.851 ms CTRL+p CTRL+q $ docker container attach alpine1 # ping -c 2 google.com PING google.com (172.217.3.174): 56 data bytes 64 bytes from 172.217.3.174: seq=0 ttl=41 time=9.606 ms 64 bytes from 172.217.3.174: seq=1 ttl=41 time=9.603 ms --- google.com ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max = 9.603/9.604/9.606 ms CTRL+p CTRL+q
Stop and remove all containers and the
alpine-net
network.$ docker container stop alpine1 alpine2 alpine3 alpine4 $ docker container rm alpine1 alpine2 alpine3 alpine4 $ docker network rm alpine-net",,,
342d836e584842fa86046634d870fcf7026b09db94a40aa55af86f8d89c6c8b8,"VFS storage driver
The VFS storage driver isn't a union filesystem. Each layer is a directory on disk, and there is no copy-on-write support. To create a new layer, a ""deep copy"" is done of the previous layer. This leads to lower performance and more space used on disk than other storage drivers. However, it is robust, stable, and works in every environment. It can also be used as a mechanism to verify other storage back-ends against, in a testing environment.
Configure Docker with the vfs
storage driver
Stop Docker.
$ sudo systemctl stop docker
Edit
/etc/docker/daemon.json
. If it doesn't yet exist, create it. Assuming that the file was empty, add the following contents.{ ""storage-driver"": ""vfs"" }
If you want to set a quota to control the maximum size the VFS storage driver can use, set the
size
option on thestorage-opts
key.{ ""storage-driver"": ""vfs"", ""storage-opts"": [""size=256M""] }
Docker doesn't start if the
daemon.json
file contains invalid JSON.Start Docker.
$ sudo systemctl start docker
Verify that the daemon is using the
vfs
storage driver. Use thedocker info
command and look forStorage Driver
.$ docker info Storage Driver: vfs ...
Docker is now using the vfs
storage driver. Docker has automatically
created the /var/lib/docker/vfs/
directory, which contains all the layers
used by running containers.
How the vfs
storage driver works
Each image layer and the writable container layer are represented on the Docker
host as subdirectories within /var/lib/docker/
. The union mount provides the
unified view of all layers. The directory names don't directly correspond to
the IDs of the layers themselves.
VFS doesn't support copy-on-write (COW). Each time a new layer is created,
it's a deep copy of its parent layer. These layers are all located under
/var/lib/docker/vfs/dir/
.
Example: Image and container on-disk constructs
The following docker pull
command shows a Docker host downloading a Docker
image comprising five layers.
$ docker pull ubuntu
Using default tag: latest
latest: Pulling from library/ubuntu
e0a742c2abfd: Pull complete
486cb8339a27: Pull complete
dc6f0d824617: Pull complete
4f7a5649a30e: Pull complete
672363445ad2: Pull complete
Digest: sha256:84c334414e2bfdcae99509a6add166bbb4fa4041dc3fa6af08046a66fed3005f
Status: Downloaded newer image for ubuntu:latest
After pulling, each of these layers is represented as a subdirectory of
/var/lib/docker/vfs/dir/
. The directory names do not correlate with the
image layer IDs shown in the docker pull
command. To see the size taken up on
disk by each layer, you can use the du -sh
command, which gives the size as a
human-readable value.
$ ls -l /var/lib/docker/vfs/dir/
total 0
drwxr-xr-x. 2 root root 19 Aug 2 18:19 3262dfbe53dac3e1ab7dcc8ad5d8c4d586a11d2ac3c4234892e34bff7f6b821e
drwxr-xr-x. 21 root root 224 Aug 2 18:23 6af21814449345f55d88c403e66564faad965d6afa84b294ae6e740c9ded2561
drwxr-xr-x. 21 root root 224 Aug 2 18:23 6d3be4585ba32f9f5cbff0110e8d07aea5f5b9fbb1439677c27e7dfee263171c
drwxr-xr-x. 21 root root 224 Aug 2 18:23 9ecd2d88ca177413ab89f987e1507325285a7418fc76d0dcb4bc021447ba2bab
drwxr-xr-x. 21 root root 224 Aug 2 18:23 a292ac6341a65bf3a5da7b7c251e19de1294bd2ec32828de621d41c7ad31f895
drwxr-xr-x. 21 root root 224 Aug 2 18:23 e92be7a4a4e3ccbb7dd87695bca1a0ea373d4f673f455491b1342b33ed91446b
$ du -sh /var/lib/docker/vfs/dir/*
4.0K /var/lib/docker/vfs/dir/3262dfbe53dac3e1ab7dcc8ad5d8c4d586a11d2ac3c4234892e34bff7f6b821e
125M /var/lib/docker/vfs/dir/6af21814449345f55d88c403e66564faad965d6afa84b294ae6e740c9ded2561
104M /var/lib/docker/vfs/dir/6d3be4585ba32f9f5cbff0110e8d07aea5f5b9fbb1439677c27e7dfee263171c
125M /var/lib/docker/vfs/dir/9ecd2d88ca177413ab89f987e1507325285a7418fc76d0dcb4bc021447ba2bab
104M /var/lib/docker/vfs/dir/a292ac6341a65bf3a5da7b7c251e19de1294bd2ec32828de621d41c7ad31f895
104M /var/lib/docker/vfs/dir/e92be7a4a4e3ccbb7dd87695bca1a0ea373d4f673f455491b1342b33ed91446b
The above output shows that three layers each take 104M and two take 125M.
These directories have only small differences from each other, but they all
consume the same amount of disk space. This is one of the disadvantages of
using the vfs
storage driver.",,,
bcdc3671a7bbbb88d6514b12fcd4b676ecb269887452b7f90bf26b0018c467c7,"Base images
All Dockerfiles start from a base image.
A base is the image that your image extends.
It refers to the contents of the FROM
instruction in the Dockerfile.
FROM debian
For most cases, you don't need to create your own base image. Docker Hub contains a vast library of Docker images that are suitable for use as a base image in your build. Docker Official Images are specifically designed as a set of hardened, battle-tested images that support a wide variety of platforms, languages, and frameworks. There are also Docker Verified Publisher images, created by trusted publishing partners, verified by Docker.
Create a base image
If you need to completely control the contents of your image, you can create
your own base image from a Linux distribution of your choosing, or use the
special FROM scratch
base:
FROM scratch
The scratch
image is typically used to create minimal images containing only
just what an application needs. See
Create a minimal base image using scratch.
To create a distribution base image, you can use a root filesystem, packaged as
a tar
file, and import it to Docker with docker import
. The process for
creating your own base image depends on the Linux distribution you want to
package. See
Create a full image using tar.
Create a minimal base image using scratch
The reserved, minimal scratch
image serves as a starting point for
building containers. Using the scratch
image signals to the build process
that you want the next command in the Dockerfile
to be the first filesystem
layer in your image.
While scratch
appears in Docker's
repository on Docker Hub,
you can't pull it, run it, or tag any image with the name scratch
.
Instead, you can refer to it in your Dockerfile
.
For example, to create a minimal container using scratch
:
# syntax=docker/dockerfile:1
FROM scratch
ADD hello /
CMD [""/hello""]
Assuming an executable binary named hello
exists at the root of the
build context.
You can build this Docker image using the following docker build
command:
$ docker build --tag hello .
To run your new image, use the docker run
command:
$ docker run --rm hello
This example image can only successfully execute as long as the hello
binary
doesn't have any runtime dependencies. Computer programs tend to depend on
certain other programs or resources to exist in the runtime environment. For
example:
- Programming language runtimes
- Dynamically linked C libraries
- CA certificates
When building a base image, or any image, this is an important aspect to
consider. And this is why creating a base image using FROM scratch
can be
difficult, for anything other than small, simple programs. On the other hand,
it's also important to include only the things you need in your image, to
reduce the image size and attack surface.
Create a full image using tar
In general, start with a working machine that is running the distribution you'd like to package as a base image, though that is not required for some tools like Debian's Debootstrap, which you can also use to build Ubuntu images.
For example, to create an Ubuntu base image:
$ sudo debootstrap focal focal > /dev/null
$ sudo tar -C focal -c . | docker import - focal
sha256:81ec9a55a92a5618161f68ae691d092bf14d700129093158297b3d01593f4ee3
$ docker run focal cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=20.04
DISTRIB_CODENAME=focal
DISTRIB_DESCRIPTION=""Ubuntu 20.04 LTS""
There are more example scripts for creating base images in the Moby GitHub repository.
More resources
For more information about building images and writing Dockerfiles, see:",,,
9f877d5da24fb44cee4d4cc0973b477488d6d1a39c8b2a3adba019aede10f5d2,"View billing history
In this section, learn how you can view your billing history, manage your invoices, and verify your renewal date. All monthly and annual subscriptions are automatically renewed at the end of the term using the original form of payment.
Important
Starting July 1, 2024, Docker will begin collecting sales tax on subscription fees in compliance with state regulations for customers in the United States. For our global customers subject to VAT, the implementation will start rolling out on July 1, 2024. Note that while the roll out begins on this date, VAT charges may not apply to all applicable subscriptions immediately.
To ensure that tax assessments are correct, make sure that your billing information and VAT/Tax ID, if applicable, are updated. If you're exempt from sales tax, see Register a tax certificate.
Invoices
Your invoice includes the following:
- Invoice number
- Date of issue
- Date due
- Your ""Bill to"" information
- Amount due (in USD)
- Description of your order, quantity if applicable, unit price, and amount (in USD)
The information listed in the Bill to section of your invoice is based on your billing information. Not all fields are required. The billing information includes the following:
- Name (required): The name of the administrator or company
- Email address (required): The email address that receives all billing-related emails for the account
- Address (required)
- Phone number
- Tax ID or VAT
You can’t make changes to a paid or unpaid billing invoice. When you update your billing information, this change won't update an existing invoice. If you need to update your billing information, make sure you do so before your subscription renewal date when your invoice is finalized. For more information, see Update the billing information.
View renewal date
You receive your invoice when the subscription renews. To verify your renewal date, sign in to the Docker Home Billing. Your renewal date and amount are displayed on your subscription plan card.
You receive your invoice when the subscription renews. To verify your renewal date:
- Sign in to Docker Hub.
- Select your user avatar to open the drop-down menu.
- Select Billing.
- Select the user or organization account to view the billing details. Here you can find your renewal date and the renewal amount.
Include your VAT number on your invoice
Note
If the VAT number field is not available, complete the Contact Support form. This field may need to be manually added.
To add or update your VAT number:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- Select Billing information from the left-hand menu.
- Select Change on your billing information card.
- Ensure the I'm purchasing as a business checkbox is checked.
- Enter your VAT number in the Tax ID section.
- Select Update.
Your VAT number will be included on your next invoice.
To add or update your VAT number:
- Sign in to Docker Hub.
- For user accounts, Select your avatar in the top-right corner, then Billing. For organizations, select the name of the organization.
- Select the Billing address link.
- In the Billing Information section, select Update information.
- Enter your VAT number in the Tax ID section.
- Select Save.
Your VAT number will be included on your next invoice.
View billing history
You can view the billing history and download past invoices for a personal account or organization.
Personal account
To view billing history:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- Select Invoices from the left-hand menu.
- Optional. Select the Invoice number to open invoice details.
- Optional. Select the Download button to download an invoice.
To view billing history:
- Sign in to Docker Hub.
- Select your avatar in the top-right corner.
- From the drop-down menu select Billing.
- Select the Payment methods and billing history link. You can find your past invoices in the Invoice History section.
From here you can download an invoice.
Organization
Note
You must be an owner of the organization to view the billing history.
To view billing history:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- Select Invoices from the left-hand menu.
- Optional. Select the invoice number to open invoice details.
- Optional. Select the download button to download an invoice.
To view billing history:
- Sign in to Docker Hub.
- Select your avatar in the top-right corner.
- From the drop-down menu select Billing.
- Select the Payment methods and billing history link. You can find your past invoices in the Invoice History section.
From here you can download an invoice.",,,
91ac12159a462b77f8ad07ef510d667c02c6cf4cabeb4a44f028be3c3eaed8c9,"Run Docker Desktop for Windows in a VM or VDI environment
In general, we recommend running Docker Desktop natively on either Mac, Linux, or Windows. However, Docker Desktop for Windows can run inside a virtual desktop provided the virtual desktop is properly configured.
To run Docker Desktop in a virtual desktop environment, it is essential nested virtualization is enabled on the virtual machine that provides the virtual desktop. This is because, under the hood, Docker Desktop is using a Linux VM in which it runs Docker Engine and the containers.
Virtual desktop support
Note
Support for running Docker Desktop on a virtual desktop is available to Docker Business customers, on VMware ESXi or Azure VMs only.
The support available from Docker extends to installing and running Docker Desktop inside the VM, once the nested virtualization is set up correctly. The only hypervisors we have successfully tested are VMware ESXi and Azure, and there is no support for other VMs. For more information on Docker Desktop support, see Get support.
For troubleshooting problems and intermittent failures that are outside of Docker's control, you should contact your hypervisor vendor. Each hypervisor vendor offers different levels of support. For example, Microsoft supports running nested Hyper-V both on-prem and on Azure, with some version constraints. This may not be the case for VMWare ESXi.
Docker does not support running multiples instances of Docker Desktop on the same machine in a VM or VDI environment.
Turn on nested virtualization
You must turn on nested virtualization before you install Docker Desktop on a virtual machine.
Turn on nested virtualization on VMware ESXi
Nested virtualization of other hypervisors like Hyper-V inside a vSphere VM is not a supported scenario. However, running Hyper-V VM in a VMware ESXi VM is technically possible and, depending on the version, ESXi includes hardware-assisted virtualization as a supported feature. For internal testing, we used a VM that had 1 CPU with 4 cores and 12GB of memory.
For steps on how to expose hardware-assisted virtualization to the guest OS, see VMware's documentation.
Turn on nested virtualization on an Azure Virtual Machine
Nested virtualization is supported by Microsoft for running Hyper-V inside an Azure VM.
For Azure virtual machines, check that the VM size chosen supports nested virtualization. Microsoft provides a helpful list on Azure VM sizes and highlights the sizes that currently support nested virtualization. For internal testing, we used D4s_v5 machines. We recommend this specification or above for optimal performance of Docker Desktop.",,,
7ceba02b49af262addd9d0947f5a1570cacd45545c54980f0da943dd18bf8337,"Using Bake with additional contexts
In addition to the main context
key that defines the build context, each
target can also define additional named contexts with a map defined with key
contexts
. These values map to the --build-context
flag in the
build
command.
Inside the Dockerfile these contexts can be used with the FROM
instruction or
--from
flag.
Supported context values are:
- Local filesystem directories
- Container images
- Git URLs
- HTTP URLs
- Name of another target in the Bake file
Pinning alpine image
# syntax=docker/dockerfile:1
FROM alpine
RUN echo ""Hello world""
target ""app"" {
contexts = {
alpine = ""docker-image://alpine:3.13""
}
}
Using a secondary source directory
FROM golang
COPY --from=src . .
# Running `docker buildx bake app` will result in `src` not pointing
# to some previous build stage but to the client filesystem, not part of the context.
target ""app"" {
contexts = {
src = ""../path/to/source""
}
}
Using a target as a build context
To use a result of one target as a build context of another, specify the target
name with target:
prefix.
FROM scratch
# syntax=docker/dockerfile:1
FROM baseapp
RUN echo ""Hello world""
target ""base"" {
dockerfile = ""baseapp.Dockerfile""
}
target ""app"" {
contexts = {
baseapp = ""target:base""
}
}
In most cases you should just use a single multi-stage Dockerfile with multiple targets for similar behavior. This case is only recommended when you have multiple Dockerfiles that can't be easily merged into one.
Deduplicate context transfer
Note
As of Buildx version 0.17.0 and later, Bake automatically de-duplicates context transfer for targets that share the same context. In addition to Buildx version 0.17.0, the builder must be running BuildKit version 0.16.0 or later, and the Dockerfile syntax must be
docker/dockerfile:1.10
or later.If you meet these requirements, you don't need to manually de-duplicate context transfer as described in this section.
- To check your Buildx version, run
docker buildx version
.- To check your BuildKit version, run
docker buildx inspect --bootstrap
and look for theBuildKit version
field.- To check your Dockerfile syntax version, check the
syntax
parser directive in your Dockerfile. If it's not present, the default version whatever comes bundled with your current version of BuildKit. To set the version explicitly, add#syntax=docker/dockerfile:1.10
at the top of your Dockerfile.
When you build targets concurrently, using groups, build contexts are loaded independently for each target. If the same context is used by multiple targets in a group, that context is transferred once for each time it's used. This can result in significant impact on build time, depending on your build configuration. For example, say you have a Bake file that defines the following group of targets:
group ""default"" {
targets = [""target1"", ""target2""]
}
target ""target1"" {
target = ""target1""
context = "".""
}
target ""target2"" {
target = ""target2""
context = "".""
}
In this case, the context .
is transferred twice when you build the default
group: once for target1
and once for target2
.
If your context is small, and if you are using a local builder, duplicate context transfers may not be a big deal. But if your build context is big, or you have a large number of targets, or you're transferring the context over a network to a remote builder, context transfer becomes a performance bottleneck.
To avoid transferring the same context multiple times, you can define a named
context that only loads the context files, and have each target that needs
those files reference that named context. For example, the following Bake file
defines a named target ctx
, which is used by both target1
and target2
:
group ""default"" {
targets = [""target1"", ""target2""]
}
target ""ctx"" {
context = "".""
target = ""ctx""
}
target ""target1"" {
target = ""target1""
contexts = {
ctx = ""target:ctx""
}
}
target ""target2"" {
target = ""target2""
contexts = {
ctx = ""target:ctx""
}
}
The named context ctx
represents a Dockerfile stage, which copies the files
from its context (.
). Other stages in the Dockerfile can now reference the
ctx
named context and, for example, mount its files with --mount=from=ctx
.
FROM scratch AS ctx
COPY --link . .
FROM golang:alpine AS target1
WORKDIR /work
RUN --mount=from=ctx \
go build -o /out/client ./cmd/client \
FROM golang:alpine AS target2
WORKDIR /work
RUN --mount=from=ctx \
go build -o /out/server ./cmd/server",,,
068265d351506e3e39c9aae85fd7cd4b0f322e8d22d0a051629549c8532351f0,"Install Docker Engine on Debian
To get started with Docker Engine on Debian, make sure you meet the prerequisites, and then follow the installation steps.
Prerequisites
Firewall limitations
Warning
Before you install Docker, make sure you consider the following security implications and firewall incompatibilities.
- If you use ufw or firewalld to manage firewall settings, be aware that when you expose container ports using Docker, these ports bypass your firewall rules. For more information, refer to Docker and ufw.
- Docker is only compatible with
iptables-nft
andiptables-legacy
. Firewall rules created withnft
are not supported on a system with Docker installed. Make sure that any firewall rulesets you use are created withiptables
orip6tables
, and that you add them to theDOCKER-USER
chain, see Packet filtering and firewalls.
OS requirements
To install Docker Engine, you need the 64-bit version of one of these Debian versions:
- Debian Bookworm 12 (stable)
- Debian Bullseye 11 (oldstable)
Docker Engine for Debian is compatible with x86_64 (or amd64), armhf, arm64, and ppc64le (ppc64el) architectures.
Uninstall old versions
Before you can install Docker Engine, you need to uninstall any conflicting packages.
Your Linux distribution may provide unofficial Docker packages, which may conflict with the official packages provided by Docker. You must uninstall these packages before you install the official version of Docker Engine.
The unofficial packages to uninstall are:
docker.io
docker-compose
docker-doc
podman-docker
Moreover, Docker Engine depends on containerd
and runc
. Docker Engine
bundles these dependencies as one bundle: containerd.io
. If you have
installed the containerd
or runc
previously, uninstall them to avoid
conflicts with the versions bundled with Docker Engine.
Run the following command to uninstall all conflicting packages:
$ for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do sudo apt-get remove $pkg; done
apt-get
might report that you have none of these packages installed.
Images, containers, volumes, and networks stored in /var/lib/docker/
aren't
automatically removed when you uninstall Docker. If you want to start with a
clean installation, and prefer to clean up any existing data, read the
uninstall Docker Engine section.
Installation methods
You can install Docker Engine in different ways, depending on your needs:
Docker Engine comes bundled with Docker Desktop for Linux. This is the easiest and quickest way to get started.
Set up and install Docker Engine from Docker's
apt
repository.Install it manually and manage upgrades manually.
Use a convenience script. Only recommended for testing and development environments.
Install using the apt
repository
Before you install Docker Engine for the first time on a new host machine, you
need to set up the Docker apt
repository. Afterward, you can install and update
Docker from the repository.
Set up Docker's
apt
repository.# Add Docker's official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \ ""deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian \ $(. /etc/os-release && echo ""$VERSION_CODENAME"") stable"" | \ sudo tee /etc/apt/sources.list.d/docker.list > /dev/null sudo apt-get update
Note
If you use a derivative distribution, such as Kali Linux, you may need to substitute the part of this command that's expected to print the version codename:
$(. /etc/os-release && echo ""$VERSION_CODENAME"")
Replace this part with the codename of the corresponding Debian release, such as
bookworm
.Install the Docker packages.
To install the latest version, run:
$ sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
To install a specific version of Docker Engine, start by listing the available versions in the repository:
# List the available versions: $ apt-cache madison docker-ce | awk '{ print $3 }' 5:28.0.1-1~debian.12~bookworm 5:28.0.0-1~debian.12~bookworm ...
Select the desired version and install:
$ VERSION_STRING=5:28.0.1-1~debian.12~bookworm $ sudo apt-get install docker-ce=$VERSION_STRING docker-ce-cli=$VERSION_STRING containerd.io docker-buildx-plugin docker-compose-plugin
Verify that the installation is successful by running the
hello-world
image:$ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Upgrade Docker Engine
To upgrade Docker Engine, follow step 2 of the installation instructions, choosing the new version you want to install.
Install from a package
If you can't use Docker's apt
repository to install Docker Engine, you can
download the deb
file for your release and install it manually. You need to
download a new file each time you want to upgrade Docker Engine.
Select your Debian version in the list.
Go to
pool/stable/
and select the applicable architecture (amd64
,armhf
,arm64
, ors390x
).Download the following
deb
files for the Docker Engine, CLI, containerd, and Docker Compose packages:containerd.io_<version>_<arch>.deb
docker-ce_<version>_<arch>.deb
docker-ce-cli_<version>_<arch>.deb
docker-buildx-plugin_<version>_<arch>.deb
docker-compose-plugin_<version>_<arch>.deb
Install the
.deb
packages. Update the paths in the following example to where you downloaded the Docker packages.$ sudo dpkg -i ./containerd.io_<version>_<arch>.deb \ ./docker-ce_<version>_<arch>.deb \ ./docker-ce-cli_<version>_<arch>.deb \ ./docker-buildx-plugin_<version>_<arch>.deb \ ./docker-compose-plugin_<version>_<arch>.deb
The Docker daemon starts automatically.
Verify that the installation is successful by running the
hello-world
image:$ sudo service docker start $ sudo docker run hello-world
This command downloads a test image and runs it in a container. When the container runs, it prints a confirmation message and exits.
You have now successfully installed and started Docker Engine.
Tip
Receiving errors when trying to run without root?
The
docker
user group exists but contains no users, which is why you’re required to usesudo
to run Docker commands. Continue to Linux postinstall to allow non-privileged users to run Docker commands and for other optional configuration steps.
Upgrade Docker Engine
To upgrade Docker Engine, download the newer package files and repeat the installation procedure, pointing to the new files.
Install using the convenience script
Docker provides a convenience script at
https://get.docker.com/ to install Docker into
development environments non-interactively. The convenience script isn't
recommended for production environments, but it's useful for creating a
provisioning script tailored to your needs. Also refer to the
install using the repository steps to learn
about installation steps to install using the package repository. The source code
for the script is open source, and you can find it in the
docker-install
repository on GitHub.
Always examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script:
- The script requires
root
orsudo
privileges to run. - The script attempts to detect your Linux distribution and version and configure your package management system for you.
- The script doesn't allow you to customize most installation parameters.
- The script installs dependencies and recommendations without asking for confirmation. This may install a large number of packages, depending on the current configuration of your host machine.
- By default, the script installs the latest stable release of Docker, containerd, and runc. When using this script to provision a machine, this may result in unexpected major version upgrades of Docker. Always test upgrades in a test environment before deploying to your production systems.
- The script isn't designed to upgrade an existing Docker installation. When using the script to update an existing installation, dependencies may not be updated to the expected version, resulting in outdated versions.
Tip
Preview script steps before running. You can run the script with the
--dry-run
option to learn what steps the script will run when invoked:$ curl -fsSL https://get.docker.com -o get-docker.sh $ sudo sh ./get-docker.sh --dry-run
This example downloads the script from https://get.docker.com/ and runs it to install the latest stable release of Docker on Linux:
$ curl -fsSL https://get.docker.com -o get-docker.sh
$ sudo sh get-docker.sh
Executing docker install script, commit: 7cae5f8b0decc17d6571f9f52eb840fbc13b2737
<...>
You have now successfully installed and started Docker Engine. The docker
service starts automatically on Debian based distributions. On RPM
based
distributions, such as CentOS, Fedora, RHEL or SLES, you need to start it
manually using the appropriate systemctl
or service
command. As the message
indicates, non-root users can't run Docker commands by default.
Use Docker as a non-privileged user, or install in rootless mode?
The installation script requires
root
orsudo
privileges to install and use Docker. If you want to grant non-root users access to Docker, refer to the post-installation steps for Linux. You can also install Docker withoutroot
privileges, or configured to run in rootless mode. For instructions on running Docker in rootless mode, refer to run the Docker daemon as a non-root user (rootless mode).
Install pre-releases
Docker also provides a convenience script at
https://test.docker.com/ to install pre-releases of
Docker on Linux. This script is equal to the script at get.docker.com
, but
configures your package manager to use the test channel of the Docker package
repository. The test channel includes both stable and pre-releases (beta
versions, release-candidates) of Docker. Use this script to get early access to
new releases, and to evaluate them in a testing environment before they're
released as stable.
To install the latest version of Docker on Linux from the test channel, run:
$ curl -fsSL https://test.docker.com -o test-docker.sh
$ sudo sh test-docker.sh
Upgrade Docker after using the convenience script
If you installed Docker using the convenience script, you should upgrade Docker using your package manager directly. There's no advantage to re-running the convenience script. Re-running it can cause issues if it attempts to re-install repositories which already exist on the host machine.
Uninstall Docker Engine
Uninstall the Docker Engine, CLI, containerd, and Docker Compose packages:
$ sudo apt-get purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras
Images, containers, volumes, or custom configuration files on your host aren't automatically removed. To delete all images, containers, and volumes:
$ sudo rm -rf /var/lib/docker $ sudo rm -rf /var/lib/containerd
Remove source list and keyrings
$ sudo rm /etc/apt/sources.list.d/docker.list $ sudo rm /etc/apt/keyrings/docker.asc
You have to delete any edited configuration files manually.
Next steps
- Continue to Post-installation steps for Linux.",,,
a85b61d1b4099d8204c6af46829a5c995bbe6e063fa2fbf5db9dbdffcd13217b,"Docker Compose
Docker Compose is a tool for defining and running multi-container applications. It is the key to unlocking a streamlined and efficient development and deployment experience.
Compose simplifies the control of your entire application stack, making it easy to manage services, networks, and volumes in a single, comprehensible YAML configuration file. Then, with a single command, you create and start all the services from your configuration file.
Compose works in all environments; production, staging, development, testing, as well as CI workflows. It also has commands for managing the whole lifecycle of your application:
- Start, stop, and rebuild services
- View the status of running services
- Stream the log output of running services
- Run a one-off command on a service",,,
2bf73b8d71daa4b54a0a63cd98eb37d3839d3bf75096fd8fbc55b12a30ea4847,"Docker Scout
Container images consist of layers and software packages, which are susceptible to vulnerabilities. These vulnerabilities can compromise the security of containers and applications.
Docker Scout is a solution for proactively enhancing your software supply chain security. By analyzing your images, Docker Scout compiles an inventory of components, also known as a Software Bill of Materials (SBOM). The SBOM is matched against a continuously updated vulnerability database to pinpoint security weaknesses.
Docker Scout is a standalone service and platform that you can interact with using Docker Desktop, Docker Hub, the Docker CLI, and the Docker Scout Dashboard. Docker Scout also facilitates integrations with third-party systems, such as container registries and CI platforms.",,,
f4325c7199942f9f869d1872af2a813bdd7f24faa1126413ed6a2cf9af191662,"FAQs for Docker Desktop for Linux
Why does Docker Desktop for Linux run a VM?
Docker Desktop for Linux runs a Virtual Machine (VM) for the following reasons:
To ensure that Docker Desktop provides a consistent experience across platforms.
During research, the most frequently cited reason for users wanting Docker Desktop for Linux was to ensure a consistent Docker Desktop experience with feature parity across all major operating systems. Utilizing a VM ensures that the Docker Desktop experience for Linux users will closely match that of Windows and macOS.
To make use of new kernel features.
Sometimes we want to make use of new operating system features. Because we control the kernel and the OS inside the VM, we can roll these out to all users immediately, even to users who are intentionally sticking on an LTS version of their machine OS.
To enhance security.
Container image vulnerabilities pose a security risk for the host environment. There is a large number of unofficial images that are not guaranteed to be verified for known vulnerabilities. Malicious users can push images to public registries and use different methods to trick users into pulling and running them. The VM approach mitigates this threat as any malware that gains root privileges is restricted to the VM environment without access to the host.
Why not run rootless Docker? Although this has the benefit of superficially limiting access to the root user so everything looks safer in ""top"", it allows unprivileged users to gain
CAP_SYS_ADMIN
in their own user namespace and access kernel APIs which are not expecting to be used by unprivileged users, resulting in vulnerabilities.To provide the benefits of feature parity and enhanced security, with minimal impact on performance.
The VM utilized by Docker Desktop for Linux uses
VirtioFS
, a shared file system that allows virtual machines to access a directory tree located on the host. Our internal benchmarking shows that with the right resource allocation to the VM, near native file system performance can be achieved with VirtioFS.As such, we have adjusted the default memory available to the VM in Docker Desktop for Linux. You can tweak this setting to your specific needs by using the Memory slider within the Settings > Resources tab of Docker Desktop.
How do I enable file sharing?
Docker Desktop for Linux uses VirtioFS as the default (and currently only) mechanism to enable file sharing between the host and Docker Desktop VM.
In order not to require elevated privileges, without
unnecessarily restricting operations on the shared files, Docker Desktop runs
the file sharing service (virtiofsd
) inside a user namespace (see
user_namespaces(7)
) with UID and GID mapping configured. As a result Docker
Desktop relies on the host being configured to enable the current user to use
subordinate ID delegation. For this to be true /etc/subuid
(see subuid(5)
)
and /etc/subgid
(see subgid(5)
) must be present. Docker Desktop only
supports subordinate ID delegation configured via files. Docker Desktop maps the
current user ID and GID to 0 in the containers. It uses the first entry
corresponding to the current user in /etc/subuid
and /etc/subgid
to set up
mappings for IDs above 0 in the containers.
| ID in container | ID on host |
|---|---|
| 0 (root) | ID of the user running DD (e.g. 1000) |
| 1 | 0 + beginning of ID range specified in /etc/subuid //etc/subgid (e.g. 100000) |
| 2 | 1 + beginning of ID range specified in /etc/subuid //etc/subgid (e.g. 100001) |
| 3 | 2 + beginning of ID range specified in /etc/subuid //etc/subgid (e.g. 100002) |
| ... | ... |
If /etc/subuid
and /etc/subgid
are missing, they need to be created.
Both should contain entries in the form -
<username>:<start of id range>:<id range size>
. For example, to allow the current user
to use IDs from 100 000 to 165 535:
$ grep ""$USER"" /etc/subuid >> /dev/null 2&>1 || (echo ""$USER:100000:65536"" | sudo tee -a /etc/subuid)
$ grep ""$USER"" /etc/subgid >> /dev/null 2&>1 || (echo ""$USER:100000:65536"" | sudo tee -a /etc/subgid)
To verify the configs have been created correctly, inspect their contents:
$ echo $USER
exampleuser
$ cat /etc/subuid
exampleuser:100000:65536
$ cat /etc/subgid
exampleuser:100000:65536
In this scenario if a shared file is chown
ed inside a Docker Desktop container
owned by a user with a UID of 1000, it shows up on the host as owned by
a user with a UID of 100999. This has the unfortunate side effect of preventing
easy access to such a file on the host. The problem is resolved by creating
a group with the new GID and adding our user to it, or by setting a recursive
ACL (see setfacl(1)
) for folders shared with the Docker Desktop VM.
Where does Docker Desktop store Linux containers?
Docker Desktop stores Linux containers and images in a single, large ""disk image"" file in the Linux filesystem. This is different from Docker on Linux, which usually stores containers and images in the /var/lib/docker
directory on the host's filesystem.
Where is the disk image file?
To locate the disk image file, select Settings from the Docker Desktop Dashboard then Advanced from the Resources tab.
The Advanced tab displays the location of the disk image. It also displays the maximum size of the disk image and the actual space the disk image is consuming. Note that other tools might display space usage of the file in terms of the maximum file size, and not the actual file size.
What if the file is too large?
If the disk image file is too large, you can:
- Move it to a bigger drive
- Delete unnecessary containers and images
- Reduce the maximum allowable size of the file
How do I move the file to a bigger drive?
To move the disk image file to a different location:
Select Settings then Advanced from the Resources tab.
In the Disk image location section, select Browse and choose a new location for the disk image.
Select Apply & Restart for the changes to take effect.
Do not move the file directly in Finder as this can cause Docker Desktop to lose track of the file.
How do I delete unnecessary containers and images?
Check whether you have any unnecessary containers and images. If your client and daemon API are running version 1.25 or later (use the docker version
command on the client to check your client and daemon API versions), you can see the detailed space usage information by running:
$ docker system df -v
Alternatively, to list images, run:
$ docker image ls
To list containers, run:
$ docker container ls -a
If there are lots of redundant objects, run the command:
$ docker system prune
This command removes all stopped containers, unused networks, dangling images, and build cache.
It might take a few minutes to reclaim space on the host depending on the format of the disk image file:
- If the file is named
Docker.raw
: space on the host should be reclaimed within a few seconds. - If the file is named
Docker.qcow2
: space will be freed by a background process after a few minutes.
Space is only freed when images are deleted. Space is not freed automatically when files are deleted inside running containers. To trigger a space reclamation at any point, run the command:
$ docker run --privileged --pid=host docker/desktop-reclaim-space
Note that many tools report the maximum file size, not the actual file size. To query the actual size of the file on the host from a terminal, run:
$ cd ~/.docker/desktop/vms/0/data
$ ls -klsh Docker.raw
2333548 -rw-r--r--@ 1 username staff 64G Dec 13 17:42 Docker.raw
In this example, the actual size of the disk is 2333548
KB, whereas the maximum size of the disk is 64
GB.
How do I reduce the maximum size of the file?
To reduce the maximum size of the disk image file:
From Docker Desktop Dashboard select Settings then Advanced from the Resources tab.
The Disk image size section contains a slider that allows you to change the maximum size of the disk image. Adjust the slider to set a lower limit.
Select Apply & Restart.
When you reduce the maximum size, the current disk image file is deleted, and therefore, all containers and images are lost.",,,
f07f67ec93a71dbac960dcb75b8fffde402e3a91bdae9689a1e57f4c03ec982e,"Docker Scout SBOMs
Image analysis uses image SBOMs to understand what packages and versions an image contains. Docker Scout uses SBOM attestations if available on the image (recommended). If no SBOM attestation is available, Docker Scout creates one by indexing the image contents.
View from CLI
To view the contents of the SBOM that Docker Scout generates, you can use the
docker scout sbom
command.
$ docker scout sbom [IMAGE]
By default, this prints the SBOM in a JSON format to stdout.
The default JSON format produced by docker scout sbom
isn't SPDX-JSON.
To output SPDX, use the --format spdx
flag:
$ docker scout sbom --format spdx [IMAGE]
To generate a human-readable list, use the --format list
flag:
$ docker scout sbom --format list alpine
Name Version Type
───────────────────────────────────────────────
alpine-baselayout 3.4.3-r1 apk
alpine-baselayout-data 3.4.3-r1 apk
alpine-keys 2.4-r1 apk
apk-tools 2.14.0-r2 apk
busybox 1.36.1-r2 apk
busybox-binsh 1.36.1-r2 apk
ca-certificates 20230506-r0 apk
ca-certificates-bundle 20230506-r0 apk
libc-dev 0.7.2-r5 apk
libc-utils 0.7.2-r5 apk
libcrypto3 3.1.2-r0 apk
libssl3 3.1.2-r0 apk
musl 1.2.4-r1 apk
musl-utils 1.2.4-r1 apk
openssl 3.1.2-r0 apk
pax-utils 1.3.7-r1 apk
scanelf 1.3.7-r1 apk
ssl_client 1.36.1-r2 apk
zlib 1.2.13-r1 apk
For more information about the docker scout sbom
command, refer to the
CLI
reference.
Attach as build attestation
You can generate the SBOM and attach it to the image at build-time as an
attestation. BuildKit provides a default
SBOM generator which is different from what Docker Scout uses.
You can configure BuildKit to use the Docker Scout SBOM generator
using the --attest
flag for the docker build
command.
The Docker Scout SBOM indexer provides richer results
and ensures better compatibility with the Docker Scout image analysis.
$ docker build --tag <org>/<image> \
--attest type=sbom,generator=docker/scout-sbom-indexer:latest \
--push .
To build images with SBOM attestations, you must use either the
containerd
image store feature, or use a docker-container
builder together with the --push
flag to push the image (with attestations)
directly to a registry. The classic image store doesn't support manifest lists
or image indices, which is required for adding attestations to an image.
Extract to file
The command for extracting the SBOM of an image to an SPDX JSON file is different depending on whether the image has been pushed to a registry or if it's a local image.
Remote image
To extract the SBOM of an image and save it to a file, you can use the docker buildx imagetools inspect
command. This command only works for images in a
registry.
$ docker buildx imagetools inspect <image> --format ""{{ json .SBOM }}"" > sbom.spdx.json
Local image
To extract the SPDX file for a local image, build the image with the local
exporter and use the scout-sbom-indexer
SBOM generator plugin.
The following command saves the SBOM to a file at build/sbom.spdx.json
.
$ docker build --attest type=sbom,generator=docker/scout-sbom-indexer:latest \
--output build .",,,
f21337ffa8dbffeb0d57e9cfff75151ae89841da965a15bd4754428f789bc414,"Docker Compose Quickstart
This tutorial aims to introduce fundamental concepts of Docker Compose by guiding you through the development of a basic Python web application.
Using the Flask framework, the application features a hit counter in Redis, providing a practical example of how Docker Compose can be applied in web development scenarios.
The concepts demonstrated here should be understandable even if you're not familiar with Python.
This is a non-normative example that just highlights the key things you can do with Compose.
Prerequisites
Make sure you have:
- Installed the latest version of Docker Compose
- A basic understanding of Docker concepts and how Docker works
Step 1: Set up
Create a directory for the project:
$ mkdir composetest $ cd composetest
Create a file called
app.py
in your project directory and paste the following code in:import time import redis from flask import Flask app = Flask(__name__) cache = redis.Redis(host='redis', port=6379) def get_hit_count(): retries = 5 while True: try: return cache.incr('hits') except redis.exceptions.ConnectionError as exc: if retries == 0: raise exc retries -= 1 time.sleep(0.5) @app.route('/') def hello(): count = get_hit_count() return f'Hello World! I have been seen {count} times.\n'
In this example,
redis
is the hostname of the redis container on the application's network and the default port,6379
is used.Note
Note the way the
get_hit_count
function is written. This basic retry loop attempts the request multiple times if the Redis service is not available. This is useful at startup while the application comes online, but also makes the application more resilient if the Redis service needs to be restarted anytime during the app's lifetime. In a cluster, this also helps handling momentary connection drops between nodes.Create another file called
requirements.txt
in your project directory and paste the following code in:flask redis
Create a
Dockerfile
and paste the following code in:# syntax=docker/dockerfile:1 FROM python:3.10-alpine WORKDIR /code ENV FLASK_APP=app.py ENV FLASK_RUN_HOST=0.0.0.0 RUN apk add --no-cache gcc musl-dev linux-headers COPY requirements.txt requirements.txt RUN pip install -r requirements.txt EXPOSE 5000 COPY . . CMD [""flask"", ""run"", ""--debug""]
This tells Docker to:
- Build an image starting with the Python 3.10 image.
- Set the working directory to
/code
. - Set environment variables used by the
flask
command. - Install gcc and other dependencies
- Copy
requirements.txt
and install the Python dependencies. - Add metadata to the image to describe that the container is listening on port 5000
- Copy the current directory
.
in the project to the workdir.
in the image. - Set the default command for the container to
flask run --debug
.
Important
Check that the
Dockerfile
has no file extension like.txt
. Some editors may append this file extension automatically which results in an error when you run the application.For more information on how to write Dockerfiles, see the Dockerfile reference.
Step 2: Define services in a Compose file
Compose simplifies the control of your entire application stack, making it easy to manage services, networks, and volumes in a single, comprehensible YAML configuration file.
Create a file called compose.yaml
in your project directory and paste
the following:
services:
web:
build: .
ports:
- ""8000:5000""
redis:
image: ""redis:alpine""
This Compose file defines two services: web
and redis
.
The web
service uses an image that's built from the Dockerfile
in the current directory.
It then binds the container and the host machine to the exposed port, 8000
. This example service uses the default port for the Flask web server, 5000
.
The redis
service uses a public
Redis
image pulled from the Docker Hub registry.
For more information on the compose.yaml
file, see
How Compose works.
Step 3: Build and run your app with Compose
With a single command, you create and start all the services from your configuration file.
From your project directory, start up your application by running
docker compose up
.$ docker compose up Creating network ""composetest_default"" with the default driver Creating composetest_web_1 ... Creating composetest_redis_1 ... Creating composetest_web_1 Creating composetest_redis_1 ... done Attaching to composetest_web_1, composetest_redis_1 web_1 | * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit) redis_1 | 1:C 17 Aug 22:11:10.480 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo redis_1 | 1:C 17 Aug 22:11:10.480 # Redis version=4.0.1, bits=64, commit=00000000, modified=0, pid=1, just started redis_1 | 1:C 17 Aug 22:11:10.480 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf web_1 | * Restarting with stat redis_1 | 1:M 17 Aug 22:11:10.483 * Running mode=standalone, port=6379. redis_1 | 1:M 17 Aug 22:11:10.483 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. web_1 | * Debugger is active! redis_1 | 1:M 17 Aug 22:11:10.483 # Server initialized redis_1 | 1:M 17 Aug 22:11:10.483 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled. web_1 | * Debugger PIN: 330-787-903 redis_1 | 1:M 17 Aug 22:11:10.483 * Ready to accept connections
Compose pulls a Redis image, builds an image for your code, and starts the services you defined. In this case, the code is statically copied into the image at build time.
Enter
http://localhost:8000/
in a browser to see the application running.If this doesn't resolve, you can also try
http://127.0.0.1:8000
.You should see a message in your browser saying:
Hello World! I have been seen 1 times.
Refresh the page.
The number should increment.
Hello World! I have been seen 2 times.
Switch to another terminal window, and type
docker image ls
to list local images.Listing images at this point should return
redis
andweb
.$ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE composetest_web latest e2c21aa48cc1 4 minutes ago 93.8MB python 3.4-alpine 84e6077c7ab6 7 days ago 82.5MB redis alpine 9d8fa9aa0e5b 3 weeks ago 27.5MB
You can inspect images with
docker inspect <tag or id>
.Stop the application, either by running
docker compose down
from within your project directory in the second terminal, or by hittingCTRL+C
in the original terminal where you started the app.
Step 4: Edit the Compose file to use Compose Watch
Edit the compose.yaml
file in your project directory to use watch
so you can preview your running Compose services which are automatically updated as you edit and save your code:
services:
web:
build: .
ports:
- ""8000:5000""
develop:
watch:
- action: sync
path: .
target: /code
redis:
image: ""redis:alpine""
Whenever a file is changed, Compose syncs the file to the corresponding location under /code
inside the container. Once copied, the bundler updates the running application without a restart.
For more information on how Compose Watch works, see Use Compose Watch. Alternatively, see Manage data in containers for other options.
Note
For this example to work, the
--debug
option is added to theDockerfile
. The--debug
option in Flask enables automatic code reload, making it possible to work on the backend API without the need to restart or rebuild the container. After changing the.py
file, subsequent API calls will use the new code, but the browser UI will not automatically refresh in this small example. Most frontend development servers include native live reload support that works with Compose.
Step 5: Re-build and run the app with Compose
From your project directory, type docker compose watch
or docker compose up --watch
to build and launch the app and start the file watch mode.
$ docker compose watch
[+] Running 2/2
✔ Container docs-redis-1 Created 0.0s
✔ Container docs-web-1 Recreated 0.1s
Attaching to redis-1, web-1
⦿ watch enabled
...
Check the Hello World
message in a web browser again, and refresh to see the
count increment.
Step 6: Update the application
To see Compose Watch in action:
Change the greeting in
app.py
and save it. For example, change theHello World!
message toHello from Docker!
:return f'Hello from Docker! I have been seen {count} times.\n'
Refresh the app in your browser. The greeting should be updated, and the counter should still be incrementing.
Once you're done, run
docker compose down
.
Step 7: Split up your services
Using multiple Compose files lets you customize a Compose application for different environments or workflows. This is useful for large applications that may use dozens of containers, with ownership distributed across multiple teams.
In your project folder, create a new Compose file called
infra.yaml
.Cut the Redis service from your
compose.yaml
file and paste it into your newinfra.yaml
file. Make sure you add theservices
top-level attribute at the top of your file. Yourinfra.yaml
file should now look like this:services: redis: image: ""redis:alpine""
In your
compose.yaml
file, add theinclude
top-level attribute along with the path to theinfra.yaml
file.include: - infra.yaml services: web: build: . ports: - ""8000:5000"" develop: watch: - action: sync path: . target: /code
Run
docker compose up
to build the app with the updated Compose files, and run it. You should see theHello world
message in your browser.
This is a simplified example, but it demonstrates the basic principle of include
and how it can make it easier to modularize complex applications into sub-Compose files. For more information on include
and working with multiple Compose files, see
Working with multiple Compose files.
Step 8: Experiment with some other commands
If you want to run your services in the background, you can pass the
-d
flag (for ""detached"" mode) todocker compose up
and usedocker compose ps
to see what is currently running:$ docker compose up -d Starting composetest_redis_1... Starting composetest_web_1... $ docker compose ps Name Command State Ports ------------------------------------------------------------------------------------- composetest_redis_1 docker-entrypoint.sh redis ... Up 6379/tcp composetest_web_1 flask run Up 0.0.0.0:8000->5000/tcp
Run
docker compose --help
to see other available commands.If you started Compose with
docker compose up -d
, stop your services once you've finished with them:$ docker compose stop
You can bring everything down, removing the containers entirely, with the
docker compose down
command.",,,
661fbb7cbf18640eb9b35ee0fc833c97ca271976d86b493fea3863f0ce4eb747,"Store configuration data using Docker Configs
About configs
Docker swarm service configs allow you to store non-sensitive information, such as configuration files, outside a service's image or running containers. This allows you to keep your images as generic as possible, without the need to bind-mount configuration files into the containers or use environment variables.
Configs operate in a similar way to secrets, except that they are not encrypted at rest and are mounted directly into the container's filesystem without the use of RAM disks. Configs can be added or removed from a service at any time, and services can share a config. You can even use configs in conjunction with environment variables or labels, for maximum flexibility. Config values can be generic strings or binary content (up to 500 kb in size).
Note
Docker configs are only available to swarm services, not to standalone containers. To use this feature, consider adapting your container to run as a service with a scale of 1.
Configs are supported on both Linux and Windows services.
Windows support
Docker includes support for configs on Windows containers, but there are differences in the implementations, which are called out in the examples below. Keep the following notable differences in mind:
Config files with custom targets are not directly bind-mounted into Windows containers, since Windows does not support non-directory file bind-mounts. Instead, configs for a container are all mounted in
C:\ProgramData\Docker\internal\configs
(an implementation detail which should not be relied upon by applications) within the container. Symbolic links are used to point from there to the desired target of the config within the container. The default target isC:\ProgramData\Docker\configs
.When creating a service which uses Windows containers, the options to specify UID, GID, and mode are not supported for configs. Configs are currently only accessible by administrators and users with
system
access within the container.On Windows, create or update a service using
--credential-spec
with theconfig://<config-name>
format. This passes the gMSA credentials file directly to nodes before a container starts. No gMSA credentials are written to disk on worker nodes. For more information, refer to Deploy services to a swarm.
How Docker manages configs
When you add a config to the swarm, Docker sends the config to the swarm manager over a mutual TLS connection. The config is stored in the Raft log, which is encrypted. The entire Raft log is replicated across the other managers, ensuring the same high availability guarantees for configs as for the rest of the swarm management data.
When you grant a newly-created or running service access to a config, the config
is mounted as a file in the container. The location of the mount point within
the container defaults to /<config-name>
in Linux containers. In Windows
containers, configs are all mounted into C:\ProgramData\Docker\configs
and
symbolic links are created to the desired location, which defaults to
C:\<config-name>
.
You can set the ownership (uid
and gid
) for the config, using either the
numerical ID or the name of the user or group. You can also specify the file
permissions (mode
). These settings are ignored for Windows containers.
- If not set, the config is owned by the user running the container
command (often
root
) and that user's default group (also oftenroot
). - If not set, the config has world-readable permissions (mode
0444
), unless aumask
is set within the container, in which case the mode is impacted by thatumask
value.
You can update a service to grant it access to additional configs or revoke its access to a given config at any time.
A node only has access to configs if the node is a swarm manager or if it is running service tasks which have been granted access to the config. When a container task stops running, the configs shared to it are unmounted from the in-memory filesystem for that container and flushed from the node's memory.
If a node loses connectivity to the swarm while it is running a task container with access to a config, the task container still has access to its configs, but cannot receive updates until the node reconnects to the swarm.
You can add or inspect an individual config at any time, or list all configs. You cannot remove a config that a running service is using. See Rotate a config for a way to remove a config without disrupting running services.
To update or roll back configs more easily, consider adding a version number or date to the config name. This is made easier by the ability to control the mount point of the config within a given container.
To update a stack, make changes to your Compose file, then re-run docker stack deploy -c <new-compose-file> <stack-name>
. If you use a new config in
that file, your services start using them. Keep in mind that configurations
are immutable, so you can't change the file for an existing service.
Instead, you create a new config to use a different file
You can run docker stack rm
to stop the app and take down the stack. This
removes any config that was created by docker stack deploy
with the same stack
name. This removes all configs, including those not referenced by services and
those remaining after a docker service update --config-rm
.
Read more about docker config
commands
Use these links to read about specific commands, or continue to the example about using configs with a service.
Examples
This section includes graduated examples which illustrate how to use Docker configs.
Note
These examples use a single-engine swarm and unscaled services for simplicity. The examples use Linux containers, but Windows containers also support configs.
Defining and using configs in compose files
The docker stack
command supports defining configs in a Compose file.
However, the configs
key is not supported for docker compose
. See
the Compose file reference for details.
Simple example: Get started with configs
This simple example shows how configs work in just a few commands. For a real-world example, continue to Advanced example: Use configs with a Nginx service.
Add a config to Docker. The
docker config create
command reads standard input because the last argument, which represents the file to read the config from, is set to-
.$ echo ""This is a config"" | docker config create my-config -
Create a
redis
service and grant it access to the config. By default, the container can access the config at/my-config
, but you can customize the file name on the container using thetarget
option.$ docker service create --name redis --config my-config redis:alpine
Verify that the task is running without issues using
docker service ps
. If everything is working, the output looks similar to this:$ docker service ps redis ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS bkna6bpn8r1a redis.1 redis:alpine ip-172-31-46-109 Running Running 8 seconds ago
Get the ID of the
redis
service task container usingdocker ps
, so that you can usedocker container exec
to connect to the container and read the contents of the config data file, which defaults to being readable by all and has the same name as the name of the config. The first command below illustrates how to find the container ID, and the second and third commands use shell completion to do this automatically.$ docker ps --filter name=redis -q 5cb1c2348a59 $ docker container exec $(docker ps --filter name=redis -q) ls -l /my-config -r--r--r-- 1 root root 12 Jun 5 20:49 my-config $ docker container exec $(docker ps --filter name=redis -q) cat /my-config This is a config
Try removing the config. The removal fails because the
redis
service is running and has access to the config.$ docker config ls ID NAME CREATED UPDATED fzwcfuqjkvo5foqu7ts7ls578 hello 31 minutes ago 31 minutes ago $ docker config rm my-config Error response from daemon: rpc error: code = 3 desc = config 'my-config' is in use by the following service: redis
Remove access to the config from the running
redis
service by updating the service.$ docker service update --config-rm my-config redis
Repeat steps 3 and 4 again, verifying that the service no longer has access to the config. The container ID is different, because the
service update
command redeploys the service.$ docker container exec -it $(docker ps --filter name=redis -q) cat /my-config cat: can't open '/my-config': No such file or directory
Stop and remove the service, and remove the config from Docker.
$ docker service rm redis $ docker config rm my-config
Simple example: Use configs in a Windows service
This is a very simple example which shows how to use configs with a Microsoft IIS service running on Docker for Windows running Windows containers on Microsoft Windows 10. It is a naive example that stores the webpage in a config.
This example assumes that you have PowerShell installed.
Save the following into a new file
index.html
.<html lang=""en""> <head><title>Hello Docker</title></head> <body> <p>Hello Docker! You have deployed a HTML page.</p> </body> </html>
If you have not already done so, initialize or join the swarm.
docker swarm init
Save the
index.html
file as a swarm config namedhomepage
.docker config create homepage index.html
Create an IIS service and grant it access to the
homepage
config.docker service create --name my-iis --publish published=8000,target=8000 --config src=homepage,target=""\inetpub\wwwroot\index.html"" microsoft/iis:nanoserver
Access the IIS service at
http://localhost:8000/
. It should serve the HTML content from the first step.Remove the service and the config.
docker service rm my-iis docker config rm homepage
Example: Use a templated config
To create a configuration in which the content will be generated using a
template engine, use the --template-driver
parameter and specify the engine
name as its argument. The template will be rendered when container is created.
Save the following into a new file
index.html.tmpl
.<html lang=""en""> <head><title>Hello Docker</title></head> <body> <p>Hello {{ env ""HELLO"" }}! I'm service {{ .Service.Name }}.</p> </body> </html>
Save the
index.html.tmpl
file as a swarm config namedhomepage
. Provide parameter--template-driver
and specifygolang
as template engine.$ docker config create --template-driver golang homepage index.html.tmpl
Create a service that runs Nginx and has access to the environment variable HELLO and to the config.
$ docker service create \ --name hello-template \ --env HELLO=""Docker"" \ --config source=homepage,target=/usr/share/nginx/html/index.html \ --publish published=3000,target=80 \ nginx:alpine
Verify that the service is operational: you can reach the Nginx server, and that the correct output is being served.
$ curl http://0.0.0.0:3000 <html lang=""en""> <head><title>Hello Docker</title></head> <body> <p>Hello Docker! I'm service hello-template.</p> </body> </html>
Advanced example: Use configs with a Nginx service
This example is divided into two parts.
The first part is all about generating
the site certificate and does not directly involve Docker configs at all, but
it sets up
the second part, where you store
and use the site certificate as a series of secrets and the Nginx configuration
as a config. The example shows how to set options on the config, such as the
target location within the container and the file permissions (mode
).
Generate the site certificate
Generate a root CA and TLS certificate and key for your site. For production
sites, you may want to use a service such as Let’s Encrypt
to generate the
TLS certificate and key, but this example uses command-line tools. This step
is a little complicated, but is only a set-up step so that you have
something to store as a Docker secret. If you want to skip these sub-steps,
you can
use Let's Encrypt to
generate the site key and certificate, name the files site.key
and
site.crt
, and skip to
Configure the Nginx container.
Generate a root key.
$ openssl genrsa -out ""root-ca.key"" 4096
Generate a CSR using the root key.
$ openssl req \ -new -key ""root-ca.key"" \ -out ""root-ca.csr"" -sha256 \ -subj '/C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA'
Configure the root CA. Edit a new file called
root-ca.cnf
and paste the following contents into it. This constrains the root CA to only sign leaf certificates and not intermediate CAs.[root_ca] basicConstraints = critical,CA:TRUE,pathlen:1 keyUsage = critical, nonRepudiation, cRLSign, keyCertSign subjectKeyIdentifier=hash
Sign the certificate.
$ openssl x509 -req -days 3650 -in ""root-ca.csr"" \ -signkey ""root-ca.key"" -sha256 -out ""root-ca.crt"" \ -extfile ""root-ca.cnf"" -extensions \ root_ca
Generate the site key.
$ openssl genrsa -out ""site.key"" 4096
Generate the site certificate and sign it with the site key.
$ openssl req -new -key ""site.key"" -out ""site.csr"" -sha256 \ -subj '/C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost'
Configure the site certificate. Edit a new file called
site.cnf
and paste the following contents into it. This constrains the site certificate so that it can only be used to authenticate a server and can't be used to sign certificates.[server] authorityKeyIdentifier=keyid,issuer basicConstraints = critical,CA:FALSE extendedKeyUsage=serverAuth keyUsage = critical, digitalSignature, keyEncipherment subjectAltName = DNS:localhost, IP:127.0.0.1 subjectKeyIdentifier=hash
Sign the site certificate.
$ openssl x509 -req -days 750 -in ""site.csr"" -sha256 \ -CA ""root-ca.crt"" -CAkey ""root-ca.key"" -CAcreateserial \ -out ""site.crt"" -extfile ""site.cnf"" -extensions server
The
site.csr
andsite.cnf
files are not needed by the Nginx service, but you need them if you want to generate a new site certificate. Protect theroot-ca.key
file.
Configure the Nginx container
Produce a very basic Nginx configuration that serves static files over HTTPS. The TLS certificate and key are stored as Docker secrets so that they can be rotated easily.
In the current directory, create a new file called
site.conf
with the following contents:server { listen 443 ssl; server_name localhost; ssl_certificate /run/secrets/site.crt; ssl_certificate_key /run/secrets/site.key; location / { root /usr/share/nginx/html; index index.html index.htm; } }
Create two secrets, representing the key and the certificate. You can store any file as a secret as long as it is smaller than 500 KB. This allows you to decouple the key and certificate from the services that use them. In these examples, the secret name and the file name are the same.
$ docker secret create site.key site.key $ docker secret create site.crt site.crt
Save the
site.conf
file in a Docker config. The first parameter is the name of the config, and the second parameter is the file to read it from.$ docker config create site.conf site.conf
List the configs:
$ docker config ls ID NAME CREATED UPDATED 4ory233120ccg7biwvy11gl5z site.conf 4 seconds ago 4 seconds ago
Create a service that runs Nginx and has access to the two secrets and the config. Set the mode to
0440
so that the file is only readable by its owner and that owner's group, not the world.$ docker service create \ --name nginx \ --secret site.key \ --secret site.crt \ --config source=site.conf,target=/etc/nginx/conf.d/site.conf,mode=0440 \ --publish published=3000,target=443 \ nginx:latest \ sh -c ""exec nginx -g 'daemon off;'""
Within the running containers, the following three files now exist:
/run/secrets/site.key
/run/secrets/site.crt
/etc/nginx/conf.d/site.conf
Verify that the Nginx service is running.
$ docker service ls ID NAME MODE REPLICAS IMAGE zeskcec62q24 nginx replicated 1/1 nginx:latest $ docker service ps nginx NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS nginx.1.9ls3yo9ugcls nginx:latest moby Running Running 3 minutes ago
Verify that the service is operational: you can reach the Nginx server, and that the correct TLS certificate is being used.
$ curl --cacert root-ca.crt https://0.0.0.0:3000 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support, refer to <a href=""https://nginx.org"">nginx.org</a>.<br/> Commercial support is available at <a href=""https://www.nginx.com"">www.nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html>
$ openssl s_client -connect 0.0.0.0:3000 -CAfile root-ca.crt CONNECTED(00000003) depth=1 /C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA verify return:1 depth=0 /C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost verify return:1 --- Certificate chain 0 s:/C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost i:/C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA --- Server certificate -----BEGIN CERTIFICATE----- … -----END CERTIFICATE----- subject=/C=US/ST=CA/L=San Francisco/O=Docker/CN=localhost issuer=/C=US/ST=CA/L=San Francisco/O=Docker/CN=Swarm Secret Example CA --- No client certificate CA names sent --- SSL handshake has read 1663 bytes and written 712 bytes --- New, TLSv1/SSLv3, Cipher is AES256-SHA Server public key is 4096 bit Secure Renegotiation IS supported Compression: NONE Expansion: NONE SSL-Session: Protocol : TLSv1 Cipher : AES256-SHA Session-ID: A1A8BF35549C5715648A12FD7B7E3D861539316B03440187D9DA6C2E48822853 Session-ID-ctx: Master-Key: F39D1B12274BA16D3A906F390A61438221E381952E9E1E05D3DD784F0135FB81353DA38C6D5C021CB926E844DFC49FC4 Key-Arg : None Start Time: 1481685096 Timeout : 300 (sec) Verify return code: 0 (ok)
Unless you are going to continue to the next example, clean up after running this example by removing the
nginx
service and the stored secrets and config.$ docker service rm nginx $ docker secret rm site.crt site.key $ docker config rm site.conf
You have now configured a Nginx service with its configuration decoupled from its image. You could run multiple sites with exactly the same image but separate configurations, without the need to build a custom image at all.
Example: Rotate a config
To rotate a config, you first save a new config with a different name than the
one that is currently in use. You then redeploy the service, removing the old
config and adding the new config at the same mount point within the container.
This example builds upon the previous one by rotating the site.conf
configuration file.
Edit the
site.conf
file locally. Addindex.php
to theindex
line, and save the file.server { listen 443 ssl; server_name localhost; ssl_certificate /run/secrets/site.crt; ssl_certificate_key /run/secrets/site.key; location / { root /usr/share/nginx/html; index index.html index.htm index.php; } }
Create a new Docker config using the new
site.conf
, calledsite-v2.conf
.$ docker config create site-v2.conf site.conf
Update the
nginx
service to use the new config instead of the old one.$ docker service update \ --config-rm site.conf \ --config-add source=site-v2.conf,target=/etc/nginx/conf.d/site.conf,mode=0440 \ nginx
Verify that the
nginx
service is fully re-deployed, usingdocker service ps nginx
. When it is, you can remove the oldsite.conf
config.$ docker config rm site.conf
To clean up, you can remove the
nginx
service, as well as the secrets and configs.$ docker service rm nginx $ docker secret rm site.crt site.key $ docker config rm site-v2.conf
You have now updated your nginx
service's configuration without the need to
rebuild its image.",,,
b9f175c108dfada883130a0d4ac7fec3a96e557dfca19d710908e4ffcd679f08,"Docker Engine 24.0 release notes
This page describes the latest changes, additions, known issues, and fixes for Docker Engine version 24.0.
For more information about:
- Deprecated and removed features, see Deprecated Engine Features.
- Changes to the Engine API, see Engine API version history.
24.0.9
2024-01-31For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Security
This release contains security fixes for the following CVEs affecting Docker Engine and its components.
| CVE | Component | Fix version | Severity |
|---|---|---|---|
| CVE-2024-21626 | runc | 1.1.12 | High, CVSS 8.6 |
| CVE-2024-24557 | Docker Engine | 24.0.9 | Medium, CVSS 6.9 |
Important
Note that this release of Docker Engine doesn't include fixes for the following known vulnerabilities in BuildKit:
To address these vulnerabilities, upgrade to Docker Engine v25.0.2.
For more information about the security issues addressed in this release, and the unaddressed vulnerabilities in BuildKit, refer to the blog post.
For details about each vulnerability, see the relevant security advisory:
Packaging updates
- Upgrade runc to v1.1.12. moby/moby#47269
- Upgrade containerd to v1.7.13 (static binaries only). moby/moby#47280
24.0.8
2024-01-25For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- Live restore: Containers with auto remove (
docker run --rm
) are no longer forcibly removed on engine restart. moby/moby#46857
Packaging updates
- Upgrade Go to
go1.20.13
. moby/moby#47054, docker/cli#4826, docker/docker-ce-packaging#975 - Upgrade containerd (static binaries only) to v1.7.12 moby/moby#47096
- Upgrade runc to v1.1.11. moby/moby#47010
24.0.7
2023-10-27For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- Write overlay2 layer metadata atomically. moby/moby#46703
- Fix ""Rootful-in-Rootless"" Docker-in-Docker on systemd version 250 and later. moby/moby#46626
- Fix
dockerd-rootless-setuptools.sh
when username contains a backslash. moby/moby#46407 - Fix a bug that would prevent network sandboxes to be fully deleted when stopping containers with no network attachments and when
dockerd --bridge=none
is used. moby/moby#46702 - Fix a bug where cancelling an API request could interrupt container restart. moby/moby#46697
- Fix an issue where containers would fail to start when providing
--ip-range
with a range larger than the subnet. docker/for-mac#6870 - Fix data corruption with zstd output. moby/moby#46709
- Fix the conditions under which the container's MAC address is applied. moby/moby#46478
- Improve the performance of the stats collector. moby/moby#46448
- Fix an issue with source policy rules ending up in the wrong order. moby/moby#46441
Packaging updates
- Add support for Fedora 39 and Ubuntu 23.10. docker/docker-ce-packaging#940, docker/docker-ce-packaging#955
- Fix
docker.socket
not getting disabled when uninstalling thedocker-ce
RPM package. docker/docker-ce-packaging#852 - Upgrade Go to
go1.20.10
. docker/docker-ce-packaging#951 - Upgrade containerd to
v1.7.6
(static binaries only). moby/moby#46103 - Upgrade the
containerd.io
package tov1.6.24
.
Security
Deny containers access to
/sys/devices/virtual/powercap
by default. This change hardens against CVE-2020-8694, CVE-2020-8695, and CVE-2020-12912, and an attack known as the PLATYPUS attack.
24.0.6
2023-09-05For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- containerd storage backend: Fix
docker ps
failing when a container image is no longer present in the content store. moby/moby#46095 - containerd storage backend: Fix
docker ps -s -a
anddocker container prune
failing when a container image config is no longer present in the content store. moby/moby#46097 - containerd storage backend: Fix
docker inspect
failing when a container image config is no longer (or was never) present in the content store. moby/moby#46244 - containerd storage backend: Fix diff and export with the
overlayfs
snapshotter by using reference-counted rootfs mounts. moby/moby#46266 - containerd storage backend: Fix a misleading error message when the image platforms available locally do not match the desired platform. moby/moby#46300
- containerd storage backend: Fix the
FROM scratch
Dockerfile instruction with the classic builder. moby/moby#46302 - containerd storage backend: Fix
mismatched image rootfs and manifest layers
errors with the classic builder. moby/moby#46310 - Warn when pulling Docker Image Format v1, and Docker Image manifest version 2, schema 1 images from all registries. moby/moby#46290
- Fix live-restore of volumes with custom volume options. moby/moby#46366
- Fix incorrectly dropping capabilities bits when running a container as a non-root user (note: this change was already effectively present due to a regression). moby/moby#46221
- Fix network isolation iptables rules preventing IPv6 Neighbor Solicitation packets from being exchanged between containers. moby/moby#46214
- Fix
dockerd.exe --register-service
not working when the binary is in the current directory on Windows. moby/moby#46215 - Add a hint suggesting the use of a PAT to
docker login
against Docker Hub. docker/cli#4500 - Improve shell startup time for users of Bash completion for the CLI. docker/cli#4517
- Improve the speed of some commands by skipping
GET /_ping
when possible. docker/cli#4508 - Fix credential scopes when using a PAT to
docker manifest inspect
an image on Docker Hub. docker/cli#4512 - Fix
docker events
not supporting--format=json
. docker/cli#4544
Packaging updates
- Upgrade Go to
go1.20.7
. moby/moby#46140, docker/cli#4476, docker/docker-ce-packaging#932 - Upgrade containerd to
v1.7.3
(static binaries only). moby/moby#46103 - Upgrade Compose to
v2.21.0
. docker/docker-ce-packaging#936
24.0.5
2023-07-24For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- The Go client now avoids using UNIX socket paths in the HTTP
Host:
header, in order to be compatible with changes introduced ingo1.20.6
. moby/moby#45962, moby/moby#45990 - containerd storage backend: Fix
Variant
not being included indocker image inspect
andGET /images/{name}/json
. moby/moby#46025 - containerd storage backend: Prevent potential garbage collection of content during image export. moby/moby#46021
- containerd storage backend: Prevent duplicate digest entries in
RepoDigests
. moby/moby#46014 - containerd storage backend: Fix operations taking place against the incorrect tag when working with an image referenced by tag and digest. moby/moby#46013
- containerd storage backend: Fix a panic caused by
EXPOSE
when building containers with the legacy builder. moby/moby#45921 - Fix a regression causing unintuitive errors to be returned when attempting to create an
overlay
network on a non-Swarm node. moby/moby#45974 - Properly report errors parsing volume specifications from the command line. docker/cli#4423
- Fix a panic caused when
auths: null
is found in the CLI config file. docker/cli#4450
Packaging updates
- Use init scripts as provided by in moby/moby
contrib/init
. docker/docker-ce-packaging#914, docker/docker-ce-packaging#926 - Drop Upstart from
contrib/init
. moby/moby#46044 - Upgrade Go to
go1.20.6
. docker/cli#4428, moby/moby#45970, docker/docker-ce-packaging#921 - Upgrade Compose to
v2.20.2
. docker/docker-ce-packaging#924 - Upgrade buildx to
v0.11.2
. docker/docker-ce-packaging#922
24.0.4
2023-07-07For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- Fix a regression introduced during 24.0.3 that causes a panic during live-restore of containers with bind mounts. moby/moby#45903
24.0.3
2023-07-06For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- containerd image store: Fix an issue where multi-platform images that did not include a manifest for the default platform could not be interacted with. moby/moby#45849
- containerd image store: Fix specious attempts to cache
FROM scratch
in container builds. moby/moby#45822 - containerd image store: Fix
docker cp
with snapshotters that cannot mount the same content multiple times. moby/moby#45780, moby/moby#45786 - containerd image store: Fix builds with
type=image
not being correctly unpacked/stored. moby/moby#45692 - containerd image store: Fix incorrectly attempting to unpack pseudo-images (including attestations) in
docker load
. moby/moby#45688 - containerd image store: Correctly set the user agent, and include additional information like the snapshotter when interacting with registries. moby/moby#45671, moby/moby#45684
- containerd image store: Fix a failure to unpack already-pulled content after switching between snapshotters. moby/moby#45678
- containerd image store: Fix images that have been re-tagged or with all tags removed being pruned while still in use. moby/moby#45857
- Fix a Swarm CSI issue where the Topology field was not propagated into NodeCSIInfo. moby/moby#45810
- Fix failures to add new Swarm managers caused by a very large raft log. moby/moby#45703, moby/swarmkit#3122, moby/swarmkit#3128
name_to_handle_at(2)
is now always allowed in the default seccomp profile. moby/moby#45833- Fix an issue that prevented encrypted Swarm overlay networks from working on ports other than the default (4789). moby/moby#45637
- Fix a failure to restore mount reference-counts during live-restore. moby/moby#45824
- Fix various networking-related failures during live-restore. moby/moby#45658, moby/moby#45659
- Fix running containers restoring with a zero (successful) exit status when the daemon is unexpectedly terminated. moby/moby#45801
- Fix a potential panic while executing healthcheck probes. moby/moby#45798
- Fix a panic caused by a race condition in container exec start. moby/moby#45794
- Fix an exception caused by attaching a terminal to an exec with a non-existent command. moby/moby#45643
- Fix
host-gateway
with BuildKit by passing the IP as a label (also requires docker/buildx#1894). moby/moby#45790 - Fix an issue where
POST /containers/{id}/stop
would forcefully terminate the container when the request was canceled, instead of waiting until the specified timeout for a 'graceful' stop. moby/moby#45774 - Fix an issue where
docker cp -a
from the root (/
) directory would fail. moby/moby#45748 - Improve compatibility with non-runc container runtimes by more correctly setting resource constraint parameters in the OCI config. moby/moby#45746
- Fix an issue caused by overlapping subuid/subgid ranges in certain configurations (e.g. LDAP) in rootless mode. moby/moby#45747, rootless-containers/rootlesskit#369
- Greatly reduce CPU and memory usage while populating the Debug section of
GET /info
. moby/moby#45856 - Fix an issue where debug information was not correctly printed during
docker info
when only the client is in debug mode. docker/cli#4393 - Fix issues related to hung connections when connecting to hosts over a SSH connection. docker/cli#4395
Packaging updates
- Upgrade Go to
go1.20.5
. moby/moby#45745, docker/cli#4351, docker/docker-ce-packaging#904 - Upgrade Compose to
v2.19.1
. docker/docker-ce-packaging#916 - Upgrade buildx to
v0.11.1
. docker/docker-ce-packaging#918
24.0.2
2023-05-26For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Bug fixes and enhancements
- Fix a panic during build when referencing locally tagged images. moby/buildkit#3899, moby/moby#45582
- Fix builds potentially failing with
exit code: 4294967295
when performing many concurrent build stages. moby/moby#45620 - Fix DNS resolution on Windows ignoring
etc/hosts
(%WINDIR%\System32\Drivers\etc\hosts
), including resolution oflocalhost
. moby/moby#45562 - Apply a workaround for a containerd bug that causes concurrent
docker exec
commands to take significantly longer than expected. moby/moby#45625 - containerd image store: Fix an issue where the image
Created
field would contain an incorrect value. moby/moby#45623 - containerd image store: Adjust the output of image pull progress so that the output has the same format regardless of whether the containerd image store is enabled. moby/moby#45602
- containerd image store: Switching between the default and containerd image store now requires a daemon restart. moby/moby#45616
Packaging updates
- Upgrade Buildx to
v0.10.5
. docker/docker-ce-packaging#900
24.0.1
2023-05-19For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
Removed
- Remove CLI completions for storage drivers removed in the 24.0 major release. docker/cli#4302
Bug fixes and enhancements
- Fix an issue where DNS query NXDOMAIN replies from external servers were forwarded to the client as SERVFAIL. moby/moby#45573
- Fix an issue where
docker pull --platform
would reportNo such image
regarding another tag pointing to the same image. moby/moby#45562 - Fix an issue where insecure registry configuration would be forgotten during config reload. moby/moby#45571
- containerd image store: Fix an issue where images which have no layers would not be listed in
docker images -a
moby/moby#45588 - API: Fix an issue where
GET /images/{id}/json
would returnnull
instead of emptyRepoTags
andRepoDigests
. moby/moby#45564 - API: Fix an issue where
POST /commit
did not accept an empty request body. moby/moby#45568
Packaging updates
- Upgrade Compose to
v2.18.1
. docker/docker-ce-packaging#896
24.0.0
2023-05-16For a full list of pull requests and changes in this release, refer to the relevant GitHub milestones:
New
- Introduce experimental support for containerd as the content store (replacing the existing storage drivers). moby/moby#43735, other moby/moby pull requests
- The
--host
CLI flag now supports a path component in assh://
host address, allowing use of an alternate socket path without configuration on the remote host. docker/cli#4073 - The
docker info
CLI command now reports a version and platform field. docker/cli#4180 - Introduce the daemon flag
--default-network-opt
to configure options for newly created networks. moby/moby#43197 - Restrict access to
AF_VSOCK
in thesocket(2)
family of syscalls in the default seccomp profile. moby/moby#44562 - Introduce support for setting OCI runtime annotations on containers. docker/cli#4156, moby/moby#45025
- Alternative runtimes can now be configured in
daemon.json
, enabling runtime names to be aliased and options to be passed. moby/moby#45032 - The
docker-init
binary will now be discovered in FHS-compliant libexec directories, in addition to thePATH
. moby/moby#45198 - API: Surface the daemon-level
--no-new-privileges
inGET /info
. moby/moby#45320
Removed
docker info
no longer reportsIndexServiceAddress
. docker/cli#4204- libnetwork: Remove fallback code for obsolete kernel versions. moby/moby#44684, moby/moby#44802
- libnetwork: Remove unused code related to classic Swarm. moby/moby#44965
- libnetwork: Remove usage of the
xt_u32
kernel module from encrypted Swarm overlay networks. moby/moby#45281 - Remove support for BuildKit's deprecated
buildinfo
in favor of standard provenance attestations. moby/moby#45097 - Remove the deprecated AUFS and legacy
overlay
storage drivers. moby/moby#45342, moby/moby#45359 - Remove the deprecated
overlay2.override_kernel_check
storage driver option. moby/moby#45368 - Remove workarounds for obsolete versions of
apparmor_parser
from the AppArmor profiles. moby/moby#45500 - API:
GET /images/json
no longer represents empty RepoTags and RepoDigests as<none>:<none>
/<none>@<none>
. Empty arrays are returned instead on API >= 1.43. moby/moby#45068
Deprecated
- Deprecate the
--oom-score-adjust
daemon option. moby/moby#45315 - API: Deprecate the
VirtualSize
field inGET /images/json
andGET /images/{id}/json
. moby/moby#45346
Bug fixes and enhancements
- The
docker stack
command no longer validates thebuild
section of Compose files. docker/cli#4214 - Fix lingering healthcheck processes after the timeout is reached. moby/moby#43739
- Reduce the overhead of container startup when using the
overlay2
storage driver. moby/moby#44285 - API: Handle multiple
before=
andsince=
filters inGET /images
. moby/moby#44503 - Fix numerous bugs in the embedded DNS resolver implementation used by user-defined networks. moby/moby#44664
- Add
execDuration
field to the map of event attributes. moby/moby#45494 - Swarm-level networks can now be created with the Windows
internal
,l2bridge
, andnat
drivers. moby/swarmkit#3121, moby/moby#45291
Packaging updates
- Update Go to
1.20.4
. docker/cli#4253, moby/moby#45456, docker/docker-ce-packaging#888 - Update
containerd
tov1.7.1
. moby/moby#45537 - Update
buildkit
tov0.11.6
. moby/moby#45367",,,
78825dc076c250f703d60fa78b0a9653e11d16363ff7e9f22e1876de4295d2f0,"Create an SSO connection
Creating a single sign-on (SSO) connection requires setting up the connection in Docker first, followed by setting up the connection in your identity provider (IdP). This guide provides steps for setting up your SSO connection in Docker and your IdP.
Tip
This guide requires copying and pasting values in both Docker and your IdP. To ensure a seamless connection process, complete all the steps in this guide in one session and keep separate browsers open for both Docker and your IdP.
Prerequisites
Make sure you have completed the following before you begin:
- Your domain is verified
- You have an account set up with an IdP
- You have completed the steps in the Configure single sign-on guide
Step one: Create an SSO connection in Docker
Note
Before creating an SSO connection in Docker, you must verify at least one domain.
- Sign in to the Admin Console.
- Select your organization or company from the Choose profile page. Note that when an organization is part of a company, you must select the company and configure the domain for the organization at the company level.
- Under Security and access, select SSO and SCIM.
- Select Create Connection and provide a name for the connection.
- Select an authentication method, SAML or Azure AD (OIDC).
- Copy the following fields to add to your IdP:
- Okta SAML: Entity ID, ACS URL
- Azure OIDC: Redirect URL
- Keep this window open so you can paste the connection information from your IdP here at the end of this guide.
- Sign in to Docker Hub.
- Select Organizations and then your organization from the list.
- On your organization page, select Settings and then Security.
- In the SSO connection table, select Create Connection and provide a name for the connection.
- Select an authentication method, SAML or Azure AD (OIDC).
- Copy the following fields to add to your IdP:
- Okta SAML: Entity ID, ACS URL
- Azure OIDC: Redirect URL
- Keep this window open so you can paste the connection information from your IdP here at the end of this guide.
Step two: Create an SSO connection in your IdP
The user interface for your IdP may differ slightly from the following steps. Refer to the documentation for your IdP to verify.
- Sign in to your Okta account.
- Select Admin to open the Okta Admin portal.
- From the left-hand navigation, select Administration.
- Select Administration and then Create App Integration.
- Select SAML 2.0 and then Next.
- Enter ""Docker Hub"" as your App Name.
- Optional. Upload a logo.
- Select Next.
- Enter the following values from Docker into their corresponding Okta fields:
- Docker ACS URL: Single Sign On URL
- Docker Entity ID: Audience URI (SP Entity ID)
- Configure the following settings in Okta:
- Name ID format:
EmailAddress
- Application username:
Email
- Update application on:
Create and update
- Name ID format:
- Select Next.
- Select the This is an internal app that we have created checkbox.
- Select Finish.
- Sign in to your Azure AD admin portal.
- Select Default Directory and then Add.
- Choose Enterprise Application and select Create your own application.
- Enter ""Docker"" for application name and select the non-gallery option.
- After the application is created, go to Single Sign-On and select SAML.
- Select Edit on the Basic SAML configuration section.
- Enter the following values from Docker into their corresponding Azure fields:
- Docker Entity ID: Identifier
- Docker ACS URL: Reply URL
- Save configuration.
- From the SAML Signing Certificate section, download your Certificate (Base64).
To create an Azure Connect (OIDC) connection, you must create an app registration, client secrets, and configure API permissions for Docker:
Create app registration
- Sign in to your Azure AD admin portal.
- Select App Registration and then New Registration.
- Enter ""Docker Hub SSO"" or similar for application name.
- Under Supported account types, specify who can use this application or access the app.
- In the Redirect URI section, select Web from the drop-down menu and paste the Redirect URI value from the Docker console into this field.
- Select Register to register the app.
- Copy the Client ID from the app's overview page. You need this information to continue configuring SSO in Docker.
Create client secrets
- Open your app in Azure AD and select Certificates & secrets.
- Select + New client secret.
- Specify the description of the secret and set how long keys can be used.
- Select Add to continue.
- Copy the secret Value field. You need this to continue configuring SSO in Docker.
Configure API permissions
- Open your app in Azure AD and navigate to your app settings.
- Select API permission and then Grant admin consent for [your tenant name].
- Select Yes to confirm.
- After confirming, select Add a permission and then Delegated permissions.
- Search for
User.Read
and select this option. - Select Add permissions to confirm.
- Verify admin consent was granted for each permission by checking the Status column.
Step three: Connect Docker and your IdP
After creating your connection in Docker and your IdP, you can cross-connect them to complete your SSO connection:
- Open your app you created in Okta and select View SAML setup instructions.
- Copy the following values from the Okta SAML setup instruction page:
- SAML Sign-in URL
- x509 Certificate
- Open Docker Hub or the Admin Console. Your SSO configuration page should still be open from Step one of this guide.
- Select Next to open the Update single-sign on connection page.
- Paste your Okta SAML Sign-in URL and x509 Certificate values in Docker.
- Select Next.
- Optional. Select a default team to provision users to and select Next.
- Verify your SSO connection details and select Create Connection.
- Open your app in Azure AD.
- Open your downloaded Certificate (Base64) in a text editor.
- Copy the following values:
- From Azure AD: Login URL
- Copy your the contents of your Certificate (Base64) file from your text editor
- Open Docker Hub or the Admin Console. Your SSO configuration page should still be open from Step one of this guide.
- Paste your Login URL and Certificate (Base64) values in Docker.
- Select Next.
- Optional. Select a default team to provision users to and select Next.
- Verify your SSO connection details and select Create Connection.
- Open Docker Hub or the Admin Console. Your SSO configuration page should still be open from Step one of this guide.
- Paste the following values from Azure AD in to Docker:
- Client ID
- Client Secret
- Azure AD Domain
- Select Next.
- Optional. Select a default team to provision users to and select Next.
- Verify your SSO connection details and select Create Connection.
Step four: Test your connection
After you've completed the SSO connection process in Docker, we recommend testing it:
- Open an incognito browser.
- Sign in to the Admin Console using your domain email address.
- The browser will redirect to your IdP's login page to authenticate.
- Authenticate through your domain email instead of using your Docker ID.
You can also test your SSO connection through the command-line interface (CLI). If you want to test through the CLI, your users must have a personal access token (PAT).
Optional: Enforce SSO
Important
If SSO isn't enforced, users can choose to sign in with either their Docker username and password or SSO.
Enforcing SSO requires users to use SSO when signing into Docker. This centralizes authentication and enforces policies set by the IdP.
- Sign in to the Admin Console.
- Select your organization or company from the Choose profile page. Note that when an organization is part of a company, you must select the company and configure the domain for the organization at the company level.
- Under Security and access, select SSO and SCIM.
- In the SSO connections table, select the Action icon and then Enable enforcement. When SSO is enforced, your users are unable to modify their email address and password, convert a user account to an organization, or set up 2FA through Docker Hub. If you want to use 2FA, you must enable 2FA through your IdP.
- Continue with the on-screen instructions and verify you've completed all tasks.
- Select Turn on enforcement to complete.
Your users must now sign in to Docker with SSO.
Note
When SSO is enforced, users can't use passwords to access the Docker CLI. Users must use a personal access token (PAT) for authentication to access the Docker CLI.
More resources
The following videos demonstrate how to enforce SSO.",,,
797b2117995924a0a8aba586b870a05eddf0743b2d5581c968fe42c749aad9fb,"Docker Engine 17.11 release notes
Table of contents
17.11.0-ce
2017-11-20
Important
Docker CE 17.11 is the first Docker release based on containerd 1.0 beta. Docker CE 17.11 and later don't recognize containers started with previous Docker versions. If you use Live Restore, you must stop all containers before upgrading to Docker CE 17.11. If you don't, any containers started by Docker versions that predate 17.11 aren't recognized by Docker after the upgrade and keep running, un-managed, on the system.
Builder
- Test & Fix build with rm/force-rm matrix moby/moby#35139
- Fix build with
--stream
with a large context moby/moby#35404
Client
- Hide help flag from help output docker/cli#645
- Support parsing of named pipes for compose volumes docker/cli#560
- [Compose] Cast values to expected type after interpolating values docker/cli#601
- Add output for ""secrets"" and ""configs"" on
docker stack deploy
docker/cli#593
- Fix flag description for
--host-add
docker/cli#648
- Do not truncate ID on docker service ps --quiet docker/cli#579
Deprecation
- Update bash completion and deprecation for synchronous service updates docker/cli#610
Logging
- copy to log driver's bufsize, fixes #34887 moby/moby#34888
- Add TCP support for GELF log driver moby/moby#34758
- Add credentials endpoint option for awslogs driver moby/moby#35055
Networking
- Fix network name masking network ID on delete moby/moby#34509
- Fix returned error code for network creation from 500 to 409 moby/moby#35030
- Fix tasks fail with error ""Unable to complete atomic operation, key modified"" docker/libnetwork#2004
Runtime
- Switch to Containerd 1.0 client moby/moby#34895
- Increase container default shutdown timeout on Windows moby/moby#35184
- LCOW: API: Add
platform
to /images/create and /build moby/moby#34642 - Stop filtering Windows manifest lists by version moby/moby#35117
- Use windows console mode constants from Azure/go-ansiterm moby/moby#35056
- Windows Daemon should respect DOCKER_TMPDIR moby/moby#35077
- Windows: Fix startup logging moby/moby#35253
- Add support for Windows version filtering on pull moby/moby#35090
- Fixes LCOW after containerd 1.0 introduced regressions moby/moby#35320
- ContainerWait on remove: don't stuck on rm fail moby/moby#34999
- oci: obey CL_UNPRIVILEGED for user namespaced daemon moby/moby#35205
- Don't abort when setting may_detach_mounts moby/moby#35172
- Fix panic on get container pid when live restore containers moby/moby#35157
- Mask
/proc/scsi
path for containers to prevent removal of devices (CVE-2017-16539) moby/moby#35399
- Update to github.com/vbatts/tar-split@v0.10.2 (CVE-2017-14992) moby/moby#35424
Swarm Mode
- Modifying integration test due to new ipam options in swarmkit moby/moby#35103
- Fix deadlock on getting swarm info moby/moby#35388
- Expand the scope of the
Err
field inTaskStatus
to also cover non-terminal errors that block the task from progressing docker/swarmkit#2287
Packaging
- Build packages for Debian 10 (Buster) docker/docker-ce-packaging#50
- Build packages for Ubuntu 17.10 (Artful) docker/docker-ce-packaging#55",,,
d3b0c786e11e89ded4edb9cd950c6c5686ac90475e16606c9dddceac0ee3942e,"Build secrets
A build secret is any piece of sensitive information, such as a password or API token, consumed as part of your application's build process.
Build arguments and environment variables are inappropriate for passing secrets to your build, because they persist in the final image. Instead, you should use secret mounts or SSH mounts, which expose secrets to your builds securely.
Types of build secrets
- Secret mounts are general-purpose mounts for passing secrets into your build. A secret mount takes a secret from the build client and makes it temporarily available inside the build container, for the duration of the build instruction. This is useful if, for example, your build needs to communicate with a private artifact server or API.
- SSH mounts are special-purpose mounts for making SSH sockets or keys available inside builds. They're commonly used when you need to fetch private Git repositories in your builds.
- Git authentication for remote contexts is a set of pre-defined secrets for when you build with a remote Git context that's also a private repository. These secrets are ""pre-flight"" secrets: they are not consumed within your build instruction, but they're used to provide the builder with the necessary credentials to fetch the context.
Using build secrets
For secret mounts and SSH mounts, using build secrets is a two-step process.
First you need to pass the secret into the docker build
command, and then you
need to consume the secret in your Dockerfile.
To pass a secret to a build, use the
docker build --secret
flag, or the
equivalent options for
Bake.
$ docker build --secret id=aws,src=$HOME/.aws/credentials .
variable ""HOME"" {
default = null
}
target ""default"" {
secret = [
""id=aws,src=${HOME}/.aws/credentials""
]
}
To consume a secret in a build and make it accessible to the RUN
instruction,
use the
--mount=type=secret
flag in the Dockerfile.
RUN --mount=type=secret,id=aws \
AWS_SHARED_CREDENTIALS_FILE=/run/secrets/aws \
aws s3 cp ...
Secret mounts
Secret mounts expose secrets to the build containers, as files or environment variables. You can use secret mounts to pass sensitive information to your builds, such as API tokens, passwords, or SSH keys.
Sources
The source of a secret can be either a
file or an
environment variable.
When you use the CLI or Bake, the type can be detected automatically. You can
also specify it explicitly with type=file
or type=env
.
The following example mounts the environment variable KUBECONFIG
to secret ID kube
,
as a file in the build container at /run/secrets/kube
.
$ docker build --secret id=kube,env=KUBECONFIG .
When you use secrets from environment variables, you can omit the env
parameter
to bind the secret to a file with the same name as the variable.
In the following example, the value of the API_TOKEN
variable
is mounted to /run/secrets/API_TOKEN
in the build container.
$ docker build --secret id=API_TOKEN .
Target
When consuming a secret in a Dockerfile, the secret is mounted to a file by
default. The default file path of the secret, inside the build container, is
/run/secrets/<id>
. You can customize how the secrets get mounted in the build
container using the target
and env
options for the RUN --mount
flag in
the Dockerfile.
The following example takes secret id aws
and mounts it to a file at
/run/secrets/aws
in the build container.
RUN --mount=type=secret,id=aws \
AWS_SHARED_CREDENTIALS_FILE=/run/secrets/aws \
aws s3 cp ...
To mount a secret as a file with a different name, use the target
option in
the --mount
flag.
RUN --mount=type=secret,id=aws,target=/root/.aws/credentials \
aws s3 cp ...
To mount a secret as an environment variable instead of a file, use the
env
option in the --mount
flag.
RUN --mount=type=secret,id=aws-key-id,env=AWS_ACCESS_KEY_ID \
--mount=type=secret,id=aws-secret-key,env=AWS_SECRET_ACCESS_KEY \
--mount=type=secret,id=aws-session-token,env=AWS_SESSION_TOKEN \
aws s3 cp ...
It's possible to use the target
and env
options together to mount a secret
as both a file and an environment variable.
SSH mounts
If the credential you want to use in your build is an SSH agent socket or key, you can use the SSH mount instead of a secret mount. Cloning private Git repositories is a common use case for SSH mounts.
The following example clones a private GitHub repository using a Dockerfile SSH mount.
# syntax=docker/dockerfile:1
FROM alpine
ADD git@github.com:me/myprivaterepo.git /src/
To pass an SSH socket the build, you use the
docker build --ssh
flag, or equivalent
options for
Bake.
$ docker buildx build --ssh default .
Git authentication for remote contexts
BuildKit supports two pre-defined build secrets, GIT_AUTH_TOKEN
and
GIT_AUTH_HEADER
. Use them to specify HTTP authentication parameters when
building with remote, private Git repositories, including:
- Building with a private Git repository as build context
- Fetching private Git repositories in a build with
ADD
For example, say you have a private GitLab project at
https://gitlab.com/example/todo-app.git
, and you want to run a build using
that repository as the build context. An unauthenticated docker build
command
fails because the builder isn't authorized to pull the repository:
$ docker build https://gitlab.com/example/todo-app.git
[+] Building 0.4s (1/1) FINISHED
=> ERROR [internal] load git source https://gitlab.com/example/todo-app.git
------
> [internal] load git source https://gitlab.com/example/todo-app.git:
0.313 fatal: could not read Username for 'https://gitlab.com': terminal prompts disabled
------
To authenticate the builder to the Git server, set the GIT_AUTH_TOKEN
environment variable to contain a valid GitLab access token, and pass it as a
secret to the build:
$ GIT_AUTH_TOKEN=$(cat gitlab-token.txt) docker build \
--secret id=GIT_AUTH_TOKEN \
https://gitlab.com/example/todo-app.git
The GIT_AUTH_TOKEN
also works with ADD
to fetch private Git repositories as
part of your build:
FROM alpine
ADD https://gitlab.com/example/todo-app.git /src
HTTP authentication scheme
By default, Git authentication over HTTP uses the Bearer authentication scheme:
Authorization: Bearer <GIT_AUTH_TOKEN>
If you need to use a Basic scheme, with a username and password, you can set
the GIT_AUTH_HEADER
build secret:
$ export GIT_AUTH_TOKEN=$(cat gitlab-token.txt)
$ export GIT_AUTH_HEADER=basic
$ docker build \
--secret id=GIT_AUTH_TOKEN \
--secret id=GIT_AUTH_HEADER \
https://gitlab.com/example/todo-app.git
BuildKit currently only supports the Bearer and Basic schemes.
Multiple hosts
You can set the GIT_AUTH_TOKEN
and GIT_AUTH_HEADER
secrets on a per-host
basis, which lets you use different authentication parameters for different
hostnames. To specify a hostname, append the hostname as a suffix to the secret
ID:
$ export GITLAB_TOKEN=$(cat gitlab-token.txt)
$ export GERRIT_TOKEN=$(cat gerrit-username-password.txt)
$ export GERRIT_SCHEME=basic
$ docker build \
--secret id=GIT_AUTH_TOKEN.gitlab.com,env=GITLAB_TOKEN \
--secret id=GIT_AUTH_TOKEN.gerrit.internal.example,env=GERRIT_TOKEN \
--secret id=GIT_AUTH_HEADER.gerrit.internal.example,env=GERRIT_SCHEME \
https://gitlab.com/example/todo-app.git",,,
cac9bee38ab4a746c2a28dd4cb03c23f33a6256ed5eae724e69db96dd3c60a73,"Local file logging driver
The local
logging driver captures output from container's stdout/stderr and
writes them to an internal storage that's optimized for performance and disk
use.
By default, the local
driver preserves 100MB of log messages per container and
uses automatic compression to reduce the size on disk. The 100MB default value is based on a 20M default size
for each file and a default count of 5 for the number of such files (to account for log rotation).
Warning
The
local
logging driver uses file-based storage. These files are designed to be exclusively accessed by the Docker daemon. Interacting with these files with external tools may interfere with Docker's logging system and result in unexpected behavior, and should be avoided.
Usage
To use the local
driver as the default logging driver, set the log-driver
and log-opt
keys to appropriate values in the daemon.json
file, which is
located in /etc/docker/
on Linux hosts or
C:\ProgramData\docker\config\daemon.json
on Windows Server. For more about
configuring Docker using daemon.json
, see
daemon.json.
The following example sets the log driver to local
and sets the max-size
option.
{
""log-driver"": ""local"",
""log-opts"": {
""max-size"": ""10m""
}
}
Restart Docker for the changes to take effect for newly created containers. Existing containers don't use the new logging configuration automatically.
You can set the logging driver for a specific container by using the
--log-driver
flag to docker container create
or docker run
:
$ docker run \
--log-driver local --log-opt max-size=10m \
alpine echo hello world
Note that local
is a bash reserved keyword, so you may need to quote it in scripts.
Options
The local
logging driver supports the following logging options:
| Option | Description | Example value |
|---|---|---|
max-size | The maximum size of the log before it's rolled. A positive integer plus a modifier representing the unit of measure (k , m , or g ). Defaults to 20m. | --log-opt max-size=10m |
max-file | The maximum number of log files that can be present. If rolling the logs creates excess files, the oldest file is removed. A positive integer. Defaults to 5. | --log-opt max-file=3 |
compress | Toggle compression of rotated log files. Enabled by default. | --log-opt compress=false |
Examples
This example starts an alpine
container which can have a maximum of 3 log
files no larger than 10 megabytes each.
$ docker run -it --log-driver local --log-opt max-size=10m --log-opt max-file=3 alpine ash",,,
196d11451f95637e8ec7e5f942976652f9ce8fdf0d43f183b691206edcc6f339,"Docker Build Cloud
Docker Build Cloud is a service that lets you build your container images faster, both locally and in CI. Builds run on cloud infrastructure optimally dimensioned for your workloads, no configuration required. The service uses a remote build cache, ensuring fast builds anywhere and for all team members.
How Docker Build Cloud works
Using Docker Build Cloud is no different from running a regular build. You invoke a
build the same way you normally would, using docker buildx build
. The
difference is in where and how that build gets executed.
By default when you invoke a build command, the build runs on a local instance of BuildKit, bundled with the Docker daemon. With Docker Build Cloud, you send the build request to a BuildKit instance running remotely, in the cloud. All data is encrypted in transit.
The remote builder executes the build steps, and sends the resulting build output to the destination that you specify. For example, back to your local Docker Engine image store, or to an image registry.
Docker Build Cloud provides several benefits over local builds:
- Improved build speed
- Shared build cache
- Native multi-platform builds
And the best part: you don't need to worry about managing builders or infrastructure. Just connect to your builders, and start building. Each cloud builder provisioned to an organization is completely isolated to a single Amazon EC2 instance, with a dedicated EBS volume for build cache, and encryption in transit. That means there are no shared processes or data between cloud builders.
Note
Docker Build Cloud is currently only available in the US East region. Users in Europe and Asia may experience increased latency compared to users based in North America.
Support for multi-region builders is on the roadmap.
Get Docker Build Cloud
To get started with Docker Build Cloud, create a Docker account. There are two options to get access to Docker Build Cloud:
- Users with a free Personal account can opt-in to a 7-day free trial, with the option to subscribe for access. To start your free trial, sign in to Docker Build Cloud Dashboard and follow the on-screen instructions.
- All users with a paid Docker subscription have access to Docker Build Cloud included with their Docker suite of products. See Docker subscriptions and features for more information.
Once you've signed up and created a builder, continue by setting up the builder in your local environment.
For information about roles and permissions related to Docker Build Cloud, see Roles and Permissions.",,,
7a0a914516f440203409798821670ff2efd06eefc87254be150d0af56277e19d,"Mirror the Docker Hub library
Use-case
If you have multiple instances of Docker running in your environment, such as multiple physical or virtual machines all running Docker, each daemon goes out to the internet and fetches an image it doesn't have locally, from the Docker repository. You can run a local registry mirror and point all your daemons there, to avoid this extra internet traffic.
Note
Docker Official Images are an intellectual property of Docker.
Alternatives
Alternatively, if the set of images you are using is well delimited, you can simply pull them manually and push them to a simple, local, private registry.
Furthermore, if your images are all built in-house, not using the Hub at all and relying entirely on your local registry is the simplest scenario.
Gotcha
It's currently not possible to mirror another private registry. Only the central Hub can be mirrored.
Note
Mirrors of Docker Hub are still subject to Docker's fair use policy.
Solution
The Registry can be configured as a pull through cache. In this mode a Registry responds to all normal docker pull requests but stores all content locally.
How does it work?
The first time you request an image from your local registry mirror, it pulls the image from the public Docker registry and stores it locally before handing it back to you. On subsequent requests, the local registry mirror is able to serve the image from its own storage.
What if the content changes on the Hub?
When a pull is attempted with a tag, the Registry checks the remote to ensure if it has the latest version of the requested content. Otherwise, it fetches and caches the latest content.
What about my disk?
In environments with high churn rates, stale data can build up in the cache. When running as a pull through cache the Registry periodically removes old content to save disk space. Subsequent requests for removed content causes a remote fetch and local re-caching.
To ensure best performance and guarantee correctness the Registry cache should
be configured to use the filesystem
driver for storage.
Run a Registry as a pull-through cache
The easiest way to run a registry as a pull through cache is to run the official
Registry image.
At least, you need to specify proxy.remoteurl
within /etc/docker/registry/config.yml
as described in the following subsection.
Multiple registry caches can be deployed over the same back-end. A single registry cache ensures that concurrent requests do not pull duplicate data, but this property does not hold true for a registry cache cluster.
Note
When using Docker Hub, all paid Docker subscriptions are limited to 5000 pulls per day. If you require a higher number of pulls, you can purchase an Enhanced Service Account add-on. See Service Accounts for more details.
Configure the cache
To configure a Registry to run as a pull through cache, the addition of a
proxy
section is required to the config file.
To access private images on the Docker Hub, a username and password can be supplied.
proxy:
remoteurl: https://registry-1.docker.io
username: [username]
password: [password]
Warning
If you specify a username and password, it's very important to understand that private resources that this user has access to Docker Hub is made available on your mirror. You must secure your mirror by implementing authentication if you expect these resources to stay private!
Warning
For the scheduler to clean up old entries,
delete
must be enabled in the registry configuration.
Configure the Docker daemon
Either pass the --registry-mirror
option when starting dockerd
manually,
or edit
/etc/docker/daemon.json
and add the registry-mirrors
key and value, to make the change persistent.
{
""registry-mirrors"": [""https://<my-docker-mirror-host>""]
}
Save the file and reload Docker for the change to take effect.
Note
Some log messages that appear to be errors are actually informational messages.
Check the
level
field to determine whether the message is warning you about an error or is giving you information. For example, this log message is informational:time=""2017-06-02T15:47:37Z"" level=info msg=""error statting local store, serving from upstream: unknown blob"" go.version=go1.7.4
It's telling you that the file doesn't exist yet in the local cache and is being pulled from upstream.",,,
6f5743333623e91173e31090321ac433beded91b0ebba030ba34da2bf9884582,"How to use secrets in Docker Compose
A secret is any piece of data, such as a password, certificate, or API key, that shouldn’t be transmitted over a network or stored unencrypted in a Dockerfile or in your application’s source code.
Docker Compose provides a way for you to use secrets without having to use environment variables to store information. If you’re injecting passwords and API keys as environment variables, you risk unintentional information exposure. Services can only access secrets when explicitly granted by a secrets
attribute within the services
top-level element.
Environment variables are often available to all processes, and it can be difficult to track access. They can also be printed in logs when debugging errors without your knowledge. Using secrets mitigates these risks.
Use secrets
Secrets are mounted as a file in /run/secrets/<secret_name>
inside the container.
Getting a secret into a container is a two-step process. First, define the secret using the top-level secrets element in your Compose file. Next, update your service definitions to reference the secrets they require with the secrets attribute. Compose grants access to secrets on a per-service basis.
Unlike the other methods, this permits granular access control within a service container via standard filesystem permissions.
Examples
Simple
In the following example, the frontend service is given access to the my_secret
secret. In the container, /run/secrets/my_secret
is set to the contents of the file ./my_secret.txt
.
services:
myapp:
image: myapp:latest
secrets:
- my_secret
secrets:
my_secret:
file: ./my_secret.txt
Advanced
services:
db:
image: mysql:latest
volumes:
- db_data:/var/lib/mysql
environment:
MYSQL_ROOT_PASSWORD_FILE: /run/secrets/db_root_password
MYSQL_DATABASE: wordpress
MYSQL_USER: wordpress
MYSQL_PASSWORD_FILE: /run/secrets/db_password
secrets:
- db_root_password
- db_password
wordpress:
depends_on:
- db
image: wordpress:latest
ports:
- ""8000:80""
environment:
WORDPRESS_DB_HOST: db:3306
WORDPRESS_DB_USER: wordpress
WORDPRESS_DB_PASSWORD_FILE: /run/secrets/db_password
secrets:
- db_password
secrets:
db_password:
file: db_password.txt
db_root_password:
file: db_root_password.txt
volumes:
db_data:
In the advanced example above:
- The
secrets
attribute under each service defines the secrets you want to inject into the specific container. - The top-level
secrets
section defines the variablesdb_password
anddb_root_password
and provides thefile
that populates their values. - The deployment of each container means Docker creates a temporary filesystem mount under
/run/secrets/<secret_name>
with their specific values.
Note
The
_FILE
environment variables demonstrated here are a convention used by some images, including Docker Official Images like mysql and postgres.
Build secrets
In the following example, the npm_token
secret is made available at build time. Its value is taken from the NPM_TOKEN
environment variable.
services:
myapp:
build:
secrets:
- npm_token
context: .
secrets:
npm_token:
environment: NPM_TOKEN",,,
5aebad352db53a70030183abd690aa8720d3878ddc62b496a8e9331134f934ba,"FAQs on companies
Are existing subscriptions affected when you create a company and add organizations to it?
You can manage subscriptions and related billing details at the organization level.
Some of my organizations don’t have a Docker Business subscription. Can I still use a parent company?
Yes, but you can only add organizations with a Docker Business subscription to a company.
What happens if one of my organizations downgrades from Docker Business, but I still need access as a company owner?
To access and manage child organizations, the organization must have a Docker Business subscription. If the organization isn’t included in this subscription, the owner of the organization must manage the organization outside of the company.
Does my organization need to prepare for downtime during the migration process?
No, you can continue with business as usual.
How many company owners can I add?
You can add a maximum of 10 company owners to a single company account.
Do company owners occupy a subscription seat?
Company owners don't occupy a seat in any organization unless they are added as a member of the organization. Since company owners have the same access as organization owners for all organizations associated with the company, it is not necessary to add company owners to an organization.
Note that when you first create a company, your account will be both a company owner and an organization owner. Your account will occupy a seat as long as you're an organization owner.
What permissions does the company owner have in the associated/nested organizations?
Company owners can navigate to the Organizations page to view all their nested organizations in a single location. They can also view or edit organization members and change single sign-on (SSO) and System for Cross-domain Identity Management (SCIM) settings. Changes to company settings impact all users in each organization under the company. For more information, see Roles and permissions.
What features are supported at the company level?
You can manage domain verification, SSO, and SCIM at the company level. The following features aren't supported at the company level, but you can manage them at the organization level:
- Image Access Management
- Registry Access Management
- User management
- Billing
To view and manage users across all the organizations under your company, you can manage users at the company level when you use the Admin Console.
Domain audit isn't supported for companies or organizations within a company.
What's required to create a company name?
A company name must be unique to that of its child organization. If a child organization requires the same name as a company, you should modify it slightly. For example, Docker Inc (parent company), Docker (child organization).
How does a company owner add an organization to the company?
You can add organizations to a company in the Admin Console. For more information, see Add organizations to a company.
How does a company owner manage SSO/SCIM settings for a company?
See your SCIM and SSO settings.
How does a company owner enable group mapping in an IdP?
See SCIM and group mapping for more information.
What's the definition of a company versus an organization?
A company is a collection of organizations that are managed together. An organization is a collection of repositories and teams that are managed together.",,,
43fcaa1f64ffaf6855ecac214792c3d34ea30351354cd8812808332f7ec28f68,"Kubernetes driver
The Kubernetes driver lets you connect your local development or CI environments to builders in a Kubernetes cluster to allow access to more powerful compute resources, optionally on multiple native architectures.
Synopsis
Run the following command to create a new builder, named kube
, that uses the
Kubernetes driver:
$ docker buildx create \
--bootstrap \
--name=kube \
--driver=kubernetes \
--driver-opt=[key=value,...]
The following table describes the available driver-specific options that you
can pass to --driver-opt
:
| Parameter | Type | Default | Description |
|---|---|---|---|
image | String | Sets the image to use for running BuildKit. | |
namespace | String | Namespace in current Kubernetes context | Sets the Kubernetes namespace. |
default-load | Boolean | false | Automatically load images to the Docker Engine image store. |
replicas | Integer | 1 | Sets the number of Pod replicas to create. See scaling BuildKit |
requests.cpu | CPU units | Sets the request CPU value specified in units of Kubernetes CPU. For example requests.cpu=100m or requests.cpu=2 | |
requests.memory | Memory size | Sets the request memory value specified in bytes or with a valid suffix. For example requests.memory=500Mi or requests.memory=4G | |
requests.ephemeral-storage | Storage size | Sets the request ephemeral-storage value specified in bytes or with a valid suffix. For example requests.ephemeral-storage=2Gi | |
limits.cpu | CPU units | Sets the limit CPU value specified in units of Kubernetes CPU. For example requests.cpu=100m or requests.cpu=2 | |
limits.memory | Memory size | Sets the limit memory value specified in bytes or with a valid suffix. For example requests.memory=500Mi or requests.memory=4G | |
limits.ephemeral-storage | Storage size | Sets the limit ephemeral-storage value specified in bytes or with a valid suffix. For example requests.ephemeral-storage=100M | |
nodeselector | CSV string | Sets the pod's nodeSelector label(s). See
node assignment. | |
annotations | CSV string | Sets additional annotations on the deployments and pods. | |
labels | CSV string | Sets additional labels on the deployments and pods. | |
tolerations | CSV string | Configures the pod's taint toleration. See node assignment. | |
serviceaccount | String | Sets the pod's serviceAccountName . | |
schedulername | String | Sets the scheduler responsible for scheduling the pod. | |
timeout | Time | 120s | Set the timeout limit that determines how long Buildx will wait for pods to be provisioned before a build. |
rootless | Boolean | false | Run the container as a non-root user. See rootless mode. |
loadbalance | String | sticky | Load-balancing strategy (sticky or random ). If set to sticky , the pod is chosen using the hash of the context path. |
qemu.install | Boolean | false | Install QEMU emulation for multi platforms support. See QEMU. |
qemu.image | String | tonistiigi/binfmt:latest | Sets the QEMU emulation image. See QEMU. |
Scaling BuildKit
One of the main advantages of the Kubernetes driver is that you can scale the number of builder replicas up and down to handle increased build load. Scaling is configurable using the following driver options:
replicas=N
This scales the number of BuildKit pods to the desired size. By default, it only creates a single pod. Increasing the number of replicas lets you take advantage of multiple nodes in your cluster.
requests.cpu
,requests.memory
,requests.ephemeral-storage
,limits.cpu
,limits.memory
,limits.ephemeral-storage
These options allow requesting and limiting the resources available to each BuildKit pod according to the official Kubernetes documentation here.
For example, to create 4 replica BuildKit pods:
$ docker buildx create \
--bootstrap \
--name=kube \
--driver=kubernetes \
--driver-opt=namespace=buildkit,replicas=4
Listing the pods, you get this:
$ kubectl -n buildkit get deployments
NAME READY UP-TO-DATE AVAILABLE AGE
kube0 4/4 4 4 8s
$ kubectl -n buildkit get pods
NAME READY STATUS RESTARTS AGE
kube0-6977cdcb75-48ld2 1/1 Running 0 8s
kube0-6977cdcb75-rkc6b 1/1 Running 0 8s
kube0-6977cdcb75-vb4ks 1/1 Running 0 8s
kube0-6977cdcb75-z4fzs 1/1 Running 0 8s
Additionally, you can use the loadbalance=(sticky|random)
option to control
the load-balancing behavior when there are multiple replicas. random
selects
random nodes from the node pool, providing an even workload distribution across
replicas. sticky
(the default) attempts to connect the same build performed
multiple times to the same node each time, ensuring better use of local cache.
For more information on scalability, see the options for
docker buildx create
.
Node assignment
The Kubernetes driver allows you to control the scheduling of BuildKit pods
using the nodeSelector
and tolerations
driver options.
You can also set the schedulername
option if you want to use a custom scheduler altogether.
You can use the annotations
and labels
driver options to apply additional
metadata to the deployments and pods that's hosting your builders.
The value of the nodeSelector
parameter is a comma-separated string of
key-value pairs, where the key is the node label and the value is the label
text. For example: ""nodeselector=kubernetes.io/arch=arm64""
The tolerations
parameter is a semicolon-separated list of taints. It accepts
the same values as the Kubernetes manifest. Each tolerations
entry specifies
a taint key and the value, operator, or effect. For example:
""tolerations=key=foo,value=bar;key=foo2,operator=exists;key=foo3,effect=NoSchedule""
These options accept CSV-delimited strings as values. Due to quoting rules for
shell commands, you must wrap the values in single quotes. You can even wrap all
of --driver-opt
in single quotes, for example:
$ docker buildx create \
--bootstrap \
--name=kube \
--driver=kubernetes \
'--driver-opt=""nodeselector=label1=value1,label2=value2"",""tolerations=key=key1,value=value1""'
Multi-platform builds
The Kubernetes driver has support for creating multi-platform images, either using QEMU or by leveraging the native architecture of nodes.
QEMU
Like the docker-container
driver, the Kubernetes driver also supports using
QEMU (user
mode) to build images for non-native platforms. Include the --platform
flag
and specify which platforms you want to output to.
For example, to build a Linux image for amd64
and arm64
:
$ docker buildx build \
--builder=kube \
--platform=linux/amd64,linux/arm64 \
-t <user>/<image> \
--push .
Warning
QEMU performs full-CPU emulation of non-native platforms, which is much slower than native builds. Compute-heavy tasks like compilation and compression/decompression will likely take a large performance hit.
Using a custom BuildKit image or invoking non-native binaries in builds may
require that you explicitly turn on QEMU using the qemu.install
option when
creating the builder:
$ docker buildx create \
--bootstrap \
--name=kube \
--driver=kubernetes \
--driver-opt=namespace=buildkit,qemu.install=true
Native
If you have access to cluster nodes of different architectures, the Kubernetes
driver can take advantage of these for native builds. To do this, use the
--append
flag of docker buildx create
.
First, create your builder with explicit support for a single architecture, for
example amd64
:
$ docker buildx create \
--bootstrap \
--name=kube \
--driver=kubernetes \
--platform=linux/amd64 \
--node=builder-amd64 \
--driver-opt=namespace=buildkit,nodeselector=""kubernetes.io/arch=amd64""
This creates a Buildx builder named kube
, containing a single builder node
named builder-amd64
. Assigning a node name using --node
is optional. Buildx
generates a random node name if you don't provide one.
Note that the Buildx concept of a node isn't the same as the Kubernetes concept of a node. A Buildx node in this case could connect multiple Kubernetes nodes of the same architecture together.
With the kube
builder created, you can now introduce another architecture into
the mix using --append
. For example, to add arm64
:
$ docker buildx create \
--append \
--bootstrap \
--name=kube \
--driver=kubernetes \
--platform=linux/arm64 \
--node=builder-arm64 \
--driver-opt=namespace=buildkit,nodeselector=""kubernetes.io/arch=arm64""
Listing your builders shows both nodes for the kube
builder:
$ docker buildx ls
NAME/NODE DRIVER/ENDPOINT STATUS PLATFORMS
kube kubernetes
builder-amd64 kubernetes:///kube?deployment=builder-amd64&kubeconfig= running linux/amd64*, linux/amd64/v2, linux/amd64/v3, linux/386
builder-arm64 kubernetes:///kube?deployment=builder-arm64&kubeconfig= running linux/arm64*
You can now build multi-arch amd64
and arm64
images, by specifying those
platforms together in your build command:
$ docker buildx build --builder=kube --platform=linux/amd64,linux/arm64 -t <user>/<image> --push .
You can repeat the buildx create --append
command for as many architectures
that you want to support.
Rootless mode
The Kubernetes driver supports rootless mode. For more information on how rootless mode works, and its requirements, see here.
To turn it on in your cluster, you can use the rootless=true
driver option:
$ docker buildx create \
--name=kube \
--driver=kubernetes \
--driver-opt=namespace=buildkit,rootless=true
This will create your pods without securityContext.privileged
.
Requires Kubernetes version 1.19 or later. Using Ubuntu as the host kernel is recommended.
Example: Creating a Buildx builder in Kubernetes
This guide shows you how to:
- Create a namespace for your Buildx resources
- Create a Kubernetes builder.
- List the available builders
- Build an image using your Kubernetes builders
Prerequisites:
- You have an existing Kubernetes cluster. If you don't already have one, you can follow along by installing minikube.
- The cluster you want to connect to is accessible via the
kubectl
command, with theKUBECONFIG
environment variable set appropriately if necessary.
Create a
buildkit
namespace.Creating a separate namespace helps keep your Buildx resources separate from other resources in the cluster.
$ kubectl create namespace buildkit namespace/buildkit created
Create a new builder with the Kubernetes driver:
$ docker buildx create \ --bootstrap \ --name=kube \ --driver=kubernetes \ --driver-opt=namespace=buildkit
Note
Remember to specify the namespace in driver options.
List available builders using
docker buildx ls
$ docker buildx ls NAME/NODE DRIVER/ENDPOINT STATUS PLATFORMS kube kubernetes kube0-6977cdcb75-k9h9m running linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386 default * docker default default running linux/amd64, linux/386
Inspect the running pods created by the build driver with
kubectl
.$ kubectl -n buildkit get deployments NAME READY UP-TO-DATE AVAILABLE AGE kube0 1/1 1 1 32s $ kubectl -n buildkit get pods NAME READY STATUS RESTARTS AGE kube0-6977cdcb75-k9h9m 1/1 Running 0 32s
The build driver creates the necessary resources on your cluster in the specified namespace (in this case,
buildkit
), while keeping your driver configuration locally.Use your new builder by including the
--builder
flag when running buildx commands. For example: :# Replace <registry> with your Docker username # and <image> with the name of the image you want to build docker buildx build \ --builder=kube \ -t <registry>/<image> \ --push .
That's it: you've now built an image from a Kubernetes pod, using Buildx.
Further reading
For more information on the Kubernetes driver, see the buildx reference.",,,
858d3b25439a54e5b532ffc9c49d88e0bae3907501aedf258947b227e5300658,"Antivirus software and Docker
When antivirus software scans files used by Docker, these files may be locked in a way that causes Docker commands to hang.
One way to reduce these problems is to add the Docker data directory
(/var/lib/docker
on Linux, %ProgramData%\docker
on Windows Server, or $HOME/Library/Containers/com.docker.docker/
on Mac) to the
antivirus's exclusion list. However, this comes with the trade-off that viruses
or malware in Docker images, writable layers of containers, or volumes are not
detected. If you do choose to exclude Docker's data directory from background
virus scanning, you may want to schedule a recurring task that stops Docker,
scans the data directory, and restarts Docker.",,,
18bb8e993f4d7eae6720cfc6eec0eb9b2e714646a60c243504b6591b1a1301ba,"Matrix targets
A matrix strategy lets you fork a single target into multiple different variants, based on parameters that you specify. This works in a similar way to Matrix strategies for GitHub Actions. You can use this to reduce duplication in your Bake definition.
The matrix attribute is a map of parameter names to lists of values. Bake builds each possible combination of values as a separate target.
Each generated target must have a unique name. To specify how target names should resolve, use the name attribute.
The following example resolves the app target to app-foo
and app-bar
. It
also uses the matrix value to define the
target build stage.
target ""app"" {
name = ""app-${tgt}""
matrix = {
tgt = [""foo"", ""bar""]
}
target = tgt
}
$ docker buildx bake --print app
[+] Building 0.0s (0/0)
{
""group"": {
""app"": {
""targets"": [
""app-foo"",
""app-bar""
]
},
""default"": {
""targets"": [
""app""
]
}
},
""target"": {
""app-bar"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""target"": ""bar""
},
""app-foo"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""target"": ""foo""
}
}
}
Multiple axes
You can specify multiple keys in your matrix to fork a target on multiple axes. When using multiple matrix keys, Bake builds every possible variant.
The following example builds four targets:
app-foo-1-0
app-foo-2-0
app-bar-1-0
app-bar-2-0
target ""app"" {
name = ""app-${tgt}-${replace(version, ""."", ""-"")}""
matrix = {
tgt = [""foo"", ""bar""]
version = [""1.0"", ""2.0""]
}
target = tgt
args = {
VERSION = version
}
}
Multiple values per matrix target
If you want to differentiate the matrix on more than just a single value, you can use maps as matrix values. Bake creates a target for each map, and you can access the nested values using dot notation.
The following example builds two targets:
app-foo-1-0
app-bar-2-0
target ""app"" {
name = ""app-${item.tgt}-${replace(item.version, ""."", ""-"")}""
matrix = {
item = [
{
tgt = ""foo""
version = ""1.0""
},
{
tgt = ""bar""
version = ""2.0""
}
]
}
target = item.tgt
args = {
VERSION = item.version
}
}",,,
e59e3474fd1824d24125cbe1768f9f1c0d7620a653fe5c203dca0db00d8bde8a,"Integrating Docker Scout with environments
You can integrate Docker Scout with your runtime environments, and get insights for your running workloads. This gives you a real-time view of your security status for your deployed artifacts.
Docker Scout lets you define multiple environments, and assign images to different environments. This gives you a complete overview of your software supply chain, and lets you view and compare deltas between environments, for example staging and production.
How you define and name your environments is up to you. You can use patterns that are meaningful to you and that matches how you ship your applications.
Assign to environments
Each environment contains references to a number of images. These references represent containers currently running in that particular environment.
For example, say you're running myorg/webapp:3.1
in production, you can
assign that tag to your production
environment. You might be running a
different version of the same image in staging, in which case you can assign
that version of the image to the staging
environment.
To add environments to Docker Scout, you can:
- Use the
docker scout env <environment> <image>
CLI command to record images to environments manually - Enable a runtime integration to automatically detect images in your environments.
Docker Scout supports the following runtime integrations:
Note
Only organization owners can create new environments and set up integrations. Additionally, Docker Scout only assigns an image to an environment if the image has been analyzed, either manually or through a registry integration.
List environments
To see all of the available environments for an organization, you can use the
docker scout env
command.
$ docker scout env
By default, this prints all environments for your personal Docker organization.
To list environments for another organization that you're a part of, use the
--org
flag.
$ docker scout env --org <org>
You can use the docker scout config
command to change the default
organization. This changes the default organization for all docker scout
commands, not just env
.
$ docker scout config organization <org>
Comparing between environments
Assigning images to environments lets you make comparisons with and between environments. This is useful for things like GitHub pull requests, for comparing the image built from the code in the PR to the corresponding image in staging or production.
You can also compare with streams using the --to-env
flag on the
docker scout compare
CLI command:
$ docker scout compare --to-env production myorg/webapp:latest
View images for an environment
To view the images for an environment:
- Go to the Images page in the Docker Scout Dashboard.
- Open the Environments drop-down menu.
- Select the environment that you want to view.
The list displays all images that have been assigned to the selected environment. If you've deployed multiple versions of the same image in an environment, all versions of the image appear in the list.
Alternatively, you can use the docker scout env
command to view the images from the terminal.
$ docker scout env production
docker/scout-demo-service:main@sha256:ef08dca54c4f371e7ea090914f503982e890ec81d22fd29aa3b012351a44e1bc
Mismatching image tags
When you've selected an environment on the Images tab, tags in the list represent the tag that was used to deploy the image. Tags are mutable, meaning that you can change the image digest that a tag refers to. If Docker Scout detects that a tag refers to an outdated digest, a warning icon displays next to the image name.",,,
0b0106470cec4142a2df50c2bb6d82321e182ec88d5d751cf5dfbe9e268ba8a1,"Docker subscriptions and features
Docker subscription plans empower development teams by providing the tools they need to ship secure, high-quality apps — fast. These plans include access to Docker's suite of products:
- Docker Desktop: The industry-leading container-first development solution that includes, Docker Engine, Docker CLI, Docker Compose, Docker Build/BuildKit, and Kubernetes.
- Docker Hub: The world's largest cloud-based container registry.
- Docker Build Cloud: Powerful cloud-based builders that accelerate build times by up to 39x.
- Docker Scout: Tooling for software supply chain security that lets you quickly assess image health and accelerate security improvements.
- Testcontainers Cloud: Container-based testing automation that provides faster tests, a unified developer experience, and more.
The following sections describe some of the key features included with your Docker subscription plan or Legacy Docker plan.
Note
Legacy Docker plans apply to Docker subscribers who last purchased or renewed their subscription before December 10, 2024. These subscribers will keep their current plan and pricing until their next renewal date that falls on or after December 10, 2024. To see purchase or renewal history, view your billing history. For more details about Docker legacy plans, see Announcing Upgraded Docker Plans.
Docker Personal
Docker Personal is ideal for open source communities, individual developers, education, and small businesses. It includes the free use of essential Docker tools as well as trials for powerful tools that'll level up your development loops.
Docker Personal includes:
- 1 included repository with continuous vulnerability analysis in Docker Scout
- Unlimited public Docker Hub repositories
- 100 pulls per hour Docker Hub image pull rate limit for authenticated users
- 7-day Docker Build Cloud trial
- 7-day Testcontainers Cloud trial
Docker Personal users who want to continue using Docker Build Cloud or Docker Testcontainers Cloud after their trial can upgrade to a Docker Pro plan at any time.
All unauthenticated users, including unauthenticated Docker Personal users, get 10 pulls per hour per IP address.
For a list of features available in each tier, see Docker Pricing.
Docker Pro
Note
Starting April 1, 2025, all users with a Pro, Team, or Business subscription will have unlimited Docker Hub pulls with fair use. Unauthenticated users and users with a free Personal account have the following pull limits:
- Unauthenticated users: 10 pulls/hour
- Authenticated users with a free account: 100 pulls/hour
Docker Pro enables individual developers to get more control of their development environment and provides an integrated and reliable developer experience. It reduces the amount of time developers spend on mundane and repetitive tasks and empowers developers to spend more time creating value for their customers. A Docker Pro subscription includes access to all tools, including Docker Desktop, Docker Hub, Docker Scout, Docker Build Cloud, and Testcontainers Cloud.
Docker Pro includes:
- 200 Docker Build Cloud build minutes per month.
- 2 included repositories with continuous vulnerability analysis in Docker Scout.
- 100 Testcontainers Cloud runtime minutes per month for use either in Docker Desktop or for CI.
- No Docker Hub image pull rate limits.
For a list of features available in each tier, see Docker Pricing.
Docker Team
Note
Starting April 1, 2025, all users with a Pro, Team, or Business subscription will have unlimited Docker Hub pulls with fair use. Unauthenticated users and users with a free Personal account have the following pull limits:
- Unauthenticated users: 10 pulls/hour
- Authenticated users with a free account: 100 pulls/hour
Docker Team offers capabilities for collaboration, productivity, and security across organizations. It enables groups of developers to unlock the full power of collaboration and sharing combined with essential security features and team management capabilities. A Docker Team subscription includes licensing for commercial use of Docker components including Docker Desktop, Docker Hub, Docker Scout, Docker Build Cloud, and Testcontainers Cloud.
Docker Team includes:
- 500 Docker Build Cloud build minutes per month.
- Unlimited Docker Scout repositories with continuous vulnerability analysis.
- 500 Testcontainers Cloud runtime minutes per month for use either in Docker Desktop or for CI.
- No Docker Hub image pull rate limits.
There are also advanced collaboration and management tools, including organization and team management with Role Based Access Control (RBAC), activity logs, and more.
For a list of features available in each tier, see Docker Pricing.
Docker Business
Note
Starting April 1, 2025, all users with a Pro, Team, or Business subscription will have unlimited Docker Hub pulls with fair use. Unauthenticated users and users with a free Personal account have the following pull limits:
- Unauthenticated users: 10 pulls/hour
- Authenticated users with a free account: 100 pulls/hour
Docker Business offers centralized management and advanced security features for enterprises that use Docker at scale. It empowers leaders to manage their Docker development environments and speed up their secure software supply chain initiatives. A Docker Business subscription includes licensing for commercial use of Docker components including Docker Desktop, Docker Hub, Docker Scout, Docker Build Cloud, and Testcontainers Cloud.
Docker Business includes:
- 1500 Docker Build Cloud build minutes per month.
- Unlimited Docker Scout repositories with continuous vulnerability analysis.
- 1500 Testcontainers Cloud runtime minutes per month for use either in Docker Desktop or for CI.
- No Docker Hub image pull rate limits.
In addition, you gain access to enterprise-grade features, such as:
- Hardened Docker Desktop
- Image Access Management which lets admins control what content developers can access
- Registry Access Management which lets admins control what registries developers can access
- Company layer to manage multiple organizations and settings
- Single sign-on
- System for Cross-domain Identity Management
For a list of features available in each tier, see Docker Pricing.
Self-serve
A self-serve Docker subscription is where everything is set up by you. You can:
- Manage your own invoices
- Add or remove seats
- Update billing and payment information
- Downgrade your subscription at any time
Sales-assisted
A sales-assisted plan refers to a Docker Business or Team subscription where everything is set up and managed by a dedicated Docker account manager.
Important
As of December 10, 2024, Docker Core, Docker Build Cloud, and Docker Scout subscription plans are no longer available and have been replaced by Docker subscription plans that provide access to all tools. If you subscribed or renewed your subscriptions before December 10, 2024, your legacy Docker plans still apply to your account until you renew. For more details, see Announcing Upgraded Docker Plans. The following describes some of the key features included with your Legacy Docker plans:
Legacy Docker plans
Legacy Docker Pro
Legacy Docker Pro enables individual developers to get more control of their development environment and provides an integrated and reliable developer experience. It reduces the amount of time developers spend on mundane and repetitive tasks and empowers developers to spend more time creating value for their customers.
Legacy Docker Pro includes:
- Unlimited public repositories
- Unlimited Scoped Access Tokens
- Unlimited collaborators for public repositories at no cost per month.
- Access to Legacy Docker Scout Free to get started with software supply chain security.
- Unlimited private repositories
- 5000 image pulls per day
- Auto Builds with 5 concurrent builds
- 300 Vulnerability Scans
For a list of features available in each legacy tier, see Legacy Docker Pricing.
Upgrade your Legacy Docker Pro plan
When you upgrade your Legacy Docker Pro plan to a Docker Pro subscription plan, your plan includes the following changes:
- Docker Build Cloud build minutes increased from 100/month to 200/month and no monthly fee.
- 2 included repositories with continuous vulnerability analysis in Docker Scout.
- 100 Testcontainers Cloud runtime minutes are now included for use either in Docker Desktop or for CI.
- Docker Hub image pull rate limits are removed.
For a list of features available in each tier, see Docker Pricing.
Legacy Docker Team
Legacy Docker Team offers capabilities for collaboration, productivity, and security across organizations. It enables groups of developers to unlock the full power of collaboration and sharing combined with essential security features and team management capabilities. A Docker Team subscription includes licensing for commercial use of Docker components including Docker Desktop and Docker Hub.
Legacy Docker Team includes:
- Everything included in legacy Docker Pro
- Unlimited teams
- Auto Builds with 15 concurrent builds
- Unlimited Vulnerability Scanning
- 5000 image pulls per day for each team member
There are also advanced collaboration and management tools, including organization and team management with Role Based Access Control (RBAC), activity logs, and more.
For a list of features available in each legacy tier, see Legacy Docker Pricing.
Upgrade your Legacy Docker Team plan
When you upgrade your Legacy Docker Team plan to a Docker Team subscription plan, your plan includes the following changes:
- Instead of paying an additional per-seat fee, Docker Build Cloud is now available to all users in your Docker plan.
- Docker Build Cloud build minutes increase from 400/mo to 500/mo.
- Docker Scout now includes unlimited repositories with continuous vulnerability analysis, an increase from 3.
- 500 Testcontainers Cloud runtime minutes are now included for use either in Docker Desktop or for CI.
- Docker Hub image pull rate limits are removed.
- The minimum number of users is 1 (lowered from 5).
For a list of features available in each tier, see Docker Pricing.
Legacy Docker Business
Legacy Docker Business offers centralized management and advanced security features for enterprises that use Docker at scale. It empowers leaders to manage their Docker development environments and speed up their secure software supply chain initiatives. A Docker Business subscription includes licensing for commercial use of Docker components including Docker Desktop and Docker Hub.
Legacy Docker Business includes:
- Everything included in legacy Docker Team
- Hardened Docker Desktop
- Image Access Management which lets admins control what content developers can access
- Registry Access Management which lets admins control what registries developers can access
- Company layer to manage multiple organizations and settings
- Single Sign-On
- System for Cross-domain Identity Management and more.
For a list of features available in each tier, see Legacy Docker Pricing.
Upgrade your Legacy Docker Business plan
When you upgrade your Legacy Docker Business plan to a Docker Business subscription plan, your plan includes the following changes:
- Instead of paying an additional per-seat fee, Docker Build Cloud is now available to all users in your Docker plan.
- Docker Build Cloud included minutes increase from 800/mo to 1500/mo.
- Docker Scout now includes unlimited repositories with continuous vulnerability analysis, an increase from 3.
- 1500 Testcontainers Cloud runtime minutes are now included for use either in Docker Desktop or for CI.
- Docker Hub image pull rate limits are removed.
For a list of features available in each tier, see Docker Pricing.
Self-serve
A self-serve Docker Business subscription is where everything is set up by you. You can:
- Manage your own invoices
- Add or remove seats
- Update billing and payment information
- Downgrade your subscription at any time
Sales-assisted
A sales-assisted Docker Business subscription where everything is set up and managed by a dedicated Docker account manager.
Legacy Docker Scout subscriptions
This section provides an overview of the legacy subscription plans for Docker Scout.
Important
As of December 10, 2024, Docker Scout subscriptions are no longer available and have been replaced by Docker subscription plans that provide access to all tools. If you subscribed or renewed your subscriptions before December 10, 2024, your legacy Docker subscriptions still apply to your account until you renew. For more details, see Announcing Upgraded Docker Plans.
Legacy Docker Scout Free
Legacy Docker Scout Free is available for organizations. If you have a Legacy Docker plan, you automatically have access to legacy Docker Scout Free.
Legacy Docker Scout Free includes:
- Unlimited local image analysis
- Up to 3 Docker Scout-enabled repositories
- SDLC integration, including policy evaluation and workload integration
- On-prem and cloud container registry integrations
- Security posture reporting
Legacy Docker Scout Team
Legacy Docker Scout Team includes:
- All the features available in legacy Docker Scout Free
- In addition to 3 Docker Scout-enabled repositories, add up to 100 repositories when you buy your subscription
Legacy Docker Scout Business
Legacy Docker Scout Business includes:
- All the features available in legacy Docker Scout Team
- Unlimited Docker Scout-enabled repositories
Upgrade your Legacy Docker Scout plan
When you upgrade your Legacy Docker Scout plan to a Docker subscription plan, your plan includes the following changes:
- Docker Business: Unlimited repositories with continuous vulnerability analysis, an increase from 3.
- Docker Team: Unlimited repositories with continuous vulnerability analysis, an increase from 3
- Docker Pro: 2 included repositories with continuous vulnerability analysis.
- Docker Personal: 1 included repository with continuous vulnerability analysis.
For a list of features available in each tier, see Docker Pricing.
Legacy Docker Build Cloud subscriptions
This section describes the features available for the different legacy Docker Build Cloud subscription tiers.
Important
As of December 10, 2024, Docker Build Cloud is only available with the new Docker Pro, Team, and Business plans. When your plan renews on or after December 10, 2024, you will see an increase in your included Build Cloud minutes each month. For more details, see Announcing Upgraded Docker Plans.
Legacy Docker Build Cloud Starter
If you have a Legacy Docker plan, a base level of Build Cloud minutes and cache are included. The features available vary depending on your Legacy Docker plan subscription tier.
Legacy Docker Pro
- 100 build minutes every month
- Available for one user
- 4 parallel builds
Legacy Docker Team
- 400 build minutes every month shared across your organization
- Option to onboard up to 100 members
- Can buy additional seats to add more minutes
Legacy Docker Business
- All the features listed for Docker Team
- 800 build minutes every month shared across your organization
Legacy Docker Build Cloud Team
Legacy Docker Build Cloud Team offers the following features:
- 200 additional build minutes per seat
- Option to buy reserve minutes
- Increased shared cache
The legacy Docker Build Cloud Team subscription is tied to a Docker organization. To use the build minutes or shared cache of a legacy Docker Build Cloud Team subscription, users must be a part of the organization associated with the subscription. See Manage seats and invites.
Legacy Docker Build Cloud Enterprise
For more details about your enterprise subscription, contact sales.
Upgrade your Legacy Docker Build Cloud plan
You no longer need to subscribe to a separate Docker Build Cloud plan to access Docker Build Cloud or to scale your minutes. When you upgrade your Legacy Docker plan to a Docker subscription plan, your plan includes the following changes:
- Docker Business: Included minutes are increased from 800/mo to 1500/mo with the option to scale more minutes.
- Docker Team: Included minutes are increased from 400/mo to 500/mo with the option to scale more minutes.
- Docker Pro: Included minutes are increased from 100/mo to 200/mo with the option to scale more minutes.
- Docker Personal: You receive a 7-day trial.
Support for subscriptions
All Docker Pro, Team, and Business subscribers receive email support for their subscriptions.",,,
9855e86a46908206ca9a303409bb766e480d95d6db6baf54258aa33c4a43315e,"Overlay network driver
The overlay
network driver creates a distributed network among multiple
Docker daemon hosts. This network sits on top of (overlays) the host-specific
networks, allowing containers connected to it to communicate securely when
encryption is enabled. Docker transparently handles routing of each packet to
and from the correct Docker daemon host and the correct destination container.
You can create user-defined overlay
networks using docker network create
,
in the same way that you can create user-defined bridge
networks. Services
or containers can be connected to more than one network at a time. Services or
containers can only communicate across networks they're each connected to.
Overlay networks are often used to create a connection between Swarm services, but you can also use it to connect standalone containers running on different hosts. When using standalone containers, it's still required that you use Swarm mode to establish a connection between the hosts.
This page describes overlay networks in general, and when used with standalone containers. For information about overlay for Swarm services, see Manage Swarm service networks.
Create an overlay network
Before you start, you must ensure that participating nodes can communicate over the network. The following table lists ports that need to be open to each host participating in an overlay network:
| Ports | Description |
|---|---|
2377/tcp | The default Swarm control plane port, is configurable with
docker swarm join --listen-addr |
4789/udp | The default overlay traffic port, configurable with
docker swarm init --data-path-addr |
7946/tcp , 7946/udp | Used for communication among nodes, not configurable |
To create an overlay network that containers on other Docker hosts can connect to, run the following command:
$ docker network create -d overlay --attachable my-attachable-overlay
The --attachable
option enables both standalone containers
and Swarm services to connect to the overlay network.
Without --attachable
, only Swarm services can connect to the network.
You can specify the IP address range, subnet, gateway, and other options. See
docker network create --help
for details.
Encrypt traffic on an overlay network
Use the --opt encrypted
flag to encrypt the application data
transmitted over the overlay network:
$ docker network create \
--opt encrypted \
--driver overlay \
--attachable \
my-attachable-multi-host-network
This enables IPsec encryption at the level of the Virtual Extensible LAN (VXLAN). This encryption imposes a non-negligible performance penalty, so you should test this option before using it in production.
Warning
Don't attach Windows containers to encrypted overlay networks.
Overlay network encryption isn't supported on Windows. Swarm doesn't report an error when a Windows host attempts to connect to an encrypted overlay network, but networking for the Windows containers is affected as follows:
- Windows containers can't communicate with Linux containers on the network
- Data traffic between Windows containers on the network isn't encrypted
Attach a container to an overlay network
Adding containers to an overlay network gives them the ability to communicate with other containers without having to set up routing on the individual Docker daemon hosts. A prerequisite for doing this is that the hosts have joined the same Swarm.
To join an overlay network named multi-host-network
with a busybox
container:
$ docker run --network multi-host-network busybox sh
Note
This only works if the overlay network is attachable (created with the
--attachable
flag).
Container discovery
Publishing ports of a container on an overlay network opens the ports to other containers on the same network. Containers are discoverable by doing a DNS lookup using the container name.
| Flag value | Description |
|---|---|
-p 8080:80 | Map TCP port 80 in the container to port 8080 on the overlay network. |
-p 8080:80/udp | Map UDP port 80 in the container to port 8080 on the overlay network. |
-p 8080:80/sctp | Map SCTP port 80 in the container to port 8080 on the overlay network. |
-p 8080:80/tcp -p 8080:80/udp | Map TCP port 80 in the container to TCP port 8080 on the overlay network, and map UDP port 80 in the container to UDP port 8080 on the overlay network. |
Connection limit for overlay networks
Due to limitations set by the Linux kernel, overlay networks become unstable and inter-container communications may break when 1000 containers are co-located on the same host.
For more information about this limitation, see moby/moby#44973.
Next steps
- Go through the overlay networking tutorial
- Learn about networking from the container's point of view
- Learn about standalone bridge networks
- Learn about Macvlan networks",,,
f89b72a6d1e0216fcfa9d2517f94daef6379f531bc5415a332e1606ede9060da,"Integrate Docker Scout with Amazon ECR
Integrating Docker Scout with Amazon Elastic Container Registry (ECR) lets you
view image insights for images hosted in ECR repositories. After integrating
Docker Scout with ECR and activating Docker Scout for a repository, pushing an
image to the repository automatically triggers image analysis. You can view
image insights using the Docker Scout Dashboard, or the docker scout
CLI
commands.
How it works
To help you integrate Docker Scout with ECR, you can use a CloudFormation stack template that creates and configures the necessary AWS resources for integrating Docker Scout with your ECR registry. For more details about the AWS resources, see CloudFormation stack template.
The following diagram shows how the Docker Scout ECR integration works.
After the integration, Docker Scout automatically pulls and analyzes images that you push to the ECR registry. Metadata about your images are stored on the Docker Scout platform, but Docker Scout doesn't store the container images themselves. For more information about how Docker Scout handles image data, see Data handling.
CloudFormation stack template
The following table describes the configuration resources.
Note
Creating these resources incurs a small, recurring cost on the AWS account. The Cost column in the table represents an estimated monthly cost of the resources, when integrating an ECR registry that gets 100 images pushed per day.
Additionally, an egress cost also applies when Docker Scout pulls the images from ECR. The egress cost is around $0.09 per GB.
| Resource type | Resource name | Description | Cost |
|---|---|---|---|
AWS::SNSTopic::Topic | SNSTopic | SNS topic for notifying Docker Scout when the AWS resources have been created. | Free |
AWS::SNS::TopicPolicy | TopicPolicy | Defines the topic for the initial setup notification. | Free |
AWS::SecretsManager::Secret | ScoutAPICredentials | Stores the credentials used by EventBridge to fire events to Scout. | $0.42 |
AWS::Events::ApiDestination | ApiDestination | Sets up the EventBridge connection to Docker Scout for sending ECR push and delete events. | $0.01 |
AWS::Events::Connection | Connection | EventBridge connection credentials to Scout. | Free |
AWS::Events::Rule | DockerScoutEcrRule | Defines the rule to send ECR pushes and deletes to Scout. | Free |
AWS::Events::Rule | DockerScoutRepoDeletedRule | Defines the rule to send ECR repository deletes to Scout. | Free |
AWS::IAM::Role | InvokeApiRole | Internal role to grant the event access to ApiDestination . | Free |
AWS::IAM::Role | AssumeRoleEcrAccess | This role has access to ScoutAPICredentials for setting up the Docker Scout integration. | Free |
Integrate your first registry
Create the CloudFormation stack in your AWS account to enable the Docker Scout integration.
Prerequisites:
- You must have access to an AWS account with permission to create resources.
- You have be an owner of the Docker organization.
To create the stack:
Go to the ECR integration page on the Docker Scout Dashboard.
Select the Create on AWS button.
This opens the Create stack wizard in the AWS CloudFormation console in a new browser tab. If you're not already signed in to AWS, you're redirected to the sign-in page first.
If the button is grayed-out, it means you're lacking the necessary permissions in the Docker organization.
Follow the steps in the Create stack wizard until the end. Choose the AWS region you want to integrate. Complete the procedure by creating the resources.
The fields in the wizard are pre-populated by the CloudFormation template, so you don't need to edit any of the fields.
When the resources have been created (the CloudFormation status shows
CREATE_COMPLETE
in the AWS console), return to the ECR integrations page in the Docker Scout Dashboard.The Integrated registries list shows the account ID and region for the ECR registry that you just integrated. If successful, the integration status is Connected.
The ECR integration is now active. For Docker Scout to start analyzing images in the registry, you need to activate it for each repository in Repository settings.
After activating repositories, images that you push are analyzed by Docker Scout. The analysis results appear in the Docker Scout Dashboard. If your repository already contains images, Docker Scout pulls and analyzes the latest image version automatically.
Integrate additional registries
To add additional registries:
Go to the ECR integration page on the Docker Scout Dashboard.
Select the Add button at the top of the list.
Complete the steps for creating the AWS resources.
When the resources have been created, return to the ECR integrations page in the Docker Scout Dashboard.
The Integrated registries list shows the account ID and region for the ECR registry that you just integrated. If successful, the integration status is Connected.
Next, activate Docker Scout for the repositories that you want to analyze in Repository settings.
Remove integration
To remove an integrated ECR registry, you must be an owner of the Docker organization.
Go to the ECR integration page on the Docker Scout Dashboard.
Find the registry that you want to remove in the list of integrated registries, and select the remove icon in the Actions column.
If the remove icon is disabled, it means that you're lacking the necessary permissions in the Docker organization.
In the dialog that opens, confirm by selecting Remove.
Important
Removing the integration from the Docker Scout dashboard doesn't remove the AWS resources in your account.
After removing the integration in Docker Scout, go to the AWS console and delete the DockerScoutECRIntegration CloudFormation stack for the integration that you want to remove.
Troubleshooting
Unable to integrate registry
Check the Status of the integration on the ECR integration page in the Docker Scout Dashboard.
If the status is Pending for a prolonged period of time, it's an indication that the integration was not yet completed on the AWS side. Select the Pending link to open the CloudFormation wizard, and complete all the steps.
An Error status indicates that something's gone wrong in the back-end. You can try removing the integration and recreating it again.
ECR images not showing in the dashboard
If image analysis results for your ECR images aren't showing up in the Docker Scout Dashboard:
Ensure that you've activated Docker Scout for the repository. View and manage active repositories in Repository settings.
Ensure that the AWS account ID and region for your registry is listed on the ECR integrations page.
The account ID and region are included in the registry hostname:
<aws_account_id>.dkr.ecr.<region>.amazonaws.com/<image>",,,
5ad103a2451857127abdbfa2ea45fb3288c4dff1aa16007d4d1d8633b08c1585,"Install Docker Desktop on Mac
Docker Desktop terms
Commercial use of Docker Desktop in larger enterprises (more than 250 employees OR more than $10 million USD in annual revenue) requires a paid subscription.
This page contains download URLs, information about system requirements, and instructions on how to install Docker Desktop for Mac.
For checksums, see Release notes.
Warning
If you're experiencing malware detection issues, follow the steps documented in docker/for-mac#7527.
System requirements
A supported version of macOS.
Important
Docker supports Docker Desktop on the most recent versions of macOS. That is, the current release of macOS and the previous two releases. As new major versions of macOS are made generally available, Docker stops supporting the oldest version and supports the newest version of macOS (in addition to the previous two releases).
At least 4 GB of RAM.
A supported version of macOS.
Important
Docker supports Docker Desktop on the most recent versions of macOS. That is, the current release of macOS and the previous two releases. As new major versions of macOS are made generally available, Docker stops supporting the oldest version and supports the newest version of macOS (in addition to the previous two releases).
At least 4 GB of RAM.
For the best experience, it's recommended that you install Rosetta 2. There is no longer a hard requirement to install Rosetta 2, however there are a few optional command line tools that still require Rosetta 2 when using Darwin/AMD64. See Known issues. To install Rosetta 2 manually from the command line, run the following command:
$ softwareupdate --install-rosetta
Install and run Docker Desktop on Mac
Tip
See the FAQs on how to install and run Docker Desktop without needing administrator privileges.
Install interactively
Download the installer using the download buttons at the top of the page, or from the release notes.
Double-click
Docker.dmg
to open the installer, then drag the Docker icon to the Applications folder. By default, Docker Desktop is installed at/Applications/Docker.app
.Double-click
Docker.app
in the Applications folder to start Docker.The Docker menu displays the Docker Subscription Service Agreement.
Here’s a summary of the key points:
- Docker Desktop is free for small businesses (fewer than 250 employees AND less than $10 million in annual revenue), personal use, education, and non-commercial open source projects.
- Otherwise, it requires a paid subscription for professional use.
- Paid subscriptions are also required for government entities.
- Docker Pro, Team, and Business subscriptions include commercial use of Docker Desktop.
Select Accept to continue.
Note that Docker Desktop won't run if you do not agree to the terms. You can choose to accept the terms at a later date by opening Docker Desktop.
For more information, see Docker Desktop Subscription Service Agreement. It is recommended that you also read the FAQs.
From the installation window, select either:
- Use recommended settings (Requires password). This lets Docker Desktop automatically set the necessary configuration settings.
- Use advanced settings. You can then set the location of the Docker CLI tools either in the system or user directory, enable the default Docker socket, and enable privileged port mapping. See Settings, for more information and how to set the location of the Docker CLI tools.
Select Finish. If you have applied any of the previous configurations that require a password in step 6, enter your password to confirm your choice.
Install from the command line
After downloading Docker.dmg
from either the download buttons at the top of the page or from the
release notes, run the following commands in a terminal to install Docker Desktop in the Applications folder:
$ sudo hdiutil attach Docker.dmg
$ sudo /Volumes/Docker/Docker.app/Contents/MacOS/install
$ sudo hdiutil detach /Volumes/Docker
By default, Docker Desktop is installed at /Applications/Docker.app
. As macOS typically performs security checks the first time an application is used, the install
command can take several minutes to run.
The install
command accepts the following flags:
--accept-license
: Accepts the Docker Subscription Service Agreement now, rather than requiring it to be accepted when the application is first run.--allowed-org=<org name>
: Requires the user to sign in and be part of the specified Docker Hub organization when running the application--user=<username>
: Performs the privileged configurations once during installation. This removes the need for the user to grant root privileges on first run. For more information, see Privileged helper permission requirements. To find the username, enterls /Users
in the CLI.--admin-settings
: Automatically creates anadmin-settings.json
file which is used by administrators to control certain Docker Desktop settings on client machines within their organization. For more information, see Settings Management.- It must be used together with the
--allowed-org=<org name>
flag. - For example:
--allowed-org=<org name> --admin-settings=""{'configurationFileVersion': 2, 'enhancedContainerIsolation': {'value': true, 'locked': false}}""
- It must be used together with the
--proxy-http-mode=<mode>
: Sets the HTTP Proxy mode. The two modes aresystem
(default) ormanual
.--override-proxy-http=<URL>
: Sets the URL of the HTTP proxy that must be used for outgoing HTTP requests. It requires--proxy-http-mode
to bemanual
.--override-proxy-https=<URL>
: Sets the URL of the HTTP proxy that must be used for outgoing HTTPS requests, requires--proxy-http-mode
to bemanual
--override-proxy-exclude=<hosts/domains>
: Bypasses proxy settings for the hosts and domains. It's a comma-separated list.
Tip
As an IT administrator, you can use endpoint management (MDM) software to identify the number of Docker Desktop instances and their versions within your environment. This can provide accurate license reporting, help ensure your machines use the latest version of Docker Desktop, and enable you to enforce sign-in.
Where to go next
- Explore Docker's subscriptions to see what Docker can offer you.
- Get started with Docker.
- Explore Docker Desktop and all its features.
- Troubleshooting describes common problems, workarounds, how to run and submit diagnostics, and submit issues.
- FAQs provide answers to frequently asked questions.
- Release notes lists component updates, new features, and improvements associated with Docker Desktop releases.
- Back up and restore data provides instructions on backing up and restoring data related to Docker.",,,
f3418dc44fa2908af95d559747bba221f8f4a52fe85380596d0e3fca385d47d5,"Docker log driver plugins
This document describes logging driver plugins for Docker.
Logging drivers enables users to forward container logs to another service for processing. Docker includes several logging drivers as built-ins, however can never hope to support all use-cases with built-in drivers. Plugins allow Docker to support a wide range of logging services without requiring to embed client libraries for these services in the main Docker codebase. See the plugin documentation for more information.
Create a logging plugin
The main interface for logging plugins uses the same JSON+HTTP RPC protocol used
by other plugin types. See the
example plugin for a
reference implementation of a logging plugin. The example wraps the built-in
jsonfilelog
log driver.
LogDriver protocol
Logging plugins must register as a LogDriver
during plugin activation. Once
activated users can specify the plugin as a log driver.
There are two HTTP endpoints that logging plugins must implement:
/LogDriver.StartLogging
Signals to the plugin that a container is starting that the plugin should start receiving logs for.
Logs will be streamed over the defined file in the request. On Linux this file is a FIFO. Logging plugins are not currently supported on Windows.
Request:
{
""File"": ""/path/to/file/stream"",
""Info"": {
""ContainerID"": ""123456""
}
}
File
is the path to the log stream that needs to be consumed. Each call to
StartLogging
should provide a different file path, even if it's a container
that the plugin has already received logs for prior. The file is created by
Docker with a randomly generated name.
Info
is details about the container that's being logged. This is fairly
free-form, but is defined by the following struct definition:
type Info struct {
Config map[string]string
ContainerID string
ContainerName string
ContainerEntrypoint string
ContainerArgs []string
ContainerImageID string
ContainerImageName string
ContainerCreated time.Time
ContainerEnv []string
ContainerLabels map[string]string
LogPath string
DaemonName string
}
ContainerID
will always be supplied with this struct, but other fields may be
empty or missing.
Response:
{
""Err"": """"
}
If an error occurred during this request, add an error message to the Err
field
in the response. If no error then you can either send an empty response ({}
)
or an empty value for the Err
field.
The driver should at this point be consuming log messages from the passed in file. If messages are unconsumed, it may cause the container to block while trying to write to its stdio streams.
Log stream messages are encoded as protocol buffers. The protobuf definitions are in the moby repository.
Since protocol buffers are not self-delimited you must decode them from the stream using the following stream format:
[size][message]
Where size
is a 4-byte big endian binary encoded uint32. size
in this case
defines the size of the next message. message
is the actual log entry.
A reference golang implementation of a stream encoder/decoder can be found here
/LogDriver.StopLogging
Signals to the plugin to stop collecting logs from the defined file. Once a response is received, the file will be removed by Docker. You must make sure to collect all logs on the stream before responding to this request or risk losing log data.
Requests on this endpoint does not mean that the container has been removed only that it has stopped.
Request:
{
""File"": ""/path/to/file/stream""
}
Response:
{
""Err"": """"
}
If an error occurred during this request, add an error message to the Err
field
in the response. If no error then you can either send an empty response ({}
)
or an empty value for the Err
field.
Optional endpoints
Logging plugins can implement two extra logging endpoints:
/LogDriver.Capabilities
Defines the capabilities of the log driver. You must implement this endpoint for Docker to be able to take advantage of any of the defined capabilities.
Request:
{}
Response:
{
""ReadLogs"": true
}
Supported capabilities:
ReadLogs
- this tells Docker that the plugin is capable of reading back logs to clients. Plugins that report that they supportReadLogs
must implement the/LogDriver.ReadLogs
endpoint
/LogDriver.ReadLogs
Reads back logs to the client. This is used when docker logs <container>
is
called.
In order for Docker to use this endpoint, the plugin must specify as much when
/LogDriver.Capabilities
is called.
Request:
{
""ReadConfig"": {},
""Info"": {
""ContainerID"": ""123456""
}
}
ReadConfig
is the list of options for reading, it is defined with the following
golang struct:
type ReadConfig struct {
Since time.Time
Tail int
Follow bool
}
Since
defines the oldest log that should be sent.Tail
defines the number of lines to read (e.g. like the commandtail -n 10
)Follow
signals that the client wants to stay attached to receive new log messages as they come in once the existing logs have been read.
Info
is the same type defined in /LogDriver.StartLogging
. It should be used
to determine what set of logs to read.
Response:
{{ log stream }}
The response should be the encoded log message using the same format as the messages that the plugin consumed from Docker.",,,
a9d24f53dc2c10c561fea10fa7fa705f9ce973100d747e63fb32992c56195956,"Change your billing cycle
You can pay for a subscription plan on a monthly or yearly billing cycle. You select your preferred billing cycle when you buy your subscription.
Note
Business plan is available only on yearly billing cycle.
If you have a monthly billing cycle, you can choose to switch to an annual billing cycle.
Note
You can't switch from an annual billing cycle to a monthly cycle.
When you change the billing cycle's duration:
- The next billing date reflects the new cycle. To find your next billing date, see View renewal date.
- The subscription's start date resets. For example, if the start date of the monthly subscription is March 1st and the end date is April 1st, then after switching the billing duration to March 15th, 2024 the new start date is March 15th, 2024, and the new end date is March 15th, 2025.
- Any unused monthly subscription is prorated and applied as credit towards the new annual period. For example, if you switch from a $10 monthly subscription to a $100 annual plan, deducting the unused monthly value (in this case $5), the migration cost becomes $95 ($100 - $5). The renewal cost after March 15, 2025 is $100.
Important
Starting July 1, 2024, Docker will begin collecting sales tax on subscription fees in compliance with state regulations for customers in the United States. For our global customers subject to VAT, the implementation will start rolling out on July 1, 2024. Note that while the roll out begins on this date, VAT charges may not apply to all applicable subscriptions immediately.
To ensure that tax assessments are correct, make sure that your billing information and VAT/Tax ID, if applicable, are updated. If you're exempt from sales tax, see Register a tax certificate.
Personal account
To change your billing cycle:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- On the plans and usage page, select Switch to annual billing.
- Verify your billing information.
- Select Continue to payment.
- Verify payment information and select Upgrade subscription.
The billing plans and usage page will now reflect your new annual plan details.
To change your billing cycle:
- Sign in to Docker Hub.
- Select your avatar in the top-right corner.
- From the drop-down menu select Billing.
- In the bottom-right of the Plan tab, select Switch to annual billing.
- Review the information displayed on the Change to an Annual subscription page and select Accept Terms and Purchase to confirm.
Organization
Note
You must be an organization owner to make changes to the payment information.
To change your organization's billing cycle:
- Sign in to Docker Home.
- Under Settings and administration, select Billing.
- On the plans and usage page, select Switch to annual billing.
- Verify your billing information.
- Select Continue to payment.
- Verify payment information and select Upgrade subscription.
To change your organization's billing cycle:
- Sign in to Docker Hub.
- Select Organizations from the top-level navigation.
- Select the organization that you want to change the payment method for.
- In the bottom-right of the Plan tab, select Switch to annual billing.
- Review the information displayed on the Change to an Annual subscription page and select Accept Terms and Purchase to confirm.",,,
cb5e78624867042c08e2de66911fc1e77754728a737c7beea493d11524ad4fe2,"Multi-platform builds
A multi-platform build refers to a single build invocation that targets
multiple different operating system or CPU architecture combinations. When
building images, this lets you create a single image that can run on multiple
platforms, such as linux/amd64
, linux/arm64
, and windows/amd64
.
Why multi-platform builds?
Docker solves the ""it works on my machine"" problem by packaging applications and their dependencies into containers. This makes it easy to run the same application on different environments, such as development, testing, and production.
But containerization by itself only solves part of the problem. Containers
share the host kernel, which means that the code that's running inside the
container must be compatible with the host's architecture. This is why you
can't run a linux/amd64
container on an arm64 host (without using emulation),
or a Windows container on a Linux host.
Multi-platform builds solve this problem by packaging multiple variants of the same application into a single image. This enables you to run the same image on different types of hardware, such as development machines running x86-64 or ARM-based Amazon EC2 instances in the cloud, without the need for emulation.
Difference between single-platform and multi-platform images
Multi-platform images have a different structure than single-platform images. Single-platform images contain a single manifest that points to a single configuration and a single set of layers. Multi-platform images contain a manifest list, pointing to multiple manifests, each of which points to a different configuration and set of layers.
When you push a multi-platform image to a registry, the registry stores the
manifest list and all the individual manifests. When you pull the image, the
registry returns the manifest list, and Docker automatically selects the
correct variant based on the host's architecture. For example, if you run a
multi-platform image on an ARM-based Raspberry Pi, Docker selects the
linux/arm64
variant. If you run the same image on an x86-64 laptop, Docker
selects the linux/amd64
variant (if you're using Linux containers).
Prerequisites
To build multi-platform images, you first need to make sure that your Docker environment is set up to support it. There are two ways you can do that:
- You can switch from the ""classic"" image store to the containerd image store.
- You can create and use a custom builder.
The ""classic"" image store of the Docker Engine does not support multi-platform images. Switching to the containerd image store ensures that your Docker Engine can push, pull, and build multi-platform images.
Creating a custom builder that uses a driver with multi-platform support,
such as the docker-container
driver, will let you build multi-platform images
without switching to a different image store. However, you still won't be able
to load the multi-platform images you build into your Docker Engine image
store. But you can push them to a container registry directly with docker build --push
.
The steps for enabling the containerd image store depends on whether you're using Docker Desktop or Docker Engine standalone:
If you're using Docker Desktop, enable the containerd image store in the Docker Desktop settings.
If you're using Docker Engine standalone, enable the containerd image store using the daemon configuration file.
To create a custom builder, use the docker buildx create
command to create a
builder that uses the docker-container
driver.
$ docker buildx create \
--name container-builder \
--driver docker-container \
--bootstrap --use
Note
Builds with the
docker-container
driver aren't automatically loaded to your Docker Engine image store. For more information, see Build drivers.
If you're using Docker Engine standalone and you need to build multi-platform images using emulation, you also need to install QEMU, see Install QEMU manually.
Build multi-platform images
When triggering a build, use the --platform
flag to define the target
platforms for the build output, such as linux/amd64
and linux/arm64
:
$ docker buildx build --platform linux/amd64,linux/arm64 .
Strategies
You can build multi-platform images using three different strategies, depending on your use case:
- Using emulation, via QEMU
- Use a builder with multiple native nodes
- Use cross-compilation with multi-stage builds
QEMU
Building multi-platform images under emulation with QEMU is the easiest way to get started if your builder already supports it. Using emulation requires no changes to your Dockerfile, and BuildKit automatically detects the architectures that are available for emulation.
Note
Emulation with QEMU can be much slower than native builds, especially for compute-heavy tasks like compilation and compression or decompression.
Use multiple native nodes or cross-compilation instead, if possible.
Docker Desktop supports running and building multi-platform images under emulation by default. No configuration is necessary as the builder uses the QEMU that's bundled within the Docker Desktop VM.
Install QEMU manually
If you're using a builder outside of Docker Desktop, such as if you're using Docker Engine on Linux, or a custom remote builder, you need to install QEMU and register the executable types on the host OS. The prerequisites for installing QEMU are:
- Linux kernel version 4.8 or later
binfmt-support
version 2.1.7 or later- The QEMU binaries must be statically compiled and registered with the
fix_binary
flag
Use the
tonistiigi/binfmt
image to
install QEMU and register the executable types on the host with a single
command:
$ docker run --privileged --rm tonistiigi/binfmt --install all
This installs the QEMU binaries and registers them with
binfmt_misc
, enabling QEMU to
execute non-native file formats for emulation.
Once QEMU is installed and the executable types are registered on the host OS,
they work transparently inside containers. You can verify your registration by
checking if F
is among the flags in /proc/sys/fs/binfmt_misc/qemu-*
.
Multiple native nodes
Using multiple native nodes provide better support for more complicated cases that QEMU can't handle, and also provides better performance.
You can add additional nodes to a builder using the --append
flag.
The following command creates a multi-node builder from Docker contexts named
node-amd64
and node-arm64
. This example assumes that you've already added
those contexts.
$ docker buildx create --use --name mybuild node-amd64
mybuild
$ docker buildx create --append --name mybuild node-arm64
$ docker buildx build --platform linux/amd64,linux/arm64 .
While this approach has advantages over emulation, managing multi-node builders introduces some overhead of setting up and managing builder clusters. Alternatively, you can use Docker Build Cloud, a service that provides managed multi-node builders on Docker's infrastructure. With Docker Build Cloud, you get native multi-platform ARM and X86 builders without the burden of maintaining them. Using cloud builders also provides additional benefits, such as a shared build cache.
After signing up for Docker Build Cloud, add the builder to your local environment and start building.
$ docker buildx create --driver cloud <ORG>/<BUILDER_NAME>
cloud-<ORG>-<BUILDER_NAME>
$ docker build \
--builder cloud-<ORG>-<BUILDER_NAME> \
--platform linux/amd64,linux/arm64,linux/arm/v7 \
--tag <IMAGE_NAME> \
--push .
For more information, see Docker Build Cloud.
Cross-compilation
Depending on your project, if the programming language you use has good support
for cross-compilation, you can leverage multi-stage builds to build binaries
for target platforms from the native architecture of the builder. Special build
arguments, such as BUILDPLATFORM
and TARGETPLATFORM
, are automatically
available for use in your Dockerfile.
In the following example, the FROM
instruction is pinned to the native
platform of the builder (using the --platform=$BUILDPLATFORM
option) to
prevent emulation from kicking in. Then the pre-defined $BUILDPLATFORM
and
$TARGETPLATFORM
build arguments are interpolated in a RUN
instruction. In
this case, the values are just printed to stdout with echo
, but this
illustrates how you would pass them to the compiler for cross-compilation.
# syntax=docker/dockerfile:1
FROM --platform=$BUILDPLATFORM golang:alpine AS build
ARG TARGETPLATFORM
ARG BUILDPLATFORM
RUN echo ""I am running on $BUILDPLATFORM, building for $TARGETPLATFORM"" > /log
FROM alpine
COPY --from=build /log /log
Examples
Here are some examples of multi-platform builds:
- Simple multi-platform build using emulation
- Multi-platform Neovim build using Docker Build Cloud
- Cross-compiling a Go application
Simple multi-platform build using emulation
This example demonstrates how to build a simple multi-platform image using emulation with QEMU. The image contains a single file that prints the architecture of the container.
Prerequisites:
- Docker Desktop, or Docker Engine with QEMU installed
- containerd image store enabled
Steps:
Create an empty directory and navigate to it:
$ mkdir multi-platform $ cd multi-platform
Create a simple Dockerfile that prints the architecture of the container:
# syntax=docker/dockerfile:1 FROM alpine RUN uname -m > /arch
Build the image for
linux/amd64
andlinux/arm64
:$ docker build --platform linux/amd64,linux/arm64 -t multi-platform .
Run the image and print the architecture:
$ docker run --rm multi-platform cat /arch
- If you're running on an x86-64 machine, you should see
x86_64
. - If you're running on an ARM machine, you should see
aarch64
.
- If you're running on an x86-64 machine, you should see
Multi-platform Neovim build using Docker Build Cloud
This example demonstrates how run a multi-platform build using Docker Build
Cloud to compile and export
Neovim binaries
for the linux/amd64
and linux/arm64
platforms.
Docker Build Cloud provides managed multi-node builders that support native multi-platform builds without the need for emulation, making it much faster to do CPU-intensive tasks like compilation.
Prerequisites:
Steps:
Create an empty directory and navigate to it:
$ mkdir docker-build-neovim $ cd docker-build-neovim
Create a Dockerfile that builds Neovim.
# syntax=docker/dockerfile:1 FROM debian:bookworm AS build WORKDIR /work RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \ --mount=type=cache,target=/var/lib/apt,sharing=locked \ apt-get update && apt-get install -y \ build-essential \ cmake \ curl \ gettext \ ninja-build \ unzip ADD https://github.com/neovim/neovim.git#stable . RUN make CMAKE_BUILD_TYPE=RelWithDebInfo FROM scratch COPY --from=build /work/build/bin/nvim /
Build the image for
linux/amd64
andlinux/arm64
using Docker Build Cloud:$ docker build \ --builder <cloud-builder> \ --platform linux/amd64,linux/arm64 \ --output ./bin .
This command builds the image using the cloud builder and exports the binaries to the
bin
directory.Verify that the binaries are built for both platforms. You should see the
nvim
binary for bothlinux/amd64
andlinux/arm64
.$ tree ./bin ./bin ├── linux_amd64 │ └── nvim └── linux_arm64 └── nvim 3 directories, 2 files
Cross-compiling a Go application
This example demonstrates how to cross-compile a Go application for multiple platforms using multi-stage builds. The application is a simple HTTP server that listens on port 8080 and returns the architecture of the container. This example uses Go, but the same principles apply to other programming languages that support cross-compilation.
Cross-compilation with Docker builds works by leveraging a series of pre-defined (in BuildKit) build arguments that give you information about platforms of the builder and the build targets. You can use these pre-defined arguments to pass the platform information to the compiler.
In Go, you can use the GOOS
and GOARCH
environment variables to specify the
target platform to build for.
Prerequisites:
- Docker Desktop or Docker Engine
Steps:
Create an empty directory and navigate to it:
$ mkdir go-server $ cd go-server
Create a base Dockerfile that builds the Go application:
# syntax=docker/dockerfile:1 FROM golang:alpine AS build WORKDIR /app ADD https://github.com/dvdksn/buildme.git#eb6279e0ad8a10003718656c6867539bd9426ad8 . RUN go build -o server . FROM alpine COPY --from=build /app/server /server ENTRYPOINT [""/server""]
This Dockerfile can't build multi-platform with cross-compilation yet. If you were to try to build this Dockerfile with
docker build
, the builder would attempt to use emulation to build the image for the specified platforms.To add cross-compilation support, update the Dockerfile to use the pre-defined
BUILDPLATFORM
andTARGETPLATFORM
build arguments. These arguments are automatically available in the Dockerfile when you use the--platform
flag withdocker build
.- Pin the
golang
image to the platform of the builder using the--platform=$BUILDPLATFORM
option. - Add
ARG
instructions for the Go compilation stages to make theTARGETOS
andTARGETARCH
build arguments available to the commands in this stage. - Set the
GOOS
andGOARCH
environment variables to the values ofTARGETOS
andTARGETARCH
. The Go compiler uses these variables to do cross-compilation.
# syntax=docker/dockerfile:1 FROM --platform=$BUILDPLATFORM golang:alpine AS build ARG TARGETOS ARG TARGETARCH WORKDIR /app ADD https://github.com/dvdksn/buildme.git#eb6279e0ad8a10003718656c6867539bd9426ad8 . RUN GOOS=${TARGETOS} GOARCH=${TARGETARCH} go build -o server . FROM alpine COPY --from=build /app/server /server ENTRYPOINT [""/server""]
# syntax=docker/dockerfile:1 FROM golang:alpine AS build WORKDIR /app ADD https://github.com/dvdksn/buildme.git#eb6279e0ad8a10003718656c6867539bd9426ad8 . RUN go build -o server . FROM alpine COPY --from=build /app/server /server ENTRYPOINT [""/server""]
# syntax=docker/dockerfile:1 -FROM golang:alpine AS build +FROM --platform=$BUILDPLATFORM golang:alpine AS build +ARG TARGETOS +ARG TARGETARCH WORKDIR /app ADD https://github.com/dvdksn/buildme.git#eb6279e0ad8a10003718656c6867539bd9426ad8 . -RUN go build -o server . +RUN GOOS=${TARGETOS} GOARCH=${TARGETARCH} go build -o server . FROM alpine COPY --from=build /app/server /server ENTRYPOINT [""/server""]
- Pin the
Build the image for
linux/amd64
andlinux/arm64
:$ docker build --platform linux/amd64,linux/arm64 -t go-server .
This example has shown how to cross-compile a Go application for multiple platforms with Docker builds. The specific steps on how to do cross-compilation may vary depending on the programming language you're using. Consult the documentation for your programming language to learn more about cross-compiling for different platforms.
Tip
You may also want to consider checking out xx - Dockerfile cross-compilation helpers.
xx
is a Docker image containing utility scripts that make cross-compiling with Docker builds easier.",,,
b78b6a72e0d92bb3157e1cb2bb48e2f666f308787b9564d31d0d8fda7f849c5e,"Building best practices
Use multi-stage builds
Multi-stage builds let you reduce the size of your final image, by creating a cleaner separation between the building of your image and the final output. Split your Dockerfile instructions into distinct stages to make sure that the resulting output only contains the files that are needed to run the application.
Using multiple stages can also let you build more efficiently by executing build steps in parallel.
See Multi-stage builds for more information.
Create reusable stages
If you have multiple images with a lot in common, consider creating a reusable stage that includes the shared components, and basing your unique stages on that. Docker only needs to build the common stage once. This means that your derivative images use memory on the Docker host more efficiently and load more quickly.
It's also easier to maintain a common base stage (""Don't repeat yourself""), than it is to have multiple different stages doing similar things.
Choose the right base image
The first step towards achieving a secure image is to choose the right base image. When choosing an image, ensure it's built from a trusted source and keep it small.
Docker Official Images are some of the most secure and dependable images on Docker Hub. Typically, Docker Official images have few or no packages containing CVEs, and are thoroughly reviewed by Docker and project maintainers.
Verified Publisher images are high-quality images published and maintained by the organizations partnering with Docker, with Docker verifying the authenticity of the content in their repositories.
Docker-Sponsored Open Source are published and maintained by open source projects sponsored by Docker through an open source program.
When you pick your base image, look out for the badges indicating that the image is part of these programs.
When building your own image from a Dockerfile, ensure you choose a minimal base image that matches your requirements. A smaller base image not only offers portability and fast downloads, but also shrinks the size of your image and minimizes the number of vulnerabilities introduced through the dependencies.
You should also consider using two types of base image: one for building and unit testing, and another (typically slimmer) image for production. In the later stages of development, your image may not require build tools such as compilers, build systems, and debugging tools. A small image with minimal dependencies can considerably lower the attack surface.
Rebuild your images often
Docker images are immutable. Building an image is taking a snapshot of that image at that moment. That includes any base images, libraries, or other software you use in your build. To keep your images up-to-date and secure, make sure to rebuild your image often, with updated dependencies.
To ensure that you're getting the latest versions of dependencies in your build,
you can use the --no-cache
option to avoid cache hits.
$ docker build --no-cache -t my-image:my-tag .
The following Dockerfile uses the 24.04
tag of the ubuntu
image. Over time,
that tag may resolve to a different underlying version of the ubuntu
image,
as the publisher rebuilds the image with new security patches and updated
libraries. Using the --no-cache
, you can avoid cache hits and ensure a fresh
download of base images and dependencies.
# syntax=docker/dockerfile:1
FROM ubuntu:24.04
RUN apt-get -y update && apt-get install -y --no-install-recommends python3
Also consider pinning base image versions.
Exclude with .dockerignore
To exclude files not relevant to the build, without restructuring your source
repository, use a .dockerignore
file. This file supports exclusion patterns
similar to .gitignore
files.
For example, to exclude all files with the .md
extension:
*.md
For information on creating one, see Dockerignore file.
Create ephemeral containers
The image defined by your Dockerfile should generate containers that are as ephemeral as possible. Ephemeral means that the container can be stopped and destroyed, then rebuilt and replaced with an absolute minimum set up and configuration.
Refer to Processes under The Twelve-factor App methodology to get a feel for the motivations of running containers in such a stateless fashion.
Don't install unnecessary packages
Avoid installing extra or unnecessary packages just because they might be nice to have. For example, you don’t need to include a text editor in a database image.
When you avoid installing extra or unnecessary packages, your images have reduced complexity, reduced dependencies, reduced file sizes, and reduced build times.
Decouple applications
Each container should have only one concern. Decoupling applications into multiple containers makes it easier to scale horizontally and reuse containers. For instance, a web application stack might consist of three separate containers, each with its own unique image, to manage the web application, database, and an in-memory cache in a decoupled manner.
Limiting each container to one process is a good rule of thumb, but it's not a hard and fast rule. For example, not only can containers be spawned with an init process, some programs might spawn additional processes of their own accord. For instance, Celery can spawn multiple worker processes, and Apache can create one process per request.
Use your best judgment to keep containers as clean and modular as possible. If containers depend on each other, you can use Docker container networks to ensure that these containers can communicate.
Sort multi-line arguments
Whenever possible, sort multi-line arguments alphanumerically to make maintenance easier.
This helps to avoid duplication of packages and make the
list much easier to update. This also makes PRs a lot easier to read and
review. Adding a space before a backslash (\
) helps as well.
Here’s an example from the buildpack-deps image:
RUN apt-get update && apt-get install -y --no-install-recommends \
bzr \
cvs \
git \
mercurial \
subversion \
&& rm -rf /var/lib/apt/lists/*
Leverage build cache
When building an image, Docker steps through the instructions in your Dockerfile, executing each in the order specified. For each instruction, Docker checks whether it can reuse the instruction from the build cache.
Understanding how the build cache works, and how cache invalidation occurs, is critical for ensuring faster builds. For more information about the Docker build cache and how to optimize your builds, see Docker build cache.
Pin base image versions
Image tags are mutable, meaning a publisher can update a tag to point to a new image. This is useful because it lets publishers update tags to point to newer versions of an image. And as an image consumer, it means you automatically get the new version when you re-build your image.
For example, if you specify FROM alpine:3.19
in your Dockerfile, 3.19
resolves to the latest patch version for 3.19
.
# syntax=docker/dockerfile:1
FROM alpine:3.19
At one point in time, the 3.19
tag might point to version 3.19.1 of the
image. If you rebuild the image 3 months later, the same tag might point to a
different version, such as 3.19.4. This publishing workflow is best practice,
and most publishers use this tagging strategy, but it isn't enforced.
The downside with this is that you're not guaranteed to get the same for every build. This could result in breaking changes, and it means you also don't have an audit trail of the exact image versions that you're using.
To fully secure your supply chain integrity, you can pin the image version to a
specific digest. By pinning your images to a digest, you're guaranteed to
always use the same image version, even if a publisher replaces the tag with a
new image. For example, the following Dockerfile pins the Alpine image to the
same tag as earlier, 3.19
, but this time with a digest reference as well.
# syntax=docker/dockerfile:1
FROM alpine:3.19@sha256:13b7e62e8df80264dbb747995705a986aa530415763a6c58f84a3ca8af9a5bcd
With this Dockerfile, even if the publisher updates the 3.19
tag, your builds
would still use the pinned image version:
13b7e62e8df80264dbb747995705a986aa530415763a6c58f84a3ca8af9a5bcd
.
While this helps you avoid unexpected changes, it's also more tedious to have to look up and include the image digest for base image versions manually each time you want to update it. And you're opting out of automated security fixes, which is likely something you want to get.
Docker Scout's default Up-to-Date Base Images policy checks whether the base image version you're using is in fact the latest version. This policy also checks if pinned digests in your Dockerfile correspond to the correct version. If a publisher updates an image that you've pinned, the policy evaluation returns a non-compliant status, indicating that you should update your image.
Docker Scout also supports an automated remediation workflow for keeping your base images up-to-date. When a new image digest is available, Docker Scout can automatically raise a pull request on your repository to update your Dockerfiles to use the latest version. This is better than using a tag that changes the version automatically, because you're in control and you have an audit trail of when and how the change occurred.
For more information about automatically updating your base images with Docker Scout, see Remediation.
Build and test your images in CI
When you check in a change to source control or create a pull request, use GitHub Actions or another CI/CD pipeline to automatically build and tag a Docker image and test it.
Dockerfile instructions
Follow these recommendations on how to properly use the Dockerfile instructions to create an efficient and maintainable Dockerfile.
FROM
Whenever possible, use current official images as the basis for your images. Docker recommends the Alpine image as it is tightly controlled and small in size (currently under 6 MB), while still being a full Linux distribution.
For more information about the FROM
instruction, see
Dockerfile reference for the FROM instruction.
LABEL
You can add labels to your image to help organize images by project, record
licensing information, to aid in automation, or for other reasons. For each
label, add a line beginning with LABEL
with one or more key-value pairs.
The following examples show the different acceptable formats. Explanatory comments are included inline.
Strings with spaces must be quoted or the spaces must be escaped. Inner
quote characters (""
), must also be escaped. For example:
# Set one or more individual labels
LABEL com.example.version=""0.0.1-beta""
LABEL vendor1=""ACME Incorporated""
LABEL vendor2=ZENITH\ Incorporated
LABEL com.example.release-date=""2015-02-12""
LABEL com.example.version.is-production=""""
An image can have more than one label. Prior to Docker 1.10, it was recommended
to combine all labels into a single LABEL
instruction, to prevent extra layers
from being created. This is no longer necessary, but combining labels is still
supported. For example:
# Set multiple labels on one line
LABEL com.example.version=""0.0.1-beta"" com.example.release-date=""2015-02-12""
The above example can also be written as:
# Set multiple labels at once, using line-continuation characters to break long lines
LABEL vendor=ACME\ Incorporated \
com.example.is-beta= \
com.example.is-production="""" \
com.example.version=""0.0.1-beta"" \
com.example.release-date=""2015-02-12""
See Understanding object labels for guidelines about acceptable label keys and values. For information about querying labels, refer to the items related to filtering in Managing labels on objects. See also LABEL in the Dockerfile reference.
RUN
Split long or complex RUN
statements on multiple lines separated with
backslashes to make your Dockerfile more readable, understandable, and
maintainable.
For example, you can chain commands with the &&
operator, and use
escape characters to break long commands into multiple lines.
RUN apt-get update && apt-get install -y --no-install-recommends \
package-bar \
package-baz \
package-foo
By default, backslash escapes a newline character, but you can change it with
the
escape
directive.
You can also use here documents to run multiple commands without chaining them with a pipeline operator:
RUN <<EOF
apt-get update
apt-get install -y --no-install-recommends \
package-bar \
package-baz \
package-foo
EOF
For more information about RUN
, see
Dockerfile reference for the RUN instruction.
apt-get
One common use case for RUN
instructions in Debian-based images is to install
software using apt-get
. Because apt-get
installs packages, the RUN apt-get
command has several counter-intuitive behaviors to look out for.
Always combine RUN apt-get update
with apt-get install
in the same RUN
statement. For example:
RUN apt-get update && apt-get install -y --no-install-recommends \
package-bar \
package-baz \
package-foo
Using apt-get update
alone in a RUN
statement causes caching issues and
subsequent apt-get install
instructions to fail. For example, this issue will occur in the following Dockerfile:
# syntax=docker/dockerfile:1
FROM ubuntu:22.04
RUN apt-get update
RUN apt-get install -y --no-install-recommends curl
After building the image, all layers are in the Docker cache. Suppose you later
modify apt-get install
by adding an extra package as shown in the following Dockerfile:
# syntax=docker/dockerfile:1
FROM ubuntu:22.04
RUN apt-get update
RUN apt-get install -y --no-install-recommends curl nginx
Docker sees the initial and modified instructions as identical and reuses the
cache from previous steps. As a result the apt-get update
isn't executed
because the build uses the cached version. Because the apt-get update
isn't
run, your build can potentially get an outdated version of the curl
and
nginx
packages.
Using RUN apt-get update && apt-get install -y --no-install-recommends
ensures your Dockerfile
installs the latest package versions with no further coding or manual
intervention. This technique is known as cache busting. You can also achieve
cache busting by specifying a package version. This is known as version pinning.
For example:
RUN apt-get update && apt-get install -y --no-install-recommends \
package-bar \
package-baz \
package-foo=1.3.*
Version pinning forces the build to retrieve a particular version regardless of what’s in the cache. This technique can also reduce failures due to unanticipated changes in required packages.
Below is a well-formed RUN
instruction that demonstrates all the apt-get
recommendations.
RUN apt-get update && apt-get install -y --no-install-recommends \
aufs-tools \
automake \
build-essential \
curl \
dpkg-sig \
libcap-dev \
libsqlite3-dev \
mercurial \
reprepro \
ruby1.9.1 \
ruby1.9.1-dev \
s3cmd=1.1.* \
&& rm -rf /var/lib/apt/lists/*
The s3cmd
argument specifies a version 1.1.*
. If the image previously
used an older version, specifying the new one causes a cache bust of apt-get update
and ensures the installation of the new version. Listing packages on
each line can also prevent mistakes in package duplication.
In addition, when you clean up the apt cache by removing /var/lib/apt/lists
it
reduces the image size, since the apt cache isn't stored in a layer. Since the
RUN
statement starts with apt-get update
, the package cache is always
refreshed prior to apt-get install
.
Official Debian and Ubuntu images
automatically run apt-get clean
, so explicit invocation is not required.
Using pipes
Some RUN
commands depend on the ability to pipe the output of one command into another, using the pipe character (|
), as in the following example:
RUN wget -O - https://some.site | wc -l > /number
Docker executes these commands using the /bin/sh -c
interpreter, which only
evaluates the exit code of the last operation in the pipe to determine success.
In the example above, this build step succeeds and produces a new image so long
as the wc -l
command succeeds, even if the wget
command fails.
If you want the command to fail due to an error at any stage in the pipe,
prepend set -o pipefail &&
to ensure that an unexpected error prevents the
build from inadvertently succeeding. For example:
RUN set -o pipefail && wget -O - https://some.site | wc -l > /number
Note
Not all shells support the
-o pipefail
option.In cases such as the
dash
shell on Debian-based images, consider using the exec form ofRUN
to explicitly choose a shell that does support thepipefail
option. For example:RUN [""/bin/bash"", ""-c"", ""set -o pipefail && wget -O - https://some.site | wc -l > /number""]
CMD
The CMD
instruction should be used to run the software contained in your
image, along with any arguments. CMD
should almost always be used in the form
of CMD [""executable"", ""param1"", ""param2""]
. Thus, if the image is for a
service, such as Apache and Rails, you would run something like CMD [""apache2"",""-DFOREGROUND""]
. Indeed, this form of the instruction is recommended
for any service-based image.
In most other cases, CMD
should be given an interactive shell, such as bash,
python and perl. For example, CMD [""perl"", ""-de0""]
, CMD [""python""]
, or CMD [""php"", ""-a""]
. Using this form means that when you execute something like
docker run -it python
, you’ll get dropped into a usable shell, ready to go.
CMD
should rarely be used in the manner of CMD [""param"", ""param""]
in
conjunction with
ENTRYPOINT
, unless
you and your expected users are already quite familiar with how ENTRYPOINT
works.
For more information about CMD
, see
Dockerfile reference for the CMD instruction.
EXPOSE
The EXPOSE
instruction indicates the ports on which a container listens
for connections. Consequently, you should use the common, traditional port for
your application. For example, an image containing the Apache web server would
use EXPOSE 80
, while an image containing MongoDB would use EXPOSE 27017
and
so on.
For external access, your users can execute docker run
with a flag indicating
how to map the specified port to the port of their choice.
For container linking, Docker provides environment variables for the path from
the recipient container back to the source (for example, MYSQL_PORT_3306_TCP
).
For more information about EXPOSE
, see
Dockerfile reference for the EXPOSE instruction.
ENV
To make new software easier to run, you can use ENV
to update the
PATH
environment variable for the software your container installs. For
example, ENV PATH=/usr/local/nginx/bin:$PATH
ensures that CMD [""nginx""]
just works.
The ENV
instruction is also useful for providing the required environment
variables specific to services you want to containerize, such as Postgres’s
PGDATA
.
Lastly, ENV
can also be used to set commonly used version numbers so that
version bumps are easier to maintain, as seen in the following example:
ENV PG_MAJOR=9.3
ENV PG_VERSION=9.3.4
RUN curl -SL https://example.com/postgres-$PG_VERSION.tar.xz | tar -xJC /usr/src/postgres && …
ENV PATH=/usr/local/postgres-$PG_MAJOR/bin:$PATH
Similar to having constant variables in a program, as opposed to hard-coding
values, this approach lets you change a single ENV
instruction to
automatically bump the version of the software in your container.
Each ENV
line creates a new intermediate layer, just like RUN
commands. This
means that even if you unset the environment variable in a future layer, it
still persists in this layer and its value can be dumped. You can test this by
creating a Dockerfile like the following, and then building it.
# syntax=docker/dockerfile:1
FROM alpine
ENV ADMIN_USER=""mark""
RUN echo $ADMIN_USER > ./mark
RUN unset ADMIN_USER
$ docker run --rm test sh -c 'echo $ADMIN_USER'
mark
To prevent this, and really unset the environment variable, use a RUN
command
with shell commands, to set, use, and unset the variable all in a single layer.
You can separate your commands with ;
or &&
. If you use the second method,
and one of the commands fails, the docker build
also fails. This is usually a
good idea. Using \
as a line continuation character for Linux Dockerfiles
improves readability. You could also put all of the commands into a shell script
and have the RUN
command just run that shell script.
# syntax=docker/dockerfile:1
FROM alpine
RUN export ADMIN_USER=""mark"" \
&& echo $ADMIN_USER > ./mark \
&& unset ADMIN_USER
CMD sh
$ docker run --rm test sh -c 'echo $ADMIN_USER'
For more information about ENV
, see
Dockerfile reference for the ENV instruction.
ADD or COPY
ADD
and COPY
are functionally similar. COPY
supports basic copying of
files into the container, from the
build context
or from a stage in a
multi-stage build.
ADD
supports features for fetching files from remote HTTPS and Git URLs, and
extracting tar files automatically when adding files from the build context.
You'll mostly want to use COPY
for copying files from one stage to another in
a multi-stage build. If you need to add files from the build context to the
container temporarily to execute a RUN
instruction, you can often substitute
the COPY
instruction with a bind mount instead. For example, to temporarily
add a requirements.txt
file for a RUN pip install
instruction:
RUN --mount=type=bind,source=requirements.txt,target=/tmp/requirements.txt \
pip install --requirement /tmp/requirements.txt
Bind mounts are more efficient than COPY
for including files from the build
context in the container. Note that bind-mounted files are only added
temporarily for a single RUN
instruction, and don't persist in the final
image. If you need to include files from the build context in the final image,
use COPY
.
The ADD
instruction is best for when you need to download a remote artifact
as part of your build. ADD
is better than manually adding files using
something like wget
and tar
, because it ensures a more precise build cache.
ADD
also has built-in support for checksum validation of the remote
resources, and a protocol for parsing branches, tags, and subdirectories from
Git URLs.
The following example uses ADD
to download a .NET installer. Combined with
multi-stage builds, only the .NET runtime remains in the final stage, no
intermediate files.
# syntax=docker/dockerfile:1
FROM scratch AS src
ARG DOTNET_VERSION=8.0.0-preview.6.23329.7
ADD --checksum=sha256:270d731bd08040c6a3228115de1f74b91cf441c584139ff8f8f6503447cebdbb \
https://dotnetcli.azureedge.net/dotnet/Runtime/$DOTNET_VERSION/dotnet-runtime-$DOTNET_VERSION-linux-arm64.tar.gz /dotnet.tar.gz
FROM mcr.microsoft.com/dotnet/runtime-deps:8.0.0-preview.6-bookworm-slim-arm64v8 AS installer
# Retrieve .NET Runtime
RUN --mount=from=src,target=/src <<EOF
mkdir -p /dotnet
tar -oxzf /src/dotnet.tar.gz -C /dotnet
EOF
FROM mcr.microsoft.com/dotnet/runtime-deps:8.0.0-preview.6-bookworm-slim-arm64v8
COPY --from=installer /dotnet /usr/share/dotnet
RUN ln -s /usr/share/dotnet/dotnet /usr/bin/dotnet
For more information about ADD
or COPY
, see the following:
ENTRYPOINT
The best use for ENTRYPOINT
is to set the image's main command, allowing that
image to be run as though it was that command, and then use CMD
as the
default flags.
The following is an example of an image for the command line tool s3cmd
:
ENTRYPOINT [""s3cmd""]
CMD [""--help""]
You can use the following command to run the image and show the command's help:
$ docker run s3cmd
Or, you can use the right parameters to execute a command, like in the following example:
$ docker run s3cmd ls s3://mybucket
This is useful because the image name can double as a reference to the binary as shown in the command above.
The ENTRYPOINT
instruction can also be used in combination with a helper
script, allowing it to function in a similar way to the command above, even
when starting the tool may require more than one step.
For example, the
Postgres Official Image
uses the following script as its ENTRYPOINT
:
#!/bin/bash
set -e
if [ ""$1"" = 'postgres' ]; then
chown -R postgres ""$PGDATA""
if [ -z ""$(ls -A ""$PGDATA"")"" ]; then
gosu postgres initdb
fi
exec gosu postgres ""$@""
fi
exec ""$@""
This script uses
the exec
Bash command so that the final running application becomes the container's PID 1. This allows the application to receive any Unix signals sent to the container. For more information, see the
ENTRYPOINT
reference.
In the following example, a helper script is copied into the container and run via ENTRYPOINT
on
container start:
COPY ./docker-entrypoint.sh /
ENTRYPOINT [""/docker-entrypoint.sh""]
CMD [""postgres""]
This script lets you interact with Postgres in several ways.
It can simply start Postgres:
$ docker run postgres
Or, you can use it to run Postgres and pass parameters to the server:
$ docker run postgres postgres --help
Lastly, you can use it to start a totally different tool, such as Bash:
$ docker run --rm -it postgres bash
For more information about ENTRYPOINT
, see
Dockerfile reference for the ENTRYPOINT instruction.
VOLUME
You should use the VOLUME
instruction to expose any database storage area,
configuration storage, or files and folders created by your Docker container. You
are strongly encouraged to use VOLUME
for any combination of mutable or user-serviceable
parts of your image.
For more information about VOLUME
, see
Dockerfile reference for the VOLUME instruction.
USER
If a service can run without privileges, use USER
to change to a non-root
user. Start by creating the user and group in the Dockerfile with something
like the following example:
RUN groupadd -r postgres && useradd --no-log-init -r -g postgres postgres
Note
Consider an explicit UID/GID.
Users and groups in an image are assigned a non-deterministic UID/GID in that the ""next"" UID/GID is assigned regardless of image rebuilds. So, if it’s critical, you should assign an explicit UID/GID.
Note
Due to an unresolved bug in the Go archive/tar package's handling of sparse files, attempting to create a user with a significantly large UID inside a Docker container can lead to disk exhaustion because
/var/log/faillog
in the container layer is filled with NULL (\0) characters. A workaround is to pass the--no-log-init
flag to useradd. The Debian/Ubuntuadduser
wrapper does not support this flag.
Avoid installing or using sudo
as it has unpredictable TTY and
signal-forwarding behavior that can cause problems. If you absolutely need
functionality similar to sudo
, such as initializing the daemon as root
but
running it as non-root
, consider using
“gosu”.
Lastly, to reduce layers and complexity, avoid switching USER
back and forth
frequently.
For more information about USER
, see
Dockerfile reference for the USER instruction.
WORKDIR
For clarity and reliability, you should always use absolute paths for your
WORKDIR
. Also, you should use WORKDIR
instead of proliferating instructions
like RUN cd … && do-something
, which are hard to read, troubleshoot, and
maintain.
For more information about WORKDIR
, see
Dockerfile reference for the WORKDIR instruction.
ONBUILD
An ONBUILD
command executes after the current Dockerfile build completes.
ONBUILD
executes in any child image derived FROM
the current image. Think
of the ONBUILD
command as an instruction that the parent Dockerfile gives
to the child Dockerfile.
A Docker build executes ONBUILD
commands before any command in a child
Dockerfile.
ONBUILD
is useful for images that are going to be built FROM
a given
image. For example, you would use ONBUILD
for a language stack image that
builds arbitrary user software written in that language within the
Dockerfile, as you can see in
Ruby’s ONBUILD
variants.
Images built with ONBUILD
should get a separate tag. For example,
ruby:1.9-onbuild
or ruby:2.0-onbuild
.
Be careful when putting ADD
or COPY
in ONBUILD
. The onbuild image
fails catastrophically if the new build's context is missing the resource being
added. Adding a separate tag, as recommended above, helps mitigate this by
allowing the Dockerfile author to make a choice.
For more information about ONBUILD
, see
Dockerfile reference for the ONBUILD instruction.",,,
34c7399c3106d053d728fa7d29d5946cfe03b9ceff42c4ba1955ca5d5a07bf71,"Manage Docker products
In this section, learn how to manage access and view usage of the Docker products for your organization. For more detailed information about each product, including how to set up and configure them, see the following manuals:
Manage access to Docker products
Access to Docker products included in your subscription is enabled by default for all users. The included products are:
- Docker Hub
- Docker Build Cloud
- Docker Desktop
- Docker Scout
Testcontainers Cloud is not enabled by default. To enable Testcontainers Cloud, see the Testcontainers Getting Started guide.
The following sections describe how to enable or disable access for these products.
Manage access to Docker Build Cloud
To learn how to initially set up and configure Docker Build Cloud, sign in to the Docker Build Cloud Dashboard and follow the on-screen instructions.
To manage access to Docker Build Cloud, sign in to Docker Build Cloud as an organization owner, select Account settings, and then manage access under Lock Docker Build Cloud.
Manage access to Docker Scout
To learn how to initially set up and configure Docker Scout for remote repositories, sign in to the Docker Scout Dashboard and follow the on-screen instructions.
To manage access to Docker Scout for use on remote repositories, sign in to the Docker Scout Dashboard and configure integrations and repository settings.
To manage access to Docker Scout for use on local images with Docker Desktop, use
Settings
Management
and set sbomIndexing
to false
to disable, or to true
to enable.
Manage access to Docker Hub
To manage access to Docker Hub, sign in to the Docker Admin Console and configure Registry Access Management or Image Access Management.
Manage access to Testcontainers Cloud
To learn how to initially set up and configure Testcontainers Cloud, sign in to Testcontainers Cloud and follow the on-screen instructions.
To manage access to Testcontainers Cloud, sign in to the Testcontainers Cloud Settings page as an organization owner, and then manage access under Lock Testcontainers Cloud.
Manage access to Docker Desktop
To manage access to Docker Desktop, you can enforce sign-in, then and manage members manually or use provisioning. With sign-in enforced, only users who are a member of your organization can use Docker Desktop after signing in.
View Docker product usage
View usage for the products on the following pages:
Docker Build Cloud: View the Build minutes page in the Docker Build Cloud Dashboard.
Docker Scout: View the Repository settings page in the Docker Scout Dashboard.
Docker Hub: View the Usage page in Docker Hub.
Testcontainers Cloud: View the Billing page in the Testcontainers Cloud Dashboard.
Docker Desktop: View the Insights page in the Docker Admin Console. For more details, see Insights.
If your usage exceeds your subscription amount, you can scale your subscription to meet your needs.",,,
8e7f66ae9d7b9cbfab3e6e3d05ec82d2a7016f059cf92efdbbf74a84b1ac6c39,"Remote Bake file definition
You can build Bake files directly from a remote Git repository or HTTPS URL:
$ docker buildx bake ""https://github.com/docker/cli.git#v20.10.11"" --print
#1 [internal] load git source https://github.com/docker/cli.git#v20.10.11
#1 0.745 e8f1871b077b64bcb4a13334b7146492773769f7 refs/tags/v20.10.11
#1 2.022 From https://github.com/docker/cli
#1 2.022 * [new tag] v20.10.11 -> v20.10.11
#1 DONE 2.9s
This fetches the Bake definition from the specified remote location and
executes the groups or targets defined in that file. If the remote Bake
definition doesn't specify a build context, the context is automatically set to
the Git remote. For example,
this case
uses https://github.com/docker/cli.git
:
{
""group"": {
""default"": {
""targets"": [""binary""]
}
},
""target"": {
""binary"": {
""context"": ""https://github.com/docker/cli.git#v20.10.11"",
""dockerfile"": ""Dockerfile"",
""args"": {
""BASE_VARIANT"": ""alpine"",
""GO_STRIP"": """",
""VERSION"": """"
},
""target"": ""binary"",
""platforms"": [""local""],
""output"": [""build""]
}
}
}
Use the local context with a remote definition
When building with a remote Bake definition, you may want to consume local
files relative to the directory where the Bake command is executed. You can
define contexts as relative to the command context using a cwd://
prefix.
target ""default"" {
context = ""cwd://""
dockerfile-inline = <<EOT
FROM alpine
WORKDIR /src
COPY . .
RUN ls -l && stop
EOT
}
$ touch foo bar
$ docker buildx bake ""https://github.com/dvdksn/buildx.git#bake-remote-example""
...
> [4/4] RUN ls -l && stop:
#8 0.101 total 0
#8 0.102 -rw-r--r-- 1 root root 0 Jul 27 18:47 bar
#8 0.102 -rw-r--r-- 1 root root 0 Jul 27 18:47 foo
#8 0.102 /bin/sh: stop: not found
You can append a path to the cwd://
prefix if you want to use a specific
local directory as a context. Note that if you do specify a path, it must be
within the working directory where the command gets executed. If you use an
absolute path, or a relative path leading outside of the working directory,
Bake will throw an error.
Local named contexts
You can also use the cwd://
prefix to define local directories in the Bake
execution context as named contexts.
The following example defines the docs
context as ./src/docs/content
,
relative to the current working directory where Bake is run as a named context.
target ""default"" {
contexts = {
docs = ""cwd://src/docs/content""
}
dockerfile = ""Dockerfile""
}
By contrast, if you omit the cwd://
prefix, the path would be resolved
relative to the build context.
Specify the Bake definition to use
When loading a Bake file from a remote Git repository, if the repository
contains more than one Bake file, you can specify which Bake definition to use
with the --file
or -f
flag:
docker buildx bake -f bake.hcl ""https://github.com/crazy-max/buildx.git#remote-with-local""
...
#4 [2/2] RUN echo ""hello world""
#4 0.270 hello world
#4 DONE 0.3s
Combine local and remote Bake definitions
You can also combine remote definitions with local ones using the cwd://
prefix with -f
.
Given the following local Bake definition in the current working directory:
# local.hcl
target ""default"" {
args = {
HELLO = ""foo""
}
}
The following example uses -f
to specify two Bake definitions:
-f bake.hcl
: this definition is loaded relative to the Git URL.-f cwd://local.hcl
: this definition is loaded relative to the current working directory where the Bake command is executed.
docker buildx bake -f bake.hcl -f cwd://local.hcl ""https://github.com/crazy-max/buildx.git#remote-with-local"" --print
{
""target"": {
""default"": {
""context"": ""https://github.com/crazy-max/buildx.git#remote-with-local"",
""dockerfile"": ""Dockerfile"",
""args"": {
""HELLO"": ""foo""
},
""target"": ""build"",
""output"": [
{
""type"": ""cacheonly""
}
]
}
}
}
One case where combining local and remote Bake definitions becomes necessary is
when you're building with a remote Bake definition in GitHub Actions and want
to use the
metadata-action to
generate tags, annotations, or labels. The metadata action generates a Bake
file available in the runner's local Bake execution context. To use both the
remote definition and the local ""metadata-only"" Bake file, specify both files
and use the cwd://
prefix for the metadata Bake file:
- name: Build
uses: docker/bake-action@v6
with:
files: |
./docker-bake.hcl
cwd://${{ steps.meta.outputs.bake-file }}
targets: build
Remote definition in a private repository
If you want to use a remote definition that lives in a private repository, you may need to specify credentials for Bake to use when fetching the definition.
If you can authenticate to the private repository using the default SSH_AUTH_SOCK
,
then you don't need to specify any additional authentication parameters for Bake.
Bake automatically uses your default agent socket.
For authentication using an HTTP token, or custom SSH agents, use the following environment variables to configure Bake's authentication strategy:",,,
c49961e311c28380e0adf2e5ce53f858e90c26d123d96ecd9737d5dc516faa7b,"Prune unused Docker objects
Docker takes a conservative approach to cleaning up unused objects (often
referred to as ""garbage collection""), such as images, containers, volumes, and
networks. These objects are generally not removed unless you explicitly ask
Docker to do so. This can cause Docker to use extra disk space. For each type of
object, Docker provides a prune
command. In addition, you can use docker system prune
to clean up multiple types of objects at once. This topic shows
how to use these prune
commands.
Prune images
The docker image prune
command allows you to clean up unused images. By
default, docker image prune
only cleans up dangling images. A dangling image
is one that isn't tagged, and isn't referenced by any container. To remove
dangling images:
$ docker image prune
WARNING! This will remove all dangling images.
Are you sure you want to continue? [y/N] y
To remove all images which aren't used by existing containers, use the -a
flag:
$ docker image prune -a
WARNING! This will remove all images without at least one container associated to them.
Are you sure you want to continue? [y/N] y
By default, you are prompted to continue. To bypass the prompt, use the -f
or
--force
flag.
You can limit which images are pruned using filtering expressions with the
--filter
flag. For example, to only consider images created more than 24
hours ago:
$ docker image prune -a --filter ""until=24h""
Other filtering expressions are available. See the
docker image prune
reference
for more examples.
Prune containers
When you stop a container, it isn't automatically removed unless you started it
with the --rm
flag. To see all containers on the Docker host, including
stopped containers, use docker ps -a
. You may be surprised how many containers
exist, especially on a development system! A stopped container's writable layers
still take up disk space. To clean this up, you can use the docker container prune
command.
$ docker container prune
WARNING! This will remove all stopped containers.
Are you sure you want to continue? [y/N] y
By default, you're prompted to continue. To bypass the prompt, use the -f
or
--force
flag.
By default, all stopped containers are removed. You can limit the scope using
the --filter
flag. For instance, the following command only removes
stopped containers older than 24 hours:
$ docker container prune --filter ""until=24h""
Other filtering expressions are available. See the
docker container prune
reference
for more examples.
Prune volumes
Volumes can be used by one or more containers, and take up space on the Docker host. Volumes are never removed automatically, because to do so could destroy data.
$ docker volume prune
WARNING! This will remove all volumes not used by at least one container.
Are you sure you want to continue? [y/N] y
By default, you are prompted to continue. To bypass the prompt, use the -f
or
--force
flag.
By default, all unused volumes are removed. You can limit the scope using
the --filter
flag. For instance, the following command only removes
volumes which aren't labelled with the keep
label:
$ docker volume prune --filter ""label!=keep""
Other filtering expressions are available. See the
docker volume prune
reference
for more examples.
Prune networks
Docker networks don't take up much disk space, but they do create iptables
rules, bridge network devices, and routing table entries. To clean these things
up, you can use docker network prune
to clean up networks which aren't used
by any containers.
$ docker network prune
WARNING! This will remove all networks not used by at least one container.
Are you sure you want to continue? [y/N] y
By default, you're prompted to continue. To bypass the prompt, use the -f
or
--force
flag.
By default, all unused networks are removed. You can limit the scope using
the --filter
flag. For instance, the following command only removes
networks older than 24 hours:
$ docker network prune --filter ""until=24h""
Other filtering expressions are available. See the
docker network prune
reference
for more examples.
Prune everything
The docker system prune
command is a shortcut that prunes images, containers,
and networks. Volumes aren't pruned by default, and you must specify the
--volumes
flag for docker system prune
to prune volumes.
$ docker system prune
WARNING! This will remove:
- all stopped containers
- all networks not used by at least one container
- all dangling images
- unused build cache
Are you sure you want to continue? [y/N] y
To also prune volumes, add the --volumes
flag:
$ docker system prune --volumes
WARNING! This will remove:
- all stopped containers
- all networks not used by at least one container
- all volumes not used by at least one container
- all dangling images
- all build cache
Are you sure you want to continue? [y/N] y
By default, you're prompted to continue. To bypass the prompt, use the -f
or
--force
flag.
By default, all unused containers, networks, and images are removed. You can
limit the scope using the --filter
flag. For instance, the following command
removes items older than 24 hours:
$ docker system prune --filter ""until=24h""
Other filtering expressions are available. See the
docker system prune
reference
for more examples.",,,
02b965e13779e0c8f25da9034a9b119aa49daa1a34d94bb70366d16d07173aec,"Use Scout with different artifact types
Some of the Docker Scout CLI commands support prefixes for specifying the location or type of artifact that you would like to analyze.
By default, image analysis with the docker scout cves
command
targets images in the local image store of the Docker Engine.
The following command always uses a local image if it exists:
$ docker scout cves <image>
If the image doesn't exist locally, Docker pulls the image before running the analysis. Analyzing the same image again would use the same local version by default, even if the tag has since changed in the registry.
By adding a registry://
prefix to the image reference,
you can force Docker Scout to analyze the registry version of the image:
$ docker scout cves registry://<image>
Supported prefixes
The supported prefixes are:
| Prefix | Description |
|---|---|
image:// (default) | Use a local image, or fall back to a registry lookup |
local:// | Use an image from the local image store (don't do a registry lookup) |
registry:// | Use an image from a registry (don't use a local image) |
oci-dir:// | Use an OCI layout directory |
archive:// | Use a tarball archive, as created by docker save |
fs:// | Use a local directory or file |
You can use prefixes with the following commands:
docker scout compare
docker scout cves
docker scout quickview
docker scout recommendations
docker scout sbom
Examples
This section contains a few examples showing how you can use prefixes
to specify artifacts for docker scout
commands.
Analyze a local project
The fs://
prefix lets you analyze local source code directly,
without having to build it into a container image.
The following docker scout quickview
command gives you an
at-a-glance vulnerability summary of the source code in the current working directory:
$ docker scout quickview fs://.
To view the details of vulnerabilities found in your local source code, you can
use the docker scout cves --details fs://.
command. Combine it with
other flags to narrow down the results to the packages and vulnerabilities that
you're interested in.
$ docker scout cves --details --only-severity high fs://.
✓ File system read
✓ Indexed 323 packages
✗ Detected 1 vulnerable package with 1 vulnerability
## Overview
│ Analyzed path
────────────────────┼──────────────────────────────
Path │ /Users/david/demo/scoutfs
vulnerabilities │ 0C 1H 0M 0L
## Packages and Vulnerabilities
0C 1H 0M 0L fastify 3.29.0
pkg:npm/fastify@3.29.0
✗ HIGH CVE-2022-39288 [OWASP Top Ten 2017 Category A9 - Using Components with Known Vulnerabilities]
https://scout.docker.com/v/CVE-2022-39288
fastify is a fast and low overhead web framework, for Node.js. Affected versions of
fastify are subject to a denial of service via malicious use of the Content-Type
header. An attacker can send an invalid Content-Type header that can cause the
application to crash. This issue has been addressed in commit fbb07e8d and will be
included in release version 4.8.1. Users are advised to upgrade. Users unable to
upgrade may manually filter out http content with malicious Content-Type headers.
Affected range : <4.8.1
Fixed version : 4.8.1
CVSS Score : 7.5
CVSS Vector : CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H
1 vulnerability found in 1 package
LOW 0
MEDIUM 0
HIGH 1
CRITICAL 0
Compare a local project to an image
With docker scout compare
, you can compare the analysis of source code on
your local filesystem with the analysis of a container image.
The following example compares local source code (fs://.
)
with a registry image registry://docker/scout-cli:latest
.
In this case, both the baseline and target for the comparison use prefixes.
$ docker scout compare fs://. --to registry://docker/scout-cli:latest --ignore-unchanged
WARN 'docker scout compare' is experimental and its behaviour might change in the future
✓ File system read
✓ Indexed 268 packages
✓ SBOM of image already cached, 234 packages indexed
## Overview
│ Analyzed File System │ Comparison Image
─────────────────────────┼────────────────────────────────────────────────┼─────────────────────────────────────────────
Path / Image reference │ /Users/david/src/docker/scout-cli-plugin │ docker/scout-cli:latest
│ │ bb0b01303584
platform │ │ linux/arm64
provenance │ https://github.com/dvdksn/scout-cli-plugin.git │ https://github.com/docker/scout-cli-plugin
│ 6ea3f7369dbdfec101ac7c0fa9d78ef05ffa6315 │ 67cb4ef78bd69545af0e223ba5fb577b27094505
vulnerabilities │ 0C 0H 1M 1L │ 0C 0H 1M 1L
│ │
size │ 7.4 MB (-14 MB) │ 21 MB
packages │ 268 (+34) │ 234
│ │
## Packages and Vulnerabilities
+ 55 packages added
- 21 packages removed
213 packages unchanged
The previous example is truncated for brevity.
View the SBOM of an image tarball
The following example shows how you can use the archive://
prefix
to get the SBOM of an image tarball, created with docker save
.
The image in this case is docker/scout-cli:latest
,
and the SBOM is exported to file sbom.spdx.json
in SPDX format.
$ docker pull docker/scout-cli:latest
latest: Pulling from docker/scout-cli
257973a141f5: Download complete
1f2083724dd1: Download complete
5c8125a73507: Download complete
Digest: sha256:13318bb059b0f8b0b87b35ac7050782462b5d0ac3f96f9f23d165d8ed68d0894
$ docker save docker/scout-cli:latest -o scout-cli.tar
$ docker scout sbom --format spdx -o sbom.spdx.json archive://scout-cli.tar
Learn more
Read about the commands and supported flags in the CLI reference documentation:",,,
18f3dd2f591bd5fe05bd1644aa1f5c213a0fc85e7fa9e4df79c75880b9b409d1,"Rootless mode
Rootless mode allows running the Docker daemon and containers as a non-root user to mitigate potential vulnerabilities in the daemon and the container runtime.
Rootless mode does not require root privileges even during the installation of the Docker daemon, as long as the prerequisites are met.
How it works
Rootless mode executes the Docker daemon and containers inside a user namespace.
This is very similar to
userns-remap
mode, except that
with userns-remap
mode, the daemon itself is running with root privileges,
whereas in rootless mode, both the daemon and the container are running without
root privileges.
Rootless mode does not use binaries with SETUID
bits or file capabilities,
except newuidmap
and newgidmap
, which are needed to allow multiple
UIDs/GIDs to be used in the user namespace.
Prerequisites
You must install
newuidmap
andnewgidmap
on the host. These commands are provided by theuidmap
package on most distributions./etc/subuid
and/etc/subgid
should contain at least 65,536 subordinate UIDs/GIDs for the user. In the following example, the usertestuser
has 65,536 subordinate UIDs/GIDs (231072-296607).
$ id -u
1001
$ whoami
testuser
$ grep ^$(whoami): /etc/subuid
testuser:231072:65536
$ grep ^$(whoami): /etc/subgid
testuser:231072:65536
Distribution-specific hint
Tip
We recommend that you use the Ubuntu kernel.
Install
dbus-user-session
package if not installed. Runsudo apt-get install -y dbus-user-session
and relogin.Install
uidmap
package if not installed. Runsudo apt-get install -y uidmap
.If running in a terminal where the user was not directly logged into, you will need to install
systemd-container
withsudo apt-get install -y systemd-container
, then switch to TheUser with the commandsudo machinectl shell TheUser@
.overlay2
storage driver is enabled by default ( Ubuntu-specific kernel patch).Ubuntu 24.04 and later enables restricted unprivileged user namespaces by default, which prevents unprivileged processes in creating user namespaces unless an AppArmor profile is configured to allow programs to use unprivileged user namespaces.
If you install
docker-ce-rootless-extras
using the deb package (apt-get install docker-ce-rootless-extras
), then the AppArmor profile forrootlesskit
is already bundled with theapparmor
deb package. With this installation method, you don't need to add any manual the AppArmor configuration. If you install the rootless extras using the installation script, however, you must add an AppArmor profile forrootlesskit
manually:Create and install the currently logged-in user's AppArmor profile:
$ filename=$(echo $HOME/bin/rootlesskit | sed -e s@^/@@ -e s@/@.@g) $ cat <<EOF > ~/${filename} abi <abi/4.0>, include <tunables/global> ""$HOME/bin/rootlesskit"" flags=(unconfined) { userns, include if exists <local/${filename}> } EOF $ sudo mv ~/${filename} /etc/apparmor.d/${filename}
Restart AppArmor.
$ systemctl restart apparmor.service
Install
dbus-user-session
package if not installed. Runsudo apt-get install -y dbus-user-session
and relogin.For Debian 11, installing
fuse-overlayfs
is recommended. Runsudo apt-get install -y fuse-overlayfs
. This step is not required on Debian 12.Rootless docker requires version of
slirp4netns
greater thanv0.4.0
(whenvpnkit
is not installed). Check you have this with$ slirp4netns --version
If you do not have this download and install with
sudo apt-get install -y slirp4netns
or download the latest release.
Installing
fuse-overlayfs
is recommended. Runsudo pacman -S fuse-overlayfs
.Add
kernel.unprivileged_userns_clone=1
to/etc/sysctl.conf
(or/etc/sysctl.d
) and runsudo sysctl --system
For openSUSE 15 and SLES 15, Installing
fuse-overlayfs
is recommended. Runsudo zypper install -y fuse-overlayfs
. This step is not required on openSUSE Tumbleweed.sudo modprobe ip_tables iptable_mangle iptable_nat iptable_filter
is required. This might be required on other distributions as well depending on the configuration.Known to work on openSUSE 15 and SLES 15.
For RHEL 8 and similar distributions, installing
fuse-overlayfs
is recommended. Runsudo dnf install -y fuse-overlayfs
. This step is not required on RHEL 9 and similar distributions.You might need
sudo dnf install -y iptables
.
Known limitations
- Only the following storage drivers are supported:
overlay2
(only if running with kernel 5.11 or later, or Ubuntu-flavored kernel)fuse-overlayfs
(only if running with kernel 4.18 or later, andfuse-overlayfs
is installed)btrfs
(only if running with kernel 4.18 or later, or~/.local/share/docker
is mounted withuser_subvol_rm_allowed
mount option)vfs
- Cgroup is supported only when running with cgroup v2 and systemd. See Limiting resources.
- Following features are not supported:
- AppArmor
- Checkpoint
- Overlay network
- Exposing SCTP ports
- To use the
ping
command, see Routing ping packets. - To expose privileged TCP/UDP ports (< 1024), see Exposing privileged ports.
IPAddress
shown indocker inspect
is namespaced inside RootlessKit's network namespace. This means the IP address is not reachable from the host withoutnsenter
-ing into the network namespace.- Host network (
docker run --net=host
) is also namespaced inside RootlessKit. - NFS mounts as the docker ""data-root"" is not supported. This limitation is not specific to rootless mode.
Install
Note
If the system-wide Docker daemon is already running, consider disabling it:
$ sudo systemctl disable --now docker.service docker.socket $ sudo rm /var/run/docker.sock
Should you choose not to shut down the
docker
service and socket, you will need to use the--force
parameter in the next section. There are no known issues, but until you shutdown and disable you're still running rootful Docker.
If you installed Docker 20.10 or later with
RPM/DEB packages, you should have dockerd-rootless-setuptool.sh
in /usr/bin
.
Run dockerd-rootless-setuptool.sh install
as a non-root user to set up the daemon:
$ dockerd-rootless-setuptool.sh install
[INFO] Creating /home/testuser/.config/systemd/user/docker.service
...
[INFO] Installed docker.service successfully.
[INFO] To control docker.service, run: `systemctl --user (start|stop|restart) docker.service`
[INFO] To run docker.service on system startup, run: `sudo loginctl enable-linger testuser`
[INFO] Make sure the following environment variables are set (or add them to ~/.bashrc):
export PATH=/usr/bin:$PATH
export DOCKER_HOST=unix:///run/user/1000/docker.sock
If dockerd-rootless-setuptool.sh
is not present, you may need to install the docker-ce-rootless-extras
package manually, e.g.,
$ sudo apt-get install -y docker-ce-rootless-extras
If you do not have permission to run package managers like apt-get
and dnf
,
consider using the installation script available at
https://get.docker.com/rootless.
Since static packages are not available for s390x
, hence it is not supported for s390x
.
$ curl -fsSL https://get.docker.com/rootless | sh
...
[INFO] Creating /home/testuser/.config/systemd/user/docker.service
...
[INFO] Installed docker.service successfully.
[INFO] To control docker.service, run: `systemctl --user (start|stop|restart) docker.service`
[INFO] To run docker.service on system startup, run: `sudo loginctl enable-linger testuser`
[INFO] Make sure the following environment variables are set (or add them to ~/.bashrc):
export PATH=/home/testuser/bin:$PATH
export DOCKER_HOST=unix:///run/user/1000/docker.sock
The binaries will be installed at ~/bin
.
See Troubleshooting if you faced an error.
Uninstall
To remove the systemd service of the Docker daemon, run dockerd-rootless-setuptool.sh uninstall
:
$ dockerd-rootless-setuptool.sh uninstall
+ systemctl --user stop docker.service
+ systemctl --user disable docker.service
Removed /home/testuser/.config/systemd/user/default.target.wants/docker.service.
[INFO] Uninstalled docker.service
[INFO] This uninstallation tool does NOT remove Docker binaries and data.
[INFO] To remove data, run: `/usr/bin/rootlesskit rm -rf /home/testuser/.local/share/docker`
Unset environment variables PATH and DOCKER_HOST if you have added them to ~/.bashrc
.
To remove the data directory, run rootlesskit rm -rf ~/.local/share/docker
.
To remove the binaries, remove docker-ce-rootless-extras
package if you installed Docker with package managers.
If you installed Docker with
https://get.docker.com/rootless (
Install without packages),
remove the binary files under ~/bin
:
$ cd ~/bin
$ rm -f containerd containerd-shim containerd-shim-runc-v2 ctr docker docker-init docker-proxy dockerd dockerd-rootless-setuptool.sh dockerd-rootless.sh rootlesskit rootlesskit-docker-proxy runc vpnkit
Usage
Daemon
The systemd unit file is installed as ~/.config/systemd/user/docker.service
.
Use systemctl --user
to manage the lifecycle of the daemon:
$ systemctl --user start docker
To launch the daemon on system startup, enable the systemd service and lingering:
$ systemctl --user enable docker
$ sudo loginctl enable-linger $(whoami)
Starting Rootless Docker as a systemd-wide service (/etc/systemd/system/docker.service
)
is not supported, even with the User=
directive.
To run the daemon directly without systemd, you need to run dockerd-rootless.sh
instead of dockerd
.
The following environment variables must be set:
$HOME
: the home directory$XDG_RUNTIME_DIR
: an ephemeral directory that is only accessible by the expected user, e,g,~/.docker/run
. The directory should be removed on every host shutdown. The directory can be on tmpfs, however, should not be under/tmp
. Locating this directory under/tmp
might be vulnerable to TOCTOU attack.
Remarks about directory paths:
- The socket path is set to
$XDG_RUNTIME_DIR/docker.sock
by default.$XDG_RUNTIME_DIR
is typically set to/run/user/$UID
. - The data dir is set to
~/.local/share/docker
by default. The data dir should not be on NFS. - The daemon config dir is set to
~/.config/docker
by default. This directory is different from~/.docker
that is used by the client.
Client
You need to specify either the socket path or the CLI context explicitly.
To specify the socket path using $DOCKER_HOST
:
$ export DOCKER_HOST=unix://$XDG_RUNTIME_DIR/docker.sock
$ docker run -d -p 8080:80 nginx
To specify the CLI context using docker context
:
$ docker context use rootless
rootless
Current context is now ""rootless""
$ docker run -d -p 8080:80 nginx
Best practices
Rootless Docker in Docker
To run Rootless Docker inside ""rootful"" Docker, use the docker:<version>-dind-rootless
image instead of docker:<version>-dind
.
$ docker run -d --name dind-rootless --privileged docker:25.0-dind-rootless
The docker:<version>-dind-rootless
image runs as a non-root user (UID 1000).
However, --privileged
is required for disabling seccomp, AppArmor, and mount
masks.
Expose Docker API socket through TCP
To expose the Docker API socket through TCP, you need to launch dockerd-rootless.sh
with DOCKERD_ROOTLESS_ROOTLESSKIT_FLAGS=""-p 0.0.0.0:2376:2376/tcp""
.
$ DOCKERD_ROOTLESS_ROOTLESSKIT_FLAGS=""-p 0.0.0.0:2376:2376/tcp"" \
dockerd-rootless.sh \
-H tcp://0.0.0.0:2376 \
--tlsverify --tlscacert=ca.pem --tlscert=cert.pem --tlskey=key.pem
Expose Docker API socket through SSH
To expose the Docker API socket through SSH, you need to make sure $DOCKER_HOST
is set on the remote host.
$ ssh -l <REMOTEUSER> <REMOTEHOST> 'echo $DOCKER_HOST'
unix:///run/user/1001/docker.sock
$ docker -H ssh://<REMOTEUSER>@<REMOTEHOST> run ...
Routing ping packets
On some distributions, ping
does not work by default.
Add net.ipv4.ping_group_range = 0 2147483647
to /etc/sysctl.conf
(or
/etc/sysctl.d
) and run sudo sysctl --system
to allow using ping
.
Exposing privileged ports
To expose privileged ports (< 1024), set CAP_NET_BIND_SERVICE
on rootlesskit
binary and restart the daemon.
$ sudo setcap cap_net_bind_service=ep $(which rootlesskit)
$ systemctl --user restart docker
Or add net.ipv4.ip_unprivileged_port_start=0
to /etc/sysctl.conf
(or
/etc/sysctl.d
) and run sudo sysctl --system
.
Limiting resources
Limiting resources with cgroup-related docker run
flags such as --cpus
, --memory
, --pids-limit
is supported only when running with cgroup v2 and systemd.
See
Changing cgroup version to enable cgroup v2.
If docker info
shows none
as Cgroup Driver
, the conditions are not satisfied.
When these conditions are not satisfied, rootless mode ignores the cgroup-related docker run
flags.
See
Limiting resources without cgroup for workarounds.
If docker info
shows systemd
as Cgroup Driver
, the conditions are satisfied.
However, typically, only memory
and pids
controllers are delegated to non-root users by default.
$ cat /sys/fs/cgroup/user.slice/user-$(id -u).slice/user@$(id -u).service/cgroup.controllers
memory pids
To allow delegation of all controllers, you need to change the systemd configuration as follows:
# mkdir -p /etc/systemd/system/user@.service.d
# cat > /etc/systemd/system/user@.service.d/delegate.conf << EOF
[Service]
Delegate=cpu cpuset io memory pids
EOF
# systemctl daemon-reload
Note
Delegating
cpuset
requires systemd 244 or later.
Limiting resources without cgroup
Even when cgroup is not available, you can still use the traditional ulimit
and
cpulimit
,
though they work in process-granularity rather than in container-granularity,
and can be arbitrarily disabled by the container process.
For example:
To limit CPU usage to 0.5 cores (similar to
docker run --cpus 0.5
):docker run <IMAGE> cpulimit --limit=50 --include-children <COMMAND>
To limit max VSZ to 64MiB (similar to
docker run --memory 64m
):docker run <IMAGE> sh -c ""ulimit -v 65536; <COMMAND>""
To limit max number of processes to 100 per namespaced UID 2000 (similar to
docker run --pids-limit=100
):docker run --user 2000 --ulimit nproc=100 <IMAGE> <COMMAND>
Troubleshooting
Unable to install with systemd when systemd is present on the system
$ dockerd-rootless-setuptool.sh install
[INFO] systemd not detected, dockerd-rootless.sh needs to be started manually:
...
rootlesskit
cannot detect systemd properly if you switch to your user via sudo su
. For users which cannot be logged-in, you must use the machinectl
command which is part of the systemd-container
package. After installing systemd-container
switch to myuser
with the following command:
$ sudo machinectl shell myuser@
Where myuser@
is your desired username and @ signifies this machine.
Errors when starting the Docker daemon
[rootlesskit:parent] error: failed to start the child: fork/exec /proc/self/exe: operation not permitted
This error occurs mostly when the value of /proc/sys/kernel/unprivileged_userns_clone
is set to 0:
$ cat /proc/sys/kernel/unprivileged_userns_clone
0
To fix this issue, add kernel.unprivileged_userns_clone=1
to
/etc/sysctl.conf
(or /etc/sysctl.d
) and run sudo sysctl --system
.
[rootlesskit:parent] error: failed to start the child: fork/exec /proc/self/exe: no space left on device
This error occurs mostly when the value of /proc/sys/user/max_user_namespaces
is too small:
$ cat /proc/sys/user/max_user_namespaces
0
To fix this issue, add user.max_user_namespaces=28633
to
/etc/sysctl.conf
(or /etc/sysctl.d
) and run sudo sysctl --system
.
[rootlesskit:parent] error: failed to setup UID/GID map: failed to compute uid/gid map: No subuid ranges found for user 1001 (""testuser"")
This error occurs when /etc/subuid
and /etc/subgid
are not configured. See
Prerequisites.
could not get XDG_RUNTIME_DIR
This error occurs when $XDG_RUNTIME_DIR
is not set.
On a non-systemd host, you need to create a directory and then set the path:
$ export XDG_RUNTIME_DIR=$HOME/.docker/xrd
$ rm -rf $XDG_RUNTIME_DIR
$ mkdir -p $XDG_RUNTIME_DIR
$ dockerd-rootless.sh
Note
You must remove the directory every time you log out.
On a systemd host, log into the host using pam_systemd
(see below).
The value is automatically set to /run/user/$UID
and cleaned up on every logout.
systemctl --user
fails with ""Failed to connect to bus: No such file or directory""
This error occurs mostly when you switch from the root user to a non-root user with sudo
:
# sudo -iu testuser
$ systemctl --user start docker
Failed to connect to bus: No such file or directory
Instead of sudo -iu <USERNAME>
, you need to log in using pam_systemd
. For example:
- Log in through the graphic console
ssh <USERNAME>@localhost
machinectl shell <USERNAME>@
The daemon does not start up automatically
You need sudo loginctl enable-linger $(whoami)
to enable the daemon to start
up automatically. See
Usage.
iptables failed: iptables -t nat -N DOCKER: Fatal: can't open lock file /run/xtables.lock: Permission denied
This error may happen with an older version of Docker when SELinux is enabled on the host.
The issue has been fixed in Docker 20.10.8.
A known workaround for older version of Docker is to run the following commands to disable SELinux for iptables
:
$ sudo dnf install -y policycoreutils-python-utils && sudo semanage permissive -a iptables_t
docker pull
errors
docker: failed to register layer: Error processing tar file(exit status 1): lchown <FILE>: invalid argument
This error occurs when the number of available entries in /etc/subuid
or
/etc/subgid
is not sufficient. The number of entries required vary across
images. However, 65,536 entries are sufficient for most images. See
Prerequisites.
docker: failed to register layer: ApplyLayer exit status 1 stdout: stderr: lchown <FILE>: operation not permitted
This error occurs mostly when ~/.local/share/docker
is located on NFS.
A workaround is to specify non-NFS data-root
directory in ~/.config/docker/daemon.json
as follows:
{""data-root"":""/somewhere-out-of-nfs""}
docker run
errors
docker: Error response from daemon: OCI runtime create failed: ...: read unix @->/run/systemd/private: read: connection reset by peer: unknown.
This error occurs on cgroup v2 hosts mostly when the dbus daemon is not running for the user.
$ systemctl --user is-active dbus
inactive
$ docker run hello-world
docker: Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:385: applying cgroup configuration for process caused: error while starting unit ""docker
-931c15729b5a968ce803784d04c7421f791d87e5ca1891f34387bb9f694c488e.scope"" with properties [{Name:Description Value:""libcontainer container 931c15729b5a968ce803784d04c7421f791d87e5ca1891f34387bb9f694c488e""} {Name:Slice Value:""use
r.slice""} {Name:PIDs Value:@au [4529]} {Name:Delegate Value:true} {Name:MemoryAccounting Value:true} {Name:CPUAccounting Value:true} {Name:IOAccounting Value:true} {Name:TasksAccounting Value:true} {Name:DefaultDependencies Val
ue:false}]: read unix @->/run/systemd/private: read: connection reset by peer: unknown.
To fix the issue, run sudo apt-get install -y dbus-user-session
or sudo dnf install -y dbus-daemon
, and then relogin.
If the error still occurs, try running systemctl --user enable --now dbus
(without sudo).
--cpus
, --memory
, and --pids-limit
are ignored
This is an expected behavior on cgroup v1 mode. To use these flags, the host needs to be configured for enabling cgroup v2. For more information, see Limiting resources.
Networking errors
This section provides troubleshooting tips for networking in rootless mode.
Networking in rootless mode is supported via network and port drivers in RootlessKit. Network performance and characteristics depend on the combination of network and port driver you use. If you're experiencing unexpected behavior or performance related to networking, review the following table which shows the configurations supported by RootlessKit, and how they compare:
| Network driver | Port driver | Net throughput | Port throughput | Source IP propagation | No SUID | Note |
|---|---|---|---|---|---|---|
slirp4netns | builtin | Slow | Fast ✅ | ❌ | ✅ | Default in a typical setup |
vpnkit | builtin | Slow | Fast ✅ | ❌ | ✅ | Default when slirp4netns isn't installed |
slirp4netns | slirp4netns | Slow | Slow | ✅ | ✅ | |
pasta | implicit | Slow | Fast ✅ | ✅ | ✅ | Experimental; Needs pasta version 2023_12_04 or later |
lxc-user-nic | builtin | Fast ✅ | Fast ✅ | ❌ | ❌ | Experimental |
bypass4netns | bypass4netns | Fast ✅ | Fast ✅ | ✅ | ✅ | Note: Not integrated to RootlessKit as it needs a custom seccomp profile |
For information about troubleshooting specific networking issues, see:
docker run -p
fails withcannot expose privileged port
- Ping doesn't work
IPAddress
shown indocker inspect
is unreachable--net=host
doesn't listen ports on the host network namespace- Network is slow
docker run -p
does not propagate source IP addresses
docker run -p
fails with cannot expose privileged port
docker run -p
fails with this error when a privileged port (< 1024) is specified as the host port.
$ docker run -p 80:80 nginx:alpine
docker: Error response from daemon: driver failed programming external connectivity on endpoint focused_swanson (9e2e139a9d8fc92b37c36edfa6214a6e986fa2028c0cc359812f685173fa6df7): Error starting userland proxy: error while calling PortManager.AddPort(): cannot expose privileged port 80, you might need to add ""net.ipv4.ip_unprivileged_port_start=0"" (currently 1024) to /etc/sysctl.conf, or set CAP_NET_BIND_SERVICE on rootlesskit binary, or choose a larger port number (>= 1024): listen tcp 0.0.0.0:80: bind: permission denied.
When you experience this error, consider using an unprivileged port instead. For example, 8080 instead of 80.
$ docker run -p 8080:80 nginx:alpine
To allow exposing privileged ports, see Exposing privileged ports.
Ping doesn't work
Ping does not work when /proc/sys/net/ipv4/ping_group_range
is set to 1 0
:
$ cat /proc/sys/net/ipv4/ping_group_range
1 0
For details, see Routing ping packets.
IPAddress
shown in docker inspect
is unreachable
This is an expected behavior, as the daemon is namespaced inside RootlessKit's
network namespace. Use docker run -p
instead.
--net=host
doesn't listen ports on the host network namespace
This is an expected behavior, as the daemon is namespaced inside RootlessKit's
network namespace. Use docker run -p
instead.
Network is slow
Docker with rootless mode uses slirp4netns as the default network stack if slirp4netns v0.4.0 or later is installed. If slirp4netns is not installed, Docker falls back to VPNKit. Installing slirp4netns may improve the network throughput.
For more information about network drivers for RootlessKit, see RootlessKit documentation.
Also, changing MTU value may improve the throughput.
The MTU value can be specified by creating ~/.config/systemd/user/docker.service.d/override.conf
with the following content:
[Service]
Environment=""DOCKERD_ROOTLESS_ROOTLESSKIT_MTU=<INTEGER>""
And then restart the daemon:
$ systemctl --user daemon-reload
$ systemctl --user restart docker
docker run -p
does not propagate source IP addresses
This is because Docker in rootless mode uses RootlessKit's builtin
port
driver by default, which doesn't support source IP propagation. To enable
source IP propagation, you can:
- Use the
slirp4netns
RootlessKit port driver - Use the
pasta
RootlessKit network driver, with theimplicit
port driver
The pasta
network driver is experimental, but provides improved throughput
performance compared to the slirp4netns
port driver. The pasta
driver
requires Docker Engine version 25.0 or later.
To change the RootlessKit networking configuration:
Create a file at
~/.config/systemd/user/docker.service.d/override.conf
.Add the following contents, depending on which configuration you would like to use:
slirp4netns
[Service] Environment=""DOCKERD_ROOTLESS_ROOTLESSKIT_NET=slirp4netns"" Environment=""DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=slirp4netns""
pasta
network driver withimplicit
port driver[Service] Environment=""DOCKERD_ROOTLESS_ROOTLESSKIT_NET=pasta"" Environment=""DOCKERD_ROOTLESS_ROOTLESSKIT_PORT_DRIVER=implicit""
Restart the daemon:
$ systemctl --user daemon-reload $ systemctl --user restart docker
For more information about networking options for RootlessKit, see:
Tips for debugging
Entering into dockerd
namespaces
The dockerd-rootless.sh
script executes dockerd
in its own user, mount, and network namespaces.
For debugging, you can enter the namespaces by running
nsenter -U --preserve-credentials -n -m -t $(cat $XDG_RUNTIME_DIR/docker.pid)
.",,,
6d23c750eb5666bc532b4d0ee4d809bc1db403f43f0f1dd5a1823581bcdf7eb2,"Best practices
Always use the latest version of WSL. At a minimum you must use WSL version 1.1.3.0., otherwise Docker Desktop may not work as expected. Testing, development, and documentation is based on the newest kernel versions. Older versions of WSL can cause:
- Docker Desktop to hang periodically or when upgrading
- Deployment via SCCM to fail
- The
vmmem.exe
to consume all memory - Network filter policies to be applied globally, not to specific objects
- GPU failures with containers
To get the best out of the file system performance when bind-mounting files, it's recommended that you store source code and other data that is bind-mounted into Linux containers. For instance, use
docker run -v <host-path>:<container-path>
in the Linux file system, rather than the Windows file system. You can also refer to the recommendation from Microsoft.- Linux containers only receive file change events, “inotify events”, if the original files are stored in the Linux filesystem. For example, some web development workflows rely on inotify events for automatic reloading when files have changed.
- Performance is much higher when files are bind-mounted from the Linux filesystem, rather than remoted from the Windows host. Therefore avoid
docker run -v /mnt/c/users:/users,
where/mnt/c
is mounted from Windows. - Instead, from a Linux shell use a command like
docker run -v ~/my-project:/sources <my-image>
where~
is expanded by the Linux shell to$HOME
.
If you have concerns about the size of the
docker-desktop-data
distribution, take a look at the WSL tooling built into Windows.- Installations of Docker Desktop version 4.30 and later no longer rely on the
docker-desktop-data
distribution; instead Docker Desktop creates and manages its own virtual hard disk (VHDX) for storage. (note, however, that Docker Desktop keeps using thedocker-desktop-data
distribution if it was already created by an earlier version of the software). - Starting from version 4.34 and later, Docker Desktop automatically manages the size of the managed VHDX and returns unused space to the operating system.
- Installations of Docker Desktop version 4.30 and later no longer rely on the
If you have concerns about CPU or memory usage, you can configure limits on the memory, CPU, and swap size allocated to the WSL 2 utility VM.",,,
7bea3d88189bbbf219946d5132953933f17dfaeecb358d79c92b1a13e216741e,"Data collection and storage in Docker Scout
Docker Scout's image analysis works by collecting metadata from the container images that you analyze. This metadata is stored on the Docker Scout platform.
Data transmission
This section describes the data that Docker Scout collects and sends to the platform.
Image metadata
Docker Scout collects the following image metadata:
- Image creation timestamp
- Image digest
- Ports exposed by the image
- Environment variable names and values
- Name and value of image labels
- Order of image layers
- Hardware architecture
- Operating system type and version
- Registry URL and type
Image digests are created for each layer of an image when the image is built and pushed to a registry. They are SHA256 digests of the contents of a layer. Docker Scout doesn't create the digests; they're read from the image manifest.
The digests are matched against your own private images and Docker's database of public images to identify images that share the same layers. The image that shares most of the layers is considered a base image match for the image that's currently being analyzed.
SBOM metadata
Software Bill of Material (SBOM) metadata is used to match package types and versions with vulnerability data to infer whether an image is affected. When the Docker Scout platform receives information from security advisories about new CVEs or other risk factors, such as leaked secrets, it cross-references this information with the SBOM. If there's a match, Docker Scout displays the results in the user interfaces where Docker Scout data is surfaced, such as the Docker Scout Dashboard and in Docker Desktop.
Docker Scout collects the following SBOM metadata:
- Package URLs (PURL)
- Package author and description
- License IDs
- Package name and namespace
- Package scheme and size
- Package type and version
- Filepath within the image
- The type of direct dependency
- Total package count
The PURLs in Docker Scout follow the purl-spec specification. Package information is derived from the contents of image, including OS-level programs and packages, and application-level packages such as maven, npm, and so on.
Environment metadata
If you integrate Docker Scout with your runtime environment via the Sysdig integration, Docker Scout collects the following data points about your deployments:
- Kubernetes namespace
- Workload name
- Workload type (for example, DaemonSet)
Local analysis
For images analyzed locally on a developer's machine, Docker Scout only transmits PURLs and layer digests. This data isn't persistently stored on the Docker Scout platform; it's only used to run the analysis.
Provenance
For images with provenance attestations, Docker Scout stores the following data in addition to the SBOM:
- Materials
- Base image
- VCS information
- Dockerfile
Data storage
For the purposes of providing the Docker Scout service, data is stored using:
- Amazon Web Services (AWS) on servers located in US East
- Google Cloud Platform (GCP) on servers located in US East
Data is used according to the processes described at docker.com/legal to provide the key capabilities of Docker Scout.",,,
c32308d04f7eb8581d815611c7897f20a41d6846b31ac125a582929475f531e1,"Image attestation storage
Buildkit supports creating and attaching attestations to build artifacts. These attestations can provide valuable information from the build process, including, but not limited to: SBOMs, SLSA Provenance, build logs, etc.
This document describes the current custom format used to store attestations, which is designed to be compatible with current registry implementations today. In the future, we may support exporting attestations in additional formats.
Attestations are stored as manifest objects in the image index, similar in style to OCI artifacts.
Properties
Attestation Manifest
Attestation manifests are attached to the root image index object, under a separate OCI image manifest. Each attestation manifest can contain multiple attestation blobs, with all the of the attestations in a manifest applying to a single platform manifest. All properties of standard OCI and Docker manifests continue to apply.
The image config
descriptor will point to a valid
image config,
however, it will not contain attestation-specific details, and should be
ignored as it is only included for compatibility purposes.
Each image layer in layers
will contain a descriptor for a single
attestation blob. The mediaType
of each layer will be
set in accordance to its contents, one of:
application/vnd.in-toto+json
(currently, the only supported option)Indicates an in-toto attestation blob
Any unknown mediaType
s should be ignored.
To assist attestation traversal, the following annotations may be set on each layer descriptor:
in-toto.io/predicate-type
This annotation will be set if the enclosed attestation is an in-toto attestation (currently, the only supported option). The annotation will be set to contain the same value as the
predicateType
property present inside the attestation.When present, this annotation may be used to find the specific attestation(s) they are looking for to avoid pulling the contents of the others.
Attestation Blob
The contents of each layer will be a blob dependent on its mediaType
.
application/vnd.in-toto+json
The blob contents will contain a full in-toto attestation statement:
{ ""_type"": ""https://in-toto.io/Statement/v0.1"", ""subject"": [ { ""name"": ""<NAME>"", ""digest"": {""<ALGORITHM>"": ""<HEX_VALUE>""} }, ... ], ""predicateType"": ""<URI>"", ""predicate"": { ... } }
The subject of the attestation should be set to be the same digest as the target manifest described in the Attestation Manifest Descriptor, or some object within.
Attestation Manifest Descriptor
Attestation manifests are attached to the root
image index,
in the manifests
key, after all the original runnable manifests. All
properties of standard OCI and Docker manifest descriptors continue to apply.
To prevent container runtimes from accidentally pulling or running the image
described in the manifest, the platform
property of the attestation manifest
will be set to unknown/unknown
, as follows:
""platform"": {
""architecture"": ""unknown"",
""os"": ""unknown""
}
To assist index traversal, the following annotations will be set on the manifest descriptor descriptor:
vnd.docker.reference.type
This annotation describes the type of the artifact, and will be set to
attestation-manifest
. If any other value is specified, the entire manifest should be ignored.vnd.docker.reference.digest
This annotation will contain the digest of the object in the image index that the attestation manifest refers to.
When present, this annotation can be used to find the matching attestation manifest for a selected image manifest.
Examples
Example showing an SBOM attestation attached to a linux/amd64
image
Image index (sha256:94acc2ca70c40f3f6291681f37ce9c767e3d251ce01c7e4e9b98ccf148c26260
):
This image index defines two descriptors: an AMD64 image sha256:23678f31..
and an attestation manifest sha256:02cb9aa7..
for that image.
{
""mediaType"": ""application/vnd.oci.image.index.v1+json"",
""schemaVersion"": 2,
""manifests"": [
{
""mediaType"": ""application/vnd.oci.image.manifest.v1+json"",
""digest"": ""sha256:23678f31b3b3586c4fb318aecfe64a96a1f0916ba8faf9b2be2abee63fa9e827"",
""size"": 1234,
""platform"": {
""architecture"": ""amd64"",
""os"": ""linux""
}
},
{
""mediaType"": ""application/vnd.oci.image.manifest.v1+json"",
""digest"": ""sha256:02cb9aa7600e73fcf41ee9f0f19cc03122b2d8be43d41ce4b21335118f5dd943"",
""size"": 1234,
""annotations"": {
""vnd.docker.reference.digest"": ""sha256:23678f31b3b3586c4fb318aecfe64a96a1f0916ba8faf9b2be2abee63fa9e827"",
""vnd.docker.reference.type"": ""attestation-manifest""
},
""platform"": {
""architecture"": ""unknown"",
""os"": ""unknown""
}
}
]
}
Attestation manifest (sha256:02cb9aa7600e73fcf41ee9f0f19cc03122b2d8be43d41ce4b21335118f5dd943
):
This attestation manifest contains one attestation that is an in-toto attestation that contains a ""https://spdx.dev/Document"" predicate, signifying that it is defining a SBOM for the image.
{
""mediaType"": ""application/vnd.oci.image.manifest.v1+json"",
""schemaVersion"": 2,
""config"": {
""mediaType"": ""application/vnd.oci.image.config.v1+json"",
""digest"": ""sha256:a781560066f20ec9c28f2115a95a886e5e71c7c7aa9d8fd680678498b82f3ea3"",
""size"": 123
},
""layers"": [
{
""mediaType"": ""application/vnd.in-toto+json"",
""digest"": ""sha256:133ae3f9bcc385295b66c2d83b28c25a9f294ce20954d5cf922dda860429734a"",
""size"": 1234,
""annotations"": {
""in-toto.io/predicate-type"": ""https://spdx.dev/Document""
}
}
]
}
Image config (sha256:a781560066f20ec9c28f2115a95a886e5e71c7c7aa9d8fd680678498b82f3ea3
):
{
""architecture"": ""unknown"",
""os"": ""unknown"",
""config"": {},
""rootfs"": {
""type"": ""layers"",
""diff_ids"": [
""sha256:133ae3f9bcc385295b66c2d83b28c25a9f294ce20954d5cf922dda860429734a""
]
}
}
Layer content (sha256:1ea07d5e55eb47ad0e6bbfa2ec180fb580974411e623814e519064c88f022f5c
):
Attestation body containing the SBOM data listing the packages used during the build in SPDX format.
{
""_type"": ""https://in-toto.io/Statement/v0.1"",
""predicateType"": ""https://spdx.dev/Document"",
""subject"": [
{
""name"": ""_"",
""digest"": {
""sha256"": ""23678f31b3b3586c4fb318aecfe64a96a1f0916ba8faf9b2be2abee63fa9e827""
}
}
],
""predicate"": {
""SPDXID"": ""SPDXRef-DOCUMENT"",
""spdxVersion"": ""SPDX-2.2"",
...",,,
84f3c9de25914e782bfc8ad6cf5783a587c4e29b5c0d5b86ae19c69fd435f776,"Add a backend to your extension
Your extension can ship a backend part with which the frontend can interact with. This page provides information on why and how to add a backend.
Before you start, make sure you have installed the latest version of Docker Desktop.
Tip
Check the Quickstart guide and
docker extension init <my-extension>
. They provide a better base for your extension as it's more up-to-date and related to your install of Docker Desktop.
Why add a backend?
Thanks to the Docker Extensions SDK, most of the time you should be able to do what you need from the Docker CLI directly from the frontend.
Nonetheless, there are some cases where you might need to add a backend to your extension. So far, extension builders have used the backend to:
- Store data in a local database and serve them back with a REST API.
- Store the extension state, for example when a button starts a long-running process, so that if you navigate away from the extension user interface and comes back, the frontend can pick up where it left off.
For more information about extension backends, see Architecture.
Add a backend to the extension
If you created your extension using the docker extension init
command, you already have a backend setup. Otherwise, you have to first create a vm
directory that contains the code and updates the Dockerfile to
containerize it.
Here is the extension folder structure with a backend:
.
├── Dockerfile # (1)
├── Makefile
├── metadata.json
├── ui
└── index.html
└── vm # (2)
├── go.mod
└── main.go
- Contains everything required to build the backend and copy it in the extension's container filesystem.
- The source folder that contains the backend code of the extension.
Although you can start from an empty directory or from the vm-ui extension
sample,
it is highly recommended that you start from the docker extension init
command and change it to suit your needs.
Tip
The
docker extension init
generates a Go backend. But you can still use it as a starting point for your own extension and use any other language like Node.js, Python, Java, .Net, or any other language and framework.
In this tutorial, the backend service simply exposes one route that returns a JSON payload that says ""Hello"".
{ ""Message"": ""Hello"" }
Important
We recommend that, the frontend and the backend communicate through sockets, and named pipes on Windows, instead of HTTP. This prevents port collision with any other running application or container running on the host. Also, some Docker Desktop users are running in constrained environments where they can't open ports on their machines. When choosing the language and framework for your backend, make sure it supports sockets connection.
package main
import (
""flag""
""log""
""net""
""net/http""
""os""
""github.com/labstack/echo""
""github.com/sirupsen/logrus""
)
func main() {
var socketPath string
flag.StringVar(&socketPath, ""socket"", ""/run/guest/volumes-service.sock"", ""Unix domain socket to listen on"")
flag.Parse()
os.RemoveAll(socketPath)
logrus.New().Infof(""Starting listening on %s\n"", socketPath)
router := echo.New()
router.HideBanner = true
startURL := """"
ln, err := listen(socketPath)
if err != nil {
log.Fatal(err)
}
router.Listener = ln
router.GET(""/hello"", hello)
log.Fatal(router.Start(startURL))
}
func listen(path string) (net.Listener, error) {
return net.Listen(""unix"", path)
}
func hello(ctx echo.Context) error {
return ctx.JSON(http.StatusOK, HTTPMessageBody{Message: ""hello world""})
}
type HTTPMessageBody struct {
Message string
}
Important
We don't have a working example for Node yet. Fill out the form and let us know if you'd like a sample for Node.
Important
We don't have a working example for Python yet. Fill out the form and let us know if you'd like a sample for Python.
Important
We don't have a working example for Java yet. Fill out the form and let us know if you'd like a sample for Java.
Important
We don't have a working example for .NET. Fill out the form and let us know if you'd like a sample for .NET.
Adapt the Dockerfile
Note
When using the
docker extension init
, it creates aDockerfile
that already contains what is needed for a Go backend.
To deploy your Go backend when installing the extension, you need first to configure the Dockerfile
, so that it:
- Builds the backend application
- Copies the binary in the extension's container filesystem
- Starts the binary when the container starts listening on the extension socket
Tip
To ease version management, you can reuse the same image to build the frontend, build the backend service, and package the extension.
# syntax=docker/dockerfile:1
FROM node:17.7-alpine3.14 AS client-builder
# ... build frontend application
# Build the Go backend
FROM golang:1.17-alpine AS builder
ENV CGO_ENABLED=0
WORKDIR /backend
COPY vm/go.* .
RUN --mount=type=cache,target=/go/pkg/mod \
--mount=type=cache,target=/root/.cache/go-build \
go mod download
COPY vm/. .
RUN --mount=type=cache,target=/go/pkg/mod \
--mount=type=cache,target=/root/.cache/go-build \
go build -trimpath -ldflags=""-s -w"" -o bin/service
FROM alpine:3.15
# ... add labels and copy the frontend application
COPY --from=builder /backend/bin/service /
CMD /service -socket /run/guest-services/extension-allthethings-extension.sock
Important
We don't have a working Dockerfile for Node yet. Fill out the form and let us know if you'd like a Dockerfile for Node.
Important
We don't have a working Dockerfile for Python yet. Fill out the form and let us know if you'd like a Dockerfile for Python.
Important
We don't have a working Dockerfile for Java yet. Fill out the form and let us know if you'd like a Dockerfile for Java.
Important
We don't have a working Dockerfile for .Net. Fill out the form and let us know if you'd like a Dockerfile for .Net.
Configure the metadata file
To start the backend service of your extension inside the VM of Docker Desktop, you have to configure the image name
in the vm
section of the metadata.json
file.
{
""vm"": {
""image"": ""${DESKTOP_PLUGIN_IMAGE}""
},
""icon"": ""docker.svg"",
""ui"": {
...
}
}
For more information on the vm
section of the metadata.json
, see
Metadata.
Warning
Do not replace the
${DESKTOP_PLUGIN_IMAGE}
placeholder in themetadata.json
file. The placeholder is replaced automatically with the correct image name when the extension is installed.
Invoke the extension backend from your frontend
Using the advanced frontend extension example, we can invoke our extension backend.
Use the Docker Desktop Client object and then invoke the /hello
route from the backend service with ddClient. extension.vm.service.get
that returns the body of the response.
Replace the ui/src/App.tsx
file with the following code:
// ui/src/App.tsx
import React, { useEffect } from 'react';
import { createDockerDesktopClient } from ""@docker/extension-api-client"";
//obtain docker desktop extension client
const ddClient = createDockerDesktopClient();
export function App() {
const ddClient = createDockerDesktopClient();
const [hello, setHello] = useState<string>();
useEffect(() => {
const getHello = async () => {
const result = await ddClient.extension.vm?.service?.get('/hello');
setHello(JSON.stringify(result));
}
getHello()
}, []);
return (
<Typography>{hello}</Typography>
);
}
Important
We don't have an example for Vue yet. Fill out the form and let us know if you'd like a sample with Vue.
Important
We don't have an example for Angular yet. Fill out the form and let us know if you'd like a sample with Angular.
Important
We don't have an example for Svelte yet. Fill out the form and let us know if you'd like a sample with Svelte.
Re-build the extension and update it
Since you have modified the configuration of the extension and added a stage in the Dockerfile, you must re-build the extension.
docker build --tag=awesome-inc/my-extension:latest .
Once built, you need to update it, or install it if you haven't already done so.
docker extension update awesome-inc/my-extension:latest
Now you can see the backend service running in the Containers view of the Docker Desktop Dashboard and watch the logs when you need to debug it.
Tip
You may need to turn on the Show system containers option in Settings to see the backend container running. See Show extension containers for more information.
Open the Docker Desktop Dashboard and select the Containers tab. You should see the response from the backend service call displayed.
What's next?
- Learn how to share and publish your extension.
- Learn more about extensions architecture.",,,
b765735f4d3b3f9ce7a4f772ef72d798e33b871f148aca0419fbe2494367bc6b,"Allowlist for Docker Desktop
Table of contents
For:
Administrators
This page contains the domain URLs that you need to add to a firewall allowlist to ensure Docker Desktop works properly within your organization.
Domain URLs to allow
| Domains | Description |
|---|---|
| https://api.segment.io | Analytics |
| https://cdn.segment.com | Analytics |
| https://experiments.docker.com | A/B testing |
| https://notify.bugsnag.com | Error reports |
| https://sessions.bugsnag.com | Error reports |
| https://auth.docker.io | Authentication |
| https://cdn.auth0.com | Authentication |
| https://login.docker.com | Authentication |
| https://desktop.docker.com | Update |
| https://hub.docker.com | Docker Hub |
| https://registry-1.docker.io | Docker Pull/Push |
| https://production.cloudflare.docker.com | Docker Pull/Push (Paid plans) |
| https://docker-images-prod.6aa30f8b08e16409b46e0173d6de2f56.r2.cloudflarestorage.com | Docker Pull/Push (Personal plan / Anonymous) |
| https://docker-pinata-support.s3.amazonaws.com | Troubleshooting |
| https://api.dso.docker.com | Docker Scout service |",,,
991d063831c56db4bb72c342c13b00603051997380be08f8a151571924469113,"Understand permission requirements for Windows
This page contains information about the permission requirements for running and installing Docker Desktop on Windows, the functionality of the privileged helper process com.docker.service
and the reasoning behind this approach.
It also provides clarity on running containers as root
as opposed to having Administrator
access on the host and the privileges of the Windows Docker engine and Windows containers.
Permission requirements
While Docker Desktop on Windows can be run without having Administrator
privileges, it does require them during installation. On installation you receive a UAC prompt which allows a privileged helper service to be installed. After that, Docker Desktop can be run without administrator privileges, provided you are members of the docker-users
group. If you performed the installation, you are automatically added to this group, but other users must be added manually. This allows the administrator to control who has access to Docker Desktop.
The reason for this approach is that Docker Desktop needs to perform a limited set of privileged operations which are conducted by the privileged helper process com.docker.service
. This approach allows, following the principle of least privilege, Administrator
access to be used only for the operations for which it is absolutely necessary, while still being able to use Docker Desktop as an unprivileged user.
Privileged helper
The privileged helper com.docker.service
is a Windows service which runs in the background with SYSTEM
privileges. It listens on the named pipe //./pipe/dockerBackendV2
. The developer runs the Docker Desktop application, which connects to the named pipe and sends commands to the service. This named pipe is protected, and only users that are part of the docker-users
group can have access to it.
The service performs the following functionalities:
- Ensuring that
kubernetes.docker.internal
is defined in the Win32 hosts file. Defining the DNS namekubernetes.docker.internal
allows Docker to share Kubernetes contexts with containers. - Ensuring that
host.docker.internal
andgateway.docker.internal
are defined in the Win32 hosts file. They point to the host local IP address and allow an application to resolve the host IP using the same name from either the host itself or a container. - Securely caching the Registry Access Management policy which is read-only for the developer.
- Creating the Hyper-V VM
""DockerDesktopVM""
and managing its lifecycle - starting, stopping and destroying it. The VM name is hard coded in the service code so the service cannot be used for creating or manipulating any other VMs. - Moving the VHDX file or folder.
- Starting and stopping the Windows Docker engine and querying whether it's running.
- Deleting all Windows containers data files.
- Checking if Hyper-V is enabled.
- Checking if the bootloader activates Hyper-V.
- Checking if required Windows features are both installed and enabled.
- Conducting healthchecks and retrieving the version of the service itself.
The service start mode depends on which container engine is selected, and, for WSL, on whether it is needed to maintain host.docker.internal
and gateway.docker.internal
in the Win32 hosts file. This is controlled by a setting under Use the WSL 2 based engine
in the settings page. When this is set, WSL engine behaves the same as Hyper-V. So:
- With Windows containers, or Hyper-v Linux containers, the service is started when the system boots and runs all the time, even when Docker Desktop isn't running. This is required so you can launch Docker Desktop without admin privileges.
- With WSL2 Linux containers, the service isn't necessary and therefore doesn't run automatically when the system boots. When you switch to Windows containers or Hyper-V Linux containers, or choose to maintain
host.docker.internal
andgateway.docker.internal
in the Win32 hosts file, a UAC prompt is displayed which asks you to accept the privileged operation to start the service. If accepted, the service is started and set to start automatically upon the next Windows boot.
Containers running as root within the Linux VM
The Linux Docker daemon and containers run in a minimal, special-purpose Linux
VM managed by Docker. It is immutable so you can’t extend it or change the
installed software. This means that although containers run by default as
root
, this doesn't allow altering the VM and doesn't grant Administrator
access to the Windows host machine. The Linux VM serves as a security boundary
and limits what resources from the host can be accessed. File sharing uses a
user-space crafted file server and any directories from the host bind mounted
into Docker containers still retain their original permissions. It doesn't give
you access to any files that it doesn’t already have access to.
Enhanced Container Isolation
In addition, Docker Desktop supports Enhanced Container Isolation mode (ECI), available to Business customers only, which further secures containers without impacting developer workflows.
ECI automatically runs all containers within a Linux user-namespace, such that root in the container is mapped to an unprivileged user inside the Docker Desktop VM. ECI uses this and other advanced techniques to further secure containers within the Docker Desktop Linux VM, such that they are further isolated from the Docker daemon and other services running inside the VM.
Windows Containers
Warning
Enabling Windows containers has important security implications.
Unlike the Linux Docker Engine and containers which run in a VM, Windows containers are implemented using operating system features, and run directly on the Windows host. If you enable Windows containers during installation, the ContainerAdministrator
user used for administration inside the container is a local administrator on the host machine. Enabling Windows containers during installation makes it so that members of the docker-users
group are able to elevate to administrators on the host. For organizations who don't want their developers to run Windows containers, a -–no-windows-containers
installer flag is available to disable their use.
Networking
For network connectivity, Docker Desktop uses a user-space process (vpnkit
), which inherits constraints like firewall rules, VPN, HTTP proxy properties etc. from the user that launched it.",,,
1af459aaba58c5c19b827fa3e231b5d4217e6e4d9b85dba1c12748145933ebdc,"Resolve the recent Docker Desktop issue on macOS
This guide provides steps to address a recent issue affecting some macOS users of Docker Desktop. The issue may prevent Docker Desktop from starting and in some cases, may also trigger inaccurate malware warnings. For more details about the incident, see the blog post.
Note
Docker Desktop versions 4.28 and earlier are not impacted by this issue.
Available solutions
There are a few options available depending on your situation:
Upgrade to Docker Desktop version 4.37.2 (recommended)
The recommended way is to upgrade to the latest Docker Desktop version which is version 4.37.2.
If possible, update directly through the app. If not, and you’re still seeing the malware pop-up, follow the steps below:
Kill the Docker process that cannot start properly:
$ sudo launchctl bootout system/com.docker.vmnetd 2>/dev/null || true $ sudo launchctl bootout system/com.docker.socket 2>/dev/null || true $ sudo rm /Library/PrivilegedHelperTools/com.docker.vmnetd || true $ sudo rm /Library/PrivilegedHelperTools/com.docker.socket || true $ ps aux | grep -i docker | awk '{print $2}' | sudo xargs kill -9 2>/dev/null
Make sure the malware pop-up is permanently closed.
Launch Docker Desktop. A privileged pop-up message displays after 5 to 10 seconds.
Enter your password.
You should now see the Docker Desktop Dashboard.
Tip
If the malware pop-up persists after completing these steps and Docker is in the Trash, try emptying the Trash and rerunning the steps.
Install a patch if you have version 4.32 - 4.36
If you can’t upgrade to the latest version and you’re seeing the malware pop-up, follow the steps below:
Kill the Docker process that cannot start properly:
$ sudo launchctl bootout system/com.docker.vmnetd 2>/dev/null || true $ sudo launchctl bootout system/com.docker.socket 2>/dev/null || true $ sudo rm /Library/PrivilegedHelperTools/com.docker.vmnetd || true $ sudo rm /Library/PrivilegedHelperTools/com.docker.socket || true $ ps aux | grep docker | awk '{print $2}' | sudo xargs kill -9 2>/dev/null
Make sure the malware pop-up is permanently closed.
Download and install the patched installer that matches your current base version. For example if you have version 4.36.0, install 4.36.1.
Launch Docker Desktop. A privileged pop-up message displays after 5 to 10 seconds.
Enter your password.
You should now see the Docker Desktop Dashboard.
Tip
If the malware pop-up persists after completing these steps and Docker is in the Trash, try emptying the Trash and rerunning the steps.
MDM script
If you are an IT administrator and your developers are seeing the malware pop-up:
Make sure your developers have a re-signed version of Docker Desktop version 4.32 or later.
Run the following script:
#!/bin/bash # Stop the docker services echo ""Stopping Docker..."" sudo pkill -i docker # Stop the vmnetd service echo ""Stopping com.docker.vmnetd service..."" sudo launchctl bootout system /Library/LaunchDaemons/com.docker.vmnetd.plist # Stop the socket service echo ""Stopping com.docker.socket service..."" sudo launchctl bootout system /Library/LaunchDaemons/com.docker.socket.plist # Remove vmnetd binary echo ""Removing com.docker.vmnetd binary..."" sudo rm -f /Library/PrivilegedHelperTools/com.docker.vmnetd # Remove socket binary echo ""Removing com.docker.socket binary..."" sudo rm -f /Library/PrivilegedHelperTools/com.docker.socket # Install new binaries echo ""Install new binaries..."" sudo cp /Applications/Docker.app/Contents/Library/LaunchServices/com.docker.vmnetd /Library/PrivilegedHelperTools/ sudo cp /Applications/Docker.app/Contents/MacOS/com.docker.socket /Library/PrivilegedHelperTools/
Homebrew casks
If you installed Docker Desktop using Homebrew casks, the recommended solution is to perform a full reinstall to resolve the issue.
To reinstall Docker Desktop, run the following commands in your terminal:
$ brew update
$ brew reinstall --cask docker
These commands will update Homebrew and completely reinstall Docker Desktop, ensuring you have the latest version with the fix applied.",,,
2cf1d6488d8681545261edac203c57758940c6279c382d5d4aa4c059cde55c48,"Environment variables precedence in Docker Compose
When the same environment variable is set in multiple sources, Docker Compose follows a precedence rule to determine the value for that variable in your container's environment.
This page contains information on the level of precedence each method of setting environmental variables takes.
The order of precedence (highest to lowest) is as follows:
- Set using
docker compose run -e
in the CLI. - Set with either the
environment
orenv_file
attribute but with the value interpolated from your shell or an environment file. (either your default.env
file, or with the--env-file
argument in the CLI). - Set using just the
environment
attribute in the Compose file. - Use of the
env_file
attribute in the Compose file. - Set in a container image in the
ENV directive.
Having any
ARG
orENV
setting in aDockerfile
evaluates only if there is no Docker Compose entry forenvironment
,env_file
orrun --env
.
Simple example
In the following example, a different value for the same environment variable in an .env
file and with the environment
attribute in the Compose file:
$ cat ./webapp.env
NODE_ENV=test
$ cat compose.yaml
services:
webapp:
image: 'webapp'
env_file:
- ./webapp.env
environment:
- NODE_ENV=production
The environment variable defined with the environment
attribute takes precedence.
$ docker compose run webapp env | grep NODE_ENV
NODE_ENV=production
Advanced example
The following table uses VALUE
, an environment variable defining the version for an image, as an example.
How the table works
Each column represents a context from where you can set a value, or substitute in a value for VALUE
.
The columns Host OS environment
and .env
file is listed only for illustration purposes. In reality, they don't result in a variable in the container by itself, but in conjunction with either the environment
or env_file
attribute.
Each row represents a combination of contexts where VALUE
is set, substituted, or both. The Result column indicates the final value for VALUE
in each scenario.
| # | docker compose run | environment attribute | env_file attribute | Image ENV | Host OS environment | .env file | Result | |
|---|---|---|---|---|---|---|---|---|
| 1 | - | - | - | - | VALUE=1.4 | VALUE=1.3 | - | |
| 2 | - | - | VALUE=1.6 | VALUE=1.5 | VALUE=1.4 | - | VALUE=1.6 | |
| 3 | - | VALUE=1.7 | - | VALUE=1.5 | VALUE=1.4 | - | VALUE=1.7 | |
| 4 | - | - | - | VALUE=1.5 | VALUE=1.4 | VALUE=1.3 | VALUE=1.5 | |
| 5 | --env VALUE=1.8 | - | - | VALUE=1.5 | VALUE=1.4 | VALUE=1.3 | VALUE=1.8 | |
| 6 | --env VALUE | - | - | VALUE=1.5 | VALUE=1.4 | VALUE=1.3 | VALUE=1.4 | |
| 7 | --env VALUE | - | - | VALUE=1.5 | - | VALUE=1.3 | VALUE=1.3 | |
| 8 | - | - | VALUE | VALUE=1.5 | VALUE=1.4 | VALUE=1.3 | VALUE=1.4 | |
| 9 | - | - | VALUE | VALUE=1.5 | - | VALUE=1.3 | VALUE=1.3 | |
| 10 | - | VALUE | - | VALUE=1.5 | VALUE=1.4 | VALUE=1.3 | VALUE=1.4 | |
| 11 | - | VALUE | - | VALUE=1.5 | - | VALUE=1.3 | VALUE=1.3 | |
| 12 | --env VALUE | VALUE=1.7 | - | VALUE=1.5 | VALUE=1.4 | VALUE=1.3 | VALUE=1.4 | |
| 13 | --env VALUE=1.8 | VALUE=1.7 | - | VALUE=1.5 | VALUE=1.4 | VALUE=1.3 | VALUE=1.8 | |
| 14 | --env VALUE=1.8 | - | VALUE=1.6 | VALUE=1.5 | VALUE=1.4 | VALUE=1.3 | VALUE=1.8 | |
| 15 | --env VALUE=1.8 | VALUE=1.7 | VALUE=1.6 | VALUE=1.5 | VALUE=1.4 | VALUE=1.3 | VALUE=1.8 |
Result explanation
Result 1: The local environment takes precedence, but the Compose file is not set to replicate this inside the container, so no such variable is set.
Result 2: The env_file
attribute in the Compose file defines an explicit value for VALUE
so the container environment is set accordingly.
Result 3: The environment
attribute in the Compose file defines an explicit value for VALUE
, so the container environment is set accordingly/
Result 4: The image's ENV
directive declares the variable VALUE
, and since the Compose file is not set to override this value, this variable is defined by image
Result 5: The docker compose run
command has the --env
flag set which an explicit value, and overrides the value set by the image.
Result 6: The docker compose run
command has the --env
flag set to replicate the value from the environment. Host OS value takes precedence and is replicated into the container's environment.
Result 7: The docker compose run
command has the --env
flag set to replicate the value from the environment. Value from .env
file is the selected to define the container's environment.
Result 8: The env_file
attribute in the Compose file is set to replicate VALUE
from the local environment. Host OS value takes precedence and is replicated into the container's environment.
Result 9: The env_file
attribute in the Compose file is set to replicate VALUE
from the local environment. Value from .env
file is the selected to define the container's environment.
Result 10: The environment
attribute in the Compose file is set to replicate VALUE
from the local environment. Host OS value takes precedence and is replicated into the container's environment.
Result 11: The environment
attribute in the Compose file is set to replicate VALUE
from the local environment. Value from .env
file is the selected to define the container's environment.
Result 12: The --env
flag has higher precedence than the environment
and env_file
attributes and is to set to replicate VALUE
from the local environment. Host OS value takes precedence and is replicated into the container's environment.
Results 13 to 15: The --env
flag has higher precedence than the environment
and env_file
attributes and so sets the value.",,,
a8b27406c78ad9918ea87e4b420126a9566c02a9cf894241f70d523c084618cb,"Get started with Policy Evaluation in Docker Scout
In software supply chain management, maintaining the security and reliability of artifacts is a top priority. Policy Evaluation in Docker Scout introduces a layer of control, on top of existing analysis capabilities. It lets you define supply chain rules for your artifacts, and helps you track how your artifacts perform, relative to your rules and thresholds, over time.
Learn how you can use Policy Evaluation to ensure that your artifacts align with established best practices.
How Policy Evaluation works
When you activate Docker Scout for a repository, images that you push are automatically analyzed. The analysis gives you insights about the composition of your images, including what packages they contain and what vulnerabilities they're exposed to. Policy Evaluation builds on top of the image analysis feature, interpreting the analysis results against the rules defined by policies.
A policy defines image quality criteria that your artifacts should fulfill. For example, the No AGPL v3 licenses policy flags any image containing packages distributed under the AGPL v3 license. If an image contains such a package, that image is non-compliant with this policy. Some policies, such as the No AGPL v3 licenses policy, are configurable. Configurable policies let you adjust the criteria to better match your organization's needs.
In Docker Scout, policies are designed to help you ratchet forward your security and supply chain stature. Where other tools focus on providing a pass or fail status, Docker Scout policies visualizes how small, incremental changes affect policy status, even when your artifacts don't meet the policy requirements (yet). By tracking how the fail gap changes over time, you more easily see whether your artifact is improving or deteriorating relative to policy.
Policies don't necessarily have to be related to application security and vulnerabilities. You can use policies to measure and track other aspects of supply chain management as well, such as open-source license usage and base image up-to-dateness.
Policy types
In Docker Scout, a policy is derived from a policy type. Policy types are templates that define the core parameters of a policy. You can compare policy types to classes in object-oriented programming, with each policy acting as an instance created from its corresponding policy type.
Docker Scout supports the following policy types:
- Severity-Based Vulnerability
- Compliant Licenses
- Up-to-Date Base Images
- High-Profile Vulnerabilities
- Supply Chain Attestations
- Default Non-Root User
- Approved Base Images
- SonarQube Quality Gates
Docker Scout automatically provides default policies for repositories where it is enabled, except for the SonarQube Quality Gates policy, which requires integration with SonarQube before use.
You can create custom policies from any of the supported policy types, or delete a default policy if it isn't applicable to your project. For more information, refer to Configure policies.
Severity-Based Vulnerability
The Severity-Based Vulnerability policy type checks whether your artifacts are exposed to known vulnerabilities.
By default, this policy only flags critical and high severity vulnerabilities where there's a fix version available. Essentially, this means that there's an easy fix that you can deploy for images that fail this policy: upgrade the vulnerable package to a version containing a fix for the vulnerability.
Images are deemed non-compliant with this policy if they contain one or more vulnerabilities that fall outside the specified policy criteria.
You can configure the parameters of this policy by creating a custom version of the policy. The following policy parameters are configurable in a custom version:
Age: The minimum number of days since the vulnerability was first published
The rationale for only flagging vulnerabilities of a certain minimum age is that newly discovered vulnerabilities shouldn't cause your evaluations to fail until you've had a chance to address them.
- Severities: Severity levels to consider (default:
Critical, High
)
Fixable vulnerabilities only: Whether or not to only report vulnerabilities with a fix version available (enabled by default).
Package types: List of package types to consider.
This option lets you specify the package types, as PURL package type definitions, that you want to include in the policy evaluation. By default, the policy considers all package types.
For more information about configuring policies, see Configure policies.
Compliant Licenses
The Compliant Licenses policy type checks whether your images contain packages distributed under an inappropriate license. Images are considered non-compliant if they contain one or more packages with such a license.
You can configure the list of licenses that this policy should look out for, and add exceptions by specifying an allow-list (in the form of PURLs). See Configure policies.
Up-to-Date Base Images
The Up-to-Date Base Images policy type checks whether the base images you use are up-to-date.
Images are considered non-compliant with this policy if the tag you used to build your image points to a different digest than what you're using. If there's a mismatch in digests, that means the base image you're using is out of date.
Your images need provenance attestations for this policy to successfully evaluate. For more information, see No base image data.
High-Profile Vulnerabilities
The High-Profile Vulnerabilities policy type checks whether your images contain vulnerabilities from Docker Scout’s curated list. This list is kept up-to-date with newly disclosed vulnerabilities that are widely recognized to be risky.
The list includes the following vulnerabilities:
- CVE-2014-0160 (OpenSSL Heartbleed)
- CVE-2021-44228 (Log4Shell)
- CVE-2023-38545 (cURL SOCKS5 heap buffer overflow)
- CVE-2023-44487 (HTTP/2 Rapid Reset)
- CVE-2024-3094 (XZ backdoor)
- CVE-2024-47176 (OpenPrinting -
cups-browsed
) - CVE-2024-47076 (OpenPrinting -
libcupsfilters
) - CVE-2024-47175 (OpenPrinting -
libppd
) - CVE-2024-47177 (OpenPrinting -
cups-filters
)
You can customize this policy to change which CVEs that are considered high-profile by configuring the policy. Custom configuration options include:
Excluded CVEs: Specify the CVEs that you want this policy to ignore.
Default:
[]
(none of the high-profile CVEs are ignored)CISA KEV: Enable tracking of vulnerabilities from CISA's Known Exploited Vulnerabilities (KEV) catalog
The CISA KEV catalog includes vulnerabilities that are actively exploited in the wild. When enabled, the policy flags images that contain vulnerabilities from the CISA KEV catalog.
Enabled by default.
For more information on policy configuration, see Configure policies.
Supply Chain Attestations
The Supply Chain Attestations policy type checks whether your images have SBOM and provenance attestations.
Images are considered non-compliant if they lack either an SBOM attestation or a provenance attestation with max mode provenance. To ensure compliance, update your build command to attach these attestations at build-time:
$ docker buildx build --provenance=true --sbom=true -t <IMAGE> --push .
For more information about building with attestations, see Attestations.
If you're using GitHub Actions to build and push your images, learn how you can configure the action to apply SBOM and provenance attestations.
Default Non-Root User
By default, containers run as the root
superuser with full system
administration privileges inside the container, unless the Dockerfile specifies
a different default user. Running containers as a privileged user weakens their
runtime security, as it means any code that runs in the container can perform
administrative actions.
The Default Non-Root User policy type detects images that are set to run as
the default root
user. To comply with this policy, images must specify a
non-root user in the image configuration. Images are non-compliant with this
policy if they don't specify a non-root default user for the runtime stage.
For non-compliant images, evaluation results show whether or not the root
user was set explicitly for the image. This helps you distinguish between
policy violations caused by images where the root
user is implicit, and
images where root
is set on purpose.
The following Dockerfile runs as root
by default despite not being explicitly set:
FROM alpine
RUN echo ""Hi""
Whereas in the following case, the root
user is explicitly set:
FROM alpine
USER root
RUN echo ""Hi""
Note
This policy only checks for the default user of the image, as set in the image configuration blob. Even if you do specify a non-root default user, it's still possible to override the default user at runtime, for example by using the
--user
flag for thedocker run
command.
To make your images compliant with this policy, use the
USER
Dockerfile instruction to set
a default user that doesn't have root privileges for the runtime stage.
The following Dockerfile snippets shows the difference between a compliant and non-compliant image.
FROM alpine AS builder
COPY Makefile ./src /
RUN make build
FROM alpine AS runtime
COPY --from=builder bin/production /app
ENTRYPOINT [""/app/production""]
FROM alpine AS builder
COPY Makefile ./src /
RUN make build
FROM alpine AS runtime
COPY --from=builder bin/production /app
USER nonroot
ENTRYPOINT [""/app/production""]
Approved Base Images
The Approved Base Images policy type ensures that the base images you use in your builds are maintained and secure.
This policy checks whether the base images used in your builds match any of the patterns specified in the policy configuration. The following table shows a few example patterns for this policy.
| Use case | Pattern |
|---|---|
| Allow all images from Docker Hub | docker.io/* |
| Allow all Docker Official Images | docker.io/library/* |
| Allow images from a specific organization | docker.io/orgname/* |
| Allow tags of a specific repository | docker.io/orgname/repository:* |
Allow images on a registry with hostname registry.example.com | registry.example.com/* |
| Allow slim tags of NodeJS images | docker.io/library/node:*-slim |
An asterisk (*
) matches up until the character that follows, or until the end
of the image reference. Note that the docker.io
prefix is required in order
to match Docker Hub images. This is the registry hostname of Docker Hub.
This policy is configurable with the following options:
Approved base image sources
Specify the image reference patterns that you want to allow. The policy evaluates the base image references against these patterns.
Default:
[*]
(any reference is an allowed base image)Only supported tags
Allow only supported tags when using Docker Official Images.
When this option is enabled, images using unsupported tags of official images as their base image trigger a policy violation. Supported tags for official images are listed in the Supported tags section of the repository overview on Docker Hub.
Enabled by default.
Only supported OS distributions
Allow only Docker Official Images of supported Linux distribution versions.
When this option is enabled, images using unsupported Linux distributions that have reached end of life (such as
ubuntu:18.04
) trigger a policy violation.Enabling this option may cause the policy to report no data if the operating system version cannot be determined.
Enabled by default.
Your images need provenance attestations for this policy to successfully evaluate. For more information, see No base image data.
SonarQube Quality Gates
The SonarQube Quality Gates policy type builds on the SonarQube integration to assess the quality of your source code. This policy works by ingesting the SonarQube code analysis results into Docker Scout.
You define the criteria for this policy using SonarQube's quality gates. SonarQube evaluates your source code against the quality gates you've defined in SonarQube. Docker Scout surfaces the SonarQube assessment as a Docker Scout policy.
Docker Scout uses
provenance
attestations or the org.opencontainers.image.revision
OCI annotation to link
SonarQube analysis results with container images. In addition to enabling the
SonarQube integration, you must also make sure that your images have either the
attestation or the label.
Once you push an image and policy evaluation completes, the results from the SonarQube quality gates display as a policy in the Docker Scout Dashboard, and in the CLI.
Note
Docker Scout can only access SonarQube analyses created after the integration is enabled. Docker Scout doesn't have access to historic evaluations. Trigger a SonarQube analysis and policy evaluation after enabling the integration to view the results in Docker Scout.
No base image data
There are cases when it's not possible to determine information about the base images used in your builds. In such cases, the Up-to-Date Base Images and Approved Base Images policies get flagged as having No data.
This ""no data"" state occurs when:
- Docker Scout doesn't know what base image tag you used
- The base image version you used has multiple tags, but not all tags are out of date
To make sure that Docker Scout always knows about your base image, you can attach provenance attestations at build-time. Docker Scout uses provenance attestations to find out the base image version.",,,
e0c57f889d8179c39b0f94f6f96d50e2f084c9ba138c4ad20e3bfc1c47c0bc6d,"Configure remote access for Docker daemon
By default, the Docker daemon listens for connections on a Unix socket to accept requests from local clients. You can configure Docker to accept requests from remote clients by configuring it to listen on an IP address and port as well as the Unix socket.
Warning
Configuring Docker to accept connections from remote clients can leave you vulnerable to unauthorized access to the host and other attacks.
It's critically important that you understand the security implications of opening Docker to the network. If steps aren't taken to secure the connection, it's possible for remote non-root users to gain root access on the host.
Remote access without TLS is not recommended, and will require explicit opt-in in a future release. For more information on how to use TLS certificates to secure this connection, see Protect the Docker daemon socket.
Enable remote access
You can enable remote access to the daemon either using a docker.service
systemd unit file for Linux distributions using systemd.
Or you can use the daemon.json
file, if your distribution doesn't use systemd.
Configuring Docker to listen for connections using both the systemd unit file
and the daemon.json
file causes a conflict that prevents Docker from starting.
Configuring remote access with systemd unit file
Use the command
sudo systemctl edit docker.service
to open an override file fordocker.service
in a text editor.Add or modify the following lines, substituting your own values.
[Service] ExecStart= ExecStart=/usr/bin/dockerd -H fd:// -H tcp://127.0.0.1:2375
Save the file.
Reload the
systemctl
configuration.$ sudo systemctl daemon-reload
Restart Docker.
$ sudo systemctl restart docker.service
Verify that the change has gone through.
$ sudo netstat -lntp | grep dockerd tcp 0 0 127.0.0.1:2375 0.0.0.0:* LISTEN 3758/dockerd
Configuring remote access with daemon.json
Set the
hosts
array in the/etc/docker/daemon.json
to connect to the Unix socket and an IP address, as follows:{ ""hosts"": [""unix:///var/run/docker.sock"", ""tcp://127.0.0.1:2375""] }
Restart Docker.
Verify that the change has gone through.
$ sudo netstat -lntp | grep dockerd tcp 0 0 127.0.0.1:2375 0.0.0.0:* LISTEN 3758/dockerd
Allow access to the remote API through a firewall
If you run a firewall on the same host as you run Docker, and you want to access
the Docker Remote API from another remote host, you must configure your firewall
to allow incoming connections on the Docker port. The default port is 2376
if
you're using TLS encrypted transport, or 2375
otherwise.
Two common firewall daemons are:
- Uncomplicated Firewall (ufw), often used for Ubuntu systems.
- firewalld, often used for RPM-based systems.
Consult the documentation for your OS and firewall. The following information might help you get started. The settings used in this instruction are permissive, and you may want to use a different configuration that locks your system down more.
For ufw, set
DEFAULT_FORWARD_POLICY=""ACCEPT""
in your configuration.For firewalld, add rules similar to the following to your policy. One for incoming requests, and one for outgoing requests.
<direct> [ <rule ipv=""ipv6"" table=""filter"" chain=""FORWARD_direct"" priority=""0""> -i zt0 -j ACCEPT </rule> ] [ <rule ipv=""ipv6"" table=""filter"" chain=""FORWARD_direct"" priority=""0""> -o zt0 -j ACCEPT </rule> ] </direct>
Make sure that the interface names and chain names are correct.
Additional information
For more detailed information on configuration options for remote access to the daemon, refer to the dockerd CLI reference.",,,
d9d6cd0eb4825946201eab211e2f68db09bfd9077fe615ff4992f6dd31e80d6d,"Docker Extensions
Docker Extensions let you use third-party tools within Docker Desktop to extend its functionality.
You can seamlessly connect your favorite development tools to your application development and deployment workflows. Augment Docker Desktop with debugging, testing, security, and networking functionalities, and create custom add-ons using the Extensions SDK.
Anyone can use Docker Extensions and there is no limit to the number of extensions you can install.
What extensions are available?
There is a mix of partner and community-built extensions and Docker-built extensions. You can explore the list of available extensions in Docker Hub or in the Extensions Marketplace within Docker Desktop.
To find out more about Docker Extensions, we recommend the video walkthrough from DockerCon 2022:",,,
da0c4699e06306208316ab4ecece37912ffbb04a7f74de7fd64146bfe13cbc73,"Overview of the Extensions SDK
The resources in this section help you create your own Docker extension.
The Docker CLI tool provides a set of commands to help you build and publish your extension, packaged as a specially formatted Docker image.
At the root of the image filesystem is a metadata.json
file which describes the content of the extension.
It's a fundamental element of a Docker extension.
An extension can contain a UI part and backend parts that run either on the host or in the Desktop virtual machine. For further information, see Architecture.
You distribute extensions through Docker Hub. However, you can develop them locally without the need to push the extension to Docker Hub. See Extensions distribution for further details.
Already built an extension?
Let us know about your experience using the feedback form.",,,
0ad3cb081a60b9b81c319b8978d60e3f5c66d40d5851f6c9cb3b0100225b7e0c,"Set up automated builds
Note
Automated builds require a Docker Pro, Team, or Business subscription.
Configure automated builds
You can configure repositories in Docker Hub so that they automatically build an image each time you push new code to your source provider. If you have automated tests configured, the new image is only pushed when the tests succeed.
From the Repositories section, select a repository to view its details.
Select the Builds tab.
Select either GitHub or Bitbucket to connect where the image's source code is stored.
Note
You may be redirected to the settings page to link the code repository service. Otherwise, if you are editing the build settings for an existing automated build, select Configure automated builds.
Select the source repository to build the Docker images from.
Note
You might need to specify an organization or user from the source code provider. Once you select a user, source code repositories appear in the Select repository drop-down list.
Optional. Enable autotests.
Review the default Build Rules.
Build rules control what Docker Hub builds into images from the contents of the source code repository, and how the resulting images are tagged within the Docker repository.
A default build rule is set up for you, which you can edit or delete. This default rule sets builds from the
Branch
in your source code repository calledmaster
ormain
, and creates a Docker image tagged withlatest
. For more information, see set up build rules.Optional. Select the plus icon to add and configure more build rules.
For each branch or tag, enable or disable the Autobuild toggle.
Only branches or tags with autobuild enabled are built, tested, and have the resulting image pushed to the repository. Branches with autobuild disabled are built for test purposes (if enabled at the repository level), but the built Docker image isn't pushed to the repository.
For each branch or tag, enable or disable the Build Caching toggle.
Build caching can save time if you are building a large image frequently or have many dependencies. Leave the build caching disabled to make sure all of your dependencies are resolved at build time, or if you have a large layer that's quicker to build locally.
Select Save to save the settings, or select Save and build to save and run an initial test.
Note
A webhook is automatically added to your source code repository to notify Docker Hub on every push. Only pushes to branches that are listed as the source for one or more tags, trigger a build.
Set up build rules
By default when you set up automated builds, a basic build rule is created for you.
This default rule watches for changes to the master
or main
branch in your source code
repository, and builds the master
or main
branch into a Docker image tagged with
latest
.
In the Build Rules section, enter one or more sources to build.
For each source:
Select the Source type to build either a tag or a branch. This tells the build system what to look for in the source code repository.
Enter the name of the Source branch or tag you want to build.
The first time you configure automated builds, a default build rule is set up for you. This default set builds from the
Branch
in your source code calledmaster
, and creates a Docker image tagged withlatest
.You can also use a regex to select which source branches or tags to build. To learn more, see regexes.
Enter the tag to apply to Docker images built from this source.
If you configured a regex to select the source, you can reference the capture groups and use its result as part of the tag. To learn more, see regexes.
Specify the Dockerfile location as a path relative to the root of the source code repository. If the Dockerfile is at the repository root, leave this path set to
/
.
Note
When Docker Hub pulls a branch from a source code repository, it performs a shallow clone - only the tip of the specified branch. Refer to Advanced options for autobuild and autotest for more information.
Environment variables for builds
You can set the values for environment variables used in your build processes when you configure an automated build. Add your build environment variables by selecting the plus icon next to the Build environment variables section, and then entering a variable name and the value.
When you set variable values from the Docker Hub UI, you can use them by the
commands you set in hooks
files. However, they're stored so that only users who have admin
access to the Docker Hub repository can see their values. This
means you can use them to store access tokens or other information that
should remain secret.
Note
The variables set on the build configuration screen are used during the build processes only and shouldn't get confused with the environment values used by your service, for example to create service links.
Advanced automated build options
At the minimum you need a build rule composed of a source branch, or tag, and a destination Docker tag to set up an automated build. You can also:
- Change where the build looks for the Dockerfile
- Set a path to the files the build should use (the build context)
- Set up multiple static tags or branches to build from
- Use regular expressions (regexes) to dynamically select source code to build and create dynamic tags
All of these options are available from the Build configuration screen for each repository. Select Repositories from the left navigation, and select the name of the repository you want to edit. Select the Builds tab, and then select Configure Automated builds.
Tag and branch builds
You can configure your automated builds so that pushes to specific branches or tags triggers a build.
In the Build Rules section, select the plus icon to add more sources to build.
Select the Source type to build either a tag or a branch.
Note
This tells the build system what type of source to look for in the code repository.
Enter the name of the Source branch or tag you want to build.
Note
You can enter a name, or use a regex to match which source branch or tag names to build. To learn more, see regexes.
Enter the tag to apply to Docker images built from this source.
Note
If you configured a regex to select the source, you can reference the capture groups and use its result as part of the tag. To learn more, see regexes.
Repeat steps 2 through 4 for each new build rule you set up.
Set the build context and Dockerfile location
Depending on how you arrange the files in your source code repository, the files required to build your images may not be at the repository root. If that's the case, you can specify a path where the build looks for the files.
The build context is the path to the files needed for the build, relative to
the root of the repository. Enter the path to these files in the Build context field. Enter /
to set the build context as the root of the source code repository.
Note
If you delete the default path
/
from the Build context field and leave it blank, the build system uses the path to the Dockerfile as the build context. However, to avoid confusion it's recommended that you specify the complete path.
You can specify the Dockerfile location as a path relative to the build
context. If the Dockerfile is at the root of the build context path, leave the
Dockerfile path set to /
. If the build context field is blank, set the path
to the Dockerfile from the root of the source repository.
Regexes and automated builds
You can specify a regular expression (regex) so that only matching branches or tags are built. You can also use the results of the regex to create the Docker tag that's applied to the built image.
You can use up to nine regular expression capture groups, or expressions enclosed in parentheses, to select a source to build, and reference
these in the Docker Tag field using {\1}
through {\9}
.
Build images with BuildKit
Autobuilds use the BuildKit build system by default. If you want to use the legacy
Docker build system, add the
environment variable
DOCKER_BUILDKIT=0
. Refer to the
BuildKit
page for more information on BuildKit.
Autobuild for teams
When you create an automated build repository in your own user account, you can start, cancel, and retry builds, and edit and delete your own repositories.
These same actions are also available for team repositories from Docker Hub if
you are an owner. If you are a member of a
team with write
permissions you can start, cancel, and retry builds in your
team's repositories, but you cannot edit the team repository settings or delete
the team repositories. If your user account has read
permission, or if you're
a member of a team with read
permission, you can view the build configuration
including any testing settings.
| Action/Permission | Read | Write | Admin | Owner |
|---|---|---|---|---|
| view build details | x | x | x | x |
| start, cancel, retry | x | x | x | |
| edit build settings | x | x | ||
| delete build | x |
Service users for team autobuilds
Note
Only owners can set up automated builds for teams.
When you set up automated builds for teams, you grant Docker Hub access to your source code repositories using OAuth tied to a specific user account. This means that Docker Hub has access to everything that the linked source provider account can access.
For organizations and teams, it's recommended you create a dedicated service account to grant access to the source provider. This ensures that no builds break as individual users' access permissions change, and that an individual user's personal projects aren't exposed to an entire organization.
This service account should have access to any repositories to be built, and must have administrative access to the source code repositories so it can manage deploy keys. If needed, you can limit this account to only a specific set of repositories required for a specific build.
If you are building repositories with linked private submodules (private
dependencies), you also need to add an override SSH_PRIVATE
environment
variable to automated builds associated with the account. For more information, see
Troubleshoot
Create a service user account on your source provider, and generate SSH keys for it.
Create a ""build"" team in your organization.
Ensure that the new ""build"" team has access to each repository and submodule you need to build.
On GitHub or Bitbucket, go to the repository's Settings page.
Add the new ""build"" team to the list of approved users.
- GitHub: Add the team in Collaborators and Teams.
- Bitbucket: Add the team in Access management.
Add the service user to the ""build"" team on the source provider.
Sign in to Docker Hub as an owner, switch to the organization, and follow the instructions to link to source code repository using the service account.
Note
You may need to sign out of your individual account on the source code provider to create the link to the service account.
Optional. Use the SSH keys you generated to set up any builds with private submodules, using the service account and the previous instructions.
What's Next?
- Customize your build process with environment variables, hooks, and more
- Add automated tests
- Manage your builds
- Troubleshoot",,,
91d8e6b296ed3bb1a61a98b2f1206b480f509464f1eacb2e6863e5e852fd679b,"Advisory database sources and matching service
Reliable information sources are key for Docker Scout's ability to surface relevant and accurate assessments of your software artifacts. Given the diversity of sources and methodologies in the industry, discrepancies in vulnerability assessment results can and do happen. This page describes how the Docker Scout advisory database and its CVE-to-package matching approach works to deal with these discrepancies.
Advisory database sources
Docker Scout aggregates vulnerability data from multiple sources. The data is continuously updated to ensure that your security posture is represented using the latest available information, in real-time.
Docker Scout uses the following package repositories and security trackers:
- AlmaLinux Security Advisory
- Alpine secdb
- Amazon Linux Security Center
- Bitnami Vulnerability Database
- CISA Known Exploited Vulnerability Catalog
- CISA Vulnrichment
- Chainguard Security Feed
- Debian Security Bug Tracker
- Exploit Prediction Scoring System (EPSS)
- GitHub Advisory Database
- GitLab Advisory Database
- Golang VulnDB
- National Vulnerability Database
- Oracle Linux Security
- Photon OS 3.0 Security Advisories
- Python Packaging Advisory Database
- RedHat Security Data
- Rocky Linux Security Advisory
- RustSec Advisory Database
- SUSE Security CVRF
- Ubuntu CVE Tracker
- Wolfi Security Feed
- inTheWild, a community-driven open database of vulnerability exploitation
When you enable Docker Scout for your Docker organization, a new database instance is provisioned on the Docker Scout platform. The database stores the Software Bill of Materials (SBOM) and other metadata about your images. When a security advisory has new information about a vulnerability, your SBOM is cross-referenced with the CVE information to detect how it affects you.
For more details on how image analysis works, see the image analysis page.
Vulnerability matching
Traditional tools often rely on broad Common Product Enumeration (CPE) matching, which can lead to many false-positive results.
Docker Scout uses Package URLs (PURLs) to match packages against CVEs, which yields more precise identification of vulnerabilities. PURLs significantly reduce the chances of false positives, focusing only on genuinely affected packages.
Supported package ecosystems
Docker Scout supports the following package ecosystems:
- .NET
- GitHub packages
- Go
- Java
- JavaScript
- PHP
- Python
- RPM
- Ruby
alpm
(Arch Linux)apk
(Alpine Linux)deb
(Debian Linux and derivatives)",,,
292cfd819114587b0e14b30a193a59d8f2b228c6587d6dcae58804101a4ff2a0,"View Docker Scout policy status
You can track policy status for your artifacts from the Docker Scout Dashboard, or using the CLI.
Dashboard
The Overview tab of the Docker Scout Dashboard displays a summary of recent changes in policy for your repositories. This summary shows images that have seen the most change in their policy evaluation between the most recent image and the previous image.
Policy status per repository
The Images tab shows the current policy status, and recent policy trend, for all images in the selected environment. The Policy status column in the list shows:
- Number of fulfilled policies versus the total number of policies
- Recent policy trends
The policy trend, denoted by the directional arrows, indicates whether an image is better, worse, or unchanged in terms of policy, compared to the previous image in the same environment.
- The green arrow pointing upwards shows the number of policies that got better in the latest pushed image.
- The red arrow pointing downwards shows the number of policies that got worse in the latest pushed image.
- The bidirectional gray arrow shows the number of policies that were unchanged in the latest version of this image.
If you select a repository, you can open the Policy tab for a detailed description of the policy delta for the most recently analyzed image and its predecessor.
Detailed results and remediation
To view the full evaluation results for an image, navigate to the image tag in the Docker Scout Dashboard and open the Policy tab. This shows a breakdown for all policy violations for the current image.
This view also provides recommendations on how to improve improve policy status for violated policies.
For vulnerability-related policies, the policy details view displays the fix version that removes the vulnerability, when a fix version is available. To fix the issue, upgrade the package version to the fix version.
For licensing-related policies, the list shows all packages whose license doesn't meet the policy criteria. To fix the issue, find a way to remove the dependency to the violating package, for example by looking for an alternative package distributed under a more appropriate license.
CLI
To view policy status for an image from the CLI, use the docker scout policy
command.
$ docker scout policy \
--org dockerscoutpolicy \
--platform linux/amd64 \
dockerscoutpolicy/email-api-service:0.0.2
✓ Pulled
✓ Policy evaluation results found
## Overview
│ Analyzed Image
─────────────┼──────────────────────────────────────────────
Target │ dockerscoutpolicy/email-api-service:0.0.2
digest │ 17b1fde0329c
platform │ linux/amd64
## Policies
Policy status FAILED (2/8 policies met, 3 missing data)
Status │ Policy │ Results
─────────┼─────────────────────────────────────────────────────┼──────────────────────────────
✓ │ No copyleft licenses │ 0 packages
! │ Default non-root user │
! │ No fixable critical or high vulnerabilities │ 2C 1H 0M 0L
✓ │ No high-profile vulnerabilities │ 0C 0H 0M 0L
? │ No outdated base images │ No data
│ │ Learn more ↗
? │ SonarQube quality gates passed │ No data
│ │ Learn more ↗
! │ Supply chain attestations │ 2 deviations
? │ No unapproved base images │ No data
...
For more information about the command, refer to the CLI reference.",,,
378eeca58858d6117071310f4f05f3cd286ca0e7c9116c3ff08a1303d9265ca7,"Manage swarm service networks
This page describes networking for swarm services.
Swarm and types of traffic
A Docker swarm generates two different kinds of traffic:
Control and management plane traffic: This includes swarm management messages, such as requests to join or leave the swarm. This traffic is always encrypted.
Application data plane traffic: This includes container traffic and traffic to and from external clients.
Key network concepts
The following three network concepts are important to swarm services:
Overlay networks manage communications among the Docker daemons participating in the swarm. You can create overlay networks, in the same way as user-defined networks for standalone containers. You can attach a service to one or more existing overlay networks as well, to enable service-to-service communication. Overlay networks are Docker networks that use the
overlay
network driver.The ingress network is a special overlay network that facilitates load balancing among a service's nodes. When any swarm node receives a request on a published port, it hands that request off to a module called
IPVS
.IPVS
keeps track of all the IP addresses participating in that service, selects one of them, and routes the request to it, over theingress
network.The
ingress
network is created automatically when you initialize or join a swarm. Most users do not need to customize its configuration, but Docker allows you to do so.The docker_gwbridge is a bridge network that connects the overlay networks (including the
ingress
network) to an individual Docker daemon's physical network. By default, each container a service is running is connected to its local Docker daemon host'sdocker_gwbridge
network.The
docker_gwbridge
network is created automatically when you initialize or join a swarm. Most users do not need to customize its configuration, but Docker allows you to do so.
Tip
See also Networking overview for more details about Swarm networking in general.
Firewall considerations
Docker daemons participating in a swarm need the ability to communicate with each other over the following ports:
- Port
7946
TCP/UDP for container network discovery. - Port
4789
UDP (configurable) for the overlay network (including ingress) data path.
When setting up networking in a Swarm, special care should be taken. Consult the tutorial for an overview.
Overlay networking
When you initialize a swarm or join a Docker host to an existing swarm, two new networks are created on that Docker host:
- An overlay network called
ingress
, which handles the control and data traffic related to swarm services. When you create a swarm service and do not connect it to a user-defined overlay network, it connects to theingress
network by default. - A bridge network called
docker_gwbridge
, which connects the individual Docker daemon to the other daemons participating in the swarm.
Create an overlay network
To create an overlay network, specify the overlay
driver when using the
docker network create
command:
$ docker network create \
--driver overlay \
my-network
The above command doesn't specify any custom options, so Docker assigns a
subnet and uses default options. You can see information about the network using
docker network inspect
.
When no containers are connected to the overlay network, its configuration is not very exciting:
$ docker network inspect my-network
[
{
""Name"": ""my-network"",
""Id"": ""fsf1dmx3i9q75an49z36jycxd"",
""Created"": ""0001-01-01T00:00:00Z"",
""Scope"": ""swarm"",
""Driver"": ""overlay"",
""EnableIPv6"": false,
""IPAM"": {
""Driver"": ""default"",
""Options"": null,
""Config"": []
},
""Internal"": false,
""Attachable"": false,
""Ingress"": false,
""Containers"": null,
""Options"": {
""com.docker.network.driver.overlay.vxlanid_list"": ""4097""
},
""Labels"": null
}
]
In the above output, notice that the driver is overlay
and that the scope is
swarm
, rather than local
, host
, or global
scopes you might see in
other types of Docker networks. This scope indicates that only hosts which are
participating in the swarm can access this network.
The network's subnet and gateway are dynamically configured when a service
connects to the network for the first time. The following example shows
the same network as above, but with three containers of a redis
service
connected to it.
$ docker network inspect my-network
[
{
""Name"": ""my-network"",
""Id"": ""fsf1dmx3i9q75an49z36jycxd"",
""Created"": ""2017-05-31T18:35:58.877628262Z"",
""Scope"": ""swarm"",
""Driver"": ""overlay"",
""EnableIPv6"": false,
""IPAM"": {
""Driver"": ""default"",
""Options"": null,
""Config"": [
{
""Subnet"": ""10.0.0.0/24"",
""Gateway"": ""10.0.0.1""
}
]
},
""Internal"": false,
""Attachable"": false,
""Ingress"": false,
""Containers"": {
""0e08442918814c2275c31321f877a47569ba3447498db10e25d234e47773756d"": {
""Name"": ""my-redis.1.ka6oo5cfmxbe6mq8qat2djgyj"",
""EndpointID"": ""950ce63a3ace13fe7ef40724afbdb297a50642b6d47f83a5ca8636d44039e1dd"",
""MacAddress"": ""02:42:0a:00:00:03"",
""IPv4Address"": ""10.0.0.3/24"",
""IPv6Address"": """"
},
""88d55505c2a02632c1e0e42930bcde7e2fa6e3cce074507908dc4b827016b833"": {
""Name"": ""my-redis.2.s7vlybipal9xlmjfqnt6qwz5e"",
""EndpointID"": ""dd822cb68bcd4ae172e29c321ced70b731b9994eee5a4ad1d807d9ae80ecc365"",
""MacAddress"": ""02:42:0a:00:00:05"",
""IPv4Address"": ""10.0.0.5/24"",
""IPv6Address"": """"
},
""9ed165407384f1276e5cfb0e065e7914adbf2658794fd861cfb9b991eddca754"": {
""Name"": ""my-redis.3.hbz3uk3hi5gb61xhxol27hl7d"",
""EndpointID"": ""f62c686a34c9f4d70a47b869576c37dffe5200732e1dd6609b488581634cf5d2"",
""MacAddress"": ""02:42:0a:00:00:04"",
""IPv4Address"": ""10.0.0.4/24"",
""IPv6Address"": """"
}
},
""Options"": {
""com.docker.network.driver.overlay.vxlanid_list"": ""4097""
},
""Labels"": {},
""Peers"": [
{
""Name"": ""moby-e57c567e25e2"",
""IP"": ""192.168.65.2""
}
]
}
]
Customize an overlay network
There may be situations where you don't want to use the default configuration
for an overlay network. For a full list of configurable options, run the
command docker network create --help
. The following are some of the most
common options to change.
Configure the subnet and gateway
By default, the network's subnet and gateway are configured automatically when
the first service is connected to the network. You can configure these when
creating a network using the --subnet
and --gateway
flags. The following
example extends the previous one by configuring the subnet and gateway.
$ docker network create \
--driver overlay \
--subnet 10.0.9.0/24 \
--gateway 10.0.9.99 \
my-network
Using custom default address pools
To customize subnet allocation for your Swarm networks, you can
optionally configure them during swarm init
.
For example, the following command is used when initializing Swarm:
$ docker swarm init --default-addr-pool 10.20.0.0/16 --default-addr-pool-mask-length 26
Whenever a user creates a network, but does not use the --subnet
command line option, the subnet for this network will be allocated sequentially from the next available subnet from the pool. If the specified network is already allocated, that network will not be used for Swarm.
Multiple pools can be configured if discontiguous address space is required. However, allocation from specific pools is not supported. Network subnets will be allocated sequentially from the IP pool space and subnets will be reused as they are deallocated from networks that are deleted.
The default mask length can be configured and is the same for all networks. It is set to /24
by default. To change the default subnet mask length, use the --default-addr-pool-mask-length
command line option.
Note
Default address pools can only be configured on
swarm init
and cannot be altered after cluster creation.
Overlay network size limitations
Docker recommends creating overlay networks with /24
blocks. The /24
overlay network blocks limit the network to 256 IP addresses.
This recommendation addresses
limitations with swarm mode.
If you need more than 256 IP addresses, do not increase the IP block size. You can either use dnsrr
endpoint mode with an external load balancer, or use multiple smaller overlay networks. See
Configure service discovery for more information about different endpoint modes.
Configure encryption of application data
Management and control plane data related to a swarm is always encrypted. For more details about the encryption mechanisms, see the Docker swarm mode overlay network security model.
Application data among swarm nodes is not encrypted by default. To encrypt this
traffic on a given overlay network, use the --opt encrypted
flag on docker network create
. This enables IPSEC encryption at the level of the vxlan. This
encryption imposes a non-negligible performance penalty, so you should test this
option before using it in production.
Note
You must customize the automatically created ingress to enable encryption. By default, all ingress traffic is unencrypted, as encryption is a network-level option.
Attach a service to an overlay network
To attach a service to an existing overlay network, pass the --network
flag to
docker service create
, or the --network-add
flag to docker service update
.
$ docker service create \
--replicas 3 \
--name my-web \
--network my-network \
nginx
Service containers connected to an overlay network can communicate with each other across it.
To see which networks a service is connected to, use docker service ls
to find
the name of the service, then docker service ps <service-name>
to list the
networks. Alternately, to see which services' containers are connected to a
network, use docker network inspect <network-name>
. You can run these commands
from any swarm node which is joined to the swarm and is in a running
state.
Configure service discovery
Service discovery is the mechanism Docker uses to route a request from your service's external clients to an individual swarm node, without the client needing to know how many nodes are participating in the service or their IP addresses or ports. You don't need to publish ports which are used between services on the same network. For instance, if you have a WordPress service that stores its data in a MySQL service, and they are connected to the same overlay network, you do not need to publish the MySQL port to the client, only the WordPress HTTP port.
Service discovery can work in two different ways: internal connection-based load-balancing at Layers 3 and 4 using the embedded DNS and a virtual IP (VIP), or external and customized request-based load-balancing at Layer 7 using DNS round robin (DNSRR). You can configure this per service.
By default, when you attach a service to a network and that service publishes one or more ports, Docker assigns the service a virtual IP (VIP), which is the ""front end"" for clients to reach the service. Docker keeps a list of all worker nodes in the service, and routes requests between the client and one of the nodes. Each request from the client might be routed to a different node.
If you configure a service to use DNS round-robin (DNSRR) service discovery, there is not a single virtual IP. Instead, Docker sets up DNS entries for the service such that a DNS query for the service name returns a list of IP addresses, and the client connects directly to one of these.
DNS round-robin is useful in cases where you want to use your own load balancer, such as HAProxy. To configure a service to use DNSRR, use the flag
--endpoint-mode dnsrr
when creating a new service or updating an existing one.
Customize the ingress network
Most users never need to configure the ingress
network, but Docker allows you
to do so. This can be useful if the automatically-chosen subnet
conflicts with one that already exists on your network, or you need to customize
other low-level network settings such as the MTU, or if you want to
enable encryption.
Customizing the ingress
network involves removing and recreating it. This is
usually done before you create any services in the swarm. If you have existing
services which publish ports, those services need to be removed before you can
remove the ingress
network.
During the time that no ingress
network exists, existing services which do not
publish ports continue to function but are not load-balanced. This affects
services which publish ports, such as a WordPress service which publishes port
80.
Inspect the
ingress
network usingdocker network inspect ingress
, and remove any services whose containers are connected to it. These are services that publish ports, such as a WordPress service which publishes port 80. If all such services are not stopped, the next step fails.Remove the existing
ingress
network:$ docker network rm ingress WARNING! Before removing the routing-mesh network, make sure all the nodes in your swarm run the same docker engine version. Otherwise, removal may not be effective and functionality of newly created ingress networks will be impaired. Are you sure you want to continue? [y/N]
Create a new overlay network using the
--ingress
flag, along with the custom options you want to set. This example sets the MTU to 1200, sets the subnet to10.11.0.0/16
, and sets the gateway to10.11.0.2
.$ docker network create \ --driver overlay \ --ingress \ --subnet=10.11.0.0/16 \ --gateway=10.11.0.2 \ --opt com.docker.network.driver.mtu=1200 \ my-ingress
Note
You can name your
ingress
network something other thaningress
, but you can only have one. An attempt to create a second one fails.Restart the services that you stopped in the first step.
Customize the docker_gwbridge
The docker_gwbridge
is a virtual bridge that connects the overlay networks
(including the ingress
network) to an individual Docker daemon's physical
network. Docker creates it automatically when you initialize a swarm or join a
Docker host to a swarm, but it is not a Docker device. It exists in the kernel
of the Docker host. If you need to customize its settings, you must do so before
joining the Docker host to the swarm, or after temporarily removing the host
from the swarm.
You need to have the brctl
application installed on your operating system in
order to delete an existing bridge. The package name is bridge-utils
.
Stop Docker.
Use the
brctl show docker_gwbridge
command to check whether a bridge device exists calleddocker_gwbridge
. If so, remove it usingbrctl delbr docker_gwbridge
.Start Docker. Do not join or initialize the swarm.
Create or re-create the
docker_gwbridge
bridge with your custom settings. This example uses the subnet10.11.0.0/16
. For a full list of customizable options, see Bridge driver options.$ docker network create \ --subnet 10.11.0.0/16 \ --opt com.docker.network.bridge.name=docker_gwbridge \ --opt com.docker.network.bridge.enable_icc=false \ --opt com.docker.network.bridge.enable_ip_masquerade=true \ docker_gwbridge
Initialize or join the swarm.
Use a separate interface for control and data traffic
By default, all swarm traffic is sent over the same interface, including control and management traffic for maintaining the swarm itself and data traffic to and from the service containers.
You can separate this traffic by passing
the --data-path-addr
flag when initializing or joining the swarm. If there are
multiple interfaces, --advertise-addr
must be specified explicitly, and
--data-path-addr
defaults to --advertise-addr
if not specified. Traffic about
joining, leaving, and managing the swarm is sent over the
--advertise-addr
interface, and traffic among a service's containers is sent
over the --data-path-addr
interface. These flags can take an IP address or
a network device name, such as eth0
.
This example initializes a swarm with a separate --data-path-addr
. It assumes
that your Docker host has two different network interfaces: 10.0.0.1 should be
used for control and management traffic and 192.168.0.1 should be used for
traffic relating to services.
$ docker swarm init --advertise-addr 10.0.0.1 --data-path-addr 192.168.0.1
This example joins the swarm managed by host 192.168.99.100:2377
and sets the
--advertise-addr
flag to eth0
and the --data-path-addr
flag to eth1
.
$ docker swarm join \
--token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2d7c \
--advertise-addr eth0 \
--data-path-addr eth1 \
192.168.99.100:2377
Publish ports on an overlay network
Swarm services connected to the same overlay network effectively expose all
ports to each other. For a port to be accessible outside of the service, that
port must be published using the -p
or --publish
flag on docker service create
or docker service update
. Both the legacy colon-separated syntax and
the newer comma-separated value syntax are supported. The longer syntax is
preferred because it is somewhat self-documenting.
| Flag value | Description |
|---|---|
| -p 8080:80 or -p published=8080,target=80 | Map TCP port 80 on the service to port 8080 on the routing mesh. |
| -p 8080:80/udp or -p published=8080,target=80,protocol=udp | Map UDP port 80 on the service to port 8080 on the routing mesh. |
| -p 8080:80/tcp -p 8080:80/udp or -p published=8080,target=80,protocol=tcp -p published=8080,target=80,protocol=udp | Map TCP port 80 on the service to TCP port 8080 on the routing mesh, and map UDP port 80 on the service to UDP port 8080 on the routing mesh. |",,,
bad5374eeeba5c2b3d36c775137cae1e880362917638143399dd4a34ec955f03,"Prune unused Docker objects
Docker takes a conservative approach to cleaning up unused objects (often
referred to as ""garbage collection""), such as images, containers, volumes, and
networks. These objects are generally not removed unless you explicitly ask
Docker to do so. This can cause Docker to use extra disk space. For each type of
object, Docker provides a prune
command. In addition, you can use docker system prune
to clean up multiple types of objects at once. This topic shows
how to use these prune
commands.
Prune images
The docker image prune
command allows you to clean up unused images. By
default, docker image prune
only cleans up dangling images. A dangling image
is one that isn't tagged, and isn't referenced by any container. To remove
dangling images:
$ docker image prune
WARNING! This will remove all dangling images.
Are you sure you want to continue? [y/N] y
To remove all images which aren't used by existing containers, use the -a
flag:
$ docker image prune -a
WARNING! This will remove all images without at least one container associated to them.
Are you sure you want to continue? [y/N] y
By default, you are prompted to continue. To bypass the prompt, use the -f
or
--force
flag.
You can limit which images are pruned using filtering expressions with the
--filter
flag. For example, to only consider images created more than 24
hours ago:
$ docker image prune -a --filter ""until=24h""
Other filtering expressions are available. See the
docker image prune
reference
for more examples.
Prune containers
When you stop a container, it isn't automatically removed unless you started it
with the --rm
flag. To see all containers on the Docker host, including
stopped containers, use docker ps -a
. You may be surprised how many containers
exist, especially on a development system! A stopped container's writable layers
still take up disk space. To clean this up, you can use the docker container prune
command.
$ docker container prune
WARNING! This will remove all stopped containers.
Are you sure you want to continue? [y/N] y
By default, you're prompted to continue. To bypass the prompt, use the -f
or
--force
flag.
By default, all stopped containers are removed. You can limit the scope using
the --filter
flag. For instance, the following command only removes
stopped containers older than 24 hours:
$ docker container prune --filter ""until=24h""
Other filtering expressions are available. See the
docker container prune
reference
for more examples.
Prune volumes
Volumes can be used by one or more containers, and take up space on the Docker host. Volumes are never removed automatically, because to do so could destroy data.
$ docker volume prune
WARNING! This will remove all volumes not used by at least one container.
Are you sure you want to continue? [y/N] y
By default, you are prompted to continue. To bypass the prompt, use the -f
or
--force
flag.
By default, all unused volumes are removed. You can limit the scope using
the --filter
flag. For instance, the following command only removes
volumes which aren't labelled with the keep
label:
$ docker volume prune --filter ""label!=keep""
Other filtering expressions are available. See the
docker volume prune
reference
for more examples.
Prune networks
Docker networks don't take up much disk space, but they do create iptables
rules, bridge network devices, and routing table entries. To clean these things
up, you can use docker network prune
to clean up networks which aren't used
by any containers.
$ docker network prune
WARNING! This will remove all networks not used by at least one container.
Are you sure you want to continue? [y/N] y
By default, you're prompted to continue. To bypass the prompt, use the -f
or
--force
flag.
By default, all unused networks are removed. You can limit the scope using
the --filter
flag. For instance, the following command only removes
networks older than 24 hours:
$ docker network prune --filter ""until=24h""
Other filtering expressions are available. See the
docker network prune
reference
for more examples.
Prune everything
The docker system prune
command is a shortcut that prunes images, containers,
and networks. Volumes aren't pruned by default, and you must specify the
--volumes
flag for docker system prune
to prune volumes.
$ docker system prune
WARNING! This will remove:
- all stopped containers
- all networks not used by at least one container
- all dangling images
- unused build cache
Are you sure you want to continue? [y/N] y
To also prune volumes, add the --volumes
flag:
$ docker system prune --volumes
WARNING! This will remove:
- all stopped containers
- all networks not used by at least one container
- all volumes not used by at least one container
- all dangling images
- all build cache
Are you sure you want to continue? [y/N] y
By default, you're prompted to continue. To bypass the prompt, use the -f
or
--force
flag.
By default, all unused containers, networks, and images are removed. You can
limit the scope using the --filter
flag. For instance, the following command
removes items older than 24 hours:
$ docker system prune --filter ""until=24h""
Other filtering expressions are available. See the
docker system prune
reference
for more examples.",,,
40dbc9a80ddaf97b6d1a6adf5593b323ccc061d5166bf29ff837ea5bf014d414,"Introduction to Bake
Bake is an abstraction for the docker build
command that lets you more easily
manage your build configuration (CLI flags, environment variables, etc.) in a
consistent way for everyone on your team.
Bake is a command built into the Buildx CLI, so as long as you have Buildx
installed, you also have access to bake, via the docker buildx bake
command.
Building a project with Bake
Here's a simple example of a docker build
command:
$ docker build -f Dockerfile -t myapp:latest .
This command builds the Dockerfile in the current directory and tags the
resulting image as myapp:latest
.
To express the same build configuration using Bake:
target ""myapp"" {
context = "".""
dockerfile = ""Dockerfile""
tags = [""myapp:latest""]
}
Bake provides a structured way to manage your build configuration, and it saves
you from having to remember all the CLI flags for docker build
every time.
With this file, building the image is as simple as running:
$ docker buildx bake myapp
For simple builds, the difference between docker build
and docker buildx bake
is minimal. However, as your build configuration grows more complex, Bake
provides a more structured way to manage that complexity, that would be
difficult to manage with CLI flags for the docker build
. It also provides a
way to share build configurations across your team, so that everyone is
building images in a consistent way, with the same configuration.
The Bake file format
You can write Bake files in HCL, YAML (Docker Compose files), or JSON. In general, HCL is the most expressive and flexible format, which is why you'll see it used in most of the examples in this documentation, and in projects that use Bake.
The properties that can be set for a target closely resemble the CLI flags for
docker build
. For instance, consider the following docker build
command:
$ docker build \
-f Dockerfile \
-t myapp:latest \
--build-arg foo=bar \
--no-cache \
--platform linux/amd64,linux/arm64 \
.
The Bake equivalent would be:
target ""myapp"" {
context = "".""
dockerfile = ""Dockerfile""
tags = [""myapp:latest""]
args = {
foo = ""bar""
}
no-cache = true
platforms = [""linux/amd64"", ""linux/arm64""]
}
Next steps
To learn more about using Bake, see the following topics:
- Learn how to define and use targets in Bake
- To see all the properties that can be set for a target, refer to the Bake file reference.",,,
b264ffe21367c0d178ffafc14516a2e9590a56c536cf21e87998798d5b459794,"Using the Docker for GitHub Copilot extension
The Docker Extension for GitHub Copilot provides a chat interface that you can use to interact with the Docker agent. You can ask questions and get help Dockerizing your project.
The Docker agent is trained to understand Docker-related questions, and provide guidance on Dockerfiles, Docker Compose files, and other Docker assets.
Setup
Before you can start interacting with the Docker agent, make sure you've installed the extension for your organization.
Enable GitHub Copilot chat in your editor or IDE
For instructions on how to use the Docker Extension for GitHub Copilot in your editor, see:
Verify the setup
You can verify that the extension has been properly installed by typing
@docker
in the Copilot Chat window. As you type, you should see the Docker
agent appear in the chat interface.
The first time you interact with the agent, you're prompted to sign in and authorize the Copilot extension with your Docker account.
Asking Docker questions in your editor
To interact with the Docker agent from within your editor or IDE:
- Open your project in your editor.
- Open the Copilot chat interface.
- Interact with the Docker agent by tagging
@docker
, followed by your question.
Asking Docker questions on GitHub.com
To interact with the Docker agent from the GitHub web interface:
Go to github.com and sign in to your account.
Go to any repository.
Select the Copilot logo in the site menu, or select the floating Copilot widget, to open the chat interface.
Interact with the Docker agent by tagging
@docker
, followed by your question.",,,
656c8a68f32f8417254fa60b88d6137ca87f329f64b9cfe1f3cacd44bbff6487,"Docker Scout health scores
Docker Scout health scores provide a security assessment, and overall supply chain health, of images on Docker Hub, helping you determine whether an image meets established security best practices. The scores range from A to F, where A represents the highest level of security and F the lowest, offering an at-a-glance view of the security posture of your images.
Only users who are members of the organization that owns the repository, and have at least “read” access to the repository, can view the health score. The score is not visible to users outside the organization or members without ""read"" access.
Viewing health scores
To view the health score of an image in Docker Hub:
- Go to Docker Hub and sign in.
- Navigate to your organization's page.
In the list of repositories, you can see the health score of each repository based on the latest pushed tag.
To view the health score of an image in Docker Desktop:
- Open Docker Desktop and sign in to your Docker account.
- Navigate to the Images view and select the Hub tab.
In the list of repositories, the Health column displays the scores of the different tags that have been pushed to Docker Hub.
The health score badge is color-coded to indicate the overall health of the repository:
- Green: A score of A or B.
- Yellow: A score of C.
- Orange: A score of D.
- Red: A score of E or F.
- Gray: An
N/A
score.
The score is also displayed on the Docker Hub page for a given repository, along with each policy that contributed to the score.
Scoring system
Health scores are determined by evaluating images against Docker Scout policies. These policies align with best practices for the software supply chain.
If your image repositories are already enrolled with Docker Scout, the health score is calculated automatically based on the policies that are enabled for your organization. This also includes any custom policies that you have configured.
If you're not using Docker Scout, the health scores show the compliance of your images with the default policies, a set of supply chain rules recommended by Docker as foundational standards for images. You can enable Docker Scout for your organization and edit the policy configurations to get a more relevant health score based on your specific policies.
Scoring process
Each policy is assigned a points value based on its type. If the image is compliant with a policy, it is awarded the points value for that policy type. The health score of an image is calculated based on the percentage of points achieved relative to the total possible points.
Policy compliance is evaluated for the image.
Points are awarded based on compliance with policies.
The points achieved percentage is calculated:
Percentage = (Points / Total) * 100
The final score is assigned based on the percentage of points achieved, as shown in the following table:
Points percentage (awarded out of total) Score More than 90% A 71% to 90% B 51% to 70% C 31% to 50% D 11% to 30% E Less than 10% F
N/A scores
Images can also be assigned an N/A
score, which can happen when:
- The image is larger than 4GB (compressed size).
- The image architecture is not
linux/amd64
orlinux/arm64
. - The image is too old and does not have fresh data for evaluation.
If you see an N/A
score, consider the following:
- If the image is too large, try reducing the size of the image.
- If the image has an unsupported architecture, rebuild the image for a supported architecture.
- If the image is too old, push a new tag to trigger a fresh evaluation.
Policy weights
Different policy types carry varying weights, which impact the score assigned to an image during evaluation, as shown in the following table.
* This policy is not enabled by default and must be configured by the user.
Evaluation
Health scores are calculated for new images pushed to Docker Hub after the feature is enabled. The health scores help you maintain high security standards and ensure your applications are built on secure and reliable images.
Repository scores
In addition to individual image scores (per tag or digest), each repository receives a health score based on the latest pushed tag, providing an overall view of the repository's security status.
Example
For an image with a total possible score of 100 points:
- If the image only deviates from one policy, worth 5 points, its score will be 95 out of 100. Since this score is above the 90th percentile, the image receives an A health score.
- If the image is non-compliant with more policies and scores 65 out of 100, it receives a C health score, reflecting its lower compliance.
Improving your health score
To improve the health score of an image, take steps to ensure that the image is compliant with the Docker Scout recommended policies.
- Go to the Docker Scout Dashboard.
- Sign in using your Docker ID.
- Go to Repository settings and enable Docker Scout for your Docker Hub image repositories.
- Analyze the policy compliance for your repositories, and take actions to ensure your images are policy-compliant.
Since policies are weighted differently, prioritize the policies with the highest scores for a greater impact on your image's overall score.",,,
cf45008d0d5a414ad5c527bd54567974987d17f9d513f4e2d11e7a5f0efbee3b,"Isolate containers with a user namespace
Linux namespaces provide isolation for running processes, limiting their access to system resources without the running process being aware of the limitations. For more information on Linux namespaces, see Linux namespaces.
The best way to prevent privilege-escalation attacks from within a container is
to configure your container's applications to run as unprivileged users. For
containers whose processes must run as the root
user within the container, you
can re-map this user to a less-privileged user on the Docker host. The mapped
user is assigned a range of UIDs which function within the namespace as normal
UIDs from 0 to 65536, but have no privileges on the host machine itself.
About remapping and subordinate user and group IDs
The remapping itself is handled by two files: /etc/subuid
and /etc/subgid
.
Each file works the same, but one is concerned with the user ID range, and the
other with the group ID range. Consider the following entry in /etc/subuid
:
testuser:231072:65536
This means that testuser
is assigned a subordinate user ID range of 231072
and the next 65536 integers in sequence. UID 231072
is mapped within the
namespace (within the container, in this case) as UID 0
(root
). UID 231073
is mapped as UID 1
, and so forth. If a process attempts to escalate privilege
outside of the namespace, the process is running as an unprivileged high-number
UID on the host, which does not even map to a real user. This means the process
has no privileges on the host system at all.
Note
It is possible to assign multiple subordinate ranges for a given user or group by adding multiple non-overlapping mappings for the same user or group in the
/etc/subuid
or/etc/subgid
file. In this case, Docker uses only the first five mappings, in accordance with the kernel's limitation of only five entries in/proc/self/uid_map
and/proc/self/gid_map
.
When you configure Docker to use the userns-remap
feature, you can optionally
specify an existing user and/or group, or you can specify default
. If you
specify default
, a user and group dockremap
is created and used for this
purpose.
Warning
Some distributions do not automatically add the new group to the
/etc/subuid
and/etc/subgid
files. If that's the case, you are may have to manually edit these files and assign non-overlapping ranges. This step is covered in Prerequisites.
It is very important that the ranges do not overlap, so that a process cannot gain access in a different namespace. On most Linux distributions, system utilities manage the ranges for you when you add or remove users.
This re-mapping is transparent to the container, but introduces some configuration complexity in situations where the container needs access to resources on the Docker host, such as bind mounts into areas of the filesystem that the system user cannot write to. From a security standpoint, it is best to avoid these situations.
Prerequisites
The subordinate UID and GID ranges must be associated with an existing user, even though the association is an implementation detail. The user owns the namespaced storage directories under
/var/lib/docker/
. If you don't want to use an existing user, Docker can create one for you and use that. If you want to use an existing username or user ID, it must already exist. Typically, this means that the relevant entries need to be in/etc/passwd
and/etc/group
, but if you are using a different authentication back-end, this requirement may translate differently.To verify this, use the
id
command:$ id testuser uid=1001(testuser) gid=1001(testuser) groups=1001(testuser)
The way the namespace remapping is handled on the host is using two files,
/etc/subuid
and/etc/subgid
. These files are typically managed automatically when you add or remove users or groups, but on some distributions, you may need to manage these files manually.Each file contains three fields: the username or ID of the user, followed by a beginning UID or GID (which is treated as UID or GID 0 within the namespace) and a maximum number of UIDs or GIDs available to the user. For instance, given the following entry:
testuser:231072:65536
This means that user-namespaced processes started by
testuser
are owned by host UID231072
(which looks like UID0
inside the namespace) through 296607 (231072 + 65536 - 1). These ranges should not overlap, to ensure that namespaced processes cannot access each other's namespaces.After adding your user, check
/etc/subuid
and/etc/subgid
to see if your user has an entry in each. If not, you need to add it, being careful to avoid overlap.If you want to use the
dockremap
user automatically created by Docker, check for thedockremap
entry in these files after configuring and restarting Docker.If there are any locations on the Docker host where the unprivileged user needs to write, adjust the permissions of those locations accordingly. This is also true if you want to use the
dockremap
user automatically created by Docker, but you can't modify the permissions until after configuring and restarting Docker.Enabling
userns-remap
effectively masks existing image and container layers, as well as other Docker objects within/var/lib/docker/
. This is because Docker needs to adjust the ownership of these resources and actually stores them in a subdirectory within/var/lib/docker/
. It is best to enable this feature on a new Docker installation rather than an existing one.Along the same lines, if you disable
userns-remap
you can't access any of the resources created while it was enabled.Check the limitations on user namespaces to be sure your use case is possible.
Enable userns-remap on the daemon
You can start dockerd
with the --userns-remap
flag or follow this
procedure to configure the daemon using the daemon.json
configuration file.
The daemon.json
method is recommended. If you use the flag, use the following
command as a model:
$ dockerd --userns-remap=""testuser:testuser""
Edit
/etc/docker/daemon.json
. Assuming the file was previously empty, the following entry enablesuserns-remap
using user and group calledtestuser
. You can address the user and group by ID or name. You only need to specify the group name or ID if it is different from the user name or ID. If you provide both the user and group name or ID, separate them by a colon (:
) character. The following formats all work for the value, assuming the UID and GID oftestuser
are1001
:testuser
testuser:testuser
1001
1001:1001
testuser:1001
1001:testuser
{ ""userns-remap"": ""testuser"" }
Note
To use the
dockremap
user and have Docker create it for you, set the value todefault
rather thantestuser
.Save the file and restart Docker.
If you are using the
dockremap
user, verify that Docker created it using theid
command.$ id dockremap uid=112(dockremap) gid=116(dockremap) groups=116(dockremap)
Verify that the entry has been added to
/etc/subuid
and/etc/subgid
:$ grep dockremap /etc/subuid dockremap:231072:65536 $ grep dockremap /etc/subgid dockremap:231072:65536
If these entries are not present, edit the files as the
root
user and assign a starting UID and GID that is the highest-assigned one plus the offset (in this case,65536
). Be careful not to allow any overlap in the ranges.Verify that previous images are not available using the
docker image ls
command. The output should be empty.Start a container from the
hello-world
image.$ docker run hello-world
Verify that a namespaced directory exists within
/var/lib/docker/
named with the UID and GID of the namespaced user, owned by that UID and GID, and not group-or-world-readable. Some of the subdirectories are still owned byroot
and have different permissions.$ sudo ls -ld /var/lib/docker/231072.231072/ drwx------ 11 231072 231072 11 Jun 21 21:19 /var/lib/docker/231072.231072/ $ sudo ls -l /var/lib/docker/231072.231072/ total 14 drwx------ 5 231072 231072 5 Jun 21 21:19 aufs drwx------ 3 231072 231072 3 Jun 21 21:21 containers drwx------ 3 root root 3 Jun 21 21:19 image drwxr-x--- 3 root root 3 Jun 21 21:19 network drwx------ 4 root root 4 Jun 21 21:19 plugins drwx------ 2 root root 2 Jun 21 21:19 swarm drwx------ 2 231072 231072 2 Jun 21 21:21 tmp drwx------ 2 root root 2 Jun 21 21:19 trust drwx------ 2 231072 231072 3 Jun 21 21:19 volumes
Your directory listing may have some differences, especially if you use a different container storage driver than
aufs
.The directories which are owned by the remapped user are used instead of the same directories directly beneath
/var/lib/docker/
and the unused versions (such as/var/lib/docker/tmp/
in the example here) can be removed. Docker does not use them whileuserns-remap
is enabled.
Disable namespace remapping for a container
If you enable user namespaces on the daemon, all containers are started with user namespaces enabled by default. In some situations, such as privileged containers, you may need to disable user namespaces for a specific container. See user namespace known limitations for some of these limitations.
To disable user namespaces for a specific container, add the --userns=host
flag to the docker container create
, docker container run
, or docker container exec
command.
There is a side effect when using this flag: user remapping will not be enabled for that container but, because the read-only (image) layers are shared between containers, ownership of the containers filesystem will still be remapped.
What this means is that the whole container filesystem will belong to the user specified in the --userns-remap
daemon config (231072
in the example above). This can lead to unexpected behavior of programs inside the container. For instance sudo
(which checks that its binaries belong to user 0
) or binaries with a setuid
flag.
User namespace known limitations
The following standard Docker features are incompatible with running a Docker daemon with user namespaces enabled:
- Sharing PID or NET namespaces with the host (
--pid=host
or--network=host
). - External (volume or storage) drivers which are unaware or incapable of using daemon user mappings.
- Using the
--privileged
mode flag ondocker run
without also specifying--userns=host
.
User namespaces are an advanced feature and require coordination with other capabilities. For example, if volumes are mounted from the host, file ownership must be pre-arranged if you need read or write access to the volume contents.
While the root user inside a user-namespaced container process has many of the
expected privileges of the superuser within the container, the Linux kernel
imposes restrictions based on internal knowledge that this is a user-namespaced
process. One notable restriction is the inability to use the mknod
command.
Permission is denied for device creation within the container when run by
the root
user.",,,
1dda35f0ba73afeb2d59d2cb182bff1fa33916a55404c830e13b7051065bf9fe,"Package and release your extension
This page contains additional information on how to package and distribute extensions.
Package your extension
Docker extensions are packaged as Docker images. The entire extension runtime including the UI, backend services (host or VM), and any necessary binary must be included in the extension image.
Every extension image must contain a metadata.json
file at the root of its filesystem that defines the
contents of the extension.
The Docker image must have several image labels, providing information about the extension. See how to use extension labels to provide extension overview information.
To package and release an extension, you must build a Docker image (docker build
), and push the image to
Docker Hub (docker push
) with a specific tag that lets you manage versions of the extension.
Release your extension
Docker image tags must follow semver conventions in order to allow fetching the latest version of the extension, and to know if there are updates available. See semver.org to learn more about semantic versioning.
Extension images must be multi-arch images so that users can install extensions on ARM/AMD hardware. These multi-arch images can include ARM/AMD specific binaries. Mac users will automatically use the right image based on their architecture. Extensions that install binaries on the host must also provide Windows binaries in the same extension image. See how to build a multi-arch image for your extension.
You can implement extensions without any constraints on the code repository. Docker doesn't need access to the code repository in order to use the extension. Also, you can manage new releases of your extension, without any dependency on Docker Desktop releases.
New releases and updates
You can release a new version of your Docker extension by pushing a new image with a new tag to Docker Hub.
Any new image pushed to an image repository corresponding to an extension defines a new version of that extension. Image tags are used to identify version numbers. Extension versions must follow semver to make it easy to understand and compare versions.
Docker Desktop scans the list of extensions published in the marketplace for new versions, and provides notifications to users when they can upgrade a specific extension. Extensions that aren't part of the Marketplace don't have automatic update notifications at the moment.
Users can download and install the newer version of any extension without updating Docker Desktop itself.
Extension API dependencies
Extensions must specify the Extension API version they rely on. Docker Desktop checks the extension's required version, and only proposes to install extensions that are compatible with the current Docker Desktop version installed. Users might need to update Docker Desktop in order to install the latest extensions available.
Extension image labels must specify the API version that the extension relies upon. This allows Docker Desktop to inspect newer versions of extension images without downloading the full extension image upfront.
License on extensions and the extension SDK
The Docker Extension SDK is licensed under the Apache 2.0 License and is free to use.
There is no constraint on how each extension should be licensed, this is up to you to decide when creating a new extension.",,,
d156b327cfa79cea4685bd20aa59dba46b25febdc65fe5df37089f15b88a0999,"Set environment variables within your container's environment
A container's environment is not set until there's an explicit entry in the service configuration to make this happen. With Compose, there are two ways you can set environment variables in your containers with your Compose file.
Tip
Don't use environment variables to pass sensitive information, such as passwords, in to your containers. Use secrets instead.
Use the environment
attribute
You can set environment variables directly in your container's environment with the
environment
attribute in your compose.yaml
.
It supports both list and mapping syntax:
services:
webapp:
environment:
DEBUG: ""true""
is equivalent to
services:
webapp:
environment:
- DEBUG=true
See
environment
attribute for more examples on how to use it.
Additional information
- You can choose not to set a value and pass the environment variables from your shell straight through to your containers. It works in the same way as
docker run -e VARIABLE ...
:web: environment: - DEBUG
The value of the DEBUG
variable in the container is taken from the value for the same variable in the shell in which Compose is run. Note that in this case no warning is issued if the DEBUG
variable in the shell environment is not set.
You can also take advantage of interpolation. In the following example, the result is similar to the one above but Compose gives you a warning if the
DEBUG
variable is not set in the shell environment or in an.env
file in the project directory.web: environment: - DEBUG=${DEBUG}
Use the env_file
attribute
A container's environment can also be set using
.env
files along with the
env_file
attribute.
services:
webapp:
env_file: ""webapp.env""
Using an .env
file lets you use the same file for use by a plain docker run --env-file ...
command, or to share the same .env
file within multiple services without the need to duplicate a long environment
YAML block.
It can also help you keep your environment variables separate from your main configuration file, providing a more organized and secure way to manage sensitive information, as you do not need to place your .env
file in the root of your project's directory.
The
env_file
attribute also lets you use multiple .env
files in your Compose application.
The paths to your .env
file, specified in the env_file
attribute, are relative to the location of your compose.yaml
file.
Important
Interpolation in
.env
files is a Docker Compose CLI feature.It is not supported when running
docker run --env-file ...
.
Additional information
- If multiple files are specified, they are evaluated in order and can override values set in previous files.
- As of Docker Compose version 2.24.0, you can set your
.env
file, defined by theenv_file
attribute, to be optional by using therequired
field. Whenrequired
is set tofalse
and the.env
file is missing, Compose silently ignores the entry.env_file: - path: ./default.env required: true # default - path: ./override.env required: false
- As of Docker Compose version 2.30.0, you can use an alternative file format for the
env_file
with theformat
attribute. For more information, seeformat
. - Values in your
.env
file can be overridden from the command line by usingdocker compose run -e
.
Set environment variables with docker compose run --env
Similar to docker run --env
, you can set environment variables temporarily with docker compose run --env
or its short form docker compose run -e
:
$ docker compose run -e DEBUG=1 web python console.py
Additional information
You can also pass a variable from the shell or your environment files by not giving it a value:
$ docker compose run -e DEBUG web python console.py
The value of the DEBUG
variable in the container is taken from the value for the same variable in the shell in which Compose is run or from the environment files.",,,
a79f87cdd9150aa662287a1c0281dcb5a6392da4a73c4f0bfb6402cb59308c2a,"Apply rolling updates to a service
In a previous step of the tutorial, you scaled the number of instances of a service. In this part of the tutorial, you deploy a service based on the Redis 7.4.0 container tag. Then you upgrade the service to use the Redis 7.4.1 container image using rolling updates.
If you haven't already, open a terminal and ssh into the machine where you run your manager node. For example, the tutorial uses a machine named
manager1
.Deploy your Redis tag to the swarm and configure the swarm with a 10 second update delay. Note that the following example shows an older Redis tag:
$ docker service create \ --replicas 3 \ --name redis \ --update-delay 10s \ redis:7.4.0 0u6a4s31ybk7yw2wyvtikmu50
You configure the rolling update policy at service deployment time.
The
--update-delay
flag configures the time delay between updates to a service task or sets of tasks. You can describe the timeT
as a combination of the number of secondsTs
, minutesTm
, or hoursTh
. So10m30s
indicates a 10 minute 30 second delay.By default the scheduler updates 1 task at a time. You can pass the
--update-parallelism
flag to configure the maximum number of service tasks that the scheduler updates simultaneously.By default, when an update to an individual task returns a state of
RUNNING
, the scheduler schedules another task to update until all tasks are updated. If at any time during an update a task returnsFAILED
, the scheduler pauses the update. You can control the behavior using the--update-failure-action
flag fordocker service create
ordocker service update
.Inspect the
redis
service:$ docker service inspect --pretty redis ID: 0u6a4s31ybk7yw2wyvtikmu50 Name: redis Service Mode: Replicated Replicas: 3 Placement: Strategy: Spread UpdateConfig: Parallelism: 1 Delay: 10s ContainerSpec: Image: redis:7.4.0 Resources: Endpoint Mode: vip
Now you can update the container image for
redis
. The swarm manager applies the update to nodes according to theUpdateConfig
policy:$ docker service update --image redis:7.4.1 redis redis
The scheduler applies rolling updates as follows by default:
- Stop the first task.
- Schedule update for the stopped task.
- Start the container for the updated task.
- If the update to a task returns
RUNNING
, wait for the specified delay period then start the next task. - If, at any time during the update, a task returns
FAILED
, pause the update.
Run
docker service inspect --pretty redis
to see the new image in the desired state:$ docker service inspect --pretty redis ID: 0u6a4s31ybk7yw2wyvtikmu50 Name: redis Service Mode: Replicated Replicas: 3 Placement: Strategy: Spread UpdateConfig: Parallelism: 1 Delay: 10s ContainerSpec: Image: redis:7.4.1 Resources: Endpoint Mode: vip
The output of
service inspect
shows if your update paused due to failure:$ docker service inspect --pretty redis ID: 0u6a4s31ybk7yw2wyvtikmu50 Name: redis ...snip... Update status: State: paused Started: 11 seconds ago Message: update paused due to failure or early termination of task 9p7ith557h8ndf0ui9s0q951b ...snip...
To restart a paused update run
docker service update <SERVICE-ID>
. For example:$ docker service update redis
To avoid repeating certain update failures, you may need to reconfigure the service by passing flags to
docker service update
.Run
docker service ps <SERVICE-ID>
to watch the rolling update:$ docker service ps redis NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR redis.1.dos1zffgeofhagnve8w864fco redis:7.4.1 worker1 Running Running 37 seconds \_ redis.1.88rdo6pa52ki8oqx6dogf04fh redis:7.4.0 worker2 Shutdown Shutdown 56 seconds ago redis.2.9l3i4j85517skba5o7tn5m8g0 redis:7.4.1 worker2 Running Running About a minute \_ redis.2.66k185wilg8ele7ntu8f6nj6i redis:7.4.0 worker1 Shutdown Shutdown 2 minutes ago redis.3.egiuiqpzrdbxks3wxgn8qib1g redis:7.4.1 worker1 Running Running 48 seconds \_ redis.3.ctzktfddb2tepkr45qcmqln04 redis:7.4.0 mmanager1 Shutdown Shutdown 2 minutes ago
Before Swarm updates all of the tasks, you can see that some are running
redis:7.4.0
while others are runningredis:7.4.1
. The output above shows the state once the rolling updates are done.
Next steps
Next, you'll learn how to drain a node in the swarm.",,,
eab7882f4cf3d5f4290ebd501111457b58dc21b474cecb55f03adaa8848c57cc,"General FAQs on SSO
Is Docker SSO available for all paid subscriptions?
Docker single sign-on (SSO) is only available with the Docker Business subscription. Upgrade your existing subscription to start using Docker SSO.
How does Docker SSO work?
Docker SSO lets users authenticate using their identity providers (IdPs) to access Docker. Docker supports Entra ID (formerly Azure AD) and any SAML 2.0 identity providers. When you enable SSO, this redirects users to your provider’s authentication page to authenticate using their email and password.
What SSO flows does Docker support?
Docker supports Service Provider Initiated (SP-initiated) SSO flow. This means users must sign in to Docker Hub or Docker Desktop to initiate the SSO authentication process.
Where can I find detailed instructions on how to configure Docker SSO?
You first need to establish an SSO connection with your identity provider, and the company email domain needs to be verified prior to establishing an SSO connection for your users. For detailed step-by-step instructions on how to configure Docker SSO, see Single Sign-on.
Does Docker SSO support multi-factor authentication (MFA)?
When an organization uses SSO, MFA is determined on the IdP level, not on the Docker platform.
Do I need a specific version of Docker Desktop for SSO?
Yes, all users in your organization must upgrade to Docker Desktop version 4.4.2 or later. Users on older versions of Docker Desktop won't be able to sign in after SSO is enforced if the company domain email is used to sign in or as the primary email associated with an existing Docker account. Your users with existing accounts can't sign in with their username and password.
Can I retain my Docker ID when using SSO?
For a personal Docker ID, a user is the account owner. A Docker ID is associated with access to the user's repositories, images, assets. A user can choose to have a company domain email on the Docker account. When enforcing SSO, the account is connected to the organization account. When enforcing SSO for an organization(s) or company, any user logging in without an existing account using verified company domain email will automatically have an account provisioned, and a new Docker ID created.
Does SAML authentication require additional attributes?
You must provide an email address as an attribute to authenticate through SAML. The ‘Name’ attribute is optional.
Does the application recognize the NameID/Unique Identifier in the SAMLResponse
subject?
The preferred format is your email address, which should also be your Name ID.
Can I use group mapping with SSO and the Azure AD (OIDC) authentication method?
No. Group mapping with SSO isn't supported with the Azure AD (OIDC) authentication method because it requires granting the OIDC app the Directory.Read.All permission, which provides access to all users, groups, and other sensitive data in the directory. Due to potential security risks, Docker doesn't support this configuration. Instead, Docker recommends configuring SCIM to enable group sync securely.
Are there any firewall rules required for SSO configuration?
No. There are no specific firewall rules required for configuring SSO, as long as the domain login.docker.com
is accessible. This domain is commonly accessible by default. However, in rare cases, some organizations may have firewall restrictions in place that block this domain. If you encounter issues during SSO setup, ensure that login.docker.com
is allowed in your network's firewall settings.",,,
971092c50ee6dc9cd7c64f997aba291f376c3fb9c06f67fbde12719c185fbc76,"Manage vulnerability exceptions
Vulnerabilities found in container images sometimes need additional context. Just because an image contains a vulnerable package, it doesn't mean that the vulnerability is exploitable. Exceptions in Docker Scout lets you acknowledge accepted risks or address false positives in image analysis.
By negating non-applicable vulnerabilities, you can make it easier for yourself and downstream consumers of your images to understand the security implications of a vulnerability in the context of an image.
In Docker Scout, exceptions are automatically factored into the results. If an image contains an exception that flags a CVE as non-applicable, then that CVE is excluded from analysis results.
Create exceptions
To create an exception for an image, you can:
- Create an exception in the GUI of Docker Scout Dashboard or Docker Desktop.
- Create a VEX document and attach it to the image.
The recommended way to create exceptions is to use Docker Scout Dashboard or Docker Desktop. The GUI provides a user-friendly interface for creating exceptions. It also lets you create exceptions for multiple images, or your entire organization, all at once.
View exceptions
To view exceptions for images, you need to have the appropriate permissions.
- Exceptions created using the GUI are visible to members of your Docker organization. Unauthenticated users or users who aren't members of your organization cannot see these exceptions.
- Exceptions created using VEX documents are visible to anyone who can pull the image, since the VEX document is stored in the image manifest or on filesystem of the image.
View exceptions in Docker Scout Dashboard or Docker Desktop
The Exceptions tab of the Vulnerabilities page in Docker Scout Dashboard lists all exceptions for for all images in your organization. From here, you can see more details about each exception, the CVEs being suppressed, the images that exceptions apply to, the type of exception and how it was created, and more.
For exceptions created using the GUI, selecting the action menu lets you edit or remove the exception.
To view all exceptions for a specific image tag:
- Go to the Images page.
- Select the tag that you want to inspect.
- Open the Exceptions tab.
- Open the Images view in Docker Desktop.
- Open the Hub tab.
- Select the tag you want to inspect.
- Open the Exceptions tab.
View exceptions in the CLI
Vulnerability exceptions are highlighted in the CLI when you run docker scout cves <image>
. If a CVE is suppressed by an exception, a SUPPRESSED
label
appears next to the CVE ID. Details about the exception are also displayed.
Important
In order to view exceptions in the CLI, you must configure the CLI to use the same Docker organization that you used to create the exceptions.
To configure an organization for the CLI, run:
$ docker scout configure organization <organization>
Replace
<organization>
with the name of your Docker organization.You can also set the organization on a per-command basis by using the
--org
flag:$ docker scout cves --org <organization> <image>
To exclude suppressed CVEs from the output, use the --ignore-suppressed
flag:
$ docker scout cves --ignore-suppressed <image>",,,
4e50c55b1015cd6224203ceb6f4aa94ca7d44882bc0cd0d2e83bae1b72346b97,"Install Docker Desktop on RHEL
Docker Desktop terms
Commercial use of Docker Desktop in larger enterprises (more than 250 employees OR more than $10 million USD in annual revenue) requires a paid subscription.
This page contains information on how to install, launch and upgrade Docker Desktop on a Red Hat Enterprise Linux (RHEL) distribution.
Prerequisites
To install Docker Desktop successfully, you must:
- Meet the general system requirements.
- Have a 64-bit version of either RHEL 8 or RHEL 9.
- Have a Docker account, as authentication is required for Docker Desktop on RHEL.
If you don't have pass
installed, or it can't be installed, you must enable
CodeReady Linux Builder (CRB) repository
and
Extra Packages for Enterprise Linux (EPEL).
$ sudo subscription-manager repos --enable codeready-builder-for-rhel-9-$(arch)-rpms
$ sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm
$ sudo dnf install pass
$ sudo subscription-manager repos --enable codeready-builder-for-rhel-8-$(arch)-rpms
$ sudo dnf install https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm
$ sudo dnf install pass
Additionally, for a GNOME desktop environment you must install AppIndicator and KStatusNotifierItem GNOME extensions. You must also enable EPEL.
$ # enable EPEL as described above
$ sudo dnf install gnome-shell-extension-appindicator
$ sudo gnome-extensions enable appindicatorsupport@rgcjonas.gmail.com
$ # enable EPEL as described above
$ sudo dnf install gnome-shell-extension-appindicator
$ sudo dnf install gnome-shell-extension-desktop-icons
$ sudo gnome-shell-extension-tool -e appindicatorsupport@rgcjonas.gmail.com
For non-GNOME desktop environments, gnome-terminal
must be installed:
$ sudo dnf install gnome-terminal
Install Docker Desktop
To install Docker Desktop on RHEL:
Set up Docker's package repository as follows:
$ sudo dnf config-manager --add-repo https://download.docker.com/linux/rhel/docker-ce.repo
Download the latest RPM package.
Install the package with dnf as follows:
$ sudo dnf install ./docker-desktop-x86_64-rhel.rpm
There are a few post-install configuration steps done through the post-install script contained in the RPM package.
The post-install script:
- Sets the capability on the Docker Desktop binary to map privileged ports and set resource limits.
- Adds a DNS name for Kubernetes to
/etc/hosts
. - Creates a symlink from
/usr/local/bin/com.docker.cli
to/usr/bin/docker
. This is because the classic Docker CLI is installed at/usr/bin/docker
. The Docker Desktop installer also installs a Docker CLI binary that includes cloud-integration capabilities and is essentially a wrapper for the Compose CLI, at/usr/local/bin/com.docker.cli
. The symlink ensures that the wrapper can access the classic Docker CLI. - Creates a symlink from
/usr/libexec/qemu-kvm
to/usr/local/bin/qemu-system-x86_64
.
Launch Docker Desktop
To start Docker Desktop for Linux:
Navigate to the Docker Desktop application in your Gnome/KDE Desktop.
Select Docker Desktop to start Docker.
The Docker Subscription Service Agreement displays.
Select Accept to continue. Docker Desktop starts after you accept the terms.
Note that Docker Desktop won't run if you do not agree to the terms. You can choose to accept the terms at a later date by opening Docker Desktop.
For more information, see Docker Desktop Subscription Service Agreement. It is recommended that you also read the FAQs.
Alternatively, open a terminal and run:
$ systemctl --user start docker-desktop
When Docker Desktop starts, it creates a dedicated context that the Docker CLI can use as a target and sets it as the current context in use. This is to avoid a clash with a local Docker Engine that may be running on the Linux host and using the default context. On shutdown, Docker Desktop resets the current context to the previous one.
The Docker Desktop installer updates Docker Compose and the Docker CLI binaries
on the host. It installs Docker Compose V2 and gives users the choice to
link it as docker-compose from the Settings panel. Docker Desktop installs
the new Docker CLI binary that includes cloud-integration capabilities in /usr/local/bin/com.docker.cli
and creates a symlink to the classic Docker CLI at /usr/local/bin
.
After you’ve successfully installed Docker Desktop, you can check the versions of these binaries by running the following commands:
$ docker compose version
Docker Compose version v2.29.1
$ docker --version
Docker version 27.1.1, build 6312585
$ docker version
Client:
Version: 23.0.5
API version: 1.42
Go version: go1.21.12
<...>
To enable Docker Desktop to start on sign in, from the Docker menu, select Settings > General > Start Docker Desktop when you sign in to your computer.
Alternatively, open a terminal and run:
$ systemctl --user enable docker-desktop
To stop Docker Desktop, select the Docker menu icon to open the Docker menu and select Quit Docker Desktop.
Alternatively, open a terminal and run:
$ systemctl --user stop docker-desktop
Important
After launching Docker Desktop for RHEL, you must sign in to your Docker account to start using Docker Desktop.
Tip
To attach Red Hat subscription data to containers, see Red Hat verified solution.
For example:
$ docker run --rm -it -v ""/etc/pki/entitlement:/etc/pki/entitlement"" -v ""/etc/rhsm:/etc/rhsm-host"" -v ""/etc/yum.repos.d/redhat.repo:/etc/yum.repos.d/redhat.repo"" registry.access.redhat.com/ubi9
Upgrade Docker Desktop
Once a new version for Docker Desktop is released, the Docker UI shows a notification. You need to first remove the previous version and then download the new package each time you want to upgrade Docker Desktop. Run:
$ sudo dnf remove docker-desktop
$ sudo dnf install ./docker-desktop-<arch>-rhel.rpm
Next steps
- Explore Docker's subscriptions to see what Docker can offer you.
- Take a look at the Docker workshop to learn how to build an image and run it as a containerized application.
- Explore Docker Desktop and all its features.
- Troubleshooting describes common problems, workarounds, how to run and submit diagnostics, and submit issues.
- FAQs provide answers to frequently asked questions.
- Release notes lists component updates, new features, and improvements associated with Docker Desktop releases.
- Back up and restore data provides instructions on backing up and restoring data related to Docker.",,,
b9a37444dda98334ca7243d31b1dadc5b309351359297231039ac355dd695c19,"Recover your Docker account
If you have lost access to both your two-factor authentication application and your recovery code:
- Sign in to your Docker account with your username and password.
- Select I've lost my authentication device and I've lost my recovery code.
- Complete the Contact Support form. You must enter the primary email address associated with your Docker ID in the Contact Support form for recovery instructions.",,,
d51e0c413c84a75f883d2856e39db2f09b60794005b13a14b3837574beb5ae57,"Trusted content
Docker Hub's trusted content provides a curated selection of high-quality, secure images designed to give developers confidence in the reliability and security of the resources they use. These images are stable, regularly updated, and adhere to industry best practices, making them a strong foundation for building and deploying applications. Docker Hub's trusted content includes, Docker Official Images, Verified Publisher images, and Docker-Sponsored Open Source Software images.
Docker Official Images
The Docker Official Images are a curated set of Docker repositories hosted on Docker Hub.
Docker recommends you use the Docker Official Images in your projects. These
images have clear documentation, promote best practices, and are regularly
updated. Docker Official Images support most common use cases, making them
perfect for new Docker users. Advanced users can benefit from more specialized
image variants as well as review Docker Official Images as part of your
Dockerfile
learning process.
Note
Use of Docker Official Images is subject to Docker's Terms of Service.
These images provide essential base repositories that serve as the starting point for the majority of users.
These include operating systems such as Ubuntu and Alpine, programming language runtimes such as Python and Node, and other essential tools such as memcached and MySQL.
The images are some of the most secure images on Docker Hub. This is particularly important as Docker Official Images are some of the most popular on Docker Hub. Typically, Docker Official images have few or no packages containing CVEs.
The images exemplify Dockerfile best practices and provide clear documentation to serve as a reference for other Dockerfile authors.
Images that are part of this program have a special badge on Docker Hub making it easier for you to identify projects that are part of Docker Official Images.
Supported tags and respective Dockerfile links
The repository description for each Docker Official Image contains a Supported tags and respective Dockerfile links section that lists all the current tags with links to the Dockerfiles that created the image with those tags. The purpose of this section is to show what image variants are available.
Tags listed on the same line all refer to the same underlying image. Multiple
tags can point to the same image. For example, in the previous screenshot taken
from the ubuntu
Docker Official Images repository, the tags 24.04
,
noble-20240225
, noble
, and devel
all refer to the same image.
The latest
tag for a Docker Official Image is often optimized for ease of use
and includes a wide variety of useful software, such as developer and build tools.
By tagging an image as latest
, the image maintainers are essentially suggesting
that image be used as the default. In other words, if you do not know what tag to
use or are unfamiliar with the underlying software, you should probably start with
the latest
image. As your understanding of the software and image variants advances,
you may find other image variants better suit your needs.
Slim images
A number of language stacks such as
Node.js,
Python, and
Ruby have slim
tag variants
designed to provide a lightweight, production-ready base image
with fewer packages.
A typical consumption pattern for slim
images is as the base image for the final stage of a
multi-staged build.
For example, you build your application in the first stage of the build
using the latest
variant and then copy your application into the final
stage based upon the slim
variant. Here is an example Dockerfile
.
FROM node:latest AS build
WORKDIR /app
COPY package.json package-lock.json ./
RUN npm ci
COPY . ./
FROM node:slim
WORKDIR /app
COPY --from=build /app /app
CMD [""node"", ""app.js""]
Alpine images
Many Docker Official Images repositories also offer alpine
variants. These
images are built on top of the
Alpine Linux
distribution rather than Debian or Ubuntu. Alpine Linux is focused on providing
a small, simple, and secure base for container images, and Docker Official
Images alpine
variants typically aim to install only necessary packages. As a
result, Docker Official Images alpine
variants are typically even smaller
than slim
variants.
The main caveat to note is that Alpine Linux uses musl libc instead of glibc. Additionally, to minimize image size, it's uncommon for Alpine-based images to include tools such as Git or Bash by default. Depending on the depth of libc requirements or assumptions in your programs, you may find yourself running into issues due to missing libraries or tools.
When you use Alpine images as a base, consider the following options in order to make your program compatible with Alpine Linux and musl:
- Compile your program against musl libc
- Statically link glibc libraries into your program
- Avoid C dependencies altogether (for example, build Go programs without CGO)
- Add the software you need yourself in your Dockerfile.
Refer to the alpine
image
description on
Docker Hub for examples on how to install packages if you are unfamiliar.
Codenames
Tags with words that look like Toy Story characters (for example, bookworm
,
bullseye
, and trixie
) or adjectives (such as focal
, jammy
, and
noble
), indicate the codename of the Linux distribution they use as a base
image. Debian release codenames are
based on Toy Story characters,
and Ubuntu's take the form of ""Adjective Animal"". For example, the
codename for Ubuntu 24.04 is ""Noble Numbat"".
Linux distribution indicators are helpful because many Docker Official Images
provide variants built upon multiple underlying distribution versions (for
example, postgres:bookworm
and postgres:bullseye
).
Other tags
Docker Official Images tags may contain other hints to the purpose of their image variant in addition to those described here. Often these tag variants are explained in the Docker Official Images repository documentation. Reading through the ""How to use this image"" and ""Image Variants"" sections will help you to understand how to use these variants.
Verified Publisher images
The Docker Verified Publisher program provides high-quality images from commercial publishers verified by Docker.
These images help development teams build secure software supply chains, minimizing exposure to malicious content early in the process to save time and money later.
Images that are part of this program have a special badge on Docker Hub making it easier for users to identify projects that Docker has verified as high-quality commercial publishers.
Docker-Sponsored Open Source Software images
The Docker-Sponsored Open Source Software (OSS) program provides images that are published and maintained by open-source projects sponsored by Docker.
Images that are part of this program have a special badge on Docker Hub making it easier for users to identify projects that Docker has verified as trusted, secure, and active open-source projects.",,,
d2f984cf398708d549c92f7dc3747974212980b202564bc88ab2772766cb307a,"Install Docker Scout
The Docker Scout CLI plugin comes pre-installed with Docker Desktop.
If you run Docker Engine without Docker Desktop, Docker Scout doesn't come pre-installed, but you can install it as a standalone binary.
Installation script
To install the latest version of the plugin, run the following commands:
$ curl -fsSL https://raw.githubusercontent.com/docker/scout-cli/main/install.sh -o install-scout.sh
$ sh install-scout.sh
Note
Always examine scripts downloaded from the internet before running them locally. Before installing, make yourself familiar with potential risks and limitations of the convenience script.
Manual installation
Download the latest release from the releases page.
Create a subdirectory under
$HOME/.docker
calledscout
.$ mkdir -p $HOME/.docker/scout
Extract the archive and move the
docker-scout
binary to the$HOME/.docker/scout
directory.Make the binary executable:
chmod +x $HOME/.docker/scout/docker-scout
.Add the
scout
subdirectory to your.docker/config.json
as a plugin directory:{ ""cliPluginsExtraDirs"": [ ""/home/<USER>/.docker/scout"" ] }
Substitute
<USER>
with your username on the system.Note
The path for
cliPluginsExtraDirs
must be an absolute path.
Download the latest release from the releases page.
Create a subdirectory under
$HOME/.docker
calledscout
.$ mkdir -p $HOME/.docker/scout
Extract the archive and move the
docker-scout
binary to the$HOME/.docker/scout
directory.Make the binary executable:
$ chmod +x $HOME/.docker/scout/docker-scout
Authorize the binary to be executable on macOS:
xattr -d com.apple.quarantine $HOME/.docker/scout/docker-scout.
Add the
scout
subdirectory to your.docker/config.json
as a plugin directory:{ ""cliPluginsExtraDirs"": [ ""/Users/<USER>/.docker/scout"" ] }
Substitute
<USER>
with your username on the system.Note
The path for
cliPluginsExtraDirs
must be an absolute path.
Download the latest release from the releases page.
Create a subdirectory under
%USERPROFILE%/.docker
calledscout
.% mkdir %USERPROFILE%\.docker\scout
Extract the archive and move the
docker-scout.exe
binary to the%USERPROFILE%\.docker\scout
directory.Add the
scout
subdirectory to your.docker\config.json
as a plugin directory:{ ""cliPluginsExtraDirs"": [ ""C:\Users\<USER>\.docker\scout"" ] }
Substitute
<USER>
with your username on the system.Note
The path for
cliPluginsExtraDirs
must be an absolute path.
Container image
The Docker Scout CLI plugin is also available as a
container image.
Use the docker/scout-cli
to run docker scout
commands without installing the CLI plugin on your host.
$ docker run -it \
-e DOCKER_SCOUT_HUB_USER=<your Docker Hub user name> \
-e DOCKER_SCOUT_HUB_PASSWORD=<your Docker Hub PAT> \
docker/scout-cli <command>
GitHub Action
The Docker Scout CLI plugin is also available as a GitHub action. You can use it in your GitHub workflows to automatically analyze images and evaluate policy compliance with each push.
Docker Scout also integrates with many more CI/CD tools, such as Jenkins, GitLab, and Azure DevOps. Learn more about the integrations available for Docker Scout.",,,
1d75c7d76f39407fc0d32a9ab9c2a67f5325572ed31137336e163a58045a26b4,"FAQs on Docker Desktop releases
How frequent will new releases be?
New releases are available roughly every month, unless there are critical fixes that need to be released sooner.
The Automatically check for updates setting in the Software updates tab is turned on by default. This means you receive notifications in the Docker menu and a notification badge on the Docker Desktop Dashboard when a new version is available.
You can also let Docker Desktop automatically download new updates in the background by selecting the Always download updates checkbox.
Sometimes new versions are rolled out gradually over a few days. Therefore, if you wait, it will turn up soon. Alternatively, you can select Check for updates in the Docker menu to get the latest version immediately.
How do I ensure that all users in my organization are using the same version?
This is managed through your IT administrator's endpoint management software.",,,
39e3d4138c94fe29f12cd677cc82192a2f643f7abd4f0e76f16630966184a912,"Organization administration overview
An organization in Docker is a collection of teams and repositories that can be managed together. A team is a group of Docker members that belong to an organization. An organization can have multiple teams. Members don't have to be added to a team to be part of an organization.
Docker users become members of an organization once they're associated with that organization by an organization owner. An organization owner is a user with administrative access to the organization.
Owners can invite users, assign them roles, create new teams, and add members to an existing team using their Docker ID or email address. An organization owner can also add additional owners to help them manage users, teams, and repositories in the organization.
The following diagram depicts the setup of an organization and how it relates to teams. Teams are an optional feature that owners can use to group members and assign permissions.
To create an organization, see Create your organization.
Learn how to administer an organization in the following sections.",,,
9da8c6ce7833f1a5b28d0c0cd175a28236fc085b6d2a78eba32160c080af2467,"Image details view
The image details view shows a breakdown of the Docker Scout analysis. You can access the image view from the Docker Scout Dashboard, the Docker Desktop Images view, and from the image tag page on Docker Hub. The image details show a breakdown of the image hierarchy (base images), image layers, packages, and vulnerabilities.
Docker Desktop first analyzes images locally, where it generates a software bill of materials (SBOM). Docker Desktop, Docker Hub, and the Docker Scout Dashboard and CLI all use the package URL (PURL) links in this SBOM to query for matching Common Vulnerabilities and Exposures (CVEs) in Docker Scout's advisory database.
Image hierarchy
The image you inspect may have one or more base images represented under Image hierarchy. This means the author of the image used other images as starting points when building the image. Often these base images are either operating system images such as Debian, Ubuntu, and Alpine, or programming language images such as PHP, Python, and Java.
Selecting each image in the chain lets you see which layers originate from each base image. Selecting the ALL row selects all layers and base images.
One or more of the base images may have updates available, which may include updated security patches that remove vulnerabilities from your image. Any base images with available updates are noted to the right of Image hierarchy.
Layers
A Docker image consists of layers. Image layers are listed from top to bottom, with the earliest layer at the top and the most recent layer at the bottom. Often, the layers at the top of the list originate from a base image, and the layers towards the bottom added by the image author, often using commands in a Dockerfile. Selecting a base image under Image hierarchy highlights with layers originate from a base image.
Selecting individual or multiple layers filters the packages and vulnerabilities on the right-hand side to show what the selected layers added.
Vulnerabilities
The Vulnerabilities tab displays a list of vulnerabilities and exploits detected in the image. The list is grouped by package, and sorted in order of severity.
You can find further information on the vulnerability or exploit, including if a fix is available, by expanding the list item.
Remediation recommendations
When you inspect an image in Docker Desktop or Docker Hub, Docker Scout can provide recommendations for improving the security of that image.
Recommendations in Docker Desktop
To view security recommendations for an image in Docker Desktop:
- Go to the Images view in Docker Desktop.
- Select the image tag that you want to view recommendations for.
- Near the top, select the Recommended fixes drop-down button.
The drop-down menu lets you choose whether you want to see recommendations for the current image or any base images used to build it:
- Recommendations for this image provides recommendations for the current image that you're inspecting.
- Recommendations for base image provides recommendations for base images used to build the image.
If the image you're viewing has no associated base images, the drop-down menu only shows the option to view recommendations for the current image.
Recommendations in Docker Hub
To view security recommendations for an image in Docker Hub:
Go to the repository page for an image where you have activated Docker Scout image analysis.
Open the Tags tab.
Select the tag that you want to view recommendations for.
Select the View recommended base image fixes button.
This opens a window which gives you recommendations for you can improve the security of your image by using better base images. See Recommendations for base image for more details.
Recommendations for current image
The recommendations for the current image view helps you determine whether the image version that you're using is out of date. If the tag you're using is referencing an old digest, the view shows a recommendation to update the tag by pulling the latest version.
Select the Pull new image button to get the updated version. Check the checkbox to remove the old version after pulling the latest.
Recommendations for base image
The base image recommendations view contains two tabs for toggling between different types of recommendations:
- Refresh base image
- Change base image
These base image recommendations are only actionable if you're the author of the image you're inspecting. This is because changing the base image for an image requires you to update the Dockerfile and re-build the image.
Refresh base image
This tab shows if the selected base image tag is the latest available version, or if it's outdated.
If the base image tag used to build the current image isn't the latest, then the delta between the two versions shows in this window. The delta information includes:
- The tag name, and aliases, of the recommended (newer) version
- The age of the current base image version
- The age of the latest available version
- The number of CVEs affecting each version
At the bottom of the window, you also receive command snippets that you can run to re-build the image using the latest version.
Change base image
This tab shows different alternative tags that you can use, and outlines the benefits and disadvantages of each tag version. Selecting the base image shows recommended options for that tag.
For example, if the image you're inspecting is using an old version of debian
as a base image, it shows recommendations for newer and more secure versions
of debian
to use. By providing more than one alternative to choose from, you
can see for yourself how the options compare with each other, and decide which
one to use.
Select a tag recommendation to see further details of the recommendation. It shows the benefits and potential disadvantages of the tag, why it's a recommended, and how to update your Dockerfile to use this version.",,,
b770673e17f1154d0d76d17e2c50054abaf76e30bd7b492ae0b1740c2f209d5e,"None network driver
Table of contents
If you want to completely isolate the networking stack of a container, you can
use the --network none
flag when starting the container. Within the container,
only the loopback device is created.
The following example shows the output of ip link show
in an alpine
container using the none
network driver.
$ docker run --rm --network none alpine:latest ip link show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
No IPv6 loopback address is configured for containers using the none
driver.
$ docker run --rm --network none --name no-net-alpine alpine:latest ip addr show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
Next steps
- Go through the host networking tutorial
- Learn about networking from the container's point of view
- Learn about bridge networks
- Learn about overlay networks
- Learn about Macvlan networks",,,
f144a6d9951e86ae704e42f3a438e6fd4207fbc818fb7f0bd473c1312617143c,"Local registry with GitHub Actions
For testing purposes you may need to create a local registry to push images into:
name: ci
on:
push:
jobs:
docker:
runs-on: ubuntu-latest
services:
registry:
image: registry:2
ports:
- 5000:5000
steps:
- name: Set up QEMU
uses: docker/setup-qemu-action@v3
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
with:
driver-opts: network=host
- name: Build and push to local registry
uses: docker/build-push-action@v6
with:
push: true
tags: localhost:5000/name/app:latest
- name: Inspect
run: |
docker buildx imagetools inspect localhost:5000/name/app:latest",,,
56bf6922428af88f95f66b7c37090e036a42a49890a2f2000e2ac948a18a691c,"Docker contexts
Introduction
This guide shows how you can use contexts to manage Docker daemons from a single client.
Each context contains all information required to manage resources on the daemon.
The docker context
command makes it easy to configure these contexts and switch between them.
As an example, a single Docker client might be configured with two contexts:
- A default context running locally
- A remote, shared context
Once these contexts are configured,
you can use the docker context use <context-name>
command
to switch between them.
Prerequisites
To follow the examples in this guide, you'll need:
- A Docker client that supports the top-level
context
command
Run docker context
to verify that your Docker client supports contexts.
The anatomy of a context
A context is a combination of several properties. These include:
- Name and description
- Endpoint configuration
- TLS info
To list available contexts, use the docker context ls
command.
$ docker context ls
NAME DESCRIPTION DOCKER ENDPOINT ERROR
default * unix:///var/run/docker.sock
This shows a single context called ""default"".
It's configured to talk to a daemon through the local /var/run/docker.sock
Unix socket.
The asterisk in the NAME
column indicates that this is the active context.
This means all docker
commands run against this context,
unless overridden with environment variables such as DOCKER_HOST
and DOCKER_CONTEXT
,
or on the command-line with the --context
and --host
flags.
Dig a bit deeper with docker context inspect
.
The following example shows how to inspect the context called default
.
$ docker context inspect default
[
{
""Name"": ""default"",
""Metadata"": {},
""Endpoints"": {
""docker"": {
""Host"": ""unix:///var/run/docker.sock"",
""SkipTLSVerify"": false
}
},
""TLSMaterial"": {},
""Storage"": {
""MetadataPath"": ""\u003cIN MEMORY\u003e"",
""TLSPath"": ""\u003cIN MEMORY\u003e""
}
}
]
Create a new context
You can create new contexts with the docker context create
command.
The following example creates a new context called docker-test
and specifies
the host endpoint of the context to TCP socket tcp://docker:2375
.
$ docker context create docker-test --docker host=tcp://docker:2375
docker-test
Successfully created context ""docker-test""
The new context is stored in a meta.json
file below ~/.docker/contexts/
.
Each new context you create gets its own meta.json
stored in a dedicated sub-directory of ~/.docker/contexts/
.
You can view the new context with docker context ls
and docker context inspect <context-name>
.
$ docker context ls
NAME DESCRIPTION DOCKER ENDPOINT ERROR
default * unix:///var/run/docker.sock
docker-test tcp://docker:2375
The current context is indicated with an asterisk (""*"").
Use a different context
You can use docker context use
to switch between contexts.
The following command will switch the docker
CLI to use the docker-test
context.
$ docker context use docker-test
docker-test
Current context is now ""docker-test""
Verify the operation by listing all contexts and ensuring the asterisk (""*"") is against the docker-test
context.
$ docker context ls
NAME DESCRIPTION DOCKER ENDPOINT ERROR
default unix:///var/run/docker.sock
docker-test * tcp://docker:2375
docker
commands will now target endpoints defined in the docker-test
context.
You can also set the current context using the DOCKER_CONTEXT
environment variable.
The environment variable overrides the context set with docker context use
.
Use the appropriate command below to set the context to docker-test
using an environment variable.
> $env:DOCKER_CONTEXT='docker-test'
$ export DOCKER_CONTEXT=docker-test
Run docker context ls
to verify that the docker-test
context is now the
active context.
You can also use the global --context
flag to override the context.
The following command uses a context called production
.
$ docker --context production container ls
Exporting and importing Docker contexts
You can use the docker context export
and docker context import
commands
to export and import contexts on different hosts.
The docker context export
command exports an existing context to a file.
The file can be imported on any host that has the docker
client installed.
Exporting and importing a context
The following example exports an existing context called docker-test
.
It will be written to a file called docker-test.dockercontext
.
$ docker context export docker-test
Written file ""docker-test.dockercontext""
Check the contents of the export file.
$ cat docker-test.dockercontext
Import this file on another host using docker context import
to create context with the same configuration.
$ docker context import docker-test docker-test.dockercontext
docker-test
Successfully imported context ""docker-test""
You can verify that the context was imported with docker context ls
.
The format of the import command is docker context import <context-name> <context-file>
.
Updating a context
You can use docker context update
to update fields in an existing context.
The following example updates the description field in the existing docker-test
context.
$ docker context update docker-test --description ""Test context""
docker-test
Successfully updated context ""docker-test""",,,
0660f3bad33054c767573711ff9540f9aaa6b9ae4cd3256a302bf5ba701e6137,"Manage company owners
Table of contents
Subscription:
Business
For:
Administrators
A company can have multiple owners. Company owners have company-wide observability and can manage company-wide settings that apply to all associated organizations. In addition, company owners have the same access as organization owners for all associated organizations. Unlike organization owners, company owners don't need to be member of an organization. When company owners aren't a member in an organization, they don't occupy a seat.
Add a company owner
- Sign in to the Admin Console.
- Select your company on the Choose profile page.
- Select Company owners.
- Select Add owner.
- Specify the user's Docker ID to search for the user.
- After you find the user, select Add company owner.
Remove a company owner
- Sign in to the Admin Console.
- Select your company on the Choose profile page.
- Select Company owners.
- Select the Action icon in the row of the company owner that your want to remove.
- Select Remove as company owner.",,,
f473f59fd3bcf73fb00beab9bf3608799a05b2b3dcec3bd8bf57b3d07f0ce6fb,"Google Cloud Logging driver
The Google Cloud Logging driver sends container logs to Google Cloud Logging Logging.
Usage
To use the gcplogs
driver as the default logging driver, set the log-driver
and log-opt
keys to appropriate values in the daemon.json
file, which is
located in /etc/docker/
on Linux hosts or
C:\ProgramData\docker\config\daemon.json
on Windows Server. For more about
configuring Docker using daemon.json
, see
daemon.json.
The following example sets the log driver to gcplogs
and sets the
gcp-meta-name
option.
{
""log-driver"": ""gcplogs"",
""log-opts"": {
""gcp-meta-name"": ""example-instance-12345""
}
}
Restart Docker for the changes to take effect.
You can set the logging driver for a specific container by using the
--log-driver
option to docker run
:
$ docker run --log-driver=gcplogs ...
If Docker detects that it's running in a Google Cloud Project, it discovers
configuration from the
instance metadata service.
Otherwise, the user must specify
which project to log to using the --gcp-project
log option and Docker
attempts to obtain credentials from the
Google Application Default Credential.
The --gcp-project
flag takes precedence over information discovered from the
metadata server, so a Docker daemon running in a Google Cloud project can be
overridden to log to a different project using --gcp-project
.
Docker fetches the values for zone, instance name and instance ID from Google Cloud metadata server. Those values can be provided via options if metadata server isn't available. They don't override the values from metadata server.
gcplogs options
You can use the --log-opt NAME=VALUE
flag to specify these additional Google
Cloud Logging driver options:
| Option | Required | Description |
|---|---|---|
gcp-project | optional | Which Google Cloud project to log to. Defaults to discovering this value from the Google Cloud metadata server. |
gcp-log-cmd | optional | Whether to log the command that the container was started with. Defaults to false. |
labels | optional | Comma-separated list of keys of labels, which should be included in message, if these labels are specified for the container. |
labels-regex | optional | Similar to and compatible with labels . A regular expression to match logging-related labels. Used for advanced
log tag options. |
env | optional | Comma-separated list of keys of environment variables, which should be included in message, if these variables are specified for the container. |
env-regex | optional | Similar to and compatible with env . A regular expression to match logging-related environment variables. Used for advanced
log tag options. |
gcp-meta-zone | optional | Zone name for the instance. |
gcp-meta-name | optional | Instance name. |
gcp-meta-id | optional | Instance ID. |
If there is collision between label
and env
keys, the value of the env
takes precedence. Both options add additional fields to the attributes of a
logging message.
The following is an example of the logging options required to log to the default logging destination which is discovered by querying the Google Cloud metadata server.
$ docker run \
--log-driver=gcplogs \
--log-opt labels=location \
--log-opt env=TEST \
--log-opt gcp-log-cmd=true \
--env ""TEST=false"" \
--label location=west \
your/application
This configuration also directs the driver to include in the payload the label
location
, the environment variable ENV
, and the command used to start the
container.
The following example shows logging options for running outside of Google
Cloud. The GOOGLE_APPLICATION_CREDENTIALS
environment variable must be set
for the daemon, for example via systemd:
[Service]
Environment=""GOOGLE_APPLICATION_CREDENTIALS=uQWVCPkMTI34bpssr1HI""
$ docker run \
--log-driver=gcplogs \
--log-opt gcp-project=test-project \
--log-opt gcp-meta-zone=west1 \
--log-opt gcp-meta-name=`hostname` \
your/application",,,
fe18e2cf1f6851f30601a03dd459e6bbdb4e5b53f2aa574edefc6a37a3468800,"Docker Desktop's Resource Saver mode
Resource Saver is a new feature available in Docker Desktop version 4.24 and later. It significantly reduces Docker Desktop's CPU and memory utilization on the host by 2 GBs or more, by automatically stopping the Docker Desktop Linux VM when no containers are running for a period of time. The default time is set to 5 minutes, but this can be adjusted to suit your needs.
With Resource Saver mode, Docker Desktop uses minimal system resources when it's idle, thereby allowing you to save battery life on your laptop and improve your multi-tasking experience.
How to configure Resource Saver
Resource Saver is enabled by default but can be disabled by navigating to the Resources tab, in Settings. You can also configure the idle timer as shown below.
If the values available aren't sufficient for your
needs, you can reconfigure it to any value, as long as the value is larger than 30 seconds, by
changing autoPauseTimeoutSeconds
in the Docker Desktop settings-store.json
file (or settings.json
for Docker Desktop versions 4.34 and earlier):
- Mac:
~/Library/Group Containers/group.com.docker/settings-store.json
- Windows:
C:\Users\[USERNAME]\AppData\Roaming\Docker\settings-store.json
- Linux:
~/.docker/desktop/settings-store.json
There's no need to restart Docker Desktop after reconfiguring.
When Docker Desktop enters Resource Saver mode:
A leaf icon displays on the Docker Desktop status bar as well as on the Docker icon in the system tray. The following image shows the Linux VM CPU and memory utilization reduced to zero when Resource Saver mode is on.
Docker commands that don't run containers, for example listing container images or volumes, don't necessarily trigger an exit from Resource Saver mode as Docker Desktop can serve such commands without unnecessarily waking up the Linux VM.
Note
Docker Desktop exits the Resource Saver mode automatically when it needs to. Commands that cause an exit from Resource Saver take a little longer to execute (about 3 to 10 seconds) as Docker Desktop restarts the Linux VM. It's generally faster on Mac and Linux, and slower on Windows with Hyper-V. Once the Linux VM is restarted, subsequent container runs occur immediately as usual.
Resource Saver mode versus Pause
Resource Saver has higher precedence than the older Pause feature, meaning that while Docker Desktop is in Resource Saver mode, manually pausing Docker Desktop is not possible (nor does it make sense since Resource Saver actually stops the Docker Desktop Linux VM). In general, we recommend keeping Resource Saver enabled as opposed to disabling it and using the manual Pause feature, as it results in much better CPU and memory savings.
Resource Saver mode on Windows
Resource Saver works a bit differently on Windows with WSL. Instead of
stopping the WSL VM, it only pauses the Docker Engine inside the
docker-desktop
WSL distribution. That's because in WSL there's a single Linux VM
shared by all WSL distributions, so Docker Desktop can't stop the Linux VM (i.e.,
the WSL Linux VM is not owned by Docker Desktop). As a result, Resource Saver
reduces CPU utilization on WSL, but it does not reduce Docker's memory
utilization.
To reduce memory utilization on WSL, we instead recommend that
users enable WSL's autoMemoryReclaim
feature as described in the
Docker Desktop WSL docs. Finally, since Docker Desktop does not
stop the Linux VM on WSL, exit from Resource Saver mode is immediate (there's
no exit delay).
Feedback
To give feedback or report any bugs you may find, create an issue on the appropriate Docker Desktop GitHub repository:",,,
9063ef5f0bf189c63068d5d90f6f02a354e3a0c57c8069eac2123a1cd5488b43,"Variables in Bake
You can define and use variables in a Bake file to set attribute values, interpolate them into other values, and perform arithmetic operations. Variables can be defined with default values, and can be overridden with environment variables.
Using variables as attribute values
Use the variable
block to define a variable.
variable ""TAG"" {
default = ""docker.io/username/webapp:latest""
}
The following example shows how to use the TAG
variable in a target.
target ""webapp"" {
context = "".""
dockerfile = ""Dockerfile""
tags = [ TAG ]
}
Interpolate variables into values
Bake supports string interpolation of variables into values. You can use the
${}
syntax to interpolate a variable into a value. The following example
defines a TAG
variable with a value of latest
.
variable ""TAG"" {
default = ""latest""
}
To interpolate the TAG
variable into the value of an attribute, use the
${TAG}
syntax.
group ""default"" {
targets = [ ""webapp"" ]
}
variable ""TAG"" {
default = ""latest""
}
target ""webapp"" {
context = "".""
dockerfile = ""Dockerfile""
tags = [""docker.io/username/webapp:${TAG}""]
}
Printing the Bake file with the --print
flag shows the interpolated value in
the resolved build configuration.
$ docker buildx bake --print
{
""group"": {
""default"": {
""targets"": [""webapp""]
}
},
""target"": {
""webapp"": {
""context"": ""."",
""dockerfile"": ""Dockerfile"",
""tags"": [""docker.io/username/webapp:latest""]
}
}
}
Validating variables
To verify that the value of a variable conforms to an expected type, value
range, or other condition, you can define custom validation rules using the
validation
block.
In the following example, validation is used to enforce a numeric constraint on
a variable value; the PORT
variable must be 1024 or higher.
# Define a variable `PORT` with a default value and a validation rule
variable ""PORT"" {
default = 3000 # Default value assigned to `PORT`
# Validation block to ensure `PORT` is a valid number within the acceptable range
validation {
condition = PORT >= 1024 # Ensure `PORT` is at least 1024
error_message = ""The variable 'PORT' must be 1024 or higher."" # Error message for invalid values
}
}
If the condition
expression evaluates to false
, the variable value is
considered invalid, whereby the build invocation fails and error_message
is
emitted. For example, if PORT=443
, the condition evaluates to false
, and
the error is raised.
Values are coerced into the expected type before the validation is set. This ensures that any overrides set with environment variables work as expected.
Validate multiple conditions
To evaluate more than one condition, define multiple validation
blocks for
the variable. All conditions must be true
.
Here’s an example:
# Define a variable `VAR` with multiple validation rules
variable ""VAR"" {
# First validation block: Ensure the variable is not empty
validation {
condition = VAR != """"
error_message = ""The variable 'VAR' must not be empty.""
}
# Second validation block: Ensure the value contains only alphanumeric characters
validation {
# VAR and the regex match must be identical:
condition = VAR == regex(""[a-zA-Z0-9]+"", VAR)
error_message = ""The variable 'VAR' can only contain letters and numbers.""
}
}
This example enforces:
- The variable must not be empty.
- The variable must match a specific character set.
For invalid inputs like VAR=""hello@world""
, the validation would fail.
Validating variable dependencies
You can reference other Bake variables in your condition expression, enabling validations that enforce dependencies between variables. This ensures that dependent variables are set correctly before proceeding.
Here’s an example:
# Define a variable `FOO`
variable ""FOO"" {}
# Define a variable `BAR` with a validation rule that references `FOO`
variable ""BAR"" {
# Validation block to ensure `FOO` is set if `BAR` is used
validation {
condition = FOO != """" # Check if `FOO` is not an empty string
error_message = ""The variable 'BAR' requires 'FOO' to be set.""
}
}
This configuration ensures that the BAR
variable can only be used if FOO
has been assigned a non-empty value. Attempting to build without setting FOO
will trigger the validation error.
Escape variable interpolation
If you want to bypass variable interpolation when parsing the Bake definition,
use double dollar signs ($${VARIABLE}
).
target ""webapp"" {
dockerfile-inline = <<EOF
FROM alpine
ARG TARGETARCH
RUN echo ""Building for $${TARGETARCH/amd64/x64}""
EOF
platforms = [""linux/amd64"", ""linux/arm64""]
}
$ docker buildx bake --progress=plain
...
#8 [linux/arm64 2/2] RUN echo ""Building for arm64""
#8 0.036 Building for arm64
#8 DONE 0.0s
#9 [linux/amd64 2/2] RUN echo ""Building for x64""
#9 0.046 Building for x64
#9 DONE 0.1s
...
Using variables in variables across files
When multiple files are specified, one file can use variables defined in
another file. In the following example, the vars.hcl
file defines a
BASE_IMAGE
variable with a default value of docker.io/library/alpine
.
variable ""BASE_IMAGE"" {
default = ""docker.io/library/alpine""
}
The following docker-bake.hcl
file defines a BASE_LATEST
variable that
references the BASE_IMAGE
variable.
variable ""BASE_LATEST"" {
default = ""${BASE_IMAGE}:latest""
}
target ""webapp"" {
contexts = {
base = BASE_LATEST
}
}
When you print the resolved build configuration, using the -f
flag to specify
the vars.hcl
and docker-bake.hcl
files, you see that the BASE_LATEST
variable is resolved to docker.io/library/alpine:latest
.
$ docker buildx bake -f vars.hcl -f docker-bake.hcl --print app
{
""target"": {
""webapp"": {
""context"": ""."",
""contexts"": {
""base"": ""docker.io/library/alpine:latest""
},
""dockerfile"": ""Dockerfile""
}
}
}
Additional resources
Here are some additional resources that show how you can use variables in Bake:
- You can override
variable
values using environment variables. See Overriding configurations for more information. - You can refer to and use global variables in functions. See HCL functions
- You can use variable values when evaluating expressions. See Expression evaluation",,,
7c0f413b4cf6fd22e4cc19d8248e2859f21c212427e810596283f75ca08ff4fb,"Inheritance in Bake
Targets can inherit attributes from other targets, using the inherits
attribute. For example, imagine that you have a target that builds a Docker
image for a development environment:
target ""app-dev"" {
args = {
GO_VERSION = ""1.23""
}
tags = [""docker.io/username/myapp:dev""]
labels = {
""org.opencontainers.image.source"" = ""https://github.com/username/myapp""
""org.opencontainers.image.author"" = ""moby.whale@example.com""
}
}
You can create a new target that uses the same build configuration, but with
slightly different attributes for a production build. In this example, the
app-release
target inherits the app-dev
target, but overrides the tags
attribute and adds a new platforms
attribute:
target ""app-release"" {
inherits = [""app-dev""]
tags = [""docker.io/username/myapp:latest""]
platforms = [""linux/amd64"", ""linux/arm64""]
}
Common reusable targets
One common inheritance pattern is to define a common target that contains
shared attributes for all or many of the build targets in the project. For
example, the following _common
target defines a common set of build
arguments:
target ""_common"" {
args = {
GO_VERSION = ""1.23""
BUILDKIT_CONTEXT_KEEP_GIT_DIR = 1
}
}
You can then inherit the _common
target in other targets to apply the shared
attributes:
target ""lint"" {
inherits = [""_common""]
dockerfile = ""./dockerfiles/lint.Dockerfile""
output = [{ type = ""cacheonly"" }]
}
target ""docs"" {
inherits = [""_common""]
dockerfile = ""./dockerfiles/docs.Dockerfile""
output = [""./docs/reference""]
}
target ""test"" {
inherits = [""_common""]
target = ""test-output""
output = [""./test""]
}
target ""binaries"" {
inherits = [""_common""]
target = ""binaries""
output = [""./build""]
platforms = [""local""]
}
Overriding inherited attributes
When a target inherits another target, it can override any of the inherited
attributes. For example, the following target overrides the args
attribute
from the inherited target:
target ""app-dev"" {
inherits = [""_common""]
args = {
GO_VERSION = ""1.17""
}
tags = [""docker.io/username/myapp:dev""]
}
The GO_VERSION
argument in app-release
is set to 1.17
, overriding the
GO_VERSION
argument from the app-dev
target.
For more information about overriding attributes, see the Overriding configurations page.
Inherit from multiple targets
The inherits
attribute is a list, meaning you can reuse attributes from
multiple other targets. In the following example, the app-release target reuses
attributes from both the app-dev
and _common
targets.
target ""_common"" {
args = {
GO_VERSION = ""1.23""
BUILDKIT_CONTEXT_KEEP_GIT_DIR = 1
}
}
target ""app-dev"" {
inherits = [""_common""]
args = {
BUILDKIT_CONTEXT_KEEP_GIT_DIR = 0
}
tags = [""docker.io/username/myapp:dev""]
labels = {
""org.opencontainers.image.source"" = ""https://github.com/username/myapp""
""org.opencontainers.image.author"" = ""moby.whale@example.com""
}
}
target ""app-release"" {
inherits = [""app-dev"", ""_common""]
tags = [""docker.io/username/myapp:latest""]
platforms = [""linux/amd64"", ""linux/arm64""]
}
When inheriting attributes from multiple targets and there's a conflict, the
target that appears last in the inherits list takes precedence. The previous
example defines the BUILDKIT_CONTEXT_KEEP_GIT_DIR
in the _common
target and
overrides it in the app-dev
target.
The app-release
target inherits both app-dev
target and the _common
target.
The BUILDKIT_CONTEXT_KEEP_GIT_DIR
argument is set to 0 in the app-dev
target
and 1 in the _common
target. The BUILDKIT_CONTEXT_KEEP_GIT_DIR
argument in
the app-release
target is set to 1, not 0, because the _common
target appears
last in the inherits list.
Reusing single attributes from targets
If you only want to inherit a single attribute from a target, you can reference
an attribute from another target using dot notation. For example, in the
following Bake file, the bar
target reuses the tags
attribute from the
foo
target:
target ""foo"" {
dockerfile = ""foo.Dockerfile""
tags = [""myapp:latest""]
}
target ""bar"" {
dockerfile = ""bar.Dockerfile""
tags = target.foo.tags
}",,,
68dd9970479e43a6aee1c4efb129df207ac11d2e159af85a2c2682d7bf6aa4f8,"Run Docker Engine in swarm mode
When you first install and start working with Docker Engine, Swarm mode is
disabled by default. When you enable Swarm mode, you work with the concept of
services managed through the docker service
command.
There are two ways to run the engine in Swarm mode:
- Create a new swarm, covered in this article.
- Join an existing swarm.
When you run the engine in Swarm mode on your local machine, you can create and test services based upon images you've created or other available images. In your production environment, Swarm mode provides a fault-tolerant platform with cluster management features to keep your services running and available.
These instructions assume you have installed the Docker Engine on a machine to serve as a manager node in your swarm.
If you haven't already, read through the Swarm mode key concepts and try the Swarm mode tutorial.
Create a swarm
When you run the command to create a swarm, Docker Engine starts running in Swarm mode.
Run
docker swarm init
to create a single-node swarm on the current node. The engine sets up the swarm
as follows:
- Switches the current node into Swarm mode.
- Creates a swarm named
default
. - Designates the current node as a leader manager node for the swarm.
- Names the node with the machine hostname.
- Configures the manager to listen on an active network interface on port
2377
. - Sets the current node to
Active
availability, meaning it can receive tasks from the scheduler. - Starts an internal distributed data store for Engines participating in the swarm to maintain a consistent view of the swarm and all services running on it.
- By default, generates a self-signed root CA for the swarm.
- By default, generates tokens for worker and manager nodes to join the swarm.
- Creates an overlay network named
ingress
for publishing service ports external to the swarm. - Creates an overlay default IP addresses and subnet mask for your networks
The output for docker swarm init
provides the connection command to use when
you join new worker nodes to the swarm:
$ docker swarm init
Swarm initialized: current node (dxn1zf6l61qsb1josjja83ngz) is now a manager.
To add a worker to this swarm, run the following command:
docker swarm join \
--token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
192.168.99.100:2377
To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.
Configuring default address pools
By default Swarm mode uses a default address pool 10.0.0.0/8
for global scope (overlay) networks. Every
network that does not have a subnet specified will have a subnet sequentially allocated from this pool. In
some circumstances it may be desirable to use a different default IP address pool for networks.
For example, if the default 10.0.0.0/8
range conflicts with already allocated address space in your network,
then it is desirable to ensure that networks use a different range without requiring swarm users to specify
each subnet with the --subnet
command.
To configure custom default address pools, you must define pools at swarm initialization using the
--default-addr-pool
command line option. This command line option uses CIDR notation for defining the subnet mask.
To create the custom address pool for Swarm, you must define at least one default address pool, and an optional default address pool subnet mask. For example, for the 10.0.0.0/27
, use the value 27
.
Docker allocates subnet addresses from the address ranges specified by the --default-addr-pool
option. For example, a command line option --default-addr-pool 10.10.0.0/16
indicates that Docker will allocate subnets from that /16
address range. If --default-addr-pool-mask-len
were unspecified or set explicitly to 24, this would result in 256 /24
networks of the form 10.10.X.0/24
.
The subnet range comes from the --default-addr-pool
, (such as 10.10.0.0/16
). The size of 16 there represents the number of networks one can create within that default-addr-pool
range. The --default-addr-pool
option may occur multiple times with each option providing additional addresses for docker to use for overlay subnets.
The format of the command is:
$ docker swarm init --default-addr-pool <IP range in CIDR> [--default-addr-pool <IP range in CIDR> --default-addr-pool-mask-length <CIDR value>]
The command to create a default IP address pool with a /16 (class B) for the 10.20.0.0
network looks like this:
$ docker swarm init --default-addr-pool 10.20.0.0/16
The command to create a default IP address pool with a /16
(class B) for the 10.20.0.0
and 10.30.0.0
networks, and to
create a subnet mask of /26
for each network looks like this:
$ docker swarm init --default-addr-pool 10.20.0.0/16 --default-addr-pool 10.30.0.0/16 --default-addr-pool-mask-length 26
In this example, docker network create -d overlay net1
will result in 10.20.0.0/26
as the allocated subnet for net1
,
and docker network create -d overlay net2
will result in 10.20.0.64/26
as the allocated subnet for net2
. This continues until
all the subnets are exhausted.
Refer to the following pages for more information:
- Swarm networking for more information about the default address pool usage
docker swarm init
CLI reference for more detail on the--default-addr-pool
flag.
Configure the advertise address
Manager nodes use an advertise address to allow other nodes in the swarm access to the Swarmkit API and overlay networking. The other nodes on the swarm must be able to access the manager node on its advertise address.
If you don't specify an advertise address, Docker checks if the system has a
single IP address. If so, Docker uses the IP address with the listening port
2377
by default. If the system has multiple IP addresses, you must specify the
correct --advertise-addr
to enable inter-manager communication and overlay
networking:
$ docker swarm init --advertise-addr <MANAGER-IP>
You must also specify the --advertise-addr
if the address where other nodes
reach the first manager node is not the same address the manager sees as its
own. For instance, in a cloud setup that spans different regions, hosts have
both internal addresses for access within the region and external addresses that
you use for access from outside that region. In this case, specify the external
address with --advertise-addr
so that the node can propagate that information
to other nodes that subsequently connect to it.
Refer to the docker swarm init
CLI reference
for more detail on the advertise address.
View the join command or update a swarm join token
Nodes require a secret token to join the swarm. The token for worker nodes is different from the token for manager nodes. Nodes only use the join-token at the moment they join the swarm. Rotating the join token after a node has already joined a swarm does not affect the node's swarm membership. Token rotation ensures an old token cannot be used by any new nodes attempting to join the swarm.
To retrieve the join command including the join token for worker nodes, run:
$ docker swarm join-token worker
To add a worker to this swarm, run the following command:
docker swarm join \
--token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
192.168.99.100:2377
This node joined a swarm as a worker.
To view the join command and token for manager nodes, run:
$ docker swarm join-token manager
To add a manager to this swarm, run the following command:
docker swarm join \
--token SWMTKN-1-59egwe8qangbzbqb3ryawxzk3jn97ifahlsrw01yar60pmkr90-bdjfnkcflhooyafetgjod97sz \
192.168.99.100:2377
Pass the --quiet
flag to print only the token:
$ docker swarm join-token --quiet worker
SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c
Be careful with the join tokens because they are the secrets necessary to join the swarm. In particular, checking a secret into version control is a bad practice because it would allow anyone with access to the application source code to add new nodes to the swarm. Manager tokens are especially sensitive because they allow a new manager node to join and gain control over the whole swarm.
We recommend that you rotate the join tokens in the following circumstances:
- If a token was checked-in by accident into a version control system, group chat or accidentally printed to your logs.
- If you suspect a node has been compromised.
- If you wish to guarantee that no new nodes can join the swarm.
Additionally, it is a best practice to implement a regular rotation schedule for any secret including swarm join tokens. We recommend that you rotate your tokens at least every 6 months.
Run swarm join-token --rotate
to invalidate the old token and generate a new
token. Specify whether you want to rotate the token for worker
or manager
nodes:
$ docker swarm join-token --rotate worker
To add a worker to this swarm, run the following command:
docker swarm join \
--token SWMTKN-1-2kscvs0zuymrsc9t0ocyy1rdns9dhaodvpl639j2bqx55uptag-ebmn5u927reawo27s3azntd44 \
192.168.99.100:2377",,,
98868a4d68feb87591fed4f6b43733410357fd2f7e548fea3cf5d0f5c3145e8c,"Select a storage driver
Ideally, very little data is written to a container's writable layer, and you use Docker volumes to write data. However, some workloads require you to be able to write to the container's writable layer. This is where storage drivers come in.
Docker supports several storage drivers, using a pluggable architecture. The storage driver controls how images and containers are stored and managed on your Docker host. After you have read the storage driver overview, the next step is to choose the best storage driver for your workloads. Use the storage driver with the best overall performance and stability in the most usual scenarios.
Note
This page discusses storage drivers for Docker Engine on Linux. If you're running the Docker daemon with Windows as the host OS, the only supported storage driver is windowsfilter. For more information, see windowsfilter.
The Docker Engine provides the following storage drivers on Linux:
| Driver | Description |
|---|---|
overlay2 | overlay2 is the preferred storage driver for all currently supported Linux distributions, and requires no extra configuration. |
fuse-overlayfs | fuse-overlayfs is preferred only for running Rootless Docker on an old host that does not provide support for rootless overlay2 . The fuse-overlayfs driver does not need to be used since Linux kernel 5.11, and overlay2 works even in rootless mode. Refer to the
rootless mode documentation for details. |
btrfs and zfs | The btrfs and zfs storage drivers allow for advanced options, such as creating ""snapshots"", but require more maintenance and setup. Each of these relies on the backing filesystem being configured correctly. |
vfs | The vfs storage driver is intended for testing purposes, and for situations where no copy-on-write filesystem can be used. Performance of this storage driver is poor, and is not generally recommended for production use. |
The Docker Engine has a prioritized list of which storage driver to use if no storage driver is explicitly configured, assuming that the storage driver meets the prerequisites, and automatically selects a compatible storage driver. You can see the order in the source code for Docker Engine 28.0.1.
Some storage drivers require you to use a specific format for the backing filesystem. If you have external requirements to use a specific backing filesystem, this may limit your choices. See Supported backing filesystems.
After you have narrowed down which storage drivers you can choose from, your choice is determined by the characteristics of your workload and the level of stability you need. See Other considerations for help in making the final decision.
Supported storage drivers per Linux distribution
Note
Modifying the storage driver by editing the daemon configuration file isn't supported on Docker Desktop. Only the default
overlay2
driver or the containerd storage are supported. The following table is also not applicable for the Docker Engine in rootless mode. For the drivers available in rootless mode, see the Rootless mode documentation.
Your operating system and kernel may not support every storage driver. For
example, btrfs
is only supported if your system uses btrfs
as storage. In
general, the following configurations work on recent versions of the Linux
distribution:
| Linux distribution | Recommended storage drivers | Alternative drivers |
|---|---|---|
| Ubuntu | overlay2 | zfs , vfs |
| Debian | overlay2 | vfs |
| CentOS | overlay2 | zfs , vfs |
| Fedora | overlay2 | zfs , vfs |
| SLES 15 | overlay2 | vfs |
| RHEL | overlay2 | vfs |
When in doubt, the best all-around configuration is to use a modern Linux
distribution with a kernel that supports the overlay2
storage driver, and to
use Docker volumes for write-heavy workloads instead of relying on writing data
to the container's writable layer.
The vfs
storage driver is usually not the best choice, and primarily intended
for debugging purposes in situations where no other storage-driver is supported.
Before using the vfs
storage driver, be sure to read about
its performance and storage characteristics and limitations.
The recommendations in the table above are known to work for a large number of users. If you use a recommended configuration and find a reproducible issue, it's likely to be fixed very quickly. If the driver that you want to use is not recommended according to this table, you can run it at your own risk. You can and should still report any issues you run into. However, such issues have a lower priority than issues encountered when using a recommended configuration.
Depending on your Linux distribution, other storage-drivers, such as btrfs
may
be available. These storage drivers can have advantages for specific use-cases,
but may require additional set-up or maintenance, which make them not recommended
for common scenarios. Refer to the documentation for those storage drivers for
details.
Supported backing filesystems
With regard to Docker, the backing filesystem is the filesystem where
/var/lib/docker/
is located. Some storage drivers only work with specific
backing filesystems.
| Storage driver | Supported backing filesystems |
|---|---|
overlay2 | xfs with ftype=1, ext4 |
fuse-overlayfs | any filesystem |
btrfs | btrfs |
zfs | zfs |
vfs | any filesystem |
Other considerations
Suitability for your workload
Among other things, each storage driver has its own performance characteristics that make it more or less suitable for different workloads. Consider the following generalizations:
overlay2
operates at the file level rather than the block level. This uses memory more efficiently, but the container's writable layer may grow quite large in write-heavy workloads.- Block-level storage drivers such as
btrfs
, andzfs
perform better for write-heavy workloads (though not as well as Docker volumes). btrfs
andzfs
require a lot of memory.zfs
is a good choice for high-density workloads such as PaaS.
More information about performance, suitability, and best practices is available in the documentation for each storage driver.
Shared storage systems and the storage driver
If you use SAN, NAS, hardware RAID, or other shared storage systems, those systems may provide high availability, increased performance, thin provisioning, deduplication, and compression. In many cases, Docker can work on top of these storage systems, but Docker doesn't closely integrate with them.
Each Docker storage driver is based on a Linux filesystem or volume manager. Be sure to follow existing best practices for operating your storage driver (filesystem or volume manager) on top of your shared storage system. For example, if using the ZFS storage driver on top of a shared storage system, be sure to follow best practices for operating ZFS filesystems on top of that specific shared storage system.
Stability
For some users, stability is more important than performance. Though Docker
considers all of the storage drivers mentioned here to be stable, some are newer
and are still under active development. In general, overlay2
provides the
highest stability.
Test with your own workloads
You can test Docker's performance when running your own workloads on different storage drivers. Make sure to use equivalent hardware and workloads to match production conditions, so you can see which storage driver offers the best overall performance.
Check your current storage driver
The detailed documentation for each individual storage driver details all of the set-up steps to use a given storage driver.
To see what storage driver Docker is currently using, use docker info
and look
for the Storage Driver
line:
$ docker info
Containers: 0
Images: 0
Storage Driver: overlay2
Backing Filesystem: xfs
<...>
To change the storage driver, see the specific instructions for the new storage driver. Some drivers require additional configuration, including configuration to physical or logical disks on the Docker host.
Important
When you change the storage driver, any existing images and containers become inaccessible. This is because their layers can't be used by the new storage driver. If you revert your changes, you can access the old images and containers again, but any that you pulled or created using the new driver are then inaccessible.",,,
caca5a5721d0259193877029769799f9b6562070a1afb2c30569e8a405783319,"Builder settings
The Builder settings page in Docker Build Cloud lets you configure disk allocation, private resource access, and firewall settings for your cloud builders in your organization. These configurations help optimize storage, enable access to private registries, and secure outbound network traffic.
Disk allocation
The Disk allocation setting lets you control how much of the available storage is dedicated to the build cache. A lower allocation increases storage available for active builds.
To make disk allocation changes, navigate to Builder settings in Docker Build Cloud and then adjust the Disk allocation slider to specify the percentage of storage used for build caching.
Any changes take effect immediately.
Tip
If you build very large images, consider allocating less storage for caching.
Private resource access
Private resource access lets cloud builders pull images and packages from private resources. This feature is useful when builds rely on self-hosted artifact repositories or private OCI registries.
For example, if your organization hosts a private PyPI repository on a private network, Docker Build Cloud would not be able to access it by default, since the cloud builder is not connected to your private network.
To enable your cloud builders to access your private resources, enter the host name and port of your private resource and then select Add.
Authentication
If your internal artifacts require authentication, make sure that you
authenticate with the repository either before or during the build. For
internal package repositories for npm or PyPI, use
build secrets
to authenticate during the build. For internal OCI registries, use docker login
to authenticate before building.
Note that if you use a private registry that requires authentication, you will
need to authenticate with docker login
twice before building. This is because
the cloud builder needs to authenticate with Docker to use the cloud builder,
and then again to authenticate with the private registry.
$ echo $DOCKER_PAT | docker login docker.io -u <username> --password-stdin
$ echo $REGISTRY_PASSWORD | docker login registry.example.com -u <username> --password-stdin
$ docker build --builder <cloud-builder> --tag registry.example.com/<image> --push .
Firewall
Firewall settings let you restrict cloud builder egress traffic to specific IP addresses. This helps enhance security by limiting external network egress from the builder.
Select the Enable firewall: Restrict cloud builder egress to specific public IP address checkbox.
Enter the IP address you want to allow.
Select Add to apply the restriction.",,,
41597c81dae3cae163afb88f7737f96023962b4a48e63f93266d11d536db5a45,"Integrate Docker Scout with SonarQube
The SonarQube integration enables Docker Scout to surface SonarQube quality gate checks through Policy Evaluation, under a new SonarQube Quality Gates Policy.
How it works
This integration uses SonarQube webhooks to notify Docker Scout of when a SonarQube project analysis has completed. When the webhook is called, Docker Scout receives the analysis results, and stores them in the database.
When you push a new image to a repository, Docker Scout evaluates the results of the SonarQube analysis record corresponding to the image. Docker Scout uses Git provenance metadata on the images, from provenance attestations or an OCI annotations, to link image repositories with SonarQube analysis results.
Note
Docker Scout doesn't have access to historic SonarQube analysis records. Only analysis results recorded after the integration is enabled will be available to Docker Scout.
Both self-managed SonarQube instances and SonarCloud are supported.
Prerequisites
To integrate Docker Scout with SonarQube, ensure that:
- Your image repository is integrated with Docker Scout.
- Your images are built with
provenance attestations,
or the
org.opencontainers.image.revision
annotation, containing information about the Git repository.
Enable the SonarQube integration
Go to the SonarQube integrations page on the Docker Scout Dashboard.
In the How to integrate section, enter a configuration name for this integration. Docker Scout uses this label as a display name for the integration, and to name the webhook.
Select Next.
Enter the configuration details for your SonarQube instance. Docker Scout uses this information to create SonarQube webhook.
In SonarQube, generate a new User token. The token requires 'Administer' permission on the specified project, or global 'Administer' permission.
Enter the token, your SonarQube URL, and the ID of your SonarQube organization. The SonarQube organization is required if you're using SonarCloud.
Select Enable configuration.
Docker Scout performs a connection test to verify that the provided details are correct, and that the token has the necessary permissions.
After a successful connection test, you're redirected to the SonarQube integration overview, which lists all your SonarQube integrations and their statuses.
From the integration overview page, you can go directly to the SonarQube Quality Gates Policy. This policy will have no results initially. To start seeing evaluation results for this policy, trigger a new SonarQube analysis of your project and push the corresponding image to a repository. For more information, refer to the policy description.",,,
fa2f48b2e02a9b1bd159ddc954f9332f38db6ee4639de3034a7459679e5e6402,"Macvlan network driver
Some applications, especially legacy applications or applications which monitor
network traffic, expect to be directly connected to the physical network. In
this type of situation, you can use the macvlan
network driver to assign a MAC
address to each container's virtual network interface, making it appear to be
a physical network interface directly connected to the physical network. In this
case, you need to designate a physical interface on your Docker host to use for
the Macvlan, as well as the subnet and gateway of the network. You can even
isolate your Macvlan networks using different physical network interfaces.
Keep the following things in mind:
You may unintentionally degrade your network due to IP address exhaustion or to ""VLAN spread"", a situation that occurs when you have an inappropriately large number of unique MAC addresses in your network.
Your networking equipment needs to be able to handle ""promiscuous mode"", where one physical interface can be assigned multiple MAC addresses.
If your application can work using a bridge (on a single Docker host) or overlay (to communicate across multiple Docker hosts), these solutions may be better in the long term.
Options
The following table describes the driver-specific options that you can pass to
--opt
when creating a network using the macvlan
driver.
| Option | Default | Description |
|---|---|---|
macvlan_mode | bridge | Sets the Macvlan mode. Can be one of: bridge , vepa , passthru , private |
parent | Specifies the parent interface to use. |
Create a Macvlan network
When you create a Macvlan network, it can either be in bridge mode or 802.1Q trunk bridge mode.
In bridge mode, Macvlan traffic goes through a physical device on the host.
In 802.1Q trunk bridge mode, traffic goes through an 802.1Q sub-interface which Docker creates on the fly. This allows you to control routing and filtering at a more granular level.
Bridge mode
To create a macvlan
network which bridges with a given physical network
interface, use --driver macvlan
with the docker network create
command. You
also need to specify the parent
, which is the interface the traffic will
physically go through on the Docker host.
$ docker network create -d macvlan \
--subnet=172.16.86.0/24 \
--gateway=172.16.86.1 \
-o parent=eth0 pub_net
If you need to exclude IP addresses from being used in the macvlan
network, such
as when a given IP address is already in use, use --aux-addresses
:
$ docker network create -d macvlan \
--subnet=192.168.32.0/24 \
--ip-range=192.168.32.128/25 \
--gateway=192.168.32.254 \
--aux-address=""my-router=192.168.32.129"" \
-o parent=eth0 macnet32
802.1Q trunk bridge mode
If you specify a parent
interface name with a dot included, such as eth0.50
,
Docker interprets that as a sub-interface of eth0
and creates the sub-interface
automatically.
$ docker network create -d macvlan \
--subnet=192.168.50.0/24 \
--gateway=192.168.50.1 \
-o parent=eth0.50 macvlan50
Use an IPvlan instead of Macvlan
In the above example, you are still using a L3 bridge. You can use ipvlan
instead, and get an L2 bridge. Specify -o ipvlan_mode=l2
.
$ docker network create -d ipvlan \
--subnet=192.168.210.0/24 \
--subnet=192.168.212.0/24 \
--gateway=192.168.210.254 \
--gateway=192.168.212.254 \
-o ipvlan_mode=l2 -o parent=eth0 ipvlan210
Use IPv6
If you have
configured the Docker daemon to allow IPv6,
you can use dual-stack IPv4/IPv6 macvlan
networks.
$ docker network create -d macvlan \
--subnet=192.168.216.0/24 --subnet=192.168.218.0/24 \
--gateway=192.168.216.1 --gateway=192.168.218.1 \
--subnet=2001:db8:abc8::/64 --gateway=2001:db8:abc8::10 \
-o parent=eth0.218 \
-o macvlan_mode=bridge macvlan216
Next steps
Learn how to use the Macvlan driver in the Macvlan networking tutorial.",,,
effa155985a88f83a7bc759c539b909507293a0725d0675ddbbada95ed4f1d06,"Syslog logging driver
The syslog
logging driver routes logs to a syslog
server. The syslog
protocol uses
a raw string as the log message and supports a limited set of metadata. The syslog
message must be formatted in a specific way to be valid. From a valid message, the
receiver can extract the following information:
- Priority: the logging level, such as
debug
,warning
,error
,info
. - Timestamp: when the event occurred.
- Hostname: where the event happened.
- Facility: which subsystem logged the message, such as
mail
orkernel
. - Process name and process ID (PID): The name and ID of the process that generated the log.
The format is defined in RFC 5424 and Docker's syslog driver implements the ABNF reference in the following way:
TIMESTAMP SP HOSTNAME SP APP-NAME SP PROCID SP MSGID
+ + + | +
| | | | |
| | | | |
+------------+ +----+ | +----+ +---------+
v v v v v
2017-04-01T17:41:05.616647+08:00 a.vm {taskid:aa,version:} 1787791 {taskid:aa,version:}
Usage
To use the syslog
driver as the default logging driver, set the log-driver
and log-opt
keys to appropriate values in the daemon.json
file, which is
located in /etc/docker/
on Linux hosts or
C:\ProgramData\docker\config\daemon.json
on Windows Server. For more about
configuring Docker using daemon.json
, see
daemon.json.
The following example sets the log driver to syslog
and sets the
syslog-address
option. The syslog-address
options supports both UDP and TCP;
this example uses UDP.
{
""log-driver"": ""syslog"",
""log-opts"": {
""syslog-address"": ""udp://1.2.3.4:1111""
}
}
Restart Docker for the changes to take effect.
Note
log-opts
configuration options in thedaemon.json
configuration file must be provided as strings. Numeric and Boolean values (such as the value forsyslog-tls-skip-verify
) must therefore be enclosed in quotes (""
).
You can set the logging driver for a specific container by using the
--log-driver
flag to docker container create
or docker run
:
$ docker run \
--log-driver syslog --log-opt syslog-address=udp://1.2.3.4:1111 \
alpine echo hello world
Options
The following logging options are supported as options for the syslog
logging
driver. They can be set as defaults in the daemon.json
, by adding them as
key-value pairs to the log-opts
JSON array. They can also be set on a given
container by adding a --log-opt <key>=<value>
flag for each option when
starting the container.
| Option | Description | Example value |
|---|---|---|
syslog-address | The address of an external syslog server. The URI specifier may be [tcp|udp|tcp+tls]://host:port , unix://path , or unixgram://path . If the transport is tcp , udp , or tcp+tls , the default port is 514 . | --log-opt syslog-address=tcp+tls://192.168.1.3:514 , --log-opt syslog-address=unix:///tmp/syslog.sock |
syslog-facility | The syslog facility to use. Can be the number or name for any valid syslog facility. See the
syslog documentation. | --log-opt syslog-facility=daemon |
syslog-tls-ca-cert | The absolute path to the trust certificates signed by the CA. Ignored if the address protocol isn't tcp+tls . | --log-opt syslog-tls-ca-cert=/etc/ca-certificates/custom/ca.pem |
syslog-tls-cert | The absolute path to the TLS certificate file. Ignored if the address protocol isn't tcp+tls . | --log-opt syslog-tls-cert=/etc/ca-certificates/custom/cert.pem |
syslog-tls-key | The absolute path to the TLS key file. Ignored if the address protocol isn't tcp+tls . | --log-opt syslog-tls-key=/etc/ca-certificates/custom/key.pem |
syslog-tls-skip-verify | If set to true , TLS verification is skipped when connecting to the syslog daemon. Defaults to false . Ignored if the address protocol isn't tcp+tls . | --log-opt syslog-tls-skip-verify=true |
tag | A string that's appended to the APP-NAME in the syslog message. By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the
log tag option documentation for customizing the log tag format. | --log-opt tag=mailer |
syslog-format | The syslog message format to use. If not specified the local Unix syslog format is used, without a specified hostname. Specify rfc3164 for the RFC-3164 compatible format, rfc5424 for RFC-5424 compatible format, or rfc5424micro for RFC-5424 compatible format with microsecond timestamp resolution. | --log-opt syslog-format=rfc5424micro |
labels | Applies when starting the Docker daemon. A comma-separated list of logging-related labels this daemon accepts. Used for advanced log tag options. | --log-opt labels=production_status,geo |
labels-regex | Applies when starting the Docker daemon. Similar to and compatible with labels . A regular expression to match logging-related labels. Used for advanced
log tag options. | --log-opt labels-regex=^(production_status|geo) |
env | Applies when starting the Docker daemon. A comma-separated list of logging-related environment variables this daemon accepts. Used for advanced log tag options. | --log-opt env=os,customer |
env-regex | Applies when starting the Docker daemon. Similar to and compatible with env . A regular expression to match logging-related environment variables. Used for advanced
log tag options. | --log-opt env-regex=^(os|customer) |",,,
8941abe0a6455b6ed525ae788ad002e0fe32ce150e66f87bdbb4f580e6257b2a,"Evaluate policy compliance in CI
Adding Policy Evaluation to your continuous integration pipelines helps you detect and prevent cases where code changes would cause policy compliance to become worse compared to your baseline.
The recommended strategy for Policy Evaluation in a CI setting involves evaluating a local image and comparing the results to a baseline. If the policy compliance for the local image is worse than the specified baseline, the CI run fails with an error. If policy compliance is better or unchanged, the CI run succeeds.
This comparison is relative, meaning that it's only concerned with whether your CI image is better or worse than your baseline. It's not an absolute check to pass or fail all policies. By measuring relative to a baseline that you define, you can quickly see if a change has a positive or negative impact on policy compliance.
How it works
When you do Policy Evaluation in CI, you run a local policy evaluation on the image you build in your CI pipeline. To run a local evaluation, the image that you evaluate must exist in the image store where your CI workflow is being run. Either build or pull the image, and then run the evaluation.
To run policy evaluation and trigger failure if compliance for your local image
is worse than your comparison baseline, you need to specify the image version
to use as a baseline. You can hard-code a specific image reference, but a
better solution is to use
environments
to automatically infer the image version from an environment. The example that
follows uses environments to compare the CI image with the image in the
production
environment.
Example
The following example on how to run policy evaluation in CI uses the
Docker
Scout GitHub Action to
execute the compare
command on an image built in CI. The compare command has
a to-env
input, which will run the comparison against an environment called
production
. The exit-on
input is set to policy
, meaning that the
comparison fails only if policy compliance has worsened.
This example doesn't assume that you're using Docker Hub as your container
registry. As a result, this workflow uses the docker/login-action
twice:
- Once for authenticating to your container registry.
- Once more for authenticating to Docker to pull the analysis results of your
production
image.
If you use Docker Hub as your container registry, you only need to authenticate once.
Note
Due to a limitation in the Docker Engine, loading multi-platform images or images with attestations to the image store isn't supported.
For the policy evaluation to work, you must load the image to the local image store of the runner. Ensure that you're building a single-platform image without attestations, and that you're loading the build results. Otherwise, the policy evaluation fails.
Also note the pull-requests: write
permission for the job. The Docker Scout
GitHub Action adds a pull request comment with the evaluation results by
default, which requires this permission. For details, see
Pull Request Comments.
name: Docker
on:
push:
tags: [""*""]
branches:
- ""main""
pull_request:
branches: [""**""]
env:
REGISTRY: docker.io
IMAGE_NAME: <IMAGE_NAME>
DOCKER_ORG: <ORG>
jobs:
build:
permissions:
pull-requests: write
runs-on: ubuntu-latest
steps:
- name: Log into registry ${{ env.REGISTRY }}
uses: docker/login-action@v3
with:
registry: ${{ env.REGISTRY }}
username: ${{ secrets.REGISTRY_USER }}
password: ${{ secrets.REGISTRY_TOKEN }}
- name: Setup Docker buildx
uses: docker/setup-buildx-action@v3
- name: Extract metadata
id: meta
uses: docker/metadata-action@v5
with:
images: ${{ env.IMAGE_NAME }}
- name: Build image
id: build-and-push
uses: docker/build-push-action@v4
with:
tags: ${{ steps.meta.outputs.tags }}
labels: ${{ steps.meta.outputs.labels }}
sbom: ${{ github.event_name != 'pull_request' }}
provenance: ${{ github.event_name != 'pull_request' }}
push: ${{ github.event_name != 'pull_request' }}
load: ${{ github.event_name == 'pull_request' }}
- name: Authenticate with Docker
uses: docker/login-action@v3
with:
username: ${{ secrets.DOCKER_USER }}
password: ${{ secrets.DOCKER_PAT }}
- name: Compare
if: ${{ github.event_name == 'pull_request' }}
uses: docker/scout-action@v1
with:
command: compare
image: ${{ steps.meta.outputs.tags }}
to-env: production
platform: ""linux/amd64""
ignore-unchanged: true
only-severities: critical,high
organization: ${{ env.DOCKER_ORG }}
exit-on: policy
The following screenshot shows what the GitHub PR comment looks like when a policy evaluation check fails because policy has become worse in the PR image compared to baseline.
This example has demonstrated how to run policy evaluation in CI with GitHub Actions. Docker Scout also supports other CI platforms. For more information, see Docker Scout CI integrations.",,,
cbff1c3d734e845394bbe3d6a7f0b447f117c412bda14c416962058999868dd1,"Manage autobuilds
Note
Automated builds require a Docker Pro, Team, or Business subscription.
Cancel or retry a build
While a build is in queue or running, a Cancel icon appears next to its build report link on the General tab and on the Builds tab. You can also select Cancel on the Build report page, or from the Timeline tab's logs display for the build.
Check your active builds
A summary of a repository's builds appears both on the repository General tab, and in the Builds tab. The Builds tab also displays a color coded bar chart of the build queue times and durations. Both views display the pending, in progress, successful, and failed builds for any tag of the repository.
From either location, you can select a build job to view its build report. The build report shows information about the build job. This includes the source repository and branch, or tag, the build logs, the build duration, creation time and location, and the user account the build occurred in.
Note
You can now view the progress of your builds every 30 seconds when you refresh the Builds page. With the in-progress build logs, you can debug your builds before they're finished.
Disable an automated build
Automated builds are enabled per branch or tag, and can be disabled and re-enabled. You might do this when you want to only build manually for a while, for example when you are doing major refactoring in your code. Disabling autobuilds doesn't disable autotests.
To disable an automated build:
From the Repositories page, select a repository, and select the Builds tab.
Select Configure automated builds to edit the repository's build settings.
In the Build Rules section, locate the branch or tag you no longer want to automatically build.
Select the Autobuild toggle next to the configuration line. When disabled the toggle is gray.
Select Save.",,,
260f53c6d6895867fac44dfcc04decb90393551038635ca9c3e53b46fd15fdd9,"Use Docker Build Cloud in CI
Using Docker Build Cloud in CI can speed up your build pipelines, which means less time spent waiting and context switching. You control your CI workflows as usual, and delegate the build execution to Docker Build Cloud.
Building with Docker Build Cloud in CI involves the following steps:
- Sign in to a Docker account.
- Set up Buildx and connect to the builder.
- Run the build.
When using Docker Build Cloud in CI, it's recommended that you push the result to a registry directly, rather than loading the image and then pushing it. Pushing directly speeds up your builds and avoids unnecessary file transfers.
If you just want to build and discard the output, export the results to the build cache or build without tagging the image. When you use Docker Build Cloud, Buildx automatically loads the build result if you build a tagged image. See Loading build results for details.
Note
Builds on Docker Build Cloud have a timeout limit of two hours. Builds that run for longer than two hours are automatically cancelled.
CI platform examples
GitHub Actions
Note
Version 4.0.0 and later of
docker/build-push-action
anddocker/bake-action
builds images with provenance attestations by default. Docker Build Cloud automatically attempts to load images to the local image store if you don't explicitly push them to a registry.This results in a conflicting scenario where if you build a tagged image without pushing it to a registry, Docker Build Cloud attempts to load images containing attestations. But the local image store on the GitHub runner doesn't support attestations, and the image load fails as a result.
If you want to load images built with
docker/build-push-action
together with Docker Build Cloud, you must disable provenance attestations by settingprovenance: false
in the GitHub Action inputs (or indocker-bake.hcl
if you use Bake).
name: ci
on:
push:
branches:
- ""main""
jobs:
docker:
runs-on: ubuntu-latest
steps:
- name: Login to Docker Hub
uses: docker/login-action@v3
with:
username: ${{ vars.DOCKER_USER }}
password: ${{ secrets.DOCKER_PAT }}
- name: Set up Docker Buildx
uses: docker/setup-buildx-action@v3
with:
driver: cloud
endpoint: ""<ORG>/default""
install: true
- name: Build and push
uses: docker/build-push-action@v6
with:
tags: ""<IMAGE>""
# For pull requests, export results to the build cache.
# Otherwise, push to a registry.
outputs: ${{ github.event_name == 'pull_request' && 'type=cacheonly' || 'type=registry' }}
GitLab
default:
image: docker:24-dind
services:
- docker:24-dind
before_script:
- docker info
- echo ""$DOCKER_PAT"" | docker login --username ""$DOCKER_USER"" --password-stdin
- |
apk add curl jq
ARCH=${CI_RUNNER_EXECUTABLE_ARCH#*/}
BUILDX_URL=$(curl -s https://raw.githubusercontent.com/docker/actions-toolkit/main/.github/buildx-lab-releases.json | jq -r "".latest.assets[] | select(endswith(\""linux-$ARCH\""))"")
mkdir -vp ~/.docker/cli-plugins/
curl --silent -L --output ~/.docker/cli-plugins/docker-buildx $BUILDX_URL
chmod a+x ~/.docker/cli-plugins/docker-buildx
- docker buildx create --use --driver cloud ${DOCKER_ORG}/default
variables:
IMAGE_NAME: <IMAGE>
DOCKER_ORG: <ORG>
# Build multi-platform image and push to a registry
build_push:
stage: build
script:
- |
docker buildx build \
--platform linux/amd64,linux/arm64 \
--tag ""${IMAGE_NAME}:${CI_COMMIT_SHORT_SHA}"" \
--push .
# Build an image and discard the result
build_cache:
stage: build
script:
- |
docker buildx build \
--platform linux/amd64,linux/arm64 \
--tag ""${IMAGE_NAME}:${CI_COMMIT_SHORT_SHA}"" \
--output type=cacheonly \
.
Circle CI
version: 2.1
jobs:
# Build multi-platform image and push to a registry
build_push:
machine:
image: ubuntu-2204:current
steps:
- checkout
- run: |
mkdir -vp ~/.docker/cli-plugins/
ARCH=amd64
BUILDX_URL=$(curl -s https://raw.githubusercontent.com/docker/actions-toolkit/main/.github/buildx-lab-releases.json | jq -r "".latest.assets[] | select(endswith(\""linux-$ARCH\""))"")
curl --silent -L --output ~/.docker/cli-plugins/docker-buildx $BUILDX_URL
chmod a+x ~/.docker/cli-plugins/docker-buildx
- run: echo ""$DOCKER_PAT"" | docker login --username $DOCKER_USER --password-stdin
- run: docker buildx create --use --driver cloud ""<ORG>/default""
- run: |
docker buildx build \
--platform linux/amd64,linux/arm64 \
--push \
--tag ""<IMAGE>"" .
# Build an image and discard the result
build_cache:
machine:
image: ubuntu-2204:current
steps:
- checkout
- run: |
mkdir -vp ~/.docker/cli-plugins/
ARCH=amd64
BUILDX_URL=$(curl -s https://raw.githubusercontent.com/docker/actions-toolkit/main/.github/buildx-lab-releases.json | jq -r "".latest.assets[] | select(endswith(\""linux-$ARCH\""))"")
curl --silent -L --output ~/.docker/cli-plugins/docker-buildx $BUILDX_URL
chmod a+x ~/.docker/cli-plugins/docker-buildx
- run: echo ""$DOCKER_PAT"" | docker login --username $DOCKER_USER --password-stdin
- run: docker buildx create --use --driver cloud ""<ORG>/default""
- run: |
docker buildx build \
--tag temp \
--output type=cacheonly \
.
workflows:
pull_request:
jobs:
- build_cache
release:
jobs:
- build_push
Buildkite
The following example sets up a Buildkite pipeline using Docker Build Cloud. The
example assumes that the pipeline name is build-push-docker
and that you
manage the Docker access token using environment hooks, but feel free to adapt
this to your needs.
Add the following environment
hook agent's hook directory:
#!/bin/bash
set -euo pipefail
if [[ ""$BUILDKITE_PIPELINE_NAME"" == ""build-push-docker"" ]]; then
export DOCKER_PAT=""<DOCKER_PERSONAL_ACCESS_TOKEN>""
fi
Create a pipeline.yml
that uses the docker-login
plugin:
env:
DOCKER_ORG: <ORG>
IMAGE_NAME: <IMAGE>
steps:
- command: ./build.sh
key: build-push
plugins:
- docker-login#v2.1.0:
username: <DOCKER_USER>
password-env: DOCKER_PAT # the variable name in the environment hook
Create the build.sh
script:
DOCKER_DIR=/usr/libexec/docker
# Get download link for latest buildx binary.
# Set $ARCH to the CPU architecture (e.g. amd64, arm64)
UNAME_ARCH=`uname -m`
case $UNAME_ARCH in
aarch64)
ARCH=""arm64"";
;;
amd64)
ARCH=""amd64"";
;;
*)
ARCH=""amd64"";
;;
esac
BUILDX_URL=$(curl -s https://raw.githubusercontent.com/docker/actions-toolkit/main/.github/buildx-lab-releases.json | jq -r "".latest.assets[] | select(endswith(\""linux-$ARCH\""))"")
# Download docker buildx with Build Cloud support
curl --silent -L --output $DOCKER_DIR/cli-plugins/docker-buildx $BUILDX_URL
chmod a+x ~/.docker/cli-plugins/docker-buildx
# Connect to your builder and set it as the default builder
docker buildx create --use --driver cloud ""$DOCKER_ORG/default""
# Cache-only image build
docker buildx build \
--platform linux/amd64,linux/arm64 \
--tag ""$IMAGE_NAME:$BUILDKITE_COMMIT"" \
--output type=cacheonly \
.
# Build, tag, and push a multi-arch docker image
docker buildx build \
--platform linux/amd64,linux/arm64 \
--push \
--tag ""$IMAGE_NAME:$BUILDKITE_COMMIT"" \
.
Jenkins
pipeline {
agent any
environment {
ARCH = 'amd64'
DOCKER_PAT = credentials('docker-personal-access-token')
DOCKER_USER = credentials('docker-username')
DOCKER_ORG = '<ORG>'
IMAGE_NAME = '<IMAGE>'
}
stages {
stage('Build') {
environment {
BUILDX_URL = sh (returnStdout: true, script: 'curl -s https://raw.githubusercontent.com/docker/actions-toolkit/main/.github/buildx-lab-releases.json | jq -r "".latest.assets[] | select(endswith(\\""linux-$ARCH\\""))""').trim()
}
steps {
sh 'mkdir -vp ~/.docker/cli-plugins/'
sh 'curl --silent -L --output ~/.docker/cli-plugins/docker-buildx $BUILDX_URL'
sh 'chmod a+x ~/.docker/cli-plugins/docker-buildx'
sh 'echo ""$DOCKER_PAT"" | docker login --username $DOCKER_USER --password-stdin'
sh 'docker buildx create --use --driver cloud ""$DOCKER_ORG/default""'
// Cache-only build
sh 'docker buildx build --platform linux/amd64,linux/arm64 --tag ""$IMAGE_NAME"" --output type=cacheonly .'
// Build and push a multi-platform image
sh 'docker buildx build --platform linux/amd64,linux/arm64 --push --tag ""$IMAGE_NAME"" .'
}
}
}
}
Travis CI
language: minimal
dist: jammy
services:
- docker
env:
global:
- IMAGE_NAME=username/repo
before_install: |
echo ""$DOCKER_PAT"" | docker login --username ""$DOCKER_USER"" --password-stdin
install: |
set -e
BUILDX_URL=$(curl -s https://raw.githubusercontent.com/docker/actions-toolkit/main/.github/buildx-lab-releases.json | jq -r "".latest.assets[] | select(endswith(\""linux-$TRAVIS_CPU_ARCH\""))"")
mkdir -vp ~/.docker/cli-plugins/
curl --silent -L --output ~/.docker/cli-plugins/docker-buildx $BUILDX_URL
chmod a+x ~/.docker/cli-plugins/docker-buildx
docker buildx create --use --driver cloud ""<ORG>/default""
script: |
docker buildx build \
--platform linux/amd64,linux/arm64 \
--push \
--tag ""$IMAGE_NAME"" .
BitBucket Pipelines
# Prerequisites: $DOCKER_USER, $DOCKER_PAT setup as deployment variables
# This pipeline assumes $BITBUCKET_REPO_SLUG as the image name
# Replace <ORG> in the `docker buildx create` command with your Docker org
image: atlassian/default-image:3
pipelines:
default:
- step:
name: Build multi-platform image
script:
- mkdir -vp ~/.docker/cli-plugins/
- ARCH=amd64
- BUILDX_URL=$(curl -s https://raw.githubusercontent.com/docker/actions-toolkit/main/.github/buildx-lab-releases.json | jq -r "".latest.assets[] | select(endswith(\""linux-$ARCH\""))"")
- curl --silent -L --output ~/.docker/cli-plugins/docker-buildx $BUILDX_URL
- chmod a+x ~/.docker/cli-plugins/docker-buildx
- echo ""$DOCKER_PAT"" | docker login --username $DOCKER_USER --password-stdin
- docker buildx create --use --driver cloud ""<ORG>/default""
- IMAGE_NAME=$BITBUCKET_REPO_SLUG
- docker buildx build
--platform linux/amd64,linux/arm64
--push
--tag ""$IMAGE_NAME"" .
services:
- docker
Shell script
#!/bin/bash
# Get download link for latest buildx binary. Set $ARCH to the CPU architecture (e.g. amd64, arm64)
ARCH=amd64
BUILDX_URL=$(curl -s https://raw.githubusercontent.com/docker/actions-toolkit/main/.github/buildx-lab-releases.json | jq -r "".latest.assets[] | select(endswith(\""linux-$ARCH\""))"")
# Download docker buildx with Build Cloud support
mkdir -vp ~/.docker/cli-plugins/
curl --silent -L --output ~/.docker/cli-plugins/docker-buildx $BUILDX_URL
chmod a+x ~/.docker/cli-plugins/docker-buildx
# Login to Docker Hub. For security reasons $DOCKER_PAT should be a Personal Access Token. See https://docs.docker.com/security/for-developers/access-tokens/
echo ""$DOCKER_PAT"" | docker login --username $DOCKER_USER --password-stdin
# Connect to your builder and set it as the default builder
docker buildx create --use --driver cloud ""<ORG>/default""
# Cache-only image build
docker buildx build \
--tag temp \
--output type=cacheonly \
.
# Build, tag, and push a multi-arch docker image
docker buildx build \
--platform linux/amd64,linux/arm64 \
--push \
--tag ""<IMAGE>"" \
.
Docker Compose
Use this implementation if you want to use docker compose build
with
Docker Build Cloud in CI.
#!/bin/bash
# Get download link for latest buildx binary. Set $ARCH to the CPU architecture (e.g. amd64, arm64)
ARCH=amd64
BUILDX_URL=$(curl -s https://raw.githubusercontent.com/docker/actions-toolkit/main/.github/buildx-lab-releases.json | jq -r "".latest.assets[] | select(endswith(\""linux-$ARCH\""))"")
COMPOSE_URL=$(curl -sL \
-H ""Accept: application/vnd.github+json"" \
-H ""Authorization: Bearer <GITHUB_TOKEN>"" \
-H ""X-GitHub-Api-Version: 2022-11-28"" \
https://api.github.com/repos/docker/compose-desktop/releases \
| jq ""[ .[] | select(.prerelease==false and .draft==false) ] | .[0].assets.[] | select(.name | endswith(\""linux-${ARCH}\"")) | .browser_download_url"")
# Download docker buildx with Build Cloud support
mkdir -vp ~/.docker/cli-plugins/
curl --silent -L --output ~/.docker/cli-plugins/docker-buildx $BUILDX_URL
curl --silent -L --output ~/.docker/cli-plugins/docker-compose $COMPOSE_URL
chmod a+x ~/.docker/cli-plugins/docker-buildx
chmod a+x ~/.docker/cli-plugins/docker-compose
# Login to Docker Hub. For security reasons $DOCKER_PAT should be a Personal Access Token. See https://docs.docker.com/security/for-developers/access-tokens/
echo ""$DOCKER_PAT"" | docker login --username $DOCKER_USER --password-stdin
# Connect to your builder and set it as the default builder
docker buildx create --use --driver cloud ""<ORG>/default""
# Build the image build
docker compose build",,,
b522dcb30f79bd1251ccd21dd91a353639a1380fc07b3813d2bbedcb4410effa,"Tags on Docker Hub
Tags let you manage multiple versions of images within a single Docker Hub
repository. By adding a specific :<tag>
to each image, such as
docs/base:testing
, you can organize and differentiate image versions for
various use cases. If no tag is specified, the image defaults to the latest
tag.
Tag a local image
To tag a local image, use one of the following methods:
- When you build an image, use
docker build -t <org-or-user-namespace>/<repo-name>[:<tag>
. - Re-tag an existing local image with
docker tag <existing-image> <org-or-user-namespace>/<repo-name>[:<tag>]
. - When you commit changes, use
docker commit <existing-container> <org-or-user-namespace>/<repo-name>[:<tag>]
.
Then, you can push this image to the repository designated by its name or tag:
$ docker push <org-or-user-namespace>/<repo-name>:<tag>
The image is then uploaded and available for use in Docker Hub.
View repository tags
You can view the available tags and the size of the associated image.
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears.
Select the Tags tab.
You can select a tag's digest to see more details.
Delete repository tags
Only the repository owner or other team members with granted permissions can delete tags.
Sign in to Docker Hub.
Select Repositories.
A list of your repositories appears.
Select a repository.
The General page for the repository appears.
Select the Tags tab.
Select the corresponding checkbox next to the tags to delete.
Select Delete.
A confirmation dialog appears.
Select Delete.",,,
7db49a9739e8d421f70a771e8799b4450394cb56a22d05ca37e04afcbb028bfd,"Insights and analytics
Insights and analytics provides usage analytics for Docker Verified Publisher (DVP) and Docker-Sponsored Open Source (DSOS) images on Docker Hub. This includes self-serve access to image and extension usage metrics for a desired time span. You can also display the number of image pulls by tag or by digest, and get breakdowns by geolocation, cloud provider, client, and more.
Tip
Head to the Docker Verified Publisher Program or Docker-Sponsored Open Source pages to learn more about the programs.
View the image's analytics data
You can find analytics data for your repositories on the Insights and
analytics dashboard at the following URL:
https://hub.docker.com/orgs/{namespace}/insights/images
. The dashboard contains a
visualization of the usage data and a table where you can download
the data as CSV files.
To view data in the chart:
- Select the data granularity: weekly or monthly
- Select the time interval: 3, 6, or 12 months
- Select one or more repositories in the list
Tip
Hovering your cursor over the chart displays a tooltip, showing precise data for points in time.
Share analytics data
You can share the visualization with others using the Share icon above the chart. This is a convenient way to share statistics with others in your organization.
Selecting the icon generates a link that's copied to your clipboard. The link preserves the display selections you made. When someone follows the link, the Insights and analytics page opens and displays the chart with the same configuration as you had set up when creating the link.
Extension analytics data
If you have published Docker Extensions in the Extension marketplace, you can also get analytics about your extension usage, available as CSV files.
You can download extension CSV reports from the Insights and analytics dashboard at the following URL:
https://hub.docker.com/orgs/{namespace}/insights/extensions
. If your Docker namespace contains extensions known in the marketplace, you will see an Extensions tab listing CSV files for your extension(s).
Exporting analytics data
You can export the analytics data either from the web dashboard, or using the DVP Data API. All members of an organization have access to the analytics data.
The data is available as a downloadable CSV file, in a weekly (Monday through Sunday) or monthly format. Monthly data is available from the first day of the following calendar month. You can import this data into your own systems, or you can analyze it manually as a spreadsheet.
Export data
Export usage data for your organization's images using the Docker Hub website by following these steps:
Sign in to Docker Hub and select Organizations.
Choose your organization and select Insights and analytics.
Set the time span for which you want to export analytics data.
The downloadable CSV files for summary and raw data appear on the right-hand side.
Export data using the API
The HTTP API endpoints are available at:
https://hub.docker.com/api/publisher/analytics/v1
. Learn how to export data
using the API in the
DVP Data API documentation.
Data points
Export data in either raw or summary format. Each format contains different data points and with different structure.
The following sections describe the available data points for each format. The Date added column shows when the field was first introduced.
Image pulls raw data
The raw data format contains the following data points. Each row in the CSV file represents an image pull.
| Data point | Description | Date added |
|---|---|---|
| Action | Request type, see
Action classification rules. One of pull_by_tag , pull_by_digest , version_check . | January 1, 2022 |
| Action day | The date part of the timestamp: YYYY-MM-DD . | January 1, 2022 |
| Country | Request origin country. | January 1, 2022 |
| Digest | Image digest. | January 1, 2022 |
| HTTP method | HTTP method used in the request, see registry API documentation for details. | January 1, 2022 |
| Host | The cloud service provider used in an event. | January 1, 2022 |
| Namespace | Docker organization (image namespace). | January 1, 2022 |
| Reference | Image digest or tag used in the request. | January 1, 2022 |
| Repository | Docker repository (image name). | January 1, 2022 |
| Tag (included when available) | Tag name that's only available if the request referred to a tag. | January 1, 2022 |
| Timestamp | Date and time of the request: YYYY-MM-DD 00:00:00 . | January 1, 2022 |
| Type | The industry from which the event originates. One of business , isp , hosting , education , null . | January 1, 2022 |
| User agent tool | The application a user used to pull an image (for example, docker or containerd ). | January 1, 2022 |
| User agent version | The version of the application used to pull an image. | January 1, 2022 |
| Domain | Request origin domain, see Privacy. | October 11, 2022 |
| Owner | The name of the organization that owns the repository. | December 19, 2022 |
Image pulls summary data
There are two levels of summary data available:
- Repository-level, a summary of every namespace and repository
- Tag- or digest-level, a summary of every namespace, repository, and reference (tag or digest)
The summary data formats contain the following data points for the selected time span:
| Data point | Description | Date added |
|---|---|---|
| Unique IP address | Number of unique IP addresses, see Privacy. | January 1, 2022 |
| Pull by tag | GET request, by digest or by tag. | January 1, 2022 |
| Pull by digest | GET or HEAD request by digest, or HEAD by digest. | January 1, 2022 |
| Version check | HEAD by tag, not followed by a GET | January 1, 2022 |
| Owner | The name of the organization that owns the repository. | December 19, 2022 |
Image pulls action classification rules
An action represents the multiple request events associated with a
docker pull
. Pulls are grouped by category to make the data more meaningful
for understanding user behavior and intent. The categories are:
- Version check
- Pull by tag
- Pull by digest
Automated systems frequently check for new versions of your images. Being able to distinguish between ""version checks"" in CI versus actual image pulls by a user grants you more insight into your users' behavior.
The following table describes the rules applied for determining intent behind pulls. To provide feedback or ask questions about these rules, fill out the Google Form.
| Starting event | Reference | Followed by | Resulting action | Use case(s) | Notes |
|---|---|---|---|---|---|
| HEAD | tag | N/A | Version check | User already has all layers existing on local machine | This is similar to the use case of a pull by tag when the user already has all the image layers existing locally, however, it differentiates the user intent and classifies accordingly. |
| GET | tag | N/A | Pull by tag | User already has all layers existing on local machine and/or the image is single-arch | |
| GET | tag | Get by different digest | Pull by tag | Image is multi-arch | Second GET by digest must be different from the first. |
| HEAD | tag | GET by same digest | Pull by tag | Image is multi-arch but some or all image layers already exist on the local machine | The HEAD by tag sends the most current digest, the following GET must be by that same digest. There may occur an additional GET, if the image is multi-arch (see the next row in this table). If the user doesn't want the most recent digest, then the user performs HEAD by digest. |
| HEAD | tag | GET by the same digest, then a second GET by a different digest | Pull by tag | Image is multi-arch | The HEAD by tag sends the most recent digest, the following GET must be by that same digest. Since the image is multi-arch, there is a second GET by a different digest. If the user doesn't want the most recent digest, then the user performs HEAD by digest. |
| HEAD | tag | GET by same digest, then a second GET by different digest | Pull by tag | Image is multi-arch | The HEAD by tag sends the most current digest, the following GET must be by that same digest. Since the image is multi-arch, there is a second GET by a different digest. If the user doesn't want the most recent digest, then the user performs HEAD by digest. |
| GET | digest | N/A | Pull by digest | User already has all layers existing on local machine and/or the image is single-arch | |
| HEAD | digest | N/A | Pull by digest | User already has all layers existing on their local machine | |
| GET | digest | GET by different digest | Pull by digest | Image is multi-arch | The second GET by digest must be different from the first. |
| HEAD | digest | GET by same digest | Pull by digest | Image is single-arch and/or image is multi-arch but some part of the image already exists on the local machine | |
| HEAD | digest | GET by same digest, then a second GET by different digest | Pull by Digest | Image is multi-arch |
Extension Summary data
There are two levels of extension summary data available:
- Core summary, with basic extension usage information: number of extension installs, uninstalls, and total install all times
The core-summary-data file contains the following data points for the selected time span:
| Data point | Description | Date added |
|---|---|---|
| Installs | Number of installs for the extension | Feb 1, 2024 |
| TotalInstalls | Number of installs for the extension all times | Feb 1, 2024 |
| Uninstalls | Number of uninstalls for the extension | Feb 1, 2024 |
| TotalUninstalls | Number of uninstalls for the extension all times | Feb 1, 2024 |
| Updates | Number of updates for the extension | Feb 1, 2024 |
- Premium summary, with advanced extension usage information: installs, uninstalls by unique users, extension opening by unique users.
The core-summary-data file contains the following data points for the selected time span:
| Data point | Description | Date added |
|---|---|---|
| Installs | Number of installs for the extension | Feb 1, 2024 |
| UniqueInstalls | Number of unique users installing the extension | Feb 1, 2024 |
| Uninstalls | Number of uninstalls for the extension | Feb 1, 2024 |
| UniqueUninstalls | Number of unique users uninstalling the extension | Feb 1, 2024 |
| Usage | Number of openings of the extension tab | Feb 1, 2024 |
| UniqueUsers | Number of unique users openings the extension tab | Feb 1, 2024 |
Changes in data over time
The insights and analytics service is continuously improved to increase the value it brings to publishers. Some changes might include adding new data points, or improving existing data to make it more useful.
Changes in the dataset, such as added or removed fields, generally only apply from the date of when the field was first introduced, and going forward.
Refer to the tables in the Data points section to see from which date a given data point is available.
Privacy
This section contains information about privacy-protecting measures that ensures consumers of content on Docker Hub remain completely anonymous.
Important
Docker never shares any Personally Identifiable Information (PII) as part of analytics data.
The image pulls summary dataset includes unique IP address count. This data point only includes the number of distinct unique IP addresses that request an image. Individual IP addresses are never shared.
The image pulls raw dataset includes user IP domains as a data point. This is the domain name
associated with the IP address used to pull an image. If the IP type is
business
, the domain represents the company or organization associated with
that IP address (for example, docker.com
). For any other IP type that's not
business
, the domain represents the internet service provider or hosting
provider used to make the request. On average, only about 30% of all pulls
classify as the business
IP type (this varies between publishers and images).",,,
2d8d4396652651e21197c2ad9e5026bd9320d17a88d1b074e91806fe0eede386,"MUI best practices
This article assumes you're following our recommended practice by using our Material UI theme. Following the steps below maximizes compatibility with Docker Desktop and minimizes the work you need to do as an extension author. They should be considered supplementary to the non-MUI-specific guidelines found in the UI Styling overview.
Assume the theme can change at any time
Resist the temptation to fine-tune your UI with precise colors, offsets and font sizings to make it look as attractive as possible. Any specializations you make today will be relative to the current MUI theme, and may look worse when the theme changes. Any part of the theme might change without warning, including (but not limited to):
- The font, or font sizes
- Border thicknesses or styles
- Colors:
- Our palette members (e.g.
red-100
) could change their RGB values - The semantic colors (e.g.
error
,primary
,textPrimary
, etc) could be changed to use a different member of our palette - Background colors (e.g. those of the page, or of dialogs) could change
- Our palette members (e.g.
- Spacings:
- The size of the basic unit of spacing,(exposed via
theme.spacing
. For instance, we may allow users to customize the density of the UI - The default spacing between paragraphs or grid items
- The size of the basic unit of spacing,(exposed via
The best way to build your UI, so that it’s robust against future theming changes, is to:
- Override the default styling as little as possible.
- Use semantic typography. e.g. use
Typography
s orLink
s with appropriatevariant
s instead of using typographical HTML elements (<a>
,<p>
,<h1>
, etc) directly. - Use canned sizes. e.g. use
size=""small""
on buttons, orfontSize=""small""
on icons, instead of specifying sizes in pixels. - Prefer semantic colors. e.g. use
error
orprimary
over explicit color codes. - Write as little CSS as possible. Write semantic markup instead. For example, if you want to space out paragraphs of text, use the
paragraph
prop on yourTypography
instances. If you want to space out something else, use aStack
orGrid
with the default spacing. - Use visual idioms you’ve seen in the Docker Desktop UI, since these are the main ones we’ll test any theme changes against.
When you go custom, centralize it
Sometimes you’ll need a piece of UI that doesn’t exist in our design system. If so, we recommend that you first reach out to us. We may already have something in our internal design system, or we may be able to expand our design system to accommodate your use case.
If you still decide to build it yourself after contacting us, try and define the new UI in a reuseable fashion. If you define your custom UI in just one place, it’ll make it easier to change in the future if our core theme changes. You could use:
- A new
variant
of an existing component - see MUI docs - A MUI mixin (a freeform bundle of reuseable styling rules defined inside a theme)
- A new reuseable component
Some of the above options require you to extend our MUI theme. See the MUI documentation on theme composition.
What's next?
- Take a look at our UI styling guide.
- Learn how to publish your extension.",,,
61a1903c0d37b7dd11b5b384f5ab2eae659e04a5719f9bf716d0aba028ef3619,"Graylog Extended Format logging driver
The gelf
logging driver is a convenient format that's understood by a number of tools such as
Graylog,
Logstash, and
Fluentd. Many tools use this format.
In GELF, every log message is a dict with the following fields:
- Version
- Host (who sent the message in the first place)
- Timestamp
- Short and long version of the message
- Any custom fields you configure yourself
Usage
To use the gelf
driver as the default logging driver, set the log-driver
and
log-opt
keys to appropriate values in the daemon.json
file, which is located
in /etc/docker/
on Linux hosts or C:\ProgramData\docker\config\daemon.json
on Windows Server. For more about configuring Docker using daemon.json
, see
daemon.json.
The following example sets the log driver to gelf
and sets the gelf-address
option.
{
""log-driver"": ""gelf"",
""log-opts"": {
""gelf-address"": ""udp://1.2.3.4:12201""
}
}
Restart Docker for the changes to take effect.
Note
log-opts
configuration options in thedaemon.json
configuration file must be provided as strings. Boolean and numeric values (such as the value forgelf-tcp-max-reconnect
) must therefore be enclosed in quotes (""
).
You can set the logging driver for a specific container by setting the
--log-driver
flag when using docker container create
or docker run
:
$ docker run \
--log-driver gelf --log-opt gelf-address=udp://1.2.3.4:12201 \
alpine echo hello world
GELF options
The gelf
logging driver supports the following options:
| Option | Required | Description | Example value |
|---|---|---|---|
gelf-address | required | The address of the GELF server. tcp and udp are the only supported URI specifier and you must specify the port. | --log-opt gelf-address=udp://192.168.0.42:12201 |
gelf-compression-type | optional | UDP Only The type of compression the GELF driver uses to compress each log message. Allowed values are gzip , zlib and none . The default is gzip . Note that enabled compression leads to excessive CPU usage, so it's highly recommended to set this to none . | --log-opt gelf-compression-type=gzip |
gelf-compression-level | optional | UDP Only The level of compression when gzip or zlib is the gelf-compression-type . An integer in the range of -1 to 9 (BestCompression). Default value is 1 (BestSpeed). Higher levels provide more compression at lower speed. Either -1 or 0 disables compression. | --log-opt gelf-compression-level=2 |
gelf-tcp-max-reconnect | optional | TCP Only The maximum number of reconnection attempts when the connection drop. A positive integer. Default value is 3. | --log-opt gelf-tcp-max-reconnect=3 |
gelf-tcp-reconnect-delay | optional | TCP Only The number of seconds to wait between reconnection attempts. A positive integer. Default value is 1. | --log-opt gelf-tcp-reconnect-delay=1 |
tag | optional | A string that's appended to the APP-NAME in the gelf message. By default, Docker uses the first 12 characters of the container ID to tag log messages. Refer to the
log tag option documentation for customizing the log tag format. | --log-opt tag=mailer |
labels | optional | Applies when starting the Docker daemon. A comma-separated list of logging-related labels this daemon accepts. Adds additional key on the extra fields, prefixed by an underscore (_ ). Used for advanced
log tag options. | --log-opt labels=production_status,geo |
labels-regex | optional | Similar to and compatible with labels . A regular expression to match logging-related labels. Used for advanced
log tag options. | --log-opt labels-regex=^(production_status|geo) |
env | optional | Applies when starting the Docker daemon. A comma-separated list of logging-related environment variables this daemon accepts. Adds additional key on the extra fields, prefixed by an underscore (_ ). Used for advanced
log tag options. | --log-opt env=os,customer |
env-regex | optional | Similar to and compatible with env . A regular expression to match logging-related environment variables. Used for advanced
log tag options. | --log-opt env-regex=^(os|customer) |
Note
The
gelf
driver doesn't support TLS for TCP connections. Messages sent to TLS-protected inputs can silently fail.
Examples
This example configures the container to use the GELF server running at
192.168.0.42
on port 12201
.
$ docker run -dit \
--log-driver=gelf \
--log-opt gelf-address=udp://192.168.0.42:12201 \
alpine sh",,,
af4f94c9fd2256de8f889c61d6a8b0b45e45bf8cf5c80550ee9649394e0ddaf2,"FAQs on organizations
What if the Docker ID I want for my organization or company is taken?
All Docker IDs are first-come, first-served except for companies that have a U.S. Trademark on a username. If you have a trademark for your namespace, Docker Support can retrieve the Docker ID for you.
How do I add an organization owner?
An existing owner can add additional team members as organization owners. You can invite a member and assign them the owner role in Docker Hub or the Docker Admin Console.
How do I know how many active users are part of my organization?
If your organization uses a Software Asset Management tool, you can use it to find out how many users have Docker Desktop installed. If your organization doesn't use this software, you can run an internal survey to find out who is using Docker Desktop. See Identify your Docker users and their Docker accounts. With a Docker Business subscription, you can manage members in your identity provider and automatically provision them to your Docker organization with SSO or SCIM.
Do users first need to authenticate with Docker before an owner can add them to an organization?
No. Organization owners can invite users with their email addresses, and also assign them to a team during the invite process.
Can I force my organization's members to authenticate before using Docker Desktop and are there any benefits?
Yes. You can enforce sign-in. Some benefits of enforcing sign-in are:
- Administrators can enforce features like Image Access Management and Registry Access Management.
- Administrators can ensure compliance by blocking Docker Desktop usage for users who don't sign in as members of the organization.
If a user has their personal email associated with a user account in Docker Hub, do they have to convert to using the organization's domain before they can be invited to join an organization?
Yes. When SSO is enabled for your organization, each user must sign in with the company’s domain. However, the user can retain their personal credentials and create a new Docker ID associated with their organization's domain.
Can I convert my personal user account (Docker ID) to an organization account?
Yes. You can convert your user account to an organization account. Once you convert a user account into an organization, it's not possible to revert it to a personal user account. For prerequisites and instructions, see Convert an account into an organization.
Our users create Docker Hub accounts through self-service. How do we know when the total number of users for the requested licenses has been met? Is it possible to add more members to the organization than the total number of licenses?
There isn't any automatic notification when the total number of users for the requested licenses has been met. However, if the number of team members exceed the number of licenses, you will receive an error informing you to contact the administrator due to lack of seats. You can add seats if needed.
How can I merge organization accounts?
You can downgrade a secondary organization and transition your users and data to a primary organization. See Merge organizations.
Do organization invitees take up seats?
Yes. A user invited to an organization will take up one of the provisioned seats, even if that user hasn’t accepted their invitation yet. Organization owners can manage the list of invitees through the Invitees tab on the organization settings page in Docker Hub, or in the Members page in Admin Console.
Do organization owners take a seat?
Yes. Organization owners will take up a seat.
What is the difference between user, invitee, seat, and member?
User refers to a Docker user with a Docker ID.
An invitee is a user that an administrator has invited to join an organization but has not yet accepted their invitation.
Seats are the number of planned members within an organization.
Member may refer to a user who has received and accepted an invitation to join an organization. Member can also refer to a member of a team within an organization.
If there are two organizations and a user belongs to both organizations, do they take up two seats?
Yes. In a scenario where a user belongs to two organizations, they take up one seat in each organization.
Is it possible to set permissions for repositories within an organization?
Yes. You can configure repository access on a per-team basis. For example, you can specify that all teams within an organization have Read and Write access to repositories A and B, whereas only specific teams have Admin access. Org owners have full administrative access to all repositories within the organization. See Configure repository permissions for a team. Administrators can also assign members the editor role, which grants administrative permissions for repositories across the namespace of the organization. See Roles and permissions.
Does my organization need to use Docker's registry?
A registry is a hosted service containing repositories of images that responds to the Registry API. Docker Hub is Docker's primary registry, but you can use Docker with other container image registries. You can access the default registry by browsing to
Docker Hub or using the docker search
command.",,,
e8ea0279d53cfdb41910d89f77aefc222a97971753ec38faad19ea44941de9c3,"Export binaries
Did you know that you can use Docker to build your application to standalone binaries? Sometimes, you don’t want to package and distribute your application as a Docker image. Use Docker to build your application, and use exporters to save the output to disk.
The default output format for docker build
is a container image. That image is
automatically loaded to your local image store, where you can run a container
from that image, or push it to a registry. Under the hood, this uses the default
exporter, called the docker
exporter.
To export your build results as files instead, you can use the --output
flag,
or -o
for short. the --output
flag lets you change the output format of
your build.
Export binaries from a build
If you specify a filepath to the docker build --output
flag, Docker exports
the contents of the build container at the end of the build to the specified
location on your host's filesystem. This uses the local
exporter.
The neat thing about this is that you can use Docker's powerful isolation and build features to create standalone binaries. This works well for Go, Rust, and other languages that can compile to a single binary.
The following example creates a simple Rust program that prints ""Hello, World!"", and exports the binary to the host filesystem.
Create a new directory for this example, and navigate to it:
$ mkdir hello-world-bin $ cd hello-world-bin
Create a Dockerfile with the following contents:
# syntax=docker/dockerfile:1 FROM rust:alpine AS build WORKDIR /src COPY <<EOT hello.rs fn main() { println!(""Hello World!""); } EOT RUN rustc -o /bin/hello hello.rs FROM scratch COPY --from=build /bin/hello / ENTRYPOINT [""/hello""]
Tip
The
COPY <<EOT
syntax is a here-document. It lets you write multi-line strings in a Dockerfile. Here it's used to create a simple Rust program inline in the Dockerfile.This Dockerfile uses a multi-stage build to compile the program in the first stage, and then copies the binary to a scratch image in the second. The final image is a minimal image that only contains the binary. This use case for the
scratch
image is common for creating minimal build artifacts for programs that don't require a full operating system to run.Build the Dockerfile and export the binary to the current working directory:
$ docker build --output=. .
This command builds the Dockerfile and exports the binary to the current working directory. The binary is named
hello
, and it's created in the current working directory.
Exporting multi-platform builds
You use the local
exporter to export binaries in combination with
multi-platform builds. This lets you
compile multiple binaries at once, that can be run on any machine of any
architecture, provided that the target platform is supported by the compiler
you use.
Continuing on the example Dockerfile in the Export binaries from a build section:
# syntax=docker/dockerfile:1
FROM rust:alpine AS build
WORKDIR /src
COPY <<EOT hello.rs
fn main() {
println!(""Hello World!"");
}
EOT
RUN rustc -o /bin/hello hello.rs
FROM scratch
COPY --from=build /bin/hello /
ENTRYPOINT [""/hello""]
You can build this Rust program for multiple platforms using the --platform
flag with the docker build
command. In combination with the --output
flag,
the build exports the binaries for each target to the specified directory.
For example, to build the program for both linux/amd64
and linux/arm64
:
$ docker build --platform=linux/amd64,linux/arm64 --output=out .
$ tree out/
out/
├── linux_amd64
│ └── hello
└── linux_arm64
└── hello
3 directories, 2 files
Additional information
In addition to the local
exporter, there are other exporters available. To
learn more about the available exporters and how to use them, see the
exporters documentation.",,,
a19c5e9349a394cf04cf5c8ca76ef8e1f19c4bb1689ae3ff4bf675c311747878,"Air-gapped containers
Air-gapped containers let you restrict containers from accessing network resources, limiting where data can be uploaded to or downloaded from.
Docker Desktop can apply a custom set of proxy rules to network traffic from containers. The proxy can be configured to:
- Accept network connections
- Reject network connections
- Tunnel through an HTTP or SOCKS proxy
You can choose:
- Which outgoing TCP ports the policy applies to. For example, only certain ports,
80
,443
or all with*
. - Whether to forward to a single HTTP or SOCKS proxy, or to have a policy per destination via a Proxy Auto-Configuration (PAC) file.
Configuration
Assuming
enforced sign-in and
Settings Management are enabled, add the new proxy configuration to the admin-settings.json
file. For example:
{
""configurationFileVersion"": 2,
""containersProxy"": {
""locked"": true,
""mode"": ""manual"",
""http"": """",
""https"": """",
""exclude"": [],
""pac"": ""http://192.168.1.16:62039/proxy.pac"",
""transparentPorts"": ""*""
}
}
The containersProxy
setting describes the policy which is applied to traffic from containers. The valid fields are:
locked
: If true, it is not possible for developers to override these settings. If false the settings are interpreted as default values which the developer can change.mode
: Same meaning as with the existingproxy
setting. Possible values aresystem
andmanual
.http
,https
,exclude
: Same meaning as with theproxy
setting. Only takes effect ifmode
is set tomanual
.pac
: URL for a PAC file. Only takes effect ifmode
ismanual
, and is considered higher priority thanhttp
,https
,exclude
.transparentPorts
: A comma-separated list of ports (e.g.""80,443,8080""
) or a wildcard (*
) indicating which ports should be proxied.
Important
Any existing
proxy
setting in theadmin-settings.json
file continues to apply to traffic from the app on the host.
Example PAC file
For general information about PAC files, see the MDN Web Docs.
The following is an example PAC file:
function FindProxyForURL(url, host) {
if (localHostOrDomainIs(host, 'internal.corp')) {
return ""PROXY 10.0.0.1:3128"";
}
if (isInNet(host, ""192.168.0.0"", ""255.255.255.0"")) {
return ""DIRECT"";
}
return ""PROXY reject.docker.internal:1234"";
}
The url
parameter is either http://host_or_ip:port
or https://host_or_ip:port
.
The hostname is normally available for outgoing requests on port 80
and 443
, but for other cases there is only an IP address.
The FindProxyForURL
can return the following values:
PROXY host_or_ip:port
: Tunnels this request through the HTTP proxyhost_or_ip:port
SOCKS5 host_or_ip:port
: Tunnels this request through the SOCKS proxyhost_or_ip:port
DIRECT
: Lets this request go direct, without a proxyPROXY reject.docker.internal:any_port
: Rejects this request
In this particular example, HTTP and HTTPS requests for internal.corp
are sent via the HTTP proxy 10.0.0.1:3128
. Requests to connect to IPs on the subnet 192.168.0.0/24
connect directly. All other requests are blocked.
To restrict traffic connecting to ports on the developers local machine,
match the special hostname host.docker.internal
.",,,
f0a525754219403cd1b051835ef286570245f1cbb59095200959a9270507a543,"Create an advanced frontend extension
To start creating your extension, you first need a directory with files which range from the extension’s source code to the required extension-specific files. This page provides information on how to set up an extension with a more advanced frontend.
Before you start, make sure you have installed the latest version of Docker Desktop.
Extension folder structure
The quickest way to create a new extension is to run docker extension init my-extension
as in the
Quickstart. This creates a new directory my-extension
that contains a fully functional extension.
Tip
The
docker extension init
generates a React based extension. But you can still use it as a starting point for your own extension and use any other frontend framework, like Vue, Angular, Svelte, etc. or even stay with vanilla Javascript.
Although you can start from an empty directory or from the react-extension
sample folder,
it's highly recommended that you start from the docker extension init
command and change it to suit your needs.
.
├── Dockerfile # (1)
├── ui # (2)
│ ├── public # (3)
│ │ └── index.html
│ ├── src # (4)
│ │ ├── App.tsx
│ │ ├── index.tsx
│ ├── package.json
│ └── package-lock.lock
│ ├── tsconfig.json
├── docker.svg # (5)
└── metadata.json # (6)
- Contains everything required to build the extension and run it in Docker Desktop.
- High-level folder containing your front-end app source code.
- Assets that aren’t compiled or dynamically generated are stored here. These can be static assets like logos or the robots.txt file.
- The src, or source folder contains all the React components, external CSS files, and dynamic assets that are brought into the component files.
- The icon that is displayed in the left-menu of the Docker Desktop Dashboard.
- A file that provides information about the extension such as the name, description, and version.
Adapting the Dockerfile
Note
When using the
docker extension init
, it creates aDockerfile
that already contains what is needed for a React extension.
Once the extension is created, you need to configure the Dockerfile
to build the extension and configure the labels
that are used to populate the extension's card in the Marketplace. Here is an example of a Dockerfile
for a React
extension:
# syntax=docker/dockerfile:1
FROM --platform=$BUILDPLATFORM node:18.9-alpine3.15 AS client-builder
WORKDIR /ui
# cache packages in layer
COPY ui/package.json /ui/package.json
COPY ui/package-lock.json /ui/package-lock.json
RUN --mount=type=cache,target=/usr/src/app/.npm \
npm set cache /usr/src/app/.npm && \
npm ci
# install
COPY ui /ui
RUN npm run build
FROM alpine
LABEL org.opencontainers.image.title=""My extension"" \
org.opencontainers.image.description=""Your Desktop Extension Description"" \
org.opencontainers.image.vendor=""Awesome Inc."" \
com.docker.desktop.extension.api.version=""0.3.3"" \
com.docker.desktop.extension.icon=""https://www.docker.com/wp-content/uploads/2022/03/Moby-logo.png"" \
com.docker.extension.screenshots="""" \
com.docker.extension.detailed-description="""" \
com.docker.extension.publisher-url="""" \
com.docker.extension.additional-urls="""" \
com.docker.extension.changelog=""""
COPY metadata.json .
COPY docker.svg .
COPY --from=client-builder /ui/build ui
Note
In the example Dockerfile, you can see that the image label
com.docker.desktop.extension.icon
is set to an icon URL. The Extensions Marketplace displays this icon without installing the extension. The Dockerfile also includesCOPY docker.svg .
to copy an icon file inside the image. This second icon file is used to display the extension UI in the Dashboard, once the extension is installed.
Important
We don't have a working Dockerfile for Vue yet. Fill out the form and let us know if you'd like a Dockerfile for Vue.
Important
We don't have a working Dockerfile for Angular yet. Fill out the form and let us know if you'd like a Dockerfile for Angular.
Important
We don't have a working Dockerfile for Svelte yet. Fill out the form and let us know if you'd like a Dockerfile for Svelte.
Configure the metadata file
In order to add a tab in Docker Desktop for your extension, you have to configure it in the metadata.json
file the root of your extension directory.
{
""icon"": ""docker.svg"",
""ui"": {
""dashboard-tab"": {
""title"": ""UI Extension"",
""root"": ""/ui"",
""src"": ""index.html""
}
}
}
The title
property is the name of the extension that is displayed in the left-menu of the Docker Desktop Dashboard.
The root
property is the path to the frontend application in the extension's container filesystem used by the
system to deploy it on the host.
The src
property is the path to the HTML entry point of the frontend application within the root
folder.
For more information on the ui
section of the metadata.json
, see
Metadata.
Build the extension and install it
Now that you have configured the extension, you need to build the extension image that Docker Desktop will use to install it.
docker build --tag=awesome-inc/my-extension:latest .
This built an image tagged awesome-inc/my-extension:latest
, you can run docker inspect awesome-inc/my-extension:latest
to see more details about it.
Finally, you can install the extension and see it appearing in the Docker Desktop Dashboard.
docker extension install awesome-inc/my-extension:latest
Use the Extension APIs client
To use the Extension APIs and perform actions with Docker Desktop, the extension must first import the
@docker/extension-api-client
library. To install it, run the command below:
npm install @docker/extension-api-client
Then call the createDockerDesktopClient
function to create a client object to call the extension APIs.
import { createDockerDesktopClient } from '@docker/extension-api-client';
const ddClient = createDockerDesktopClient();
When using Typescript, you can also install @docker/extension-api-client-types
as a dev dependency. This will
provide you with type definitions for the extension APIs and auto-completion in your IDE.
npm install @docker/extension-api-client-types --save-dev
For example, you can use the docker.cli.exec
function to get the list of all the containers via the docker ps --all
command and display the result in a table.
Replace the ui/src/App.tsx
file with the following code:
// ui/src/App.tsx
import React, { useEffect } from 'react';
import {
Paper,
Stack,
Table,
TableBody,
TableCell,
TableContainer,
TableHead,
TableRow,
Typography
} from ""@mui/material"";
import { createDockerDesktopClient } from ""@docker/extension-api-client"";
//obtain docker desktop extension client
const ddClient = createDockerDesktopClient();
export function App() {
const [containers, setContainers] = React.useState<any[]>([]);
useEffect(() => {
// List all containers
ddClient.docker.cli.exec('ps', ['--all', '--format', '""{{json .}}""']).then((result) => {
// result.parseJsonLines() parses the output of the command into an array of objects
setContainers(result.parseJsonLines());
});
}, []);
return (
<Stack>
<Typography data-testid=""heading"" variant=""h3"" role=""title"">
Container list
</Typography>
<Typography
data-testid=""subheading""
variant=""body1""
color=""text.secondary""
sx={{ mt: 2 }}
>
Simple list of containers using Docker Extensions SDK.
</Typography>
<TableContainer sx={{mt:2}}>
<Table>
<TableHead>
<TableRow>
<TableCell>Container id</TableCell>
<TableCell>Image</TableCell>
<TableCell>Command</TableCell>
<TableCell>Created</TableCell>
<TableCell>Status</TableCell>
</TableRow>
</TableHead>
<TableBody>
{containers.map((container) => (
<TableRow
key={container.ID}
sx={{ '&:last-child td, &:last-child th': { border: 0 } }}
>
<TableCell>{container.ID}</TableCell>
<TableCell>{container.Image}</TableCell>
<TableCell>{container.Command}</TableCell>
<TableCell>{container.CreatedAt}</TableCell>
<TableCell>{container.Status}</TableCell>
</TableRow>
))}
</TableBody>
</Table>
</TableContainer>
</Stack>
);
}
Important
We don't have an example for Vue yet. Fill out the form and let us know if you'd like a sample with Vue.
Important
We don't have an example for Angular yet. Fill out the form and let us know if you'd like a sample with Angular.
Important
We don't have an example for Svelte yet. Fill out the form and let us know if you'd like a sample with Svelte.
Policies enforced for the front-end code
Extension UI code is rendered in a separate electron session and doesn't have a node.js environment initialized, nor direct access to the electron APIs.
This is to limit the possible unexpected side effects to the overall Docker Dashboard.
The extension UI code can't perform privileged tasks, such as making changes to the system, or spawning sub-processes, except by using the SDK APIs provided with the extension framework. The Extension UI code can also perform interactions with Docker Desktop, such as navigating to various places in the Dashboard, only through the extension SDK APIs.
Extensions UI parts are isolated from each other and extension UI code is running in its own session for each extension. Extensions can't access other extensions’ session data.
localStorage
is one of the mechanisms of a browser’s web storage. It allows users to save data as key-value pairs in the browser for later use. localStorage
doesn't clear data when the browser (the extension pane) closes. This makes it ideal for persisting data when navigating out of the extension to other parts of Docker Desktop.
If your extension uses localStorage
to store data, other extensions running in Docker Desktop can't access the local storage of your extension. The extension’s local storage is persisted even after Docker Desktop is stopped or restarted. When an extension is upgraded, its local storage is persisted, whereas when it is uninstalled, its local storage is completely removed.
Re-build the extension and update it
Since you have modified the code of the extension, you must build again the extension.
$ docker build --tag=awesome-inc/my-extension:latest .
Once built, you need to update it.
$ docker extension update awesome-inc/my-extension:latest
Now you can see the backend service running in the containers tab of the Docker Desktop Dashboard and watch the logs when you need to debug it.
Tip
You can turn on hot reloading to avoid the need to rebuild the extension every time you make a change.
What's next?
- Add a backend to your extension.
- Learn how to test and debug your extension.
- Learn how to setup CI for your extension.
- Learn more about extensions architecture.
- For more information and guidelines on building the UI, see the Design and UI styling section.
- If you want to set up user authentication for the extension, see Authentication.",,,
