{"test_cases": [{"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>io.erasurecode.codec.rs-legacy.rawcoders</name>\n  <value>rs-legacy_java</value>\n  <description>\n    Comma separated raw coder implementations for the rs-legacy codec. The earlier\n    factory is prior to followings in case of failure of creating raw coders.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.assumed.role.session.duration</name>\n  <value>40m</value>\n  <description>\n    Duration of assumed roles before a refresh is attempted.\n    Used when session tokens are requested.\n    Range: 15m to 1h\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.connection.ssl.enabled</name>\n  <value>false</value>\n  <description>Enables or disables SSL connections to AWS services.\n    Also sets the default port to use for the s3a proxy settings,\n    when not explicitly set in fs.s3a.proxy.port.</description>\n</property>\n\n<property>\n  <name>fs.s3a.metadatastore.fail.on.write.error</name>\n  <value>true</value>\n  <description>\n    When true (default), FileSystem write operations generate\n    org.apache.hadoop.fs.s3a.MetadataPersistenceException if the metadata\n    cannot be saved to the metadata store.  When false, failures to save to\n    metadata store are logged at ERROR level, but the overall FileSystem\n    write operation succeeds.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.retry.limit</name>\n  <value>14</value>\n  <description>\n    Number of times to retry any repeatable S3 client request on failure,\n    excluding throttling requests and S3Guard inconsistency resolution.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.select.input.csv.record.delimiter</name>\n  <value>\\n</value>\n  <description>In S3 Select queries over CSV files: the record delimiter.\n    \\t is remapped to the TAB character, \\r to CR \\n to newline. \\\\ to \\\n    and \\\" to \"\n  </description>\n</property>\n\n<property>\n  <name>ftp.bytes-per-checksum</name>\n  <value>512</value>\n  <description>The number of bytes per checksum.  Must not be larger than\n  ftp.stream-buffer-size</description>\n</property>\n\n<property>\n  <name>fs.defaultFS</name>\n  <value>/</value>\n  <description>The name of the default file system.  A URI whose\n  scheme and authority determine the FileSystem implementation.  The\n  uri's scheme determines the config property (fs.SCHEME.impl) naming\n  the FileSystem implementation class.  The uri's authority is used to\n  determine the host, port, etc. for a filesystem.</description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": false}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.client.https.keystore.resource</name>\n  <value>ssl-client.xml</value>\n  <description>Resource file from which ssl client keystore\n  information will be extracted\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.shared.edits.dir</name>\n  <value>/valid/dir1</value>\n  <description>A directory on shared storage between the multiple namenodes\n  in an HA cluster. This directory will be written by the active and read\n  by the standby in order to keep the namespaces synchronized. This directory\n  does not need to be listed in dfs.namenode.edits.dir above. It should be\n  left empty in a non-HA cluster.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.keytab.file</name>\n  <value>/valid/file1</value>\n  <description>\n    The keytab file used by each NameNode daemon to login as its\n    service principal. The principal name is configured with\n    dfs.namenode.kerberos.principal.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.fileio.profiling.sampling.percentage</name>\n  <value>-1</value>\n  <description>\n    This setting controls the percentage of file I/O events which will be\n    profiled for DataNode disk statistics. The default value of 0 disables\n    disk statistics. Set to an integer value between 1 and 100 to enable disk\n    statistics.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.deadnode.detection.probe.deadnode.threads</name>\n  <value>1</value>\n    <description>\n      The maximum number of threads to use for probing dead node.\n    </description>\n</property>\n\n<property>\n  <name>dfs.namenode.storageinfo.defragment.timeout.ms</name>\n  <value>1</value>\n  <description>\n    Timeout value in ms for the StorageInfo compaction run.\n  </description>\n</property>\n\n<property>\n  <name>dfs.qjournal.http.open.timeout.ms</name>\n  <value>30000</value>\n  <description>\n    Timeout in milliseconds when open a new HTTP connection to remote\n    journals.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.lease-hard-limit-sec</name>\n  <value>2400</value>\n    <description>\n      Determines the namenode automatic lease recovery interval in seconds.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.search.attr.group.name</name>\n  <value>cn</value>\n  <description>\n    The attribute of the group object that identifies the group name. The\n    default will usually be appropriate for all LDAP systems.\n  </description>\n</property>\n\n<property>\n  <name>fs.AbstractFileSystem.swebhdfs.impl</name>\n  <value>org.apache.hadoop.fs.SWebHdfs</value>\n  <description>The FileSystem for swebhdfs: uris.</description>\n</property>\n\n<property>\n  <name>fs.s3a.block.size</name>\n  <value>16M</value>\n  <description>Block size to use when reading files using s3a: file system.\n    A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.select.input.csv.quote.escape.character</name>\n  <value>\\\\</value>\n  <description>In S3 Select queries over CSV files: quote escape character.\n    \\t is remapped to the TAB character, \\r to CR \\n to newline. \\\\ to \\\n    and \\\" to \"\n  </description>\n</property>\n\n<property>\n  <name>ftp.bytes-per-checksum</name>\n  <value>1024</value>\n  <description>The number of bytes per checksum.  Must not be larger than\n  ftp.stream-buffer-size</description>\n</property>\n\n<property>\n  <name>ha.health-monitor.connect-retry-interval.ms</name>\n  <value>1000</value>\n  <description>\n    How often to retry connecting to the service.\n  </description>\n</property>\n\n<property>\n  <name>ha.failover-controller.graceful-fence.rpc-timeout.ms</name>\n  <value>2500</value>\n  <description>\n    Timeout that the FC waits for the old active to go to standby\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.kms.client.authentication.retry-count</name>\n  <value>2</value>\n  <description>\n    Number of time to retry connecting to KMS on authentication failure\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    \n<property>\n  <name>yarn.resourcemanager.configuration.provider-class</name>\n  <value>org.apache.hadoop.yarn.LocalConfigurationProvider</value>\n    <description>The class to use as the configuration provider.\n    If org.apache.hadoop.yarn.LocalConfigurationProvider is used,\n    the local configuration will be loaded.\n    If org.apache.hadoop.yarn.FileSystemBasedConfigurationProvider is used,\n    the configuration which will be loaded should be uploaded to remote File system first.\n    </description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.container-localizer.log.level</name>\n  <value>INFO</value>\n    <description>\n      The log level for container localizer while it is an independent process.\n    </description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.linux-container-executor.resources-handler.class</name>\n  <value>org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler</value>\n    <description>The class which should help the LCE handle resources.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.windows-container.memory-limit.enabled</name>\n  <value>true</value>\n    <description>This flag determines whether memory limit will be set for the Windows Job\n    Object of the containers launched by the default container executor.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.resourcemanager.minimum.version</name>\n  <value>NONE</value>\n    <description>The minimum allowed version of a resourcemanager that a nodemanager will connect to.  \n      The valid values are NONE (no version checking), EqualToNM (the resourcemanager's version is \n      equal to or greater than the NM version), or a Version String.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.container-retry-minimum-interval-ms</name>\n  <value>500</value>\n    <description>Minimum container restart interval in milliseconds.</description>\n</property>\n\n<property>\n  <name>yarn.is.minicluster</name>\n  <value>false</value>\n    <description>\n    Set to true for MiniYARNCluster unit tests\n    </description>\n</property>\n\n<property>\n  <name>yarn.client.load.resource-types.from-server</name>\n  <value>false</value>\n    <description>\n      Provides an option for client to load supported resource types from RM\n      instead of depending on local resource-types.xml file.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "yarn", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    \n<property>\n  <name>yarn.nodemanager.container-executor.exit-code-file.timeout-ms</name>\n  <value>2000</value>\n    <description>\n      How long the container executor should wait for the exit code file to\n      appear after a reacquired container has exited.\n    </description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.local-cache.max-files-per-directory</name>\n  <value>16384</value>\n    <description>It limits the maximum number of files which will be localized\n      in a single local directory. If the limit is reached then sub-directories\n      will be created and new files will be localized in them. If it is set to\n      a value less than or equal to 36 [which are sub-directories (0-9 and then\n      a-z)] then NodeManager will fail to start. For example; [for public\n      cache] if this is configured with a value of 40 ( 4 files +\n      36 sub-directories) and the local-dir is \"/tmp/local-dir1\" then it will\n      allow 4 files to be created directly inside \"/tmp/local-dir1/filecache\".\n      For files that are localized further it will create a sub-directory \"0\"\n      inside \"/tmp/local-dir1/filecache\" and will localize files inside it\n      until it becomes full. If a file is removed from a sub-directory that\n      is marked full, then that sub-directory will be used back again to\n      localize files.\n   </description>\n</property>\n\n<property>\n  <name>yarn.log-aggregation-status.time-out.ms</name>\n  <value>300000</value>\n    <description>\n    How long for ResourceManager to wait for NodeManager to report its\n    log aggregation status. If waiting time of which the log aggregation\n    status is reported from NodeManager exceeds the configured value, RM\n    will report log aggregation status for this NodeManager as TIME_OUT.\n    This configuration will be used in NodeManager as well to decide\n    whether and when to delete the cached log aggregation status.\n    </description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.vmem-pmem-ratio</name>\n  <value>4.2</value>\n    <description>Ratio between virtual memory to physical memory when\n    setting memory limits for containers. Container allocations are\n    expressed in terms of physical memory, and virtual memory usage\n    is allowed to exceed this allocation by this ratio.\n    </description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.health-checker.script.timeout-ms</name>\n  <value>1200000</value>\n    <description>Script time out period.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds.min</name>\n  <value>3600</value>\n    <description>Defines the positive minimum hard limit for\n    \"yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds\".\n    If this configuration has been set less than its default value (3600)\n    the NodeManager may raise a warning.\n    </description>\n</property>\n\n<property>\n  <name>yarn.timeline-service.webapp.rest-csrf.custom-header</name>\n  <value>X-XSRF-Header</value>\n    <description>\n      Optional parameter that indicates the custom header name to use for CSRF\n      protection.\n    </description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.numa-awareness.enabled</name>\n  <value>false</value>\n    <description>\n    Whether to enable the NUMA awareness for containers in Node Manager.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "yarn", "is_synthetic": true}}, {"input": "minSessionTimeout=4000000000\n\nlocalSessionsUpgradingEnabled=true\n\nsyncLimit=10\n\nautopurge.snapRetainCount=1\n\nclientPort=3001\n\nelectionAlg=6\n\nstandaloneEnabled=true\n\nmaxClientCnxns=120\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.namenode.max-corrupt-file-blocks-returned</name>\n  <value>200</value>\n  <description>\n      The maximum number of corrupt file blocks listed by NameNode Web UI,\n      JMX and other client request.\n  </description>\n</property>\n\n<property>\n  <name>dfs.blockreport.split.threshold</name>\n  <value>500000</value>\n    <description>If the number of blocks on the DataNode is below this\n    threshold then it will send block reports for all Storage Directories\n    in a single message.\n\n    If the number of blocks exceeds this threshold then the DataNode will\n    send block reports for each Storage Directory in separate messages.\n\n    Set to zero to always split.\n    </description>\n</property>\n\n<property>\n  <name>dfs.datanode.directoryscan.interval</name>\n  <value>1s</value>\n  <description>Interval in seconds for Datanode to scan data directories and\n  reconcile the difference between blocks in memory and on the disk.\n  Support multiple time unit suffix(case insensitive), as described\n  in dfs.heartbeat.interval.If no time unit is specified then seconds\n  is assumed.\n  </description>\n</property>\n\n<property>\n  <name>dfs.bytes-per-checksum</name>\n  <value>256</value>\n  <description>The number of bytes per checksum.  Must not be larger than\n  dfs.stream-buffer-size</description>\n</property>\n\n<property>\n  <name>dfs.namenode.list.cache.directives.num.responses</name>\n  <value>200</value>\n  <description>\n    This value controls the number of cache directives that the NameNode will\n    send over the wire in response to a listDirectives RPC.\n  </description>\n</property>\n\n<property>\n  <name>dfs.webhdfs.socket.read-timeout</name>\n  <value>60s</value>\n  <description>\n    Socket timeout for reading data from WebHDFS servers. This\n    prevents a WebHDFS client from hanging if the server stops sending\n    data. Value is followed by a unit specifier: ns, us, ms, s, m, h,\n    d for nanoseconds, microseconds, milliseconds, seconds, minutes,\n    hours, days respectively. Values should provide units,\n    but milliseconds are assumed.\n  </description>\n</property>\n\n<property>\n  <name>dfs.webhdfs.netty.low.watermark</name>\n  <value>32768</value>\n  <description>\n    Low watermark configuration to Netty for Datanode WebHdfs.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.blockreport.queue.size</name>\n  <value>true</value>\n    <description>\n      The queue size of BlockReportProcessingThread in BlockManager.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.master.info.port</name>\n  <value>3001</value>\n    <description>The port for the HBase Master web UI.\n    Set to -1 if you do not want a UI instance run.</description>\n</property>\n\n<property>\n  <name>hbase.zookeeper.leaderport</name>\n  <value>1944</value>\n    <description>Port used by ZooKeeper for leader election.\n    See https://zookeeper.apache.org/doc/r3.3.3/zookeeperStarted.html#sc_RunningReplicatedZooKeeper\n    for more information.</description>\n</property>\n\n<property>\n  <name>hbase.zookeeper.property.dataDir</name>\n  <value>/valid/file2</value>\n    <description>Property from ZooKeeper's config zoo.cfg.\n    The directory where the snapshot is stored.</description>\n</property>\n\n<property>\n  <name>hbase.zookeeper.property.clientPort</name>\n  <value>file://</value>\n    <description>Property from ZooKeeper's config zoo.cfg.\n    The port at which the clients will connect.</description>\n</property>\n\n<property>\n  <name>hbase.bulkload.retries.number</name>\n  <value>20</value>\n    <description>Maximum retries.  This is maximum number of iterations\n    to atomic bulk loads are attempted in the face of splitting operations\n    0 means never give up.</description>\n</property>\n\n<property>\n  <name>hfile.index.block.max.size</name>\n  <value>131072</value>\n      <description>When the size of a leaf-level, intermediate-level, or root-level\n          index block in a multi-level block index grows to this size, the\n          block is written out and a new block is started.</description>\n</property>\n\n<property>\n  <name>hbase.auth.key.update.interval</name>\n  <value>172800000</value>\n    <description>The update interval for master key for authentication tokens\n    in servers in milliseconds.  Only used when HBase security is enabled.</description>\n</property>\n\n<property>\n  <name>hbase.data.umask.enable</name>\n  <value>true</value>\n    <description>Enable, if true, that file permissions should be assigned\n      to the files written by the regionserver</description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "quorum.auth.enableSasl=false\n\nstandaloneEnabled=true\n\ninitLimit=10\n\nclientPortAddress=0.0.0.0:3001\n\nportUnification=false\n\npeerType=participant\n\nclientPort=3001\n\nserver.1=127.0.0.1:1234;1235\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": false}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.namenode.lifeline.rpc-address</name>\n  <value>127.0.0.1</value>\n  <description>\n    NameNode RPC lifeline address.  This is an optional separate RPC address\n    that can be used to isolate health checks and liveness to protect against\n    resource exhaustion in the main RPC handler pool.  In the case of\n    HA/Federation where multiple NameNodes exist, the name service ID is added\n    to the name e.g. dfs.namenode.lifeline.rpc-address.ns1.  The value of this\n    property will take the form of nn-host1:rpc-port.  If this property is not\n    defined, then the NameNode will not start a lifeline RPC server.  By\n    default, the property is not defined.\n  </description>\n</property>\n\n<property>\n  <name>nfs.server.port</name>\n  <value>65536</value>\n  <description>\n      Specify the port number used by Hadoop NFS.\n  </description>\n</property>\n\n<property>\n  <name>nfs.rtmax</name>\n  <value>2097152</value>\n  <description>This is the maximum size in bytes of a READ request\n    supported by the NFS gateway. If you change this, make sure you\n    also update the nfs mount's rsize(add rsize= # of bytes to the \n    mount directive).\n  </description>\n</property>\n\n<property>\n  <name>dfs.block.local-path-access.user</name>\n  <value>samsuper</value>\n  <description>\n    Comma separated list of the users allowed to open block files\n    on legacy short-circuit local read.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.quota.init-threads</name>\n  <value>8</value>\n  <description>\n    The number of concurrent threads to be used in quota initialization. The\n    speed of quota initialization also affects the namenode fail-over latency.\n    If the size of name space is big, try increasing this.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.read.striped.threadpool.size</name>\n  <value>36</value>\n  <description>\n    The maximum number of threads used for parallel reading\n    in striped layout.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.write.max-packets-in-flight</name>\n  <value>80</value>\n  <description>\n    The maximum number of DFSPackets allowed in flight.\n  </description>\n</property>\n\n<property>\n  <name>dfs.disk.balancer.plan.threshold.percent</name>\n  <value>20</value>\n    <description>\n      The percentage threshold value for volume Data Density in a plan.\n      If the absolute value of volume Data Density which is out of\n      threshold value in a node, it means that the volumes corresponding to\n      the disks should do the balancing in the plan. The default value is 10.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.namenode.heartbeat.recheck-interval</name>\n  <value>600000</value>\n  <description>\n    This time decides the interval to check for expired datanodes.\n    With this value and dfs.heartbeat.interval, the interval of\n    deciding the datanode is stale or not is also calculated.\n    The unit of this configuration is millisecond.\n  </description>\n</property>\n\n<property>\n  <name>dfs.default.chunk.view.size</name>\n  <value>16384</value>\n  <description>The number of bytes to view for a file on the browser.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.name.dir.restore</name>\n  <value>true</value>\n  <description>Set to true to enable NameNode to attempt recovering a\n      previously failed dfs.namenode.name.dir. When enabled, a recovery of any\n      failed directory is attempted during checkpoint.</description>\n</property>\n\n<property>\n  <name>dfs.namenode.fs-limits.max-blocks-per-file</name>\n  <value>20000</value>\n    <description>Maximum number of blocks per file, enforced by the Namenode on\n        write. This prevents the creation of extremely large files which can\n        degrade performance.</description>\n</property>\n\n<property>\n  <name>dfs.journalnode.https-bind-host</name>\n  <value>256.256.256.256</value>\n  <description>\n    The actual address the HTTP server will bind to. If this optional address\n    is set, it overrides only the hostname portion of\n    dfs.journalnode.https-address. This is useful for making the JournalNode\n    HTTP server listen on all interfaces by setting it to 0.0.0.0.\n  </description>\n</property>\n\n<property>\n  <name>dfs.cachereport.intervalMsec</name>\n  <value>5000</value>\n  <description>\n    Determines cache reporting interval in milliseconds.  After this amount of\n    time, the DataNode sends a full report of its cache state to the NameNode.\n    The NameNode uses the cache report to update its map of cached blocks to\n    DataNode locations.\n\n    This configuration has no effect if in-memory caching has been disabled by\n    setting dfs.datanode.max.locked.memory to 0 (which is the default).\n\n    If the native libraries are not available to the DataNode, this\n    configuration has no effect.\n  </description>\n</property>\n\n<property>\n  <name>dfs.user.home.dir.prefix</name>\n  <value>/valid/file2</value>\n  <description>The directory to prepend to user name to get the user's\n    home direcotry.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.transfer.socket.send.buffer.size</name>\n  <value>0</value>\n  <description>\n    Socket send buffer size for DataXceiver (mirroring packets to downstream\n    in pipeline). This may affect TCP connection throughput.\n    If it is set to zero or negative value, no buffer size will be set\n    explicitly, thus enable tcp auto-tuning on some system.\n    The default value is 0.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "secureClientPort=3001\n\nstandaloneEnabled=false\n\nsslQuorumReloadCertFiles=true\n\nmaxSessionTimeout=8000\n\nsyncLimit=10\n\nmaxClientCnxns=120\n\ndataDir=/tmp/test\n\nsnapDir=\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": false}}, {"input": "alluxio.job.worker.web.port=file://\n\nalluxio.worker.management.tier.promote.quota.percent=90\n\nalluxio.underfs.oss.connection.ttl=-2\n\nalluxio.network.host.resolution.timeout=10sec\n\nalluxio.jvm.monitor.info.threshold=10sec\n\nalluxio.zookeeper.election.path=/valid/file2\n\nalluxio.user.block.master.client.pool.gc.threshold=1sec\n\nalluxio.user.file.sequential.pread.threshold=4MB\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "CSRF_COOKIE_PATH=tmp////staging\n\nEMAIL_SUBJECT_PREFIX='[Django] '\n\nCSRF_TRUSTED_ORIGINS=[]\n\nUSE_I18N=True\n\nYEAR_MONTH_FORMAT='F Y'\n\nUSE_X_FORWARDED_PORT=True\n\nCSRF_COOKIE_HTTPONLY=False\n\nEMAIL_SSL_CERTFILE=None\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "django", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.rootdir</name>\n  <value>tmp////staging</value>\n    <description>The directory shared by region servers and into\n    which HBase persists.  The URL should be 'fully-qualified'\n    to include the filesystem scheme.  For example, to specify the\n    HDFS directory '/hbase' where the HDFS instance's namenode is\n    running at namenode.example.org on port 9000, set this value to:\n    hdfs://namenode.example.org:9000/hbase.  By default, we write\n    to whatever ${hbase.tmp.dir} is set too -- usually /tmp --\n    so change this configuration or else all data will be lost on\n    machine restart.</description>\n</property>\n\n<property>\n  <name>hbase.local.dir</name>\n  <value>/valid/file1</value>\n    <description>Directory on the local filesystem to be used\n    as a local storage.</description>\n</property>\n\n<property>\n  <name>hbase.master.logcleaner.ttl</name>\n  <value>300000</value>\n    <description>How long a WAL remain in the archive ({hbase.rootdir}/oldWALs) directory,\n    after which it will be cleaned by a Master thread. The value is in milliseconds.</description>\n</property>\n\n<property>\n  <name>hbase.thrift.maxWorkerThreads</name>\n  <value>500</value>\n    <description>The maximum size of the thread pool. When the pending request queue\n    overflows, new threads are created until their number reaches this number.\n    After that, the server starts dropping connections.</description>\n</property>\n\n<property>\n  <name>hbase.thrift.maxQueuedRequests</name>\n  <value>2000</value>\n    <description>The maximum number of pending Thrift connections waiting in the queue. If\n     there are no idle threads in the pool, the server queues requests. Only\n     when the queue overflows, new threads are added, up to\n     hbase.thrift.maxQueuedRequests threads.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.thrift.framed</name>\n  <value>true</value>\n    <description>Use Thrift TFramedTransport on the server side.\n      This is the recommended transport for thrift servers and requires a similar setting\n      on the client side. Changing this to false will select the default transport,\n      vulnerable to DoS when malformed requests are issued due to THRIFT-601.\n    </description>\n</property>\n\n<property>\n  <name>hbase.status.multicast.address.ip</name>\n  <value>127.0.0.1</value>\n    <description>\n      Multicast address to use for the status publication by multicast.\n    </description>\n</property>\n\n<property>\n  <name>hbase.mob.file.cache.size</name>\n  <value>500</value>\n    <description>\n      Number of opened file handlers to cache.\n      A larger value will benefit reads by providing more file handlers per mob\n      file cache and would reduce frequent file opening and closing.\n      However, if this is set too high, this could lead to a \"too many opened file handlers\"\n      The default value is 1000.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "ssl=off\n\nssl_ecdh_curve=prime256v1'\n\nwal_compression=off\n\ntcp_keepalives_idle=0\n\ntimezone_abbreviations='Default'\n\nmaintenance_work_mem=1MB\n\ncommit_siblings=1\n\ngeqo_generations=1\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "postgresql", "is_synthetic": true}}, {"input": "alluxio.master.backup.connect.interval.min=1sec\n\nalluxio.master.embedded.journal.addresses=127.0.0.1\n\nalluxio.user.network.streaming.keepalive.timeout=60sec\n\nalluxio.master.backup.state.lock.interrupt.cycle.enabled=false\n\nalluxio.job.master.hostname=${alluxio.master.hostname}\n\nalluxio.master.persistence.max.total.wait.time=10day\n\nalluxio.master.metastore.inode.inherit.owner.and.group=true\n\nalluxio.zookeeper.leader.path=/valid/file2\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>fs.s3a.select.output.csv.quote.fields</name>\n  <value>always</value>\n  <description>\n    In S3 Select queries: should fields in generated CSV Files be quoted?\n    One of: \"always\", \"asneeded\".\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.etag.checksum.enabled</name>\n  <value>true</value>\n  <description>\n    Should calls to getFileChecksum() return the etag value of the remote\n    object.\n    WARNING: if enabled, distcp operations between HDFS and S3 will fail unless\n    -skipcrccheck is set.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3.sleepTimeSeconds</name>\n  <value>15</value>\n  <description>The number of seconds to sleep between each S3 retry.\n  </description>\n</property>\n\n<property>\n  <name>ipc.client.connection.maxidletime</name>\n  <value>20000</value>\n  <description>The maximum time in msec after which a client will bring down the\n               connection to the server.\n  </description>\n</property>\n\n<property>\n  <name>ha.failover-controller.graceful-fence.connection.retries</name>\n  <value>2</value>\n  <description>\n    FC connection retries for graceful fencing\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.crypto.cipher.suite</name>\n  <value>AES/CTR/NoPadding</value>\n  <description>\n    Cipher suite for crypto codec.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.java.secure.random.algorithm</name>\n  <value>SHA1PRNG</value>\n  <description>\n    The java secure random algorithm.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.tags.system</name>\n  <value>HDFS</value>\n    <description>\n      System tags to group related properties together.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "set-max-listpack-value=64\n\ncluster-announce-port=1\n\nenable-debug-command=no\n\ntls-session-cache-size=2500\n\nlatency-monitor-threshold=0\n\ndir=./\n\nproto-max-bulk-len=512mb\n\nstream-node-max-entries=100\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.client.local.interfaces</name>\n  <value>eth2</value>\n  <description>A comma separated list of network interface names to use\n    for data transfer between the client and datanodes. When creating\n    a connection to read from or write to a datanode, the client\n    chooses one of the specified interfaces at random and binds its\n    socket to the IP of that interface. Individual names may be\n    specified as either an interface name (eg \"eth0\"), a subinterface\n    name (eg \"eth0:0\"), or an IP address (which may be specified using\n    CIDR notation to match a range of IPs).\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.fileio.profiling.sampling.percentage</name>\n  <value>0</value>\n  <description>\n    This setting controls the percentage of file I/O events which will be\n    profiled for DataNode disk statistics. The default value of 0 disables\n    disk statistics. Set to an integer value between 1 and 100 to enable disk\n    statistics.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.caching.enabled</name>\n  <value>true</value>\n  <description>\n    Set to true to enable block caching.  This flag enables the NameNode to\n    maintain a mapping of cached blocks to DataNodes via processing DataNode\n    cache reports.  Based on these reports and addition and removal of caching\n    directives, the NameNode will schedule caching and uncaching work.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.inotify.max.events.per.rpc</name>\n  <value>2000</value>\n  <description>Maximum number of events that will be sent to an inotify client\n    in a single RPC response. The default value attempts to amortize away\n    the overhead for this RPC while avoiding huge memory requirements for the\n    client and NameNode (1000 events should consume no more than 1 MB.)\n  </description>\n</property>\n\n<property>\n  <name>datanode.https.port</name>\n  <value>-1</value>\n  <description>\n    HTTPS port for DataNode.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.key.provider.cache.expiry</name>\n  <value>432000000</value>\n  <description>\n    DFS client security key cache expiration in milliseconds.\n  </description>\n</property>\n\n<property>\n  <name>dfs.content-summary.limit</name>\n  <value>5000</value>\n  <description>\n    The maximum content summary counts allowed in one locking period. 0 or a negative number\n    means no limit (i.e. no yielding).\n  </description>\n</property>\n\n<property>\n  <name>dfs.storage.policy.satisfier.self.retry.timeout.millis</name>\n  <value>150000</value>\n  <description>\n    If any of file related block movements not at all reported by datanode,\n    then after this timeout(in milliseconds), the item will be added back to movement needed list\n    at namenode which will be retried for block movements.\n    The default value is 5 * 60 * 1000 (5 mins)\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.ipc.server.callqueue.scan.ratio</name>\n  <value>0</value>\n    <description>Given the number of read call queues, calculated from the total number\n      of call queues multiplied by the callqueue.read.ratio, the scan.ratio property\n      will split the read call queues into small-read and long-read queues.\n      A value lower than 0.5 means that there will be less long-read queues than short-read queues.\n      A value of 0.5 means that there will be the same number of short-read and long-read queues.\n      A value greater than 0.5 means that there will be more long-read queues than short-read queues\n      A value of 0 or 1 indicate to use the same set of queues for gets and scans.\n\n      Example: Given the total number of read call queues being 8\n      a scan.ratio of 0 or 1 means that: 8 queues will contain both long and short read requests.\n      a scan.ratio of 0.3 means that: 2 queues will contain only long-read requests\n      and 6 queues will contain only short-read requests.\n      a scan.ratio of 0.5 means that: 4 queues will contain only long-read requests\n      and 4 queues will contain only short-read requests.\n      a scan.ratio of 0.8 means that: 6 queues will contain only long-read requests\n      and 2 queues will contain only short-read requests.\n    </description>\n</property>\n\n<property>\n  <name>hbase.client.max.total.tasks</name>\n  <value>60</value>\n    <description>The maximum number of concurrent mutation tasks a single HTable instance will\n    send to the cluster.</description>\n</property>\n\n<property>\n  <name>hbase.client.max.perregion.tasks</name>\n  <value>100</value>\n    <description>The maximum number of concurrent mutation tasks the client will\n    maintain to a single Region. That is, if there is already\n    hbase.client.max.perregion.tasks writes in progress for this region, new puts\n    won't be sent to this region until some writes finishes.</description>\n</property>\n\n<property>\n  <name>hbase.snapshot.restore.take.failsafe.snapshot</name>\n  <value>true</value>\n    <description>Set to true to take a snapshot before the restore operation.\n      The snapshot taken will be used in case of failure, to restore the previous state.\n      At the end of the restore operation this snapshot will be deleted</description>\n</property>\n\n<property>\n  <name>hbase.status.multicast.address.ip</name>\n  <value>127.0.0.1</value>\n    <description>\n      Multicast address to use for the status publication by multicast.\n    </description>\n</property>\n\n<property>\n  <name>hbase.http.filter.initializers</name>\n  <value>org.apache.hadoop.hbase.http.lib.StaticUserWebFilter</value>\n    <description>\n      A comma separated list of class names. Each class in the list must extend\n      org.apache.hadoop.hbase.http.FilterInitializer. The corresponding Filter will\n      be initialized. Then, the Filter will be applied to all user facing jsp\n      and servlet web pages.\n      The ordering of the list defines the ordering of the filters.\n      The default StaticUserWebFilter add a user principal as defined by the\n      hbase.http.staticuser.user property.\n    </description>\n</property>\n\n<property>\n  <name>hbase.mob.cache.evict.period</name>\n  <value>1800</value>\n    <description>\n      The amount of time in seconds before the mob cache evicts cached mob files.\n      The default value is 3600 seconds.\n    </description>\n</property>\n\n<property>\n  <name>hbase.mob.compaction.batch.size</name>\n  <value>50</value>\n    <description>\n      The max number of the mob files that is allowed in a batch of the mob compaction.\n      The mob compaction merges the small mob files to bigger ones. If the number of the\n      small files is very large, it could lead to a \"too many opened file handlers\" in the merge.\n      And the merge has to be split into batches. This value limits the number of mob files\n      that are selected in a batch of the mob compaction. The default value is 100.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.regionserver.logroll.errors.tolerated</name>\n  <value>4</value>\n    <description>The number of consecutive WAL close errors we will allow\n    before triggering a server abort.  A setting of 0 will cause the\n    region server to abort if closing the current WAL writer fails during\n    log rolling.  Even a small value (2 or 3) will allow a region server\n    to ride over transient HDFS errors.</description>\n</property>\n\n<property>\n  <name>hbase.hregion.memstore.mslab.enabled</name>\n  <value>false</value>\n    <description>\n      Enables the MemStore-Local Allocation Buffer,\n      a feature which works to prevent heap fragmentation under\n      heavy write loads. This can reduce the frequency of stop-the-world\n      GC pauses on large heaps.</description>\n</property>\n\n<property>\n  <name>hbase.hstore.compaction.ratio.offpeak</name>\n  <value>5.0F</value>\n    <description>Allows you to set a different (by default, more aggressive) ratio for determining\n      whether larger StoreFiles are included in compactions during off-peak hours. Works in the\n      same way as hbase.hstore.compaction.ratio. Only applies if hbase.offpeak.start.hour and\n      hbase.offpeak.end.hour are also enabled.</description>\n</property>\n\n<property>\n  <name>hbase.hstore.time.to.purge.deletes</name>\n  <value>1</value>\n    <description>The amount of time to delay purging of delete markers with future timestamps. If\n      unset, or set to 0, all delete markers, including those with future timestamps, are purged\n      during the next major compaction. Otherwise, a delete marker is kept until the major compaction\n      which occurs after the marker's timestamp plus the value of this setting, in milliseconds.\n    </description>\n</property>\n\n<property>\n  <name>hbase.rest.port</name>\n  <value>3001</value>\n    <description>The port for the HBase REST server.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.thrift.compact</name>\n  <value>true</value>\n    <description>Use Thrift TCompactProtocol binary serialization protocol.</description>\n</property>\n\n<property>\n  <name>hbase.snapshot.restore.failsafe.name</name>\n  <value>hbase-failsafe-{snapshot.name}-{restore.timestamp}</value>\n    <description>Name of the failsafe snapshot taken by the restore operation.\n      You can use the {snapshot.name}, {table.name} and {restore.timestamp} variables\n      to create a name based on what you are restoring.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.storefile.refresh.period</name>\n  <value>-1</value>\n    <description>\n      The period (in milliseconds) for refreshing the store files for the secondary regions. 0\n      means this feature is disabled. Secondary regions sees new files (from flushes and\n      compactions) from primary once the secondary region refreshes the list of files in the\n      region (there is no notification mechanism). But too frequent refreshes might cause\n      extra Namenode pressure. If the files cannot be refreshed for longer than HFile TTL\n      (hbase.master.hfilecleaner.ttl) the requests are rejected. Configuring HFile TTL to a larger\n      value is also recommended with this setting.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.regionserver.handler.count</name>\n  <value>60</value>\n    <description>Count of RPC Listener instances spun up on RegionServers.\n      Same property is used by the Master for count of master handlers.\n      Too many handlers can be counter-productive. Make it a multiple of\n      CPU count. If mostly read-only, handlers count close to cpu count\n      does well. Start with twice the CPU count and tune from there.</description>\n</property>\n\n<property>\n  <name>hbase.client.scanner.timeout.period</name>\n  <value>60000</value>\n    <description>Client scanner lease period in milliseconds.</description>\n</property>\n\n<property>\n  <name>hbase.hregion.memstore.flush.size</name>\n  <value>268435456</value>\n    <description>\n    Memstore will be flushed to disk if size of the memstore\n    exceeds this number of bytes.  Value is checked by a thread that runs\n    every hbase.server.thread.wakefrequency.</description>\n</property>\n\n<property>\n  <name>hbase.hregion.majorcompaction</name>\n  <value>302400000</value>\n    <description>Time between major compactions, expressed in milliseconds. Set to 0 to disable\n      time-based automatic major compactions. User-requested and size-based major compactions will\n      still run. This value is multiplied by hbase.hregion.majorcompaction.jitter to cause\n      compaction to start at a somewhat-random time during a given window of time. The default value\n      is 7 days, expressed in milliseconds. If major compactions are causing disruption in your\n      environment, you can configure them to run at off-peak times for your deployment, or disable\n      time-based major compactions by setting this parameter to 0, and run major compactions in a\n      cron job or by another external mechanism.</description>\n</property>\n\n<property>\n  <name>hbase.rest.support.proxyuser</name>\n  <value>false</value>\n    <description>Enables running the REST server to support proxy-user mode.</description>\n</property>\n\n<property>\n  <name>hbase.thrift.maxWorkerThreads</name>\n  <value>2000</value>\n    <description>The maximum size of the thread pool. When the pending request queue\n    overflows, new threads are created until their number reaches this number.\n    After that, the server starts dropping connections.</description>\n</property>\n\n<property>\n  <name>hbase.region.replica.replication.enabled</name>\n  <value>false</value>\n    <description>\n      Whether asynchronous WAL replication to the secondary region replicas is enabled or not.\n      If this is enabled, a replication peer named \"region_replica_replication\" will be created\n      which will tail the logs and replicate the mutations to region replicas for tables that\n      have region replication > 1. If this is enabled once, disabling this replication also\n      requires disabling the replication peer using shell or Admin java class.\n      Replication to secondary region replicas works over standard inter-cluster replication.\n    </description>\n</property>\n\n<property>\n  <name>hbase.snapshot.region.timeout</name>\n  <value>150000</value>\n    <description>\n       Timeout for regionservers to keep threads in snapshot request pool waiting.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.ha.tail-edits.rolledits.timeout</name>\n  <value>60</value>\n  <description>The timeout in seconds of calling rollEdits RPC on Active NN.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.mmap.cache.size</name>\n  <value>256</value>\n  <description>\n    When zero-copy reads are used, the DFSClient keeps a cache of recently used\n    memory mapped regions.  This parameter controls the maximum number of\n    entries that we will keep in that cache.\n\n    The larger this number is, the more file descriptors we will potentially\n    use for memory-mapped files.  mmaped files also use virtual address space.\n    You may need to increase your ulimit virtual address space limits before\n    increasing the client mmap cache size.\n\n    Note that you can still do zero-copy reads when this size is set to 0.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.fs-limits.max-xattrs-per-inode</name>\n  <value>64</value>\n  <description>\n    Maximum number of extended attributes per inode.\n  </description>\n</property>\n\n<property>\n  <name>dfs.balancer.max-no-move-interval</name>\n  <value>60000</value>\n  <description>\n    If this specified amount of time has elapsed and no block has been moved\n    out of a source DataNode, on more effort will be made to move blocks out of\n    this DataNode in the current Balancer iteration.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.socketcache.capacity</name>\n  <value>32</value>\n  <description>\n    Socket cache capacity (in entries) for short-circuit reads.\n    If this value is set to 0, the client socket cache is disabled.\n  </description>\n</property>\n\n<property>\n  <name>dfs.storage.policy.satisfier.datanode.cache.refresh.interval.ms</name>\n  <value>300000</value>\n  <description>\n    How often to refresh the datanode storages cache in milliseconds. This cache\n    keeps live datanode storage reports fetched from namenode. After elapsed time,\n    it will again fetch latest datanodes from namenode.\n    By default, this parameter is set to 5 minutes.\n  </description>\n</property>\n\n<property>\n  <name>dfs.provided.aliasmap.text.read.file</name>\n  <value>/valid/file1</value>\n    <description>\n        The path specifying the provided block map as a text file, specified as\n        a URI.\n    </description>\n</property>\n\n<property>\n  <name>dfs.qjm.operations.timeout</name>\n  <value>120s</value>\n    <description>\n      Common key to set timeout for related operations in\n      QuorumJournalManager. This setting supports multiple time unit suffixes\n      as described in dfs.heartbeat.interval.\n      If no suffix is specified then milliseconds is assumed.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.datanode.http.address</name>\n  <value>0.0.0.0:3001</value>\n  <description>\n    The datanode http server address and port.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.handler.count</name>\n  <value>10</value>\n  <description>The number of server threads for the datanode.</description>\n</property>\n\n<property>\n  <name>dfs.domain.socket.disable.interval.seconds</name>\n  <value>1200</value>\n  <description>\n    The interval that a DataNode is disabled for future Short-Circuit Reads,\n    after an error happens during a Short-Circuit Read. Setting this to 0 will\n    not disable Short-Circuit Reads at all after errors happen. Negative values\n    are invalid.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.processcommands.threshold</name>\n  <value>2s</value>\n    <description>The threshold in milliseconds at which we will log a slow\n      command processing in BPServiceActor. By default, this parameter is set\n      to 2 seconds.\n    </description>\n</property>\n\n<property>\n  <name>dfs.storage.policy.permissions.superuser-only</name>\n  <value>false</value>\n  <description>\n    Allow only superuser role to change the storage policy on files and\n    directories.\n  </description>\n</property>\n\n<property>\n  <name>dfs.checksum.type</name>\n  <value>CRC32C</value>\n  <description>\n    Checksum type\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.socketcache.capacity</name>\n  <value>32</value>\n  <description>\n    Socket cache capacity (in entries) for short-circuit reads.\n    If this value is set to 0, the client socket cache is disabled.\n  </description>\n</property>\n\n<property>\n  <name>dfs.webhdfs.netty.high.watermark</name>\n  <value>131070</value>\n  <description>\n    High watermark configuration to Netty for Datanode WebHdfs.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.block.access.token.lifetime</name>\n  <value>300</value>\n  <description>The lifetime of access tokens in minutes.</description>\n</property>\n\n<property>\n  <name>dfs.namenode.num.extra.edits.retained</name>\n  <value>2000000</value>\n  <description>The number of extra transactions which should be retained\n  beyond what is minimally necessary for a NN restart.\n  It does not translate directly to file's age, or the number of files kept,\n  but to the number of transactions (here \"edits\" means transactions).\n  One edit file may contain several transactions (edits).\n  During checkpoint, NameNode will identify the total number of edits to retain as extra by\n  checking the latest checkpoint transaction value, subtracted by the value of this property.\n  Then, it scans edits files to identify the older ones that don't include the computed range of\n  retained transactions that are to be kept around, and purges them subsequently.\n  The retainment can be useful for audit purposes or for an HA setup where a remote Standby Node may have\n  been offline for some time and need to have a longer backlog of retained\n  edits in order to start again.\n  Typically each edit is on the order of a few hundred bytes, so the default\n  of 1 million edits should be on the order of hundreds of MBs or low GBs.\n\n  NOTE: Fewer extra edits may be retained than value specified for this setting\n  if doing so would mean that more segments would be retained than the number\n  configured by dfs.namenode.max.extra.edits.segments.retained.\n  </description>\n</property>\n\n<property>\n  <name>dfs.image.transfer.chunksize</name>\n  <value>131072</value>\n  <description>\n        Chunksize in bytes to upload the checkpoint.\n        Chunked streaming is used to avoid internal buffering of contents\n        of image file of huge size.\n        Support multiple size unit suffix(case insensitive), as described\n        in dfs.blocksize.\n  </description>\n</property>\n\n<property>\n  <name>dfs.image.parallel.load</name>\n  <value>false</value>\n  <description>\n        If true, write sub-section entries to the fsimage index so it can\n        be loaded in parallel. Also controls whether parallel loading\n        will be used for an image previously created with sub-sections.\n        If the image contains sub-sections and this is set to false,\n        parallel loading will not be used.\n        Parallel loading is not compatible with image compression,\n        so if dfs.image.compress is set to true this setting will be\n        ignored and no parallel loading will occur.\n        Enabling this feature may impact rolling upgrades and downgrades if\n        the previous version does not support this feature. If the feature was\n        enabled and a downgrade is required, first set this parameter to\n        false and then save the namespace to create a fsimage with no\n        sub-sections and then perform the downgrade.\n  </description>\n</property>\n\n<property>\n  <name>dfs.http.client.failover.sleep.max.millis</name>\n  <value>30000</value>\n  <description>\n    Specify the upper bound of sleep time in milliseconds between\n    retries or failovers for WebHDFS client.\n  </description>\n</property>\n\n<property>\n  <name>dfs.balancer.max-iteration-time</name>\n  <value>2400000</value>\n  <description>\n    Maximum amount of time while an iteration can be run by the Balancer. After\n    this time the Balancer will stop the iteration, and reevaluate the work\n    needs to be done to Balance the cluster. The default value is 20 minutes.\n  </description>\n</property>\n\n<property>\n  <name>dfs.provided.aliasmap.class</name>\n  <value>org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap</value>\n    <description>\n      The class that is used to specify the input format of the blocks on\n      provided storages. The default is\n      org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap which uses\n      file regions to describe blocks. The file regions are specified as a\n      delimited text file. Each file region is a 6-tuple containing the\n      block id, remote file path, offset into file, length of block, the\n      block pool id containing the block, and the generation stamp of the\n      block.\n    </description>\n</property>\n\n<property>\n  <name>dfs.journalnode.edits.dir.perm</name>\n  <value>800</value>\n    <description>\n      Permissions for the directories on on the local filesystem where\n      the DFS journal node stores the edits. The permissions can either be\n      octal or symbolic.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.authorization</name>\n  <value>false</value>\n  <description>Is service-level authorization enabled?</description>\n</property>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.search.attr.member</name>\n  <value>member</value>\n  <description>\n    The attribute of the group object that identifies the users that are\n    members of the group. The default will usually be appropriate for\n    any LDAP installation.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.select.output.csv.quote.fields</name>\n  <value>always</value>\n  <description>\n    In S3 Select queries: should fields in generated CSV Files be quoted?\n    One of: \"always\", \"asneeded\".\n  </description>\n</property>\n\n<property>\n  <name>ipc.[port_number].weighted-cost.lockfree</name>\n  <value>2</value>\n  <description>The weight multiplier to apply to the time spent in the\n    LOCKFREE phase which do not involve holding a lock.\n    See org.apache.hadoop.ipc.ProcessingDetails.Timing for more details on\n    this phase. This property applies to WeightedTimeCostProvider.\n  </description>\n</property>\n\n<property>\n  <name>ftp.blocksize</name>\n  <value>67108864</value>\n  <description>Block size</description>\n</property>\n\n<property>\n  <name>hadoop.http.cross-origin.enabled</name>\n  <value>false</value>\n  <description>Enable/disable the cross-origin (CORS) filter.</description>\n</property>\n\n<property>\n  <name>hadoop.security.crypto.codec.classes.aes.ctr.nopadding</name>\n  <value>org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec</value>\n  <description>\n    Comma-separated list of crypto codec implementations for AES/CTR/NoPadding.\n    The first implementation will be used if available, others are fallbacks.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.tags.system</name>\n  <value>YARN</value>\n    <description>\n      System tags to group related properties together.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "enable-module-command=no\n\nprotected-mode=yes\n\npidfile=/valid/file1.pid\n\ntls-ca-cert-file=ca.crt\n\ntls-port=12758\n\noom-score-adj-values=0 200 800\n\nproc-title-template=\"{title} {listen-addr} {server-mode}\"\n\nstop-writes-on-bgsave-error=yes\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "databases=32\n\nhll-sparse-max-bytes=3000\n\nactiverehashing=yes\n\nproc-title-template=\"{title} {listen-addr} {server-mode}\"\n\ndaemonize=no\n\ncluster-announce-port=2\n\nalways-show-logo=no\n\nrepl-diskless-load=disabled\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.master.info.bindAddress</name>\n  <value>0.0.0.0</value>\n    <description>The bind address for the HBase Master web UI\n    </description>\n</property>\n\n<property>\n  <name>hbase.regionserver.regionSplitLimit</name>\n  <value>1000</value>\n    <description>\n      Limit for the number of regions after which no more region splitting\n      should take place. This is not hard limit for the number of regions\n      but acts as a guideline for the regionserver to stop splitting after\n      a certain limit. Default is set to 1000.\n    </description>\n</property>\n\n<property>\n  <name>hbase.zookeeper.property.maxClientCnxns</name>\n  <value>600</value>\n    <description>Property from ZooKeeper's config zoo.cfg.\n    Limit on number of concurrent connections (at the socket level) that a\n    single client, identified by IP address, may make to a single member of\n    the ZooKeeper ensemble. Set high to avoid zk connection issues running\n    standalone and pseudo-distributed.</description>\n</property>\n\n<property>\n  <name>hbase.client.scanner.timeout.period</name>\n  <value>60000</value>\n    <description>Client scanner lease period in milliseconds.</description>\n</property>\n\n<property>\n  <name>hbase.rpc.timeout</name>\n  <value>120000</value>\n    <description>This is for the RPC layer to define how long (millisecond) HBase client applications\n        take for a remote call to time out. It uses pings to check connections\n        but will eventually throw a TimeoutException.</description>\n</property>\n\n<property>\n  <name>hbase.dynamic.jars.dir</name>\n  <value>//hadoop/io/local</value>\n    <description>\n      The directory from which the custom filter JARs can be loaded\n      dynamically by the region server without the need to restart. However,\n      an already loaded filter/co-processor class would not be un-loaded. See\n      HBASE-1936 for more details.\n\n      Does not apply to coprocessors.\n    </description>\n</property>\n\n<property>\n  <name>hbase.rest.csrf.enabled</name>\n  <value>true</value>\n  <description>\n    Set to true to enable protection against cross-site request forgery (CSRF)\n\t</description>\n</property>\n\n<property>\n  <name>hbase.mob.compactor.class</name>\n  <value>org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactor</value>\n    <description>\n      Implementation of mob compactor, the default one is PartitionedMobCompactor.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "alluxio.master.backup.entry.buffer.count=10000\n\nalluxio.worker.session.timeout=2min\n\nalluxio.master.rpc.addresses=127.0.0.1\n\nalluxio.locality.compare.node.ip=true\n\nalluxio.worker.allocator.class=alluxio.worker.block.allocator.MaxFreeAllocator\n\nalluxio.worker.tieredstore.free.ahead.bytes=1\n\nalluxio.master.metastore.inode.enumerator.buffer.count=20000\n\nalluxio.master.file.access.time.updater.shutdown.timeout=10sec\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "enable_indexscan=on\n\nmax_parallel_apply_workers_per_subscription=4\n\nwal_decode_buffer_size=512kB\n\nlog_statement='none'\n\nclient_encoding=sql_ascii\n\nmax_files_per_process=500\n\nlc_monetary='C'\n\nwork_mem=4MB\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "postgresql", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.regionserver.info.port.auto</name>\n  <value>false</value>\n    <description>Whether or not the Master or RegionServer\n    UI should search for a port to bind to. Enables automatic port\n    search if hbase.regionserver.info.port is already in use.\n    Useful for testing, turned off by default.</description>\n</property>\n\n<property>\n  <name>hbase.hregion.majorcompaction</name>\n  <value>604800000</value>\n    <description>Time between major compactions, expressed in milliseconds. Set to 0 to disable\n      time-based automatic major compactions. User-requested and size-based major compactions will\n      still run. This value is multiplied by hbase.hregion.majorcompaction.jitter to cause\n      compaction to start at a somewhat-random time during a given window of time. The default value\n      is 7 days, expressed in milliseconds. If major compactions are causing disruption in your\n      environment, you can configure them to run at off-peak times for your deployment, or disable\n      time-based major compactions by setting this parameter to 0, and run major compactions in a\n      cron job or by another external mechanism.</description>\n</property>\n\n<property>\n  <name>hbase.hstore.compaction.max.size</name>\n  <value>4611686018427387903</value>\n    <description>A StoreFile (or a selection of StoreFiles, when using ExploringCompactionPolicy)\n      larger than this size will be excluded from compaction. The effect of\n      raising hbase.hstore.compaction.max.size is fewer, larger StoreFiles that do not get\n      compacted often. If you feel that compaction is happening too often without much benefit, you\n      can try raising this value. Default: the value of LONG.MAX_VALUE, expressed in bytes.</description>\n</property>\n\n<property>\n  <name>hfile.format.version</name>\n  <value>1</value>\n      <description>The HFile format version to use for new files.\n      Version 3 adds support for tags in hfiles (See http://hbase.apache.org/book.html#hbase.tags).\n      Also see the configuration 'hbase.replication.rpc.codec'.\n      </description>\n</property>\n\n<property>\n  <name>hbase.data.umask</name>\n  <value>002</value>\n    <description>File permissions that should be used to write data\n      files when hbase.data.umask.enable is true</description>\n</property>\n\n<property>\n  <name>hbase.snapshot.restore.take.failsafe.snapshot</name>\n  <value>false</value>\n    <description>Set to true to take a snapshot before the restore operation.\n      The snapshot taken will be used in case of failure, to restore the previous state.\n      At the end of the restore operation this snapshot will be deleted</description>\n</property>\n\n<property>\n  <name>hbase.coordinated.state.manager.class</name>\n  <value>org.apache.hadoop.hbase.coordination.ZkCoordinatedStateManager</value>\n    <description>Fully qualified name of class implementing coordinated state manager.</description>\n</property>\n\n<property>\n  <name>hbase.region.replica.replication.enabled</name>\n  <value>true</value>\n    <description>\n      Whether asynchronous WAL replication to the secondary region replicas is enabled or not.\n      If this is enabled, a replication peer named \"region_replica_replication\" will be created\n      which will tail the logs and replicate the mutations to region replicas for tables that\n      have region replication > 1. If this is enabled once, disabling this replication also\n      requires disabling the replication peer using shell or Admin java class.\n      Replication to secondary region replicas works over standard inter-cluster replication.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "cluster-announce-ip=127.0.0.1\n\nunixsocket=/run/redis.sock\n\ndir=./\n\ntcp-backlog=1022\n\nenable-protected-configs=no\n\nreplica-announce-ip=127.0.0.1\n\ncluster-config-file=nodes-6379.conf\n\nstream-node-max-entries=100\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.group.mapping</name>\n  <value>org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback</value>\n  <description>\n    Class for user to group mapping (get groups for a given user) for ACL.\n    The default implementation,\n    org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback,\n    will determine if the Java Native Interface (JNI) is available. If JNI is\n    available the implementation will use the API within hadoop to resolve a\n    list of groups for a user. If JNI is not available then the shell\n    implementation, ShellBasedUnixGroupsMapping, is used.  This implementation\n    shells out to the Linux/Unix environment with the\n    <code>bash -c groups</code> command to resolve a list of groups for a user.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.groups.cache.secs</name>\n  <value>150</value>\n  <description>\n    This is the config controlling the validity of the entries in the cache\n    containing the user->group mapping. When this duration has expired,\n    then the implementation of the group mapping provider is invoked to get\n    the groups of the user and then cached back.\n  </description>\n</property>\n\n<property>\n  <name>io.compression.codec.bzip2.library</name>\n  <value>system-native</value>\n  <description>The native-code library to be used for compression and\n  decompression by the bzip2 codec.  This library could be specified\n  either by by name or the full pathname.  In the former case, the\n  library is located by the dynamic linker, usually searching the\n  directories specified in the environment variable LD_LIBRARY_PATH.\n\n  The value of \"system-native\" indicates that the default system\n  library should be used.  To indicate that the algorithm should\n  operate entirely in Java, specify \"java-builtin\".</description>\n</property>\n\n<property>\n  <name>fs.df.interval</name>\n  <value>60000</value>\n  <description>Disk usage statistics refresh interval in msec.</description>\n</property>\n\n<property>\n  <name>ipc.client.low-latency</name>\n  <value>false</value>\n  <description>Use low-latency QoS markers for IPC connections.\n  </description>\n</property>\n\n<property>\n  <name>net.topology.table.file.name</name>\n  <value>/valid/file2</value>\n  <description> The file name for a topology file, which is used when the\n    net.topology.node.switch.mapping.impl property is set to\n    org.apache.hadoop.net.TableMapping. The file format is a two column text\n    file, with columns separated by whitespace. The first column is a DNS or\n    IP address and the second column specifies the rack where the address maps.\n    If no entry corresponding to a host in the cluster is found, then\n    /default-rack is assumed.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.http.cross-origin.allowed-origins</name>\n  <value>*</value>\n  <description>Comma separated list of origins that are allowed for web services\n    needing cross-origin (CORS) support. If a value in the list contains an\n    asterix (*), a regex pattern, escaping any dots ('.' -> '\\.') and replacing\n    the asterix such that it captures any characters ('*' -> '.*'), is generated.\n    Values prefixed with 'regex:' are interpreted directly as regular expressions,\n    e.g. use the expression 'regex:https?:\\/\\/foo\\.bar:([0-9]+)?' to allow any\n    origin using the 'http' or 'https' protocol in the domain 'foo.bar' on any\n    port. The use of simple wildcards ('*') is discouraged, and only available for\n    backward compatibility.</description>\n</property>\n\n<property>\n  <name>hadoop.registry.zk.connection.timeout.ms</name>\n  <value>15000</value>\n    <description>\n      Zookeeper connection timeout in milliseconds\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "replica-announce-port=1234\n\nstream-node-max-bytes=2048\n\naclfile=/etc/redis/users.acl\n\ndaemonize=no\n\nloglevel=notice\n\ncluster-announce-ip=10.1.1.5\n\nhash-max-listpack-entries=1024\n\ncluster-announce-port=1\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "appendonly=no\n\nappendfsync=always\n\nactiverehashing=yes\n\nalways-show-logo=no\n\ndir=./\n\nrepl-diskless-sync-max-replicas=2\n\naof-rewrite-incremental-fsync=yes\n\nport=3189\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "alluxio.job.master.web.bind.host=127.0.0.1\n\nalluxio.worker.block.master.client.pool.size=22\n\nalluxio.user.block.read.retry.sleep.base=250ms\n\nalluxio.worker.network.writer.buffer.size.messages=16\n\nalluxio.master.embedded.journal.catchup.retry.wait=2s\n\nalluxio.worker.tieredstore.level0.alias=MEM\n\nalluxio.work.dir=/valid/dir1\n\nalluxio.user.client.cache.eviction.retries=10\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.search.attr.member</name>\n  <value>member</value>\n  <description>\n    The attribute of the group object that identifies the users that are\n    members of the group. The default will usually be appropriate for\n    any LDAP installation.\n  </description>\n</property>\n\n<property>\n  <name>fs.AbstractFileSystem.viewfs.impl</name>\n  <value>org.apache.hadoop.fs.viewfs.ViewFs</value>\n  <description>The AbstractFileSystem for view file system for viewfs: uris\n  (ie client side mount table:).</description>\n</property>\n\n<property>\n  <name>fs.s3a.committer.threads</name>\n  <value>1</value>\n  <description>\n    Number of threads in committers for parallel operations on files\n    (upload, commit, abort, delete...)\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.select.output.csv.quote.fields</name>\n  <value>always</value>\n  <description>\n    In S3 Select queries: should fields in generated CSV Files be quoted?\n    One of: \"always\", \"asneeded\".\n  </description>\n</property>\n\n<property>\n  <name>ftp.client-write-packet-size</name>\n  <value>65536</value>\n  <description>Packet size for clients to write</description>\n</property>\n\n<property>\n  <name>fs.permissions.umask-mode</name>\n  <value>022</value>\n  <description>\n    The umask used when creating files and directories.\n    Can be in octal or in symbolic. Examples are:\n    \"022\" (octal for u=rwx,g=r-x,o=r-x in symbolic),\n    or \"u=rwx,g=rwx,o=\" (symbolic for 007 in octal).\n  </description>\n</property>\n\n<property>\n  <name>ha.failover-controller.cli-check.rpc-timeout.ms</name>\n  <value>40000</value>\n  <description>\n    Timeout that the CLI (manual) FC waits for monitorHealth, getServiceState\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.crypto.codec.classes.aes.ctr.nopadding</name>\n  <value>org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec</value>\n  <description>\n    Comma-separated list of crypto codec implementations for AES/CTR/NoPadding.\n    The first implementation will be used if available, others are fallbacks.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.namenode.decommission.blocks.per.interval</name>\n  <value>250000</value>\n  <description>The approximate number of blocks to process per decommission\n    or maintenance interval, as defined in dfs.namenode.decommission.interval.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.scan.period.hours</name>\n  <value>504</value>\n  <description>\n        If this is positive, the DataNode will not scan any\n        individual block more than once in the specified scan period.\n        If this is negative, the block scanner is disabled.\n        If this is set to zero, then the default value of 504 hours\n        or 3 weeks is used. Prior versions of HDFS incorrectly documented\n        that setting this key to zero will disable the block scanner.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.metrics.logger.period.seconds</name>\n  <value>600</value>\n  <description>\n    This setting controls how frequently the NameNode logs its metrics. The\n    logging configuration must also define one or more appenders for\n    NameNodeMetricsLog for the metrics to be logged.\n    NameNode metrics logging is disabled if this value is set to zero or\n    less than zero.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.fsdatasetcache.max.threads.per.volume</name>\n  <value>8</value>\n  <description>\n    The maximum number of threads per volume to use for caching new data\n    on the datanode. These threads consume both I/O and CPU. This can affect\n    normal datanode operations.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.inotify.max.events.per.rpc</name>\n  <value>500</value>\n  <description>Maximum number of events that will be sent to an inotify client\n    in a single RPC response. The default value attempts to amortize away\n    the overhead for this RPC while avoiding huge memory requirements for the\n    client and NameNode (1000 events should consume no more than 1 MB.)\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.socketcache.capacity</name>\n  <value>32</value>\n  <description>\n    Socket cache capacity (in entries) for short-circuit reads.\n    If this value is set to 0, the client socket cache is disabled.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.missing.checkpoint.periods.before.shutdown</name>\n  <value>6</value>\n  <description>\n    The number of checkpoint period windows (as defined by the property\n    dfs.namenode.checkpoint.period) allowed by the Namenode to perform\n    saving the namespace before shutdown.\n  </description>\n</property>\n\n<property>\n  <name>dfs.journalnode.edits.dir.perm</name>\n  <value>hbase</value>\n    <description>\n      Permissions for the directories on on the local filesystem where\n      the DFS journal node stores the edits. The permissions can either be\n      octal or symbolic.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.master.infoserver.redirect</name>\n  <value>false</value>\n    <description>Whether or not the Master listens to the Master web\n      UI port (hbase.master.info.port) and redirects requests to the web\n      UI server shared by the Master and RegionServer. Config. makes\n      sense when Master is serving Regions (not the default).</description>\n</property>\n\n<property>\n  <name>zookeeper.znode.acl.parent</name>\n  <value>acl</value>\n    <description>Root ZNode for access control lists.</description>\n</property>\n\n<property>\n  <name>hbase.normalizer.period</name>\n  <value>150000</value>\n    <description>Period at which the region normalizer runs in the Master.</description>\n</property>\n\n<property>\n  <name>io.storefile.bloom.block.size</name>\n  <value>131072</value>\n      <description>The size in bytes of a single block (\"chunk\") of a compound Bloom\n          filter. This size is approximate, because Bloom blocks can only be\n          inserted at data block boundaries, and the number of keys per data\n          block varies.</description>\n</property>\n\n<property>\n  <name>hbase.ipc.client.tcpnodelay</name>\n  <value>true</value>\n    <description>Set no delay on rpc socket connections.  See\n    http://docs.oracle.com/javase/1.5.0/docs/api/java/net/Socket.html#getTcpNoDelay()</description>\n</property>\n\n<property>\n  <name>hbase.ipc.server.fallback-to-simple-auth-allowed</name>\n  <value>true</value>\n    <description>When a server is configured to require secure connections, it will\n      reject connection attempts from clients using SASL SIMPLE (unsecure) authentication.\n      This setting allows secure servers to accept SASL SIMPLE connections from clients\n      when the client requests.  When false (the default), the server will not allow the fallback\n      to SIMPLE authentication, and will reject the connection.  WARNING: This setting should ONLY\n      be used as a temporary measure while converting clients over to secure authentication.  It\n      MUST BE DISABLED for secure operation.</description>\n</property>\n\n<property>\n  <name>hbase.client.scanner.max.result.size</name>\n  <value>1048576</value>\n    <description>Maximum number of bytes returned when calling a scanner's next method.\n    Note that when a single row is larger than this limit the row is still returned completely.\n    The default value is 2MB, which is good for 1ge networks.\n    With faster and/or high latency networks this value should be increased.\n    </description>\n</property>\n\n<property>\n  <name>hbase.mob.delfile.max.count</name>\n  <value>6</value>\n    <description>\n      The max number of del files that is allowed in the mob compaction.\n      In the mob compaction, when the number of existing del files is larger than\n      this value, they are merged until number of del files is not larger this value.\n      The default value is 3.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.instrumentation.requires.admin</name>\n  <value>false</value>\n  <description>\n    Indicates if administrator ACLs are required to access\n    instrumentation servlets (JMX, METRICS, CONF, STACKS).\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.posix.attr.gid.name</name>\n  <value>gidNumber</value>\n  <description>\n    The attribute of posixAccount indicating the group id.\n  </description>\n</property>\n\n<property>\n  <name>io.erasurecode.codec.rs.rawcoders</name>\n  <value>rs_native</value>\n  <description>\n    Comma separated raw coder implementations for the rs codec. The earlier\n    factory is prior to followings in case of failure of creating raw coders.\n  </description>\n</property>\n\n<property>\n  <name>fs.AbstractFileSystem.viewfs.impl</name>\n  <value>org.apache.hadoop.fs.viewfs.ViewFs</value>\n  <description>The AbstractFileSystem for view file system for viewfs: uris\n  (ie client side mount table:).</description>\n</property>\n\n<property>\n  <name>fs.s3a.connection.timeout</name>\n  <value>400000</value>\n  <description>Socket connection timeout in milliseconds.</description>\n</property>\n\n<property>\n  <name>fs.s3a.s3guard.ddb.table.capacity.write</name>\n  <value>-1</value>\n  <description>\n    Provisioned throughput requirements for write operations in terms of\n    capacity units for the DynamoDB table.\n    If set to 0 (the default), new tables are created with \"per-request\" capacity.\n    Refer to related configuration option fs.s3a.s3guard.ddb.table.capacity.read\n  </description>\n</property>\n\n<property>\n  <name>hadoop.registry.secure</name>\n  <value>true</value>\n    <description>\n      Key to set if the registry is secure. Turning it on\n      changes the permissions policy from \"open access\"\n      to restrictions on kerberos with the option of\n      a user adding one or more auth key pairs down their\n      own tree.\n    </description>\n</property>\n\n<property>\n  <name>fs.adl.oauth2.access.token.provider.type</name>\n  <value>ClientCredential</value>\n    <description>\n      Defines Azure Active Directory OAuth2 access token provider type.\n      Supported types are ClientCredential, RefreshToken, MSI, DeviceCode,\n      and Custom.\n      The ClientCredential type requires property fs.adl.oauth2.client.id,\n      fs.adl.oauth2.credential, and fs.adl.oauth2.refresh.url.\n      The RefreshToken type requires property fs.adl.oauth2.client.id and\n      fs.adl.oauth2.refresh.token.\n      The MSI type reads optional property fs.adl.oauth2.msi.port, if specified.\n      The DeviceCode type requires property\n      fs.adl.oauth2.devicecode.clientapp.id.\n      The Custom type requires property fs.adl.oauth2.access.token.provider.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "alluxio.job.master.rpc.port=1.1\n\nalluxio.worker.rpc.port=3001\n\nalluxio.security.authentication.type=SIMPLE\n\nalluxio.worker.management.tier.align.reserved.bytes=1GB\n\nalluxio.user.file.master.client.pool.gc.interval=240sec\n\nalluxio.user.rpc.retry.base.sleep=100ms\n\nalluxio.master.daily.backup.time=05:00\n\nalluxio.user.file.master.client.pool.gc.threshold=240sec\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "alluxio.proxy.web.bind.host=0.0.256.0\n\nalluxio.job.master.finished.job.purge.count=-2\n\nalluxio.user.ufs.block.read.location.policy.deterministic.hash.shards=1\n\nalluxio.worker.management.tier.promote.quota.percent=180\n\nalluxio.fuse.cached.paths.max=500\n\nalluxio.worker.management.backoff.strategy=ANY\n\nalluxio.worker.network.shutdown.timeout=15sec\n\nalluxio.web.file.info.enabled=true\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    \n<property>\n  <name>yarn.scheduler.minimum-allocation-mb</name>\n  <value>2048</value>\n    <description>The minimum allocation for every container request at the RM\n    in MBs. Memory requests lower than this will be set to the value of this\n    property. Additionally, a node manager that is configured to have less memory\n    than this value will be shut down by the resource manager.</description>\n</property>\n\n<property>\n  <name>yarn.resourcemanager.proxy-user-privileges.enabled</name>\n  <value>true</value>\n  <description>If true, ResourceManager will have proxy-user privileges.\n    Use case: In a secure cluster, YARN requires the user hdfs delegation-tokens to\n    do localization and log-aggregation on behalf of the user. If this is set to true,\n    ResourceManager is able to request new hdfs delegation tokens on behalf of\n    the user. This is needed by long-running-service, because the hdfs tokens\n    will eventually expire and YARN requires new valid tokens to do localization\n    and log-aggregation. Note that to enable this use case, the corresponding\n    HDFS NameNode has to configure ResourceManager as the proxy-user so that\n    ResourceManager can itself ask for new tokens on behalf of the user when\n    tokens are past their max-life-time.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.collector-service.address</name>\n  <value>${yarn.nodemanager.hostname}:8048</value>\n    <description>Address where the collector service IPC is.</description>\n</property>\n\n<property>\n  <name>yarn.log-aggregation.file-controller.TFile.class</name>\n  <value>org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController</value>\n    <description>Class that supports TFile read and write operations.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.container-diagnostics-maximum-size</name>\n  <value>20000</value>\n    <description>Maximum size of contain's diagnostics to keep for relaunching\n      container case.</description>\n</property>\n\n<property>\n  <name>yarn.client.application-client-protocol.poll-interval-ms</name>\n  <value>200</value>\n    <description>The interval that the yarn client library uses to poll the\n    completion status of the asynchronous API of application client protocol.\n    </description>\n</property>\n\n<property>\n  <name>yarn.intermediate-data-encryption.enable</name>\n  <value>false</value>\n    <description>\n    Enable/disable intermediate-data encryption at YARN level. For now,\n    this only is used by the FileSystemRMStateStore to setup right\n    file-system security attributes.\n    </description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.containers-launcher.class</name>\n  <value>org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher</value>\n    <description>\n      Containers launcher implementation for determining how containers\n      are launched within NodeManagers.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "yarn", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.master.port</name>\n  <value>16000</value>\n    <description>The port the HBase Master should bind to.</description>\n</property>\n\n<property>\n  <name>hbase.master.logcleaner.plugins</name>\n  <value>org.apache.hadoop.hbase.master.cleaner.TimeToLiveProcedureWALCleaner</value>\n    <description>A comma-separated list of BaseLogCleanerDelegate invoked by\n    the LogsCleaner service. These WAL cleaners are called in order,\n    so put the cleaner that prunes the most files in front. To\n    implement your own BaseLogCleanerDelegate, just put it in HBase's classpath\n    and add the fully qualified class name here. Always add the above\n    default log cleaners in the list.</description>\n</property>\n\n<property>\n  <name>hbase.client.retries.number</name>\n  <value>30</value>\n    <description>Maximum retries.  Used as maximum for all retryable\n    operations such as the getting of a cell's value, starting a row update,\n    etc.  Retry interval is a rough function based on hbase.client.pause.  At\n    first we retry at this interval but then with backoff, we pretty quickly reach\n    retrying every ten seconds.  See HConstants#RETRY_BACKOFF for how the backup\n    ramps up.  Change this setting and hbase.client.pause to suit your workload.</description>\n</property>\n\n<property>\n  <name>hfile.index.block.max.size</name>\n  <value>131072</value>\n      <description>When the size of a leaf-level, intermediate-level, or root-level\n          index block in a multi-level block index grows to this size, the\n          block is written out and a new block is started.</description>\n</property>\n\n<property>\n  <name>hbase.auth.key.update.interval</name>\n  <value>172800000</value>\n    <description>The update interval for master key for authentication tokens\n    in servers in milliseconds.  Only used when HBase security is enabled.</description>\n</property>\n\n<property>\n  <name>hbase.auth.token.max.lifetime</name>\n  <value>1209600000</value>\n    <description>The maximum lifetime in milliseconds after which an\n    authentication token expires.  Only used when HBase security is enabled.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.storefile.refresh.period</name>\n  <value>-1</value>\n    <description>\n      The period (in milliseconds) for refreshing the store files for the secondary regions. 0\n      means this feature is disabled. Secondary regions sees new files (from flushes and\n      compactions) from primary once the secondary region refreshes the list of files in the\n      region (there is no notification mechanism). But too frequent refreshes might cause\n      extra Namenode pressure. If the files cannot be refreshed for longer than HFile TTL\n      (hbase.master.hfilecleaner.ttl) the requests are rejected. Configuring HFile TTL to a larger\n      value is also recommended with this setting.\n    </description>\n</property>\n\n<property>\n  <name>hbase.http.staticuser.user</name>\n  <value>dr.stack</value>\n    <description>\n      The user name to filter as, on static web filters\n      while rendering content. An example use is the HDFS\n      web UI (user to be used for browsing files).\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>fs.s3a.paging.maximum</name>\n  <value>5000</value>\n  <description>How many keys to request from S3 when doing\n     directory listings at a time.</description>\n</property>\n\n<property>\n  <name>fs.s3a.change.detection.mode</name>\n  <value>server</value>\n  <description>\n    Determines how change detection is applied to alert to inconsistent S3\n    objects read during or after an overwrite. Value 'server' indicates to apply\n    the attribute constraint directly on GetObject requests to S3. Value 'client'\n    means to do a client-side comparison of the attribute value returned in the\n    response.  Value 'server' would not work with third-party S3 implementations\n    that do not support these constraints on GetObject. Values 'server' and\n    'client' generate RemoteObjectChangedException when a mismatch is detected.\n    Value 'warn' works like 'client' but generates only a warning.  Value 'none'\n    will ignore change detection completely.\n  </description>\n</property>\n\n<property>\n  <name>ipc.[port_number].weighted-cost.lockfree</name>\n  <value>1</value>\n  <description>The weight multiplier to apply to the time spent in the\n    LOCKFREE phase which do not involve holding a lock.\n    See org.apache.hadoop.ipc.ProcessingDetails.Timing for more details on\n    this phase. This property applies to WeightedTimeCostProvider.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.http.authentication.type</name>\n  <value>simple</value>\n  <description>\n    Defines authentication used for Oozie HTTP endpoint.\n    Supported values are: simple | kerberos | #AUTHENTICATION_HANDLER_CLASSNAME#\n  </description>\n</property>\n\n<property>\n  <name>hadoop.http.cross-origin.max-age</name>\n  <value>900</value>\n  <description>The number of seconds a pre-flighted request can be cached\n    for web services needing cross-origin (CORS) support.</description>\n</property>\n\n<property>\n  <name>hadoop.http.staticuser.user</name>\n  <value>dr.who</value>\n  <description>\n    The user name to filter as, on static web filters\n    while rendering content. An example use is the HDFS\n    web UI (user to be used for browsing files).\n  </description>\n</property>\n\n<property>\n  <name>hadoop.registry.jaas.context</name>\n  <value>Client</value>\n    <description>\n      Key to define the JAAS context. Used in secure\n      mode\n    </description>\n</property>\n\n<property>\n  <name>hadoop.metrics.jvm.use-thread-mxbean</name>\n  <value>true</value>\n    <description>\n      Whether or not ThreadMXBean is used for getting thread info in JvmMetrics,\n      ThreadGroup approach is preferred for better performance.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "alluxio.job.master.client.threads=-100\n\nalluxio.worker.tieredstore.level0.dirs.quota=${alluxio.worker.ramdisk.size}\n\nalluxio.master.daily.backup.state.lock.sleep.duration=5m\n\nalluxio.master.ufs.active.sync.retry.timeout=20sec\n\nalluxio.user.network.streaming.keepalive.timeout=60sec\n\nalluxio.user.worker.list.refresh.interval=2min\n\nalluxio.master.daily.backup.files.retained=3\n\nalluxio.web.threads=0\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.service.shutdown.timeout</name>\n  <value>30s</value>\n    <description>\n      Timeout to wait for each shutdown operation to complete.\n      If a hook takes longer than this time to complete, it will be interrupted,\n      so the service will shutdown. This allows the service shutdown\n      to recover from a blocked operation.\n      Some shutdown hooks may need more time than this, for example when\n      a large amount of data needs to be uploaded to an object store.\n      In this situation: increase the timeout.\n\n      The minimum duration of the timeout is 1 second, \"1s\".\n    </description>\n</property>\n\n<property>\n  <name>hadoop.rpc.protection</name>\n  <value>authentication</value>\n  <description>A comma-separated list of protection values for secured sasl\n      connections. Possible values are authentication, integrity and privacy.\n      authentication means authentication only and no integrity or privacy;\n      integrity implies authentication and integrity are enabled; and privacy\n      implies all of authentication, integrity and privacy are enabled.\n      hadoop.security.saslproperties.resolver.class can be used to override\n      the hadoop.rpc.protection for a connection at the server side.\n  </description>\n</property>\n\n<property>\n  <name>fs.ftp.transfer.mode</name>\n  <value>BLOCK_TRANSFER_MODE</value>\n  <description>\n    Set FTP's transfer mode based on configuration. Valid values are\n    STREAM_TRANSFER_MODE, BLOCK_TRANSFER_MODE and COMPRESSED_TRANSFER_MODE.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.path.style.access</name>\n  <value>true</value>\n  <description>Enable S3 path style access ie disabling the default virtual hosting behaviour.\n    Useful for S3A-compliant storage providers as it removes the need to set up DNS for virtual hosting.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.attempts.maximum</name>\n  <value>10</value>\n  <description>How many times we should retry commands on transient errors.</description>\n</property>\n\n<property>\n  <name>fs.s3a.select.input.csv.quote.character</name>\n  <value>\"</value>\n  <description>In S3 Select queries over CSV files: quote character.\n    \\t is remapped to the TAB character, \\r to CR \\n to newline. \\\\ to \\\n    and \\\" to \"\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.kms.client.failover.sleep.max.millis</name>\n  <value>1000</value>\n  <description>\n    Expert only. The time to wait, in milliseconds, between failover\n    attempts increases exponentially as a function of the number of\n    attempts made so far, with a random factor of +/- 50%. This option\n    specifies the maximum value to wait between failovers.\n    Specifically, the time between two failover attempts will not\n    exceed +/- 50% of hadoop.security.client.failover.sleep.max.millis\n    milliseconds.\n  </description>\n</property>\n\n<property>\n  <name>ipc.client.bind.wildcard.addr</name>\n  <value>false</value>\n    <description>When set to true Clients will bind socket to wildcard\n      address. (i.e 0.0.0.0)\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.connection.timeout.ms</name>\n  <value>120000</value>\n  <description>\n    This property is the connection timeout (in milliseconds) for LDAP\n    operations. If the LDAP provider doesn't establish a connection within the\n    specified period, it will abort the connect attempt. Non-positive value\n    means no LDAP connection timeout is specified in which case it waits for the\n    connection to establish until the underlying network times out.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.kerberos.keytab.login.autorenewal.enabled</name>\n  <value>false</value>\n  <description>Used to enable automatic renewal of keytab based kerberos login.\n    By default the automatic renewal is disabled for keytab based kerberos login.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.aws.credentials.provider</name>\n  <value>org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider,org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,com.amazonaws.auth.EnvironmentVariableCredentialsProvider,org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider</value>\n  <description>\n    Comma-separated class names of credential provider classes which implement\n    com.amazonaws.auth.AWSCredentialsProvider.\n\n    When S3A delegation tokens are not enabled, this list will be used\n    to directly authenticate with S3 and DynamoDB services.\n    When S3A Delegation tokens are enabled, depending upon the delegation\n    token binding it may be used\n    to communicate wih the STS endpoint to request session/role\n    credentials.\n\n    These are loaded and queried in sequence for a valid set of credentials.\n    Each listed class must implement one of the following means of\n    construction, which are attempted in order:\n    * a public constructor accepting java.net.URI and\n        org.apache.hadoop.conf.Configuration,\n    * a public constructor accepting org.apache.hadoop.conf.Configuration,\n    * a public static method named getInstance that accepts no\n       arguments and returns an instance of\n       com.amazonaws.auth.AWSCredentialsProvider, or\n    * a public default constructor.\n\n    Specifying org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider allows\n    anonymous access to a publicly accessible S3 bucket without any credentials.\n    Please note that allowing anonymous access to an S3 bucket compromises\n    security and therefore is unsuitable for most use cases. It can be useful\n    for accessing public data sets without requiring AWS credentials.\n\n    If unspecified, then the default list of credential provider classes,\n    queried in sequence, is:\n    * org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider: looks\n       for session login secrets in the Hadoop configuration.\n    * org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider:\n       Uses the values of fs.s3a.access.key and fs.s3a.secret.key.\n    * com.amazonaws.auth.EnvironmentVariableCredentialsProvider: supports\n        configuration of AWS access key ID and secret access key in\n        environment variables named AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY,\n        and AWS_SESSION_TOKEN as documented in the AWS SDK.\n    * org.apache.hadoop.fs.s3a.auth.IAMInstanceCredentialsProvider: picks up\n       IAM credentials of any EC2 VM or AWS container in which the process is running.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.select.input.csv.field.delimiter</name>\n  <value>,</value>\n  <description>In S3 Select queries over CSV files: the field delimiter.\n    \\t is remapped to the TAB character, \\r to CR \\n to newline. \\\\ to \\\n    and \\\" to \"\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.ssl.channel.mode</name>\n  <value>default_jsse</value>\n  <description>\n    If secure connections to S3 are enabled, configures the SSL\n    implementation used to encrypt connections to S3. Supported values are:\n    \"default_jsse\", \"default_jsse_with_gcm\", \"default\", and \"openssl\".\n    \"default_jsse\" uses the Java Secure Socket Extension package (JSSE).\n    However, when running on Java 8, the GCM cipher is removed from the list\n    of enabled ciphers. This is due to performance issues with GCM in Java 8.\n    \"default_jsse_with_gcm\" uses the JSSE with the default list of cipher\n    suites. \"default_jsse_with_gcm\" is equivalent to the behavior prior to\n    this feature being introduced. \"default\" attempts to use OpenSSL rather\n    than the JSSE for SSL encryption, if OpenSSL libraries cannot be loaded,\n    it falls back to the \"default_jsse\" behavior. \"openssl\" attempts to use\n    OpenSSL as well, but fails if OpenSSL libraries cannot be loaded.\n  </description>\n</property>\n\n<property>\n  <name>io.mapfile.bloom.error.rate</name>\n  <value>0.01</value>\n  <description>The rate of false positives in BloomFilter-s used in BloomMapFile.\n  As this value decreases, the size of BloomFilter-s increases exponentially. This\n  value is the probability of encountering false positives (default is 0.5%).\n  </description>\n</property>\n\n<property>\n  <name>ipc.client.idlethreshold</name>\n  <value>4000000000</value>\n  <description>Defines the threshold number of connections after which\n               connections will be inspected for idleness.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.http.authentication.simple.anonymous.allowed</name>\n  <value>true</value>\n  <description>\n    Indicates if anonymous requests are allowed when using 'simple' authentication.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.namenode.lazypersist.file.scrub.interval.sec</name>\n  <value>600</value>\n  <description>\n    The NameNode periodically scans the namespace for LazyPersist files with\n    missing blocks and unlinks them from the namespace. This configuration key\n    controls the interval between successive scans. If this value is set to 0,\n    the file scrubber is disabled.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.handler.count</name>\n  <value>5</value>\n  <description>The number of Namenode RPC server threads that listen to\n  requests from clients.\n  If dfs.namenode.servicerpc-address is not configured then\n  Namenode RPC server threads listen to requests from all nodes.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.service.handler.count</name>\n  <value>10</value>\n  <description>The number of Namenode RPC server threads that listen to\n  requests from DataNodes and from all other non-client nodes.\n  dfs.namenode.service.handler.count will be valid only if\n  dfs.namenode.servicerpc-address is configured.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.deadnode.detection.rpc.threads</name>\n  <value>40</value>\n    <description>\n      The maximum number of threads to use for calling RPC call to recheck the liveness of dead node.\n    </description>\n</property>\n\n<property>\n  <name>dfs.client.retry.interval-ms.get-last-block-length</name>\n  <value>8000</value>\n  <description>\n    Retry interval in milliseconds to wait between retries in getting\n    block lengths from the datanodes.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.max.op.size</name>\n  <value>104857600</value>\n  <description>\n    Maximum opcode size in bytes.\n  </description>\n</property>\n\n<property>\n  <name>dfs.storage.policy.satisfier.mode</name>\n  <value>none</value>\n  <description>\n    Following values are supported - external, none.\n    If external, StoragePolicySatisfier will be enabled and started as an independent service outside namenode.\n    If none, StoragePolicySatisfier is disabled.\n  </description>\n</property>\n\n<property>\n  <name>dfs.storage.policy.satisfier.work.multiplier.per.iteration</name>\n  <value>0</value>\n  <description>\n    *Note*: Advanced property. Change with caution.\n    This determines the total amount of block transfers to begin in\n    one iteration, for satisfy the policy. The actual number is obtained by\n    multiplying this multiplier with the total number of live nodes in the\n    cluster. The result number is the number of blocks to begin transfers\n    immediately. This number can be any positive, non-zero integer.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.permissions.superusergroup</name>\n  <value>xdgroup</value>\n  <description>The name of the group of super-users.\n    The value should be a single group name.\n  </description>\n</property>\n\n<property>\n  <name>dfs.blockreport.split.threshold</name>\n  <value>1000000</value>\n    <description>If the number of blocks on the DataNode is below this\n    threshold then it will send block reports for all Storage Directories\n    in a single message.\n\n    If the number of blocks exceeds this threshold then the DataNode will\n    send block reports for each Storage Directory in separate messages.\n\n    Set to zero to always split.\n    </description>\n</property>\n\n<property>\n  <name>dfs.namenode.delegation.key.update-interval</name>\n  <value>43200000</value>\n  <description>The update interval for master key for delegation tokens \n       in the namenode in milliseconds.\n  </description>\n</property>\n\n<property>\n  <name>dfs.ha.automatic-failover.enabled</name>\n  <value>true</value>\n  <description>\n    Whether automatic failover is enabled. See the HDFS High\n    Availability documentation for details on automatic HA\n    configuration.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.mmap.enabled</name>\n  <value>false</value>\n  <description>\n    If this is set to false, the client won't attempt to perform memory-mapped reads.\n  </description>\n</property>\n\n<property>\n  <name>dfs.balancer.moverThreads</name>\n  <value>1000</value>\n  <description>\n    Thread pool size for executing block moves.\n    moverThreadAllocator\n  </description>\n</property>\n\n<property>\n  <name>dfs.data.transfer.client.tcpnodelay</name>\n  <value>false</value>\n  <description>\n    If true, set TCP_NODELAY to sockets for transferring data from DFS client.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.gc.time.monitor.sleep.interval.ms</name>\n  <value>1s</value>\n    <description>\n      Determines the sleep interval in the window. The GcTimeMonitor wakes up in\n      the sleep interval periodically to compute the gc time proportion. The\n      shorter the interval the preciser the GcTimePercentage. The sleep interval\n      must be shorter than the window size.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.ha.tail-edits.rolledits.timeout</name>\n  <value>60</value>\n  <description>The timeout in seconds of calling rollEdits RPC on Active NN.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.kerberos.principal.pattern</name>\n  <value>*</value>\n  <description>\n    A client-side RegEx that can be configured to control\n    allowed realms to authenticate with (useful in cross-realm env.)\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.state.context.enabled</name>\n  <value>true</value>\n  <description>\n    Whether enable namenode sending back its current txnid back to client.\n    Setting this to true is required by Consistent Read from Standby feature.\n    But for regular cases, this should be set to false to avoid the overhead\n    of updating and maintaining this state.\n  </description>\n</property>\n\n<property>\n  <name>dfs.balancer.max-iteration-time</name>\n  <value>2400000</value>\n  <description>\n    Maximum amount of time while an iteration can be run by the Balancer. After\n    this time the Balancer will stop the iteration, and reevaluate the work\n    needs to be done to Balance the cluster. The default value is 20 minutes.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.snapshot.skip.capture.accesstime-only-change</name>\n  <value>false</value>\n  <description>\n    If accessTime of a file/directory changed but there is no other\n    modification made to the file/directory, the changed accesstime will\n    not be captured in next snapshot. However, if there is other modification\n    made to the file/directory, the latest access time will be captured\n    together with the modification in next snapshot.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.snapshot.skiplist.max.levels</name>\n  <value>0</value>\n  <description>\n    Maximum no of the skip levels to be maintained in the skip list for\n    storing directory snapshot diffs. By default, it is set to 0 and a linear\n    list will be used to store the directory snapshot diffs.\n  </description>\n</property>\n\n<property>\n  <name>dfs.storage.policy.satisfier.queue.limit</name>\n  <value>1000</value>\n  <description>\n    Storage policy satisfier queue size. This queue contains the currently\n    scheduled file's inode ID for statisfy the policy.\n    Default value is 1000.\n  </description>\n</property>\n\n<property>\n  <name>dfs.quota.by.storage.type.enabled</name>\n  <value>true</value>\n  <description>\n    If true, enables quotas based on storage type.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "enable_mergejoin=on\n\ncpu_tuple_cost=0.01\n\nautovacuum_vacuum_cost_delay=1ms\n\nmin_wal_size=80MB\n\ndefault_transaction_isolation='read committed'\n\nenable_hashjoin=on\n\nenable_parallel_hash=on\n\npassword_encryption=scram-sha-256\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "postgresql", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.zookeeper.quorum</name>\n  <value>localhost</value>\n    <description>Comma separated list of servers in the ZooKeeper ensemble\n    (This config. should have been named hbase.zookeeper.ensemble).\n    For example, \"host1.mydomain.com,host2.mydomain.com,host3.mydomain.com\".\n    By default this is set to localhost for local and pseudo-distributed modes\n    of operation. For a fully-distributed setup, this should be set to a full\n    list of ZooKeeper ensemble servers. If HBASE_MANAGES_ZK is set in hbase-env.sh\n    this is the list of servers which hbase will start/stop ZooKeeper on as\n    part of cluster start/stop.  Client-side, we will take this list of\n    ensemble members and put it together with the hbase.zookeeper.property.clientPort\n    config. and pass it into zookeeper constructor as the connectString\n    parameter.</description>\n</property>\n\n<property>\n  <name>hbase.ipc.server.callqueue.handler.factor</name>\n  <value>0.05</value>\n    <description>Factor to determine the number of call queues.\n      A value of 0 means a single queue shared between all the handlers.\n      A value of 1 means that each handler has its own queue.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.hlog.writer.impl</name>\n  <value>org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter</value>\n    <description>The WAL file writer implementation.</description>\n</property>\n\n<property>\n  <name>hbase.client.max.perregion.tasks</name>\n  <value>0</value>\n    <description>The maximum number of concurrent mutation tasks the client will\n    maintain to a single Region. That is, if there is already\n    hbase.client.max.perregion.tasks writes in progress for this region, new puts\n    won't be sent to this region until some writes finishes.</description>\n</property>\n\n<property>\n  <name>hbase.hstore.compaction.ratio.offpeak</name>\n  <value>10.0</value>\n    <description>Allows you to set a different (by default, more aggressive) ratio for determining\n      whether larger StoreFiles are included in compactions during off-peak hours. Works in the\n      same way as hbase.hstore.compaction.ratio. Only applies if hbase.offpeak.start.hour and\n      hbase.offpeak.end.hour are also enabled.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.hostname</name>\n  <value>127.0.0.1</value>\n    <description>This config is for experts: don't set its value unless you really know what you are doing.\n    When set to a non-empty value, this represents the (external facing) hostname for the underlying server.\n    See https://issues.apache.org/jira/browse/HBASE-12954 for details.</description>\n</property>\n\n<property>\n  <name>hbase.display.keys</name>\n  <value>false</value>\n    <description>When this is set to true the webUI and such will display all start/end keys\n                 as part of the table details, region names, etc. When this is set to false,\n                 the keys are hidden.</description>\n</property>\n\n<property>\n  <name>hbase.data.umask</name>\n  <value>ciri</value>\n    <description>File permissions that should be used to write data\n      files when hbase.data.umask.enable is true</description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "alluxio.underfs.s3.threads.max=4000000000\n\nalluxio.master.metastore=ROCKS\n\nalluxio.master.log.config.report.heartbeat.interval=10h\n\nalluxio.master.embedded.journal.retry.cache.expiry.time=60s\n\nalluxio.master.metastore.dir=/valid/file1\n\nalluxio.master.journal.temporary.file.gc.threshold=1min\n\nalluxio.worker.tieredstore.level2.watermark.high.ratio=1.9\n\nalluxio.master.ufs.active.sync.interval=60sec\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "CACHE_MIDDLEWARE_SECONDS=true\n\nSESSION_SAVE_EVERY_REQUEST=False\n\nCSRF_COOKIE_SAMESITE='Lax'\n\nDEFAULT_AUTO_FIELD='django.db.models.AutoField'\n\nCSRF_FAILURE_VIEW='django.views.csrf.csrf_failure'\n\nSECURE_HSTS_PRELOAD=False\n\nSESSION_COOKIE_NAME='sessionid'\n\nEMAIL_HOST='localhost'\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "django", "is_synthetic": true}}, {"input": "STATIC_ROOT=//var/www/example.com/static\n\nFILE_UPLOAD_DIRECTORY_PERMISSIONS=None\n\nSESSION_COOKIE_NAME='sessionid'\n\nDEFAULT_CHARSET='utf-8'\n\nSESSION_FILE_PATH=None\n\nDECIMAL_SEPARATOR='.'\n\nSECURE_CROSS_ORIGIN_OPENER_POLICY='same-origin'\n\nCSRF_COOKIE_HTTPONLY=False\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "django", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.datanode.data.dir.perm</name>\n  <value>file://</value>\n  <description>Permissions for the directories on on the local filesystem where\n  the DFS data node store its blocks. The permissions can either be octal or\n  symbolic.</description>\n</property>\n\n<property>\n  <name>dfs.namenode.resource.checked.volumes.minimum</name>\n  <value>0</value>\n  <description>\n    The minimum number of redundant NameNode storage volumes required.\n  </description>\n</property>\n\n<property>\n  <name>dfs.image.compression.codec</name>\n  <value>org.apache.hadoop.io.compress.DefaultCodec</value>\n  <description>If the dfs image is compressed, how should they be compressed?\n               This has to be a codec defined in io.compression.codecs.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.audit.loggers</name>\n  <value>default</value>\n  <description>\n    List of classes implementing audit loggers that will receive audit events.\n    These should be implementations of org.apache.hadoop.hdfs.server.namenode.AuditLogger.\n    The special value \"default\" can be used to reference the default audit\n    logger, which uses the configured log system. Installing custom audit loggers\n    may affect the performance and stability of the NameNode. Refer to the custom\n    logger's documentation for more details.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.path.based.cache.refresh.interval.ms</name>\n  <value>60000</value>\n  <description>\n    The amount of milliseconds between subsequent path cache rescans.  Path\n    cache rescans are when we calculate which blocks should be cached, and on\n    what datanodes.\n\n    By default, this parameter is set to 30 seconds.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.deadnode.detection.probe.deadnode.threads</name>\n  <value>1</value>\n    <description>\n      The maximum number of threads to use for probing dead node.\n    </description>\n</property>\n\n<property>\n  <name>dfs.namenode.edekcacheloader.interval.ms</name>\n  <value>1000</value>\n  <description>When KeyProvider is configured, the interval time of warming\n    up edek cache on NN starts up / becomes active. All edeks will be loaded\n    from KMS into provider cache. The edek cache loader will try to warm up the\n    cache until succeed or NN leaves active state.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.hedged.read.threadpool.size</name>\n  <value>0</value>\n  <description>\n    Support 'hedged' reads in DFSClient. To enable this feature, set the parameter\n    to a positive number. The threadpool size is how many threads to dedicate\n    to the running of these 'hedged', concurrent reads in your client.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.namenode.replication.min</name>\n  <value>2</value>\n  <description>Minimal block replication. \n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.resource.checked.volumes.minimum</name>\n  <value>1</value>\n  <description>\n    The minimum number of redundant NameNode storage volumes required.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.redundancy.interval.seconds</name>\n  <value>1s</value>\n  <description>The periodicity in seconds with which the namenode computes \n  low redundancy work for datanodes. Support multiple time unit suffix(case insensitive),\n  as described in dfs.heartbeat.interval.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.num.extra.edits.retained</name>\n  <value>2000000</value>\n  <description>The number of extra transactions which should be retained\n  beyond what is minimally necessary for a NN restart.\n  It does not translate directly to file's age, or the number of files kept,\n  but to the number of transactions (here \"edits\" means transactions).\n  One edit file may contain several transactions (edits).\n  During checkpoint, NameNode will identify the total number of edits to retain as extra by\n  checking the latest checkpoint transaction value, subtracted by the value of this property.\n  Then, it scans edits files to identify the older ones that don't include the computed range of\n  retained transactions that are to be kept around, and purges them subsequently.\n  The retainment can be useful for audit purposes or for an HA setup where a remote Standby Node may have\n  been offline for some time and need to have a longer backlog of retained\n  edits in order to start again.\n  Typically each edit is on the order of a few hundred bytes, so the default\n  of 1 million edits should be on the order of hundreds of MBs or low GBs.\n\n  NOTE: Fewer extra edits may be retained than value specified for this setting\n  if doing so would mean that more segments would be retained than the number\n  configured by dfs.namenode.max.extra.edits.segments.retained.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.mmap.cache.timeout.ms</name>\n  <value>1800000</value>\n  <description>\n    The minimum length of time that we will keep an mmap entry in the cache\n    between uses.  If an entry is in the cache longer than this, and nobody\n    uses it, it will be removed by a background thread.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.lock-reporting-threshold-ms</name>\n  <value>300</value>\n  <description>When thread waits to obtain a lock, or a thread holds a lock for\n    more than the threshold, a log message will be written. Note that\n    dfs.lock.suppress.warning.interval ensures a single log message is\n    emitted per interval for waiting threads and a single message for holding\n    threads to avoid excessive logging.\n  </description>\n</property>\n\n<property>\n  <name>dfs.http.client.failover.max.attempts</name>\n  <value>30</value>\n  <description>\n    Specify the max number of failover attempts for WebHDFS client\n    in case of network exception.\n  </description>\n</property>\n\n<property>\n  <name>dfs.balancer.getBlocks.min-block-size</name>\n  <value>20971520</value>\n  <description>\n    Minimum block threshold size in bytes to ignore when fetching a source's\n    block list.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "tickTime=1500\n\ninitLimit=10\n\nmaxSessionTimeout=0\n\nreconfigEnabled=false\n\nautopurge.purgeInterval=1\n\nstandaloneEnabled=false\n\nsyncLimit=1\n\nquorum.auth.learner.saslLoginContext=QuorumLearner\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": true}}, {"input": "alluxio.underfs.web.titles=Index of,Directory listing for\n\nalluxio.user.file.persist.on.rename=true\n\nalluxio.table.transform.manager.job.history.retention.time=300sec\n\nalluxio.master.daily.backup.state.lock.timeout=1h\n\nalluxio.master.file.access.time.journal.flush.interval=1h\n\nalluxio.job.master.hostname=${alluxio.master.hostname}\n\nalluxio.master.standby.heartbeat.interval=2min\n\nalluxio.master.journal.gc.period=4min\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    \n<property>\n  <name>yarn.scheduler.maximum-allocation-mb</name>\n  <value>16384</value>\n    <description>The maximum allocation for every container request at the RM\n    in MBs. Memory requests higher than this will throw an\n    InvalidResourceRequestException.</description>\n</property>\n\n<property>\n  <name>yarn.client.failover-no-ha-proxy-provider</name>\n  <value>org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider</value>\n    <description>When HA is not enabled, the class to be used by Clients, AMs and\n      NMs to retry connecting to the Active RM. It should extend\n      org.apache.hadoop.yarn.client.RMFailoverProxyProvider</description>\n</property>\n\n<property>\n  <name>yarn.resourcemanager.delegation-token-renewer.thread-timeout</name>\n  <value>120s</value>\n    <description>\n    RM DelegationTokenRenewer thread timeout\n    </description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.resource.percentage-physical-cpu-limit</name>\n  <value>100</value>\n    <description>Percentage of CPU that can be allocated\n    for containers. This setting allows users to limit the amount of\n    CPU that YARN containers use. Currently functional only\n    on Linux using cgroups. The default is to use 100% of CPU.\n    </description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.runtime.linux.sandbox-mode.local-dirs.permissions</name>\n  <value>read</value>\n    <description>Permissions for application local directories.</description>\n</property>\n\n<property>\n  <name>yarn.timeline-service.client.internal-timers-ttl-secs</name>\n  <value>420</value>\n    <description>\n      How long the internal Timer Tasks can be alive in writer. If there is no\n      write operation for this configured time, the internal timer tasks will\n      be close.\n    </description>\n</property>\n\n<property>\n  <name>yarn.resourcemanager.nm-container-queuing.queue-limit-stdev</name>\n  <value>1.0f</value>\n    <description>\n    Value of standard deviation used for calculation of queue limit thresholds.\n    </description>\n</property>\n\n<property>\n  <name>yarn.timeline-service.reader.webapp.address</name>\n  <value>127.0.0.1</value>\n    <description>The http address of the timeline reader web application.</description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "yarn", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.datanode.dns.interface</name>\n  <value>default</value>\n   <description>\n     The name of the Network Interface from which a data node should\n     report its IP address. e.g. eth2. This setting may be required for some\n     multi-homed nodes where the DataNodes are assigned multiple hostnames\n     and it is desirable for the DataNodes to use a non-default hostname.\n\n     Prefer using hadoop.security.dns.interface over\n     dfs.datanode.dns.interface.\n   </description>\n</property>\n\n<property>\n  <name>dfs.namenode.lifeline.handler.ratio</name>\n  <value>0.2</value>\n  <description>\n    A ratio applied to the value of dfs.namenode.handler.count, which then\n    provides the number of RPC server threads the NameNode runs for handling the\n    lifeline RPC server.  For example, if dfs.namenode.handler.count is 100, and\n    dfs.namenode.lifeline.handler.factor is 0.10, then the NameNode starts\n    100 * 0.10 = 10 threads for handling the lifeline RPC server.  It is common\n    to tune the value of dfs.namenode.handler.count as a function of the number\n    of DataNodes in a cluster.  Using this property allows for the lifeline RPC\n    server handler threads to be tuned automatically without needing to touch a\n    separate property.  Lifeline message processing is lightweight, so it is\n    expected to require many fewer threads than the main NameNode RPC server.\n    This property is not used if dfs.namenode.lifeline.handler.count is defined,\n    which sets an absolute thread count.  This property has no effect if\n    dfs.namenode.lifeline.rpc-address is not defined.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.safemode.extension</name>\n  <value>60000</value>\n  <description>\n    Determines extension of safe mode in milliseconds after the threshold level\n    is reached.  Support multiple time unit suffix (case insensitive), as\n    described in dfs.heartbeat.interval.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.failed.volumes.tolerated</name>\n  <value>-1</value>\n  <description>The number of volumes that are allowed to\n  fail before a datanode stops offering service. By default\n  any volume failure will cause a datanode to shutdown.\n  The value should be greater than or equal to -1 , -1 represents minimum\n  1 valid volume.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.edit.log.autoroll.check.interval.ms</name>\n  <value>300000</value>\n  <description>\n    How often an active namenode will check if it needs to roll its edit log,\n    in milliseconds.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.write.max-packets-in-flight</name>\n  <value>40</value>\n  <description>\n    The maximum number of DFSPackets allowed in flight.\n  </description>\n</property>\n\n<property>\n  <name>dfs.provided.acls.import.enabled</name>\n  <value>false</value>\n    <description>\n      Set to true to inherit ACLs (Access Control Lists) from remote stores\n      during mount. Disabled by default, i.e., ACLs are not inherited from\n      remote stores. Note had HDFS ACLs have to be enabled\n      (dfs.namenode.acls.enabled must be set to true) for this to take effect.\n    </description>\n</property>\n\n<property>\n  <name>httpfs.buffer.size</name>\n  <value>ciri/ciri</value>\n    <description>\n      The size buffer to be used when creating or opening httpfs filesystem IO stream.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>zookeeper.znode.parent</name>\n  <value>/hbase</value>\n    <description>Root ZNode for HBase in ZooKeeper. All of HBase's ZooKeeper\n      files that are configured with a relative path will go under this node.\n      By default, all of HBase's ZooKeeper file paths are configured with a\n      relative path, so they will all go under this directory unless changed.\n    </description>\n</property>\n\n<property>\n  <name>hbase.client.keyvalue.maxsize</name>\n  <value>5242880</value>\n    <description>Specifies the combined maximum allowed size of a KeyValue\n    instance. This is to set an upper boundary for a single entry saved in a\n    storage file. Since they cannot be split it helps avoiding that a region\n    cannot be split any further because the data is too large. It seems wise\n    to set this to a fraction of the maximum region size. Setting it to zero\n    or less disables the check.</description>\n</property>\n\n<property>\n  <name>hbase.client.scanner.timeout.period</name>\n  <value>30000</value>\n    <description>Client scanner lease period in milliseconds.</description>\n</property>\n\n<property>\n  <name>hbase.rest.support.proxyuser</name>\n  <value>true</value>\n    <description>Enables running the REST server to support proxy-user mode.</description>\n</property>\n\n<property>\n  <name>hbase.thrift.maxWorkerThreads</name>\n  <value>500</value>\n    <description>The maximum size of the thread pool. When the pending request queue\n    overflows, new threads are created until their number reaches this number.\n    After that, the server starts dropping connections.</description>\n</property>\n\n<property>\n  <name>hbase.rootdir.perms</name>\n  <value>1400</value>\n    <description>FS Permissions for the root data subdirectory in a secure (kerberos) setup.\n    When master starts, it creates the rootdir with this permissions or sets the permissions\n    if it does not match.</description>\n</property>\n\n<property>\n  <name>hbase.security.authentication</name>\n  <value>simple</value>\n    <description>\n      Controls whether or not secure authentication is enabled for HBase.\n      Possible values are 'simple' (no authentication), and 'kerberos'.\n    </description>\n</property>\n\n<property>\n  <name>hbase.mob.compactor.class</name>\n  <value>org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactor</value>\n    <description>\n      Implementation of mob compactor, the default one is PartitionedMobCompactor.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "proxy-dial-timeout: 2000\n\nproxy-refresh-interval: 60000\n\nlogger: zap\n\nproxy: 'off'\n\nquota-backend-bytes: 0\n\nenable-v2: true\n\nforce-new-cluster: true\n\nelection-timeout: 500\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "etcd", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.namenode.edits.dir</name>\n  <value>/valid/dir2</value>\n  <description>Determines where on the local filesystem the DFS name node\n      should store the transaction (edits) file. If this is a comma-delimited list\n      of directories then the transaction file is replicated in all of the\n      directories, for redundancy. Default value is same as dfs.namenode.name.dir\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.edits.journal-plugin.qjournal</name>\n  <value>org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager</value>\n</property>\n\n<property>\n  <name>dfs.namenode.service.handler.count</name>\n  <value>10</value>\n  <description>The number of Namenode RPC server threads that listen to\n  requests from DataNodes and from all other non-client nodes.\n  dfs.namenode.service.handler.count will be valid only if\n  dfs.namenode.servicerpc-address is configured.\n  </description>\n</property>\n\n<property>\n  <name>dfs.bytes-per-checksum</name>\n  <value>1024</value>\n  <description>The number of bytes per checksum.  Must not be larger than\n  dfs.stream-buffer-size</description>\n</property>\n\n<property>\n  <name>dfs.namenode.checkpoint.dir</name>\n  <value>/valid/dir1</value>\n  <description>Determines where on the local filesystem the DFS secondary\n      name node should store the temporary images to merge.\n      If this is a comma-delimited list of directories then the image is\n      replicated in all of the directories for redundancy.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.num.checkpoints.retained</name>\n  <value>4</value>\n  <description>The number of image checkpoint files (fsimage_*) that will be retained by\n  the NameNode and Secondary NameNode in their storage directories. All edit\n  logs (stored on edits_* files) necessary to recover an up-to-date namespace from the oldest retained\n  checkpoint will also be retained.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.failover.proxy.provider</name>\n  <value>dfs.client.failover.proxy.provider.mycluster</value>\n  <description>\n    The prefix (plus a required nameservice ID) for the class name of the configured\n    Failover proxy provider for the host. For normal HA mode, please consult\n    the \"Configuration Details\" section of the HDFS High Availability documentation.\n    For observer reading mode, please choose a custom class--ObserverReadProxyProvider.\n  </description>\n</property>\n\n<property>\n  <name>dfs.nameservices</name>\n  <value>sacluster</value>\n  <description>\n    Comma-separated list of nameservices.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": false}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    \n<property>\n  <name>yarn.nodemanager.collector-service.thread-count</name>\n  <value>5</value>\n    <description>Number of threads collector service uses.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.resource.memory.enabled</name>\n  <value>false</value>\n    <description>Whether YARN CGroups memory tracking is enabled.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.health-checker.interval-ms</name>\n  <value>600000</value>\n    <description>Frequency of running node health scripts.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.runtime.linux.runc.image-tag-to-manifest-plugin</name>\n  <value>org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.runc.ImageTagToManifestPlugin</value>\n    <description>The runC image tag to manifest plugin\n      class to be used.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.runtime.linux.sandbox-mode.local-dirs.permissions</name>\n  <value>read</value>\n    <description>Permissions for application local directories.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.sleep-delay-before-sigkill.ms</name>\n  <value>-10</value>\n    <description>No. of ms to wait between sending a SIGTERM and SIGKILL to a container</description>\n</property>\n\n<property>\n  <name>yarn.timeline-service.keytab</name>\n  <value>/valid/file2</value>\n    <description>The Kerberos keytab for the timeline server.</description>\n</property>\n\n<property>\n  <name>yarn.router.pipeline.cache-max-size</name>\n  <value>50</value>\n    <description>\n      Size of LRU cache for Router ClientRM Service and RMAdmin Service.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "yarn", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.namenode.redundancy.considerLoad.factor</name>\n  <value>2.0</value>\n    <description>The factor by which a node's load can exceed the average\n      before being rejected for writes, only if considerLoad is true.\n    </description>\n</property>\n\n<property>\n  <name>dfs.permissions.superusergroup</name>\n  <value>supergroup</value>\n  <description>The name of the group of super-users.\n    The value should be a single group name.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.read.shortcircuit</name>\n  <value>false</value>\n  <description>\n    This configuration parameter turns on short-circuit local reads.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.reject-unresolved-dn-topology-mapping</name>\n  <value>true</value>\n  <description>\n    If the value is set to true, then namenode will reject datanode\n    registration if the topology mapping for a datanode is not resolved and\n    NULL is returned (script defined by net.topology.script.file.name fails\n    to execute). Otherwise, datanode will be registered and the default rack\n    will be assigned as the topology path. Topology paths are important for\n    data resiliency, since they define fault domains. Thus it may be unwanted\n    behavior to allow datanode registration with the default rack if the\n    resolving topology failed.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.reencrypt.batch.size</name>\n  <value>2000</value>\n  <description>How many EDEKs should the re-encrypt thread process in one batch.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.shared.edits.dir</name>\n  <value>/tmp/share</value>\n  <description>A directory on shared storage between the multiple namenodes\n  in an HA cluster. This directory will be written by the active and read\n  by the standby in order to keep the namespaces synchronized. This directory\n  does not need to be listed in dfs.namenode.edits.dir above. It should be\n  left empty in a non-HA cluster.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.max.block.acquire.failures</name>\n  <value>6</value>\n  <description>\n    Maximum failures allowed when trying to get block information from a specific datanode.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.provided.enabled</name>\n  <value>false</value>\n    <description>\n      Enables the Namenode to handle provided storages.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": false}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>fs.s3a.s3guard.ddb.table.capacity.write</name>\n  <value>1</value>\n  <description>\n    Provisioned throughput requirements for write operations in terms of\n    capacity units for the DynamoDB table.\n    If set to 0 (the default), new tables are created with \"per-request\" capacity.\n    Refer to related configuration option fs.s3a.s3guard.ddb.table.capacity.read\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.committer.magic.enabled</name>\n  <value>false</value>\n  <description>\n    Enable support in the filesystem for the S3 \"Magic\" committer.\n    When working with AWS S3, S3Guard must be enabled for the destination\n    bucket, as consistent metadata listings are required.\n  </description>\n</property>\n\n<property>\n  <name>fs.wasb.impl</name>\n  <value>org.apache.hadoop.fs.azure.NativeAzureFileSystem</value>\n  <description>The implementation class of the Native Azure Filesystem</description>\n</property>\n\n<property>\n  <name>ipc.server.reuseaddr</name>\n  <value>false</value>\n  <description>Enables the SO_REUSEADDR TCP option on the server.\n    Useful if BindException often prevents a certain service to be restarted\n    because the server side is stuck in TIME_WAIT state.\n  </description>\n</property>\n\n<property>\n  <name>tfile.fs.input.buffer.size</name>\n  <value>262144</value>\n  <description>\n    Buffer size used for FSDataInputStream in bytes.\n  </description>\n</property>\n\n<property>\n  <name>ha.zookeeper.session-timeout.ms</name>\n  <value>10000</value>\n  <description>\n    The session timeout to use when the ZKFC connects to ZooKeeper.\n    Setting this value to a lower value implies that server crashes\n    will be detected more quickly, but risks triggering failover too\n    aggressively in the case of a transient error or network blip.\n  </description>\n</property>\n\n<property>\n  <name>ha.failover-controller.graceful-fence.rpc-timeout.ms</name>\n  <value>5000</value>\n  <description>\n    Timeout that the FC waits for the old active to go to standby\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.kms.client.encrypted.key.cache.num.refill.threads</name>\n  <value>2</value>\n  <description>\n    Number of threads to use for refilling depleted EncryptedKeyVersion\n    cache Queues\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>fs.s3a.retry.throttle.interval</name>\n  <value>1ms</value>\n  <description>\n    Initial between retry attempts on throttled requests, +/- 50%. chosen at random.\n    i.e. for an intial value of 3000ms, the initial delay would be in the range 1500ms to 4500ms.\n    Backoffs are exponential; again randomness is used to avoid the thundering heard problem.\n    500ms is the default value used by the AWS S3 Retry policy.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.ssl.channel.mode</name>\n  <value>default_jsse</value>\n  <description>\n    If secure connections to S3 are enabled, configures the SSL\n    implementation used to encrypt connections to S3. Supported values are:\n    \"default_jsse\", \"default_jsse_with_gcm\", \"default\", and \"openssl\".\n    \"default_jsse\" uses the Java Secure Socket Extension package (JSSE).\n    However, when running on Java 8, the GCM cipher is removed from the list\n    of enabled ciphers. This is due to performance issues with GCM in Java 8.\n    \"default_jsse_with_gcm\" uses the JSSE with the default list of cipher\n    suites. \"default_jsse_with_gcm\" is equivalent to the behavior prior to\n    this feature being introduced. \"default\" attempts to use OpenSSL rather\n    than the JSSE for SSL encryption, if OpenSSL libraries cannot be loaded,\n    it falls back to the \"default_jsse\" behavior. \"openssl\" attempts to use\n    OpenSSL as well, but fails if OpenSSL libraries cannot be loaded.\n  </description>\n</property>\n\n<property>\n  <name>s3.bytes-per-checksum</name>\n  <value>1024</value>\n  <description>The number of bytes per checksum.  Must not be larger than\n  s3.stream-buffer-size</description>\n</property>\n\n<property>\n  <name>fs.wasbs.impl</name>\n  <value>org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure</value>\n  <description>The implementation class of the Secure Native Azure Filesystem</description>\n</property>\n\n<property>\n  <name>ipc.[port_number].weighted-cost.lockexclusive</name>\n  <value>50</value>\n  <description>The weight multiplier to apply to the time spent in the\n    processing phase which holds an exclusive (write) lock.\n    This property applies to WeightedTimeCostProvider.\n  </description>\n</property>\n\n<property>\n  <name>file.stream-buffer-size</name>\n  <value>8192</value>\n  <description>The size of buffer to stream files.\n  The size of this buffer should probably be a multiple of hardware\n  page size (4096 on Intel x86), and it determines how much data is\n  buffered during read and write operations.</description>\n</property>\n\n<property>\n  <name>nfs.exports.allowed.hosts</name>\n  <value>* rw</value>\n  <description>\n    By default, the export can be mounted by any client. The value string\n    contains machine name and access privilege, separated by whitespace\n    characters. The machine name format can be a single host, a Java regular\n    expression, or an IPv4 address. The access privilege uses rw or ro to\n    specify read/write or read-only access of the machines to exports. If the\n    access privilege is not provided, the default is read-only. Entries are separated by \";\".\n    For example: \"192.168.0.0/22 rw ; host.*\\.example\\.com ; host1.test.org ro;\".\n    Only the NFS gateway needs to restart after this property is updated.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.prometheus.endpoint.enabled</name>\n  <value>true</value>\n    <description>\n      If set to true, prometheus compatible metric page on the HTTP servers\n      is enabled via '/prom' endpoint.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "dynamic_shared_memory_type=NOEXIST_LOCAL_DATA_CONNECTION_MODE\n\nshared_buffers=128MB\n\ntransform_null_equals=off\n\ndefault_transaction_read_only=off\n\nclient_encoding=sql_ascii\n\nlc_time='C'\n\nmax_prepared_transactions=1\n\nlog_statement_sample_rate=0.5\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "postgresql", "is_synthetic": true}}, {"input": "tls-port=file://\n\nreplica-announce-port=2468\n\nreplica-announce-ip=5.5.5.5\n\ncluster-config-file=nodes-6379.conf\n\ncluster-announce-bus-port=12760\n\nacllog-max-len=128\n\nappendonly=no\n\naof-timestamp-enabled=no\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "repl-backlog-size=1mm\n\nreplica-priority=200\n\nappendfilename=\"appendonly.aof\"\n\nappendfsync=everysec\n\naof-rewrite-incremental-fsync=yes\n\noom-score-adj-values=0 200 800\n\nenable-debug-command=no\n\ndir=./\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.bind.users</name>\n  <value>xdsuper</value>\n  <description>\n    Aliases of users to be used to bind as when connecting to the LDAP\n    server(s). Each alias will have to have its distinguished name and\n    password specified through:\n    hadoop.security.group.mapping.ldap.bind.user\n    and a password configuration such as:\n    hadoop.security.group.mapping.ldap.bind.password.alias\n\n    For example, if:\n    hadoop.security.group.mapping.ldap.bind.users=alias1,alias2\n\n    then the following configuration is valid:\n    hadoop.security.group.mapping.ldap.bind.users.alias1.bind.user=bindUser1\n    hadoop.security.group.mapping.ldap.bind.users.alias1.bind.password.alias=\n    bindPasswordAlias1\n    hadoop.security.group.mapping.ldap.bind.users.alias2.bind.user=bindUser2\n    hadoop.security.group.mapping.ldap.bind.users.alias2.bind.password.alias=\n    bindPasswordAlias2\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.bind.user</name>\n  <value>samsuper</value>\n  <description>\n    The distinguished name of the user to bind as when connecting to the LDAP\n    server. This may be left blank if the LDAP server supports anonymous binds.\n  </description>\n</property>\n\n<property>\n  <name>fs.ftp.impl</name>\n  <value>org.apache.hadoop.fs.ftp.FTPFileSystem</value>\n  <description>The implementation class of the FTP FileSystem</description>\n</property>\n\n<property>\n  <name>fs.s3a.connection.ssl.enabled</name>\n  <value>true</value>\n  <description>Enables or disables SSL connections to AWS services.\n    Also sets the default port to use for the s3a proxy settings,\n    when not explicitly set in fs.s3a.proxy.port.</description>\n</property>\n\n<property>\n  <name>fs.s3a.multipart.purge.age</name>\n  <value>86400</value>\n  <description>Minimum age in seconds of multipart uploads to purge\n    on startup if \"fs.s3a.multipart.purge\" is true\n  </description>\n</property>\n\n<property>\n  <name>ipc.[port_number].weighted-cost.response</name>\n  <value>2</value>\n  <description>The weight multiplier to apply to the time spent in the\n    RESPONSE phase which do not involve holding a lock.\n    See org.apache.hadoop.ipc.ProcessingDetails.Timing for more details on\n    this phase. This property applies to WeightedTimeCostProvider.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.http.cross-origin.max-age</name>\n  <value>3600</value>\n  <description>The number of seconds a pre-flighted request can be cached\n    for web services needing cross-origin (CORS) support.</description>\n</property>\n\n<property>\n  <name>ha.health-monitor.rpc.connect.max.retries</name>\n  <value>1</value>\n  <description>\n    The number of retries on connect error when establishing RPC proxy\n    connection to NameNode, used for monitorHealth() calls.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.blocksize</name>\n  <value>134217728</value>\n  <description>\n      The default block size for new files, in bytes.\n      You can use the following suffix (case insensitive):\n      k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.),\n      Or provide complete size in bytes (such as 134217728 for 128 MB).\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.redundancy.queue.restart.iterations</name>\n  <value>1200</value>\n  <description>When picking blocks from the low redundancy queues, reset the\n    bookmarked iterator after the set number of iterations to ensure any blocks\n    which were not processed on the first pass are retried before the iterators\n    would naturally reach their end point. This ensures blocks are retried\n    more frequently when there are many pending blocks or blocks are\n    continuously added to the queues preventing the iterator reaching its\n    natural endpoint.\n    The default setting of 2400 combined with the default of\n    dfs.namenode.redundancy.interval.seconds means the iterators will be reset\n    approximately every 2 hours.\n    Setting this parameter to zero disables the feature and the iterators will\n    be reset only when the end of all queues has been reached.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.path.based.cache.block.map.allocation.percent</name>\n  <value>0.25</value>\n  <description>\n    The percentage of the Java heap which we will allocate to the cached blocks\n    map.  The cached blocks map is a hash map which uses chained hashing.\n    Smaller maps may be accessed more slowly if the number of cached blocks is\n    large; larger maps will consume more memory.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.fs-limits.max-xattr-size</name>\n  <value>16384</value>\n  <description>\n    The maximum combined size of the name and value of an extended attribute\n    in bytes. It should be larger than 0, and less than or equal to maximum\n    size hard limit which is 32768.\n    Support multiple size unit suffix(case insensitive), as described in\n    dfs.blocksize.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.cache.revocation.polling.ms</name>\n  <value>250</value>\n  <description>How often the DataNode should poll to see if the clients have\n    stopped using a replica that the DataNode wants to uncache.\n  </description>\n</property>\n\n<property>\n  <name>dfs.block.misreplication.processing.limit</name>\n  <value>20000</value>\n  <description>\n    Maximum number of blocks to process for initializing replication queues.\n  </description>\n</property>\n\n<property>\n  <name>dfs.content-summary.sleep-microsec</name>\n  <value>1000</value>\n  <description>\n    The length of time in microseconds to put the thread to sleep, between reaquiring the locks\n    in content summary computation.\n  </description>\n</property>\n\n<property>\n  <name>dfs.provided.aliasmap.leveldb.path</name>\n  <value>/valid/file1</value>\n    <description>\n      The read/write path for the leveldb-based alias map\n      (org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.LevelDBFileRegionAliasMap).\n      The path has to be explicitly configured when this alias map is used.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "secureClientPort=-1\n\nquorum.auth.serverRequireSasl=true\n\nportUnification=true\n\nlocalSessionsUpgradingEnabled=false\n\nquorum.cnxn.threads.size=10\n\ndataLogDir=/valid/dir2\n\ntickTime=3000\n\nmaxClientCnxns=60\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.regionserver.info.bindAddress</name>\n  <value>-1.0.0.0</value>\n    <description>The address for the HBase RegionServer web UI</description>\n</property>\n\n<property>\n  <name>hbase.rpc.shortoperation.timeout</name>\n  <value>5000</value>\n    <description>This is another version of \"hbase.rpc.timeout\". For those RPC operation\n        within cluster, we rely on this configuration to set a short timeout limitation\n        for short operation. For example, short rpc timeout for region server's trying\n        to report to active master can benefit quicker master failover process.</description>\n</property>\n\n<property>\n  <name>hbase.ipc.client.tcpnodelay</name>\n  <value>false</value>\n    <description>Set no delay on rpc socket connections.  See\n    http://docs.oracle.com/javase/1.5.0/docs/api/java/net/Socket.html#getTcpNoDelay()</description>\n</property>\n\n<property>\n  <name>hbase.defaults.for.version</name>\n  <value>@@@VERSION@@@</value>\n    <description>This defaults file was compiled for version ${project.version}. This variable is used\n    to make sure that a user doesn't have an old version of hbase-default.xml on the\n    classpath.</description>\n</property>\n\n<property>\n  <name>hbase.thrift.maxWorkerThreads</name>\n  <value>1000</value>\n    <description>The maximum size of the thread pool. When the pending request queue\n    overflows, new threads are created until their number reaches this number.\n    After that, the server starts dropping connections.</description>\n</property>\n\n<property>\n  <name>hbase.data.umask</name>\n  <value>000</value>\n    <description>File permissions that should be used to write data\n      files when hbase.data.umask.enable is true</description>\n</property>\n\n<property>\n  <name>hbase.snapshot.restore.take.failsafe.snapshot</name>\n  <value>false</value>\n    <description>Set to true to take a snapshot before the restore operation.\n      The snapshot taken will be used in case of failure, to restore the previous state.\n      At the end of the restore operation this snapshot will be deleted</description>\n</property>\n\n<property>\n  <name>hbase.mob.compaction.chore.period</name>\n  <value>604800</value>\n    <description>\n      The period that MobCompactionChore runs. The unit is second.\n      The default value is one week.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "tls-ca-cert-dir=/\\etc/ssl/certs\n\nreplica-lazy-flush=no\n\nslowlog-log-slower-than=20000\n\ncluster-announce-port=1\n\naof-rewrite-incremental-fsync=yes\n\ndir=./\n\nrepl-diskless-sync=yes\n\njemalloc-bg-thread=yes\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "alluxio.job.worker.web.bind.host=1.1.1.1.1.1\n\nalluxio.master.backup.state.lock.interrupt.cycle.enabled=true\n\nalluxio.user.conf.cluster.default.enabled=false\n\nalluxio.underfs.hdfs.remote=false\n\nalluxio.user.network.rpc.keepalive.timeout=1sec\n\nalluxio.user.file.create.ttl=-2\n\nalluxio.user.file.master.client.pool.gc.threshold=240sec\n\nalluxio.master.worker.connect.wait.time=1sec\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.directory.search.timeout</name>\n  <value>5000</value>\n  <description>\n    The attribute applied to the LDAP SearchControl properties to set a\n    maximum time limit when searching and awaiting a result.\n    Set to 0 if infinite wait period is desired.\n    Default is 10 seconds. Units in milliseconds.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.uid.cache.secs</name>\n  <value>28800</value>\n    <description>\n        This is the config controlling the validity of the entries in the cache\n        containing the userId to userName and groupId to groupName used by\n        NativeIO getFstat().\n    </description>\n</property>\n\n<property>\n  <name>io.serializations</name>\n  <value>org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization</value>\n  <description>A list of serialization classes that can be used for\n  obtaining serializers and deserializers.</description>\n</property>\n\n<property>\n  <name>fs.s3a.threads.max</name>\n  <value>64</value>\n  <description>The total number of threads available in the filesystem for data\n    uploads *or any other queued filesystem operation*.</description>\n</property>\n\n<property>\n  <name>fs.s3a.retry.throttle.interval</name>\n  <value>1ms</value>\n  <description>\n    Initial between retry attempts on throttled requests, +/- 50%. chosen at random.\n    i.e. for an intial value of 3000ms, the initial delay would be in the range 1500ms to 4500ms.\n    Backoffs are exponential; again randomness is used to avoid the thundering heard problem.\n    500ms is the default value used by the AWS S3 Retry policy.\n  </description>\n</property>\n\n<property>\n  <name>ipc.[port_number].scheduler.impl</name>\n  <value>org.apache.hadoop.ipc.DefaultRpcScheduler</value>\n  <description>The fully qualified name of a class to use as the\n    implementation of the scheduler. The default implementation is\n    org.apache.hadoop.ipc.DefaultRpcScheduler (no-op scheduler) when not using\n    FairCallQueue. If using FairCallQueue, defaults to\n    org.apache.hadoop.ipc.DecayRpcScheduler. Use\n    org.apache.hadoop.ipc.DecayRpcScheduler in conjunction with the Fair Call\n    Queue.\n  </description>\n</property>\n\n<property>\n  <name>net.topology.node.switch.mapping.impl</name>\n  <value>org.apache.hadoop.net.ScriptBasedMapping</value>\n  <description> The default implementation of the DNSToSwitchMapping. It\n    invokes a script specified in net.topology.script.file.name to resolve\n    node names. If the value for net.topology.script.file.name is not set, the\n    default value of DEFAULT_RACK is returned for all node names.\n  </description>\n</property>\n\n<property>\n  <name>fs.client.resolve.topology.enabled</name>\n  <value>false</value>\n    <description>Whether the client machine will use the class specified by\n      property net.topology.node.switch.mapping.impl to compute the network\n      distance between itself and remote machines of the FileSystem. Additional\n      properties might need to be configured depending on the class specified\n      in net.topology.node.switch.mapping.impl. For example, if\n      org.apache.hadoop.net.ScriptBasedMapping is used, a valid script file\n      needs to be specified in net.topology.script.file.name.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "max-snapshots: true\n\nadvertise-client-urls: http://localhost:2379\n\ninitial-cluster-token: 'etcd-cluster'\n\nmax-wals: 5\n\nlisten-peer-urls: http://localhost:2380\n\nproxy-write-timeout: 10000\n\nstrict-reconfig-check: true\n\nheartbeat-interval: 200\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "etcd", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.master.logcleaner.ttl</name>\n  <value>600000</value>\n    <description>How long a WAL remain in the archive ({hbase.rootdir}/oldWALs) directory,\n    after which it will be cleaned by a Master thread. The value is in milliseconds.</description>\n</property>\n\n<property>\n  <name>hbase.hstore.compaction.ratio.offpeak</name>\n  <value>6.0F</value>\n    <description>Allows you to set a different (by default, more aggressive) ratio for determining\n      whether larger StoreFiles are included in compactions during off-peak hours. Works in the\n      same way as hbase.hstore.compaction.ratio. Only applies if hbase.offpeak.start.hour and\n      hbase.offpeak.end.hour are also enabled.</description>\n</property>\n\n<property>\n  <name>hbase.offpeak.start.hour</name>\n  <value>-1</value>\n    <description>The start of off-peak hours, expressed as an integer between 0 and 23, inclusive.\n      Set to -1 to disable off-peak.</description>\n</property>\n\n<property>\n  <name>hbase.auth.token.max.lifetime</name>\n  <value>604800000</value>\n    <description>The maximum lifetime in milliseconds after which an\n    authentication token expires.  Only used when HBase security is enabled.</description>\n</property>\n\n<property>\n  <name>hbase.table.max.rowsize</name>\n  <value>2147483648</value>\n    <description>\n      Maximum size of single row in bytes (default is 1 Gb) for Get'ting\n      or Scan'ning without in-row scan flag set. If row size exceeds this limit\n      RowTooBigException is thrown to client.\n    </description>\n</property>\n\n<property>\n  <name>hbase.regionserver.thrift.compact</name>\n  <value>false</value>\n    <description>Use Thrift TCompactProtocol binary serialization protocol.</description>\n</property>\n\n<property>\n  <name>hbase.column.max.version</name>\n  <value>2</value>\n    <description>New column family descriptors will use this value as the default number of versions\n      to keep.</description>\n</property>\n\n<property>\n  <name>hbase.security.exec.permission.checks</name>\n  <value>false</value>\n    <description>\n      If this setting is enabled and ACL based access control is active (the\n      AccessController coprocessor is installed either as a system coprocessor\n      or on a table as a table coprocessor) then you must grant all relevant\n      users EXEC privilege if they require the ability to execute coprocessor\n      endpoint calls. EXEC privilege, like any other permission, can be\n      granted globally to a user, or to a user on a per table or per namespace\n      basis. For more information on coprocessor endpoints, see the coprocessor\n      section of the HBase online manual. For more information on granting or\n      revoking permissions using the AccessController, see the security\n      section of the HBase online manual.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "NUMBER_GROUPING=-5.5\n\nLANGUAGE_COOKIE_DOMAIN=None\n\nUSE_TZ=False\n\nCSRF_COOKIE_SECURE=True\n\nMESSAGE_STORAGE='django.contrib.messages.storage.fallback.FallbackStorage'\n\nTEMPLATES=[]\n\nEMAIL_SUBJECT_PREFIX='[Django] '\n\nEMAIL_SSL_CERTFILE=None\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "django", "is_synthetic": true}}, {"input": "alluxio.job.worker.data.port=ciri\n\nalluxio.master.update.check.enabled=true\n\nalluxio.worker.tieredstore.level0.watermark.high.ratio=0.95\n\nalluxio.job.worker.rpc.port=3000\n\nalluxio.web.threads=1\n\nalluxio.master.journal.type=EMBEDDED\n\nalluxio.underfs.web.titles=Index of\n\nalluxio.worker.jvm.monitor.enabled=true\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.credential.clear-text-fallback</name>\n  <value>false</value>\n  <description>\n    true or false to indicate whether or not to fall back to storing credential\n    password as clear text. The default value is true. This property only works\n    when the password can't not be found from credential providers.\n  </description>\n</property>\n\n<property>\n  <name>fs.default.name</name>\n  <value>file:///</value>\n  <description>Deprecated. Use (fs.defaultFS) property\n  instead</description>\n</property>\n\n<property>\n  <name>fs.AbstractFileSystem.har.impl</name>\n  <value>org.apache.hadoop.fs.HarFs</value>\n  <description>The AbstractFileSystem for har: uris.</description>\n</property>\n\n<property>\n  <name>fs.s3a.threads.max</name>\n  <value>64</value>\n  <description>The total number of threads available in the filesystem for data\n    uploads *or any other queued filesystem operation*.</description>\n</property>\n\n<property>\n  <name>ipc.server.listen.queue.size</name>\n  <value>128</value>\n  <description>Indicates the length of the listen queue for servers accepting\n               client connections.\n  </description>\n</property>\n\n<property>\n  <name>ha.failover-controller.active-standby-elector.zk.op.retries</name>\n  <value>3</value>\n  <description>\n    The number of zookeeper operation retry times in ActiveStandbyElector\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.key.provider.path</name>\n  <value>/valid/file1</value>\n  <description>\n    The KeyProvider to use when managing zone keys, and interacting with\n    encryption keys when reading and writing to an encryption zone.\n    For hdfs clients, the provider path will be same as namenode's\n    provider path.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.http.sni.host.check.enabled</name>\n  <value>false</value>\n    <description>\n      Enable Server Name Indication (SNI) host check for HTTPS enabled server.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.regionserver.handler.count</name>\n  <value>30</value>\n    <description>Count of RPC Listener instances spun up on RegionServers.\n      Same property is used by the Master for count of master handlers.\n      Too many handlers can be counter-productive. Make it a multiple of\n      CPU count. If mostly read-only, handlers count close to cpu count\n      does well. Start with twice the CPU count and tune from there.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.dns.interface</name>\n  <value>default</value>\n    <description>The name of the Network Interface from which a region server\n      should report its IP address.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.region.split.policy</name>\n  <value>org.apache.hadoop.hbase.regionserver.SteppingSplitPolicy</value>\n    <description>\n      A split policy determines when a region should be split. The various\n      other split policies that are available currently are BusyRegionSplitPolicy,\n      ConstantSizeRegionSplitPolicy, DisabledRegionSplitPolicy,\n      DelimitedKeyPrefixRegionSplitPolicy, KeyPrefixRegionSplitPolicy, and\n      SteppingSplitPolicy. DisabledRegionSplitPolicy blocks manual region splitting.\n    </description>\n</property>\n\n<property>\n  <name>hbase.hregion.memstore.block.multiplier</name>\n  <value>4</value>\n    <description>\n    Block updates if memstore has hbase.hregion.memstore.block.multiplier\n    times hbase.hregion.memstore.flush.size bytes.  Useful preventing\n    runaway memstore during spikes in update traffic.  Without an\n    upper-bound, memstore fills such that when it flushes the\n    resultant flush files take a long time to compact or split, or\n    worse, we OOME.</description>\n</property>\n\n<property>\n  <name>hbase.ipc.client.fallback-to-simple-auth-allowed</name>\n  <value>true</value>\n    <description>When a client is configured to attempt a secure connection, but attempts to\n      connect to an insecure server, that server may instruct the client to\n      switch to SASL SIMPLE (unsecure) authentication. This setting controls\n      whether or not the client will accept this instruction from the server.\n      When false (the default), the client will not allow the fallback to SIMPLE\n      authentication, and will abort the connection.</description>\n</property>\n\n<property>\n  <name>hbase.snapshot.enabled</name>\n  <value>false</value>\n    <description>Set to true to allow snapshots to be taken / restored / cloned.</description>\n</property>\n\n<property>\n  <name>hbase.status.multicast.address.ip</name>\n  <value>1.1.1.1.1.1</value>\n    <description>\n      Multicast address to use for the status publication by multicast.\n    </description>\n</property>\n\n<property>\n  <name>hbase.snapshot.master.timeout.millis</name>\n  <value>150000</value>\n    <description>\n       Timeout for master for the snapshot procedure execution.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    \n<property>\n  <name>yarn.resourcemanager.address</name>\n  <value>${yarn.resourcemanager.hostname}:8032</value>\n    <description>The address of the applications manager interface in the RM.</description>\n</property>\n\n<property>\n  <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>\n  <value>true</value>\n    <description>Enable automatic failover.\n      By default, it is enabled only when HA is enabled</description>\n</property>\n\n<property>\n  <name>yarn.resourcemanager.zk-delegation-token-node.split-index</name>\n  <value>0</value>\n    <description>Index at which the RM Delegation Token ids will be split so\n      that the delegation token znodes stored in the zookeeper RM state store\n      will be stored as two different znodes (parent-child). The split is done\n      from the end. For instance, with no split, a delegation token znode will\n      be of the form RMDelegationToken_123456789. If the value of this config is\n      1, the delegation token znode will be broken into two parts:\n      RMDelegationToken_12345678 and 9 respectively with former being the parent\n      node. This config can take values from 0 to 4. 0 means there will be no\n      split. If the value is outside this range, it will be treated as 0 (i.e.\n      no split). A value larger than 0 (up to 4) should be configured if you are\n      running a large number of applications, with long-lived delegation tokens\n      and state store operations (e.g. failover) are failing due to LenError in\n      Zookeeper.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.process-kill-wait.ms</name>\n  <value>10000</value>\n    <description>Max time to wait for a process to come up when trying to cleanup a container</description>\n</property>\n\n<property>\n  <name>yarn.timeline-service.hostname</name>\n  <value>x.0.0.0.0.0</value>\n    <description>The hostname of the timeline service web application.</description>\n</property>\n\n<property>\n  <name>yarn.registry.class</name>\n  <value>org.apache.hadoop.registry.client.impl.FSRegistryOperationsService</value>\n    <description>The registry implementation to use.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.amrmproxy.address</name>\n  <value>0.0.0.0:3001</value>\n    <description>\n    The address of the AMRMProxyService listener.\n    </description>\n</property>\n\n<property>\n  <name>yarn.resourcemanager.nm-container-queuing.max-queue-length</name>\n  <value>7</value>\n    <description>\n    Max length of container queue at NodeManager.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "yarn", "is_synthetic": true}}, {"input": "SESSION_COOKIE_HTTPONLY=False\n\nSECURE_CROSS_ORIGIN_OPENER_POLICY='same-origin'\n\nLOGGING={}\n\nNUMBER_GROUPING=2\n\nCSRF_COOKIE_DOMAIN=None\n\nMANAGERS=ADMINS\n\nSECURE_REFERRER_POLICY='same-origin'\n\nTEMPLATES=[]\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "django", "is_synthetic": true}}, {"input": "aclfile=dev/urandom///users.acl\n\nauto-aof-rewrite-min-size=32mb\n\nlazyfree-lazy-eviction=no\n\ndaemonize=no\n\njemalloc-bg-thread=yes\n\nport=12758\n\naof-rewrite-incremental-fsync=yes\n\nunixsocketperm=1400\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.zookeeper.property.dataDir</name>\n  <value>/valid/file1</value>\n    <description>Property from ZooKeeper's config zoo.cfg.\n    The directory where the snapshot is stored.</description>\n</property>\n\n<property>\n  <name>hbase.zookeeper.property.maxClientCnxns</name>\n  <value>150</value>\n    <description>Property from ZooKeeper's config zoo.cfg.\n    Limit on number of concurrent connections (at the socket level) that a\n    single client, identified by IP address, may make to a single member of\n    the ZooKeeper ensemble. Set high to avoid zk connection issues running\n    standalone and pseudo-distributed.</description>\n</property>\n\n<property>\n  <name>hbase.defaults.for.version.skip</name>\n  <value>true</value>\n    <description>Set to true to skip the 'hbase.defaults.for.version' check.\n    Setting this to true can be useful in contexts other than\n    the other side of a maven generation; i.e. running in an\n    IDE.  You'll want to set this boolean to true to avoid\n    seeing the RuntimeException complaint: \"hbase-default.xml file\n    seems to be for and old version of HBase (\\${hbase.version}), this\n    version is X.X.X-SNAPSHOT\"</description>\n</property>\n\n<property>\n  <name>hbase.table.lock.enable</name>\n  <value>true</value>\n    <description>Set to true to enable locking the table in zookeeper for schema change operations.\n    Table locking from master prevents concurrent schema modifications to corrupt table\n    state.</description>\n</property>\n\n<property>\n  <name>hbase.thrift.maxQueuedRequests</name>\n  <value>1000</value>\n    <description>The maximum number of pending Thrift connections waiting in the queue. If\n     there are no idle threads in the pool, the server queues requests. Only\n     when the queue overflows, new threads are added, up to\n     hbase.thrift.maxQueuedRequests threads.</description>\n</property>\n\n<property>\n  <name>hbase.rootdir.perms</name>\n  <value>350</value>\n    <description>FS Permissions for the root data subdirectory in a secure (kerberos) setup.\n    When master starts, it creates the rootdir with this permissions or sets the permissions\n    if it does not match.</description>\n</property>\n\n<property>\n  <name>hbase.snapshot.enabled</name>\n  <value>true</value>\n    <description>Set to true to allow snapshots to be taken / restored / cloned.</description>\n</property>\n\n<property>\n  <name>hbase.snapshot.region.timeout</name>\n  <value>150000</value>\n    <description>\n       Timeout for regionservers to keep threads in snapshot request pool waiting.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "cluster-enabled=no\n\ncluster-config-file=nodes-6378.conf\n\ntimeout=0\n\nhash-max-listpack-value=128\n\nrepl-diskless-sync-delay=5\n\ncluster-announce-bus-port=3190\n\nlist-max-listpack-size=-1\n\nprotected-mode=yes\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.regionserver.info.port</name>\n  <value>16030</value>\n    <description>The port for the HBase RegionServer web UI\n    Set to -1 if you do not want the RegionServer UI to run.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.info.bindAddress</name>\n  <value>2.2.2.2.2.2</value>\n    <description>The address for the HBase RegionServer web UI</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.hlog.writer.impl</name>\n  <value>org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter</value>\n    <description>The WAL file writer implementation.</description>\n</property>\n\n<property>\n  <name>hbase.hregion.memstore.flush.size</name>\n  <value>134217728</value>\n    <description>\n    Memstore will be flushed to disk if size of the memstore\n    exceeds this number of bytes.  Value is checked by a thread that runs\n    every hbase.server.thread.wakefrequency.</description>\n</property>\n\n<property>\n  <name>hbase.storescanner.parallel.seek.threads</name>\n  <value>20</value>\n    <description>\n      The default thread pool size if parallel-seeking feature enabled.</description>\n</property>\n\n<property>\n  <name>hbase.replication.rpc.codec</name>\n  <value>org.apache.hadoop.hbase.codec.KeyValueCodecWithTags</value>\n  \t<description>\n  \t\tThe codec that is to be used when replication is enabled so that\n  \t\tthe tags are also replicated. This is used along with HFileV3 which\n  \t\tsupports tags in them.  If tags are not used or if the hfile version used\n  \t\tis HFileV2 then KeyValueCodec can be used as the replication codec. Note that\n  \t\tusing KeyValueCodecWithTags for replication when there are no tags causes no harm.\n  \t</description>\n</property>\n\n<property>\n  <name>hbase.replication.source.maxthreads</name>\n  <value>1</value>\n    <description>\n        The maximum number of threads any replication source will use for\n        shipping edits to the sinks in parallel. This also limits the number of\n        chunks each replication batch is broken into. Larger values can improve\n        the replication throughput between the master and slave clusters. The\n        default of 10 will rarely need to be changed.\n    </description>\n</property>\n\n<property>\n  <name>hbase.master.mob.ttl.cleaner.period</name>\n  <value>86400</value>\n    <description>\n      The period that ExpiredMobFileCleanerChore runs. The unit is second.\n      The default value is one day. The MOB file name uses only the date part of\n      the file creation time in it. We use this time for deciding TTL expiry of\n      the files. So the removal of TTL expired files might be delayed. The max\n      delay might be 24 hrs.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.master.logcleaner.plugins</name>\n  <value>org.apache.hadoop.hbase.master.cleaner.TimeToLiveProcedureWALCleaner</value>\n    <description>A comma-separated list of BaseLogCleanerDelegate invoked by\n    the LogsCleaner service. These WAL cleaners are called in order,\n    so put the cleaner that prunes the most files in front. To\n    implement your own BaseLogCleanerDelegate, just put it in HBase's classpath\n    and add the fully qualified class name here. Always add the above\n    default log cleaners in the list.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.info.port.auto</name>\n  <value>false</value>\n    <description>Whether or not the Master or RegionServer\n    UI should search for a port to bind to. Enables automatic port\n    search if hbase.regionserver.info.port is already in use.\n    Useful for testing, turned off by default.</description>\n</property>\n\n<property>\n  <name>hbase.master.regions.recovery.check.interval</name>\n  <value>1200000</value>\n    <description>\n      Regions Recovery Chore interval in milliseconds.\n      This chore keeps running at this interval to\n      find all regions with configurable max store file ref count\n      and reopens them.\n    </description>\n</property>\n\n<property>\n  <name>hbase.zookeeper.property.initLimit</name>\n  <value>1</value>\n    <description>Property from ZooKeeper's config zoo.cfg.\n    The number of ticks that the initial synchronization phase can take.</description>\n</property>\n\n<property>\n  <name>hbase.hregion.majorcompaction.jitter</name>\n  <value>1.0</value>\n    <description>A multiplier applied to hbase.hregion.majorcompaction to cause compaction to occur\n      a given amount of time either side of hbase.hregion.majorcompaction. The smaller the number,\n      the closer the compactions will happen to the hbase.hregion.majorcompaction\n      interval.</description>\n</property>\n\n<property>\n  <name>hbase.master.loadbalancer.class</name>\n  <value>org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer</value>\n    <description>\n      Class used to execute the regions balancing when the period occurs.\n      See the class comment for more on how it works\n      http://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.html\n      It replaces the DefaultLoadBalancer as the default (since renamed\n      as the SimpleLoadBalancer).\n    </description>\n</property>\n\n<property>\n  <name>hbase.security.exec.permission.checks</name>\n  <value>false</value>\n    <description>\n      If this setting is enabled and ACL based access control is active (the\n      AccessController coprocessor is installed either as a system coprocessor\n      or on a table as a table coprocessor) then you must grant all relevant\n      users EXEC privilege if they require the ability to execute coprocessor\n      endpoint calls. EXEC privilege, like any other permission, can be\n      granted globally to a user, or to a user on a per table or per namespace\n      basis. For more information on coprocessor endpoints, see the coprocessor\n      section of the HBase online manual. For more information on granting or\n      revoking permissions using the AccessController, see the security\n      section of the HBase online manual.\n    </description>\n</property>\n\n<property>\n  <name>hbase.master.wait.on.service.seconds</name>\n  <value>30</value>\n    <description>Default is 5 minutes. Make it 30 seconds for tests. See\n    HBASE-19794 for some context.</description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>io.file.buffer.size</name>\n  <value>4096</value>\n  <description>The size of buffer for use in sequence files.\n  The size of this buffer should probably be a multiple of hardware\n  page size (4096 on Intel x86), and it determines how much data is\n  buffered during read and write operations.</description>\n</property>\n\n<property>\n  <name>io.bytes.per.checksum</name>\n  <value>16384</value>\n  <description>The number of bytes per checksum.  Must not be larger than\n  io.file.buffer.size.</description>\n</property>\n\n<property>\n  <name>fs.s3a.multipart.purge.age</name>\n  <value>43200</value>\n  <description>Minimum age in seconds of multipart uploads to purge\n    on startup if \"fs.s3a.multipart.purge\" is true\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.committer.threads</name>\n  <value>16</value>\n  <description>\n    Number of threads in committers for parallel operations on files\n    (upload, commit, abort, delete...)\n  </description>\n</property>\n\n<property>\n  <name>ipc.client.idlethreshold</name>\n  <value>2000</value>\n  <description>Defines the threshold number of connections after which\n               connections will be inspected for idleness.\n  </description>\n</property>\n\n<property>\n  <name>ipc.[port_number].weighted-cost.lockfree</name>\n  <value>0</value>\n  <description>The weight multiplier to apply to the time spent in the\n    LOCKFREE phase which do not involve holding a lock.\n    See org.apache.hadoop.ipc.ProcessingDetails.Timing for more details on\n    this phase. This property applies to WeightedTimeCostProvider.\n  </description>\n</property>\n\n<property>\n  <name>ftp.stream-buffer-size</name>\n  <value>8192</value>\n  <description>The size of buffer to stream files.\n  The size of this buffer should probably be a multiple of hardware\n  page size (4096 on Intel x86), and it determines how much data is\n  buffered during read and write operations.</description>\n</property>\n\n<property>\n  <name>adl.http.timeout</name>\n  <value>-1</value>\n    <description>\n      Base timeout (in milliseconds) for HTTP requests from the ADL SDK. Values\n      of zero or less cause the SDK default to be used instead.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "event_triggers=on\n\nlog_executor_stats=off\n\nwal_skip_threshold=2MB\n\nbytea_output='hex'\n\nautovacuum_vacuum_cost_limit=-2\n\nssl_ciphers='HIGH:MEDIUM:+3DES:!aNULL'\n\ndynamic_library_path='$libdir'\n\nlog_duration=off\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "postgresql", "is_synthetic": true}}, {"input": "autopurge.snapRetainCount=1\n\nquorum.auth.server.saslLoginContext=QuorumServer\n\nminSessionTimeout=0\n\nlocalSessionsEnabled=true\n\ninitLimit=1\n\nquorum.auth.learnerRequireSasl=true\n\nsyncLimit=10\n\ndataLogDir=/valid/dir1\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": true}}, {"input": "alluxio.underfs.s3.admin.threads.max=true\n\nalluxio.master.backup.entry.buffer.count=10000\n\nalluxio.master.ufs.active.sync.event.rate.interval=120sec\n\nalluxio.security.authentication.type=SIMPLE\n\nalluxio.master.backup.connect.interval.max=30sec\n\nalluxio.user.network.streaming.flowcontrol.window=2MB\n\nalluxio.user.logging.threshold=10s\n\nalluxio.master.mount.table.root.shared=false\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.regionserver.info.bindAddress</name>\n  <value>0.0.0.0</value>\n    <description>The address for the HBase RegionServer web UI</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.global.memstore.size</name>\n  <value>0.5</value>\n    <description>Maximum size of all memstores in a region server before new\n      updates are blocked and flushes are forced. Defaults to 40% of heap (0.4).\n      Updates are blocked and flushes are forced until size of all memstores\n      in a region server hits hbase.regionserver.global.memstore.size.lower.limit.\n      The default value in this configuration has been intentionally left empty in order to\n      honor the old hbase.regionserver.global.memstore.upperLimit property if present.\n    </description>\n</property>\n\n<property>\n  <name>hbase.client.perserver.requests.threshold</name>\n  <value>4294967294</value>\n    <description>The max number of concurrent pending requests for one server in all client threads\n    (process level). Exceeding requests will be thrown ServerTooBusyException immediately to prevent\n    user's threads being occupied and blocked by only one slow region server. If you use a fix\n    number of threads to access HBase in a synchronous way, set this to a suitable value which is\n    related to the number of threads will help you. See\n    https://issues.apache.org/jira/browse/HBASE-16388 for details.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.thread.compaction.throttle</name>\n  <value>2684354560</value>\n    <description>There are two different thread pools for compactions, one for large compactions and\n      the other for small compactions. This helps to keep compaction of lean tables (such as\n      hbase:meta) fast. If a compaction is larger than this threshold, it\n      goes into the large compaction pool. In most cases, the default value is appropriate. Default:\n      2 x hbase.hstore.compaction.max x hbase.hregion.memstore.flush.size (which defaults to 128MB).\n      The value field assumes that the value of hbase.hregion.memstore.flush.size is unchanged from\n      the default.</description>\n</property>\n\n<property>\n  <name>hbase.storescanner.parallel.seek.enable</name>\n  <value>10000</value>\n    <description>\n      Enables StoreFileScanner parallel-seeking in StoreScanner,\n      a feature which can reduce response latency under special conditions.</description>\n</property>\n\n<property>\n  <name>hbase.coprocessor.enabled</name>\n  <value>false</value>\n    <description>Enables or disables coprocessor loading. If 'false'\n    (disabled), any other coprocessor related configuration will be ignored.\n    </description>\n</property>\n\n<property>\n  <name>hbase.coprocessor.abortonerror</name>\n  <value>false</value>\n      <description>Set to true to cause the hosting server (master or regionserver)\n      to abort if a coprocessor fails to load, fails to initialize, or throws an\n      unexpected Throwable object. Setting this to false will allow the server to\n      continue execution but the system wide state of the coprocessor in question\n      will become inconsistent as it will be properly executing in only a subset\n      of servers, so this is most useful for debugging only.</description>\n</property>\n\n<property>\n  <name>hbase.data.umask.enable</name>\n  <value>false</value>\n    <description>Enable, if true, that file permissions should be assigned\n      to the files written by the regionserver</description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>fs.azure.user.agent.prefix</name>\n  <value>unknown</value>\n    <description>\n      WASB passes User-Agent header to the Azure back-end. The default value\n      contains WASB version, Java Runtime version, Azure Client library version,\n      and the value of the configuration option fs.azure.user.agent.prefix.\n    </description>\n</property>\n\n<property>\n  <name>fs.AbstractFileSystem.ftp.impl</name>\n  <value>org.apache.hadoop.fs.ftp.FtpFs</value>\n  <description>The FileSystem for Ftp: uris.</description>\n</property>\n\n<property>\n  <name>fs.s3a.committer.staging.tmp.path</name>\n  <value>tmp/staging</value>\n  <description>\n    Path in the cluster filesystem for temporary data.\n    This is for HDFS, not the local filesystem.\n    It is only for the summary data of each file, not the actual\n    data being committed.\n    Using an unqualified path guarantees that the full path will be\n    generated relative to the home directory of the user creating the job,\n    hence private (assuming home directory permissions are secure).\n  </description>\n</property>\n\n<property>\n  <name>fs.azure.secure.mode</name>\n  <value>true</value>\n  <description>\n    Config flag to identify the mode in which fs.azure.NativeAzureFileSystem needs\n    to run under. Setting it \"true\" would make fs.azure.NativeAzureFileSystem use\n    SAS keys to communicate with Azure storage.\n  </description>\n</property>\n\n<property>\n  <name>ipc.client.tcpnodelay</name>\n  <value>false</value>\n  <description>Use TCP_NODELAY flag to bypass Nagle's algorithm transmission delays.\n  </description>\n</property>\n\n<property>\n  <name>ipc.[port_number].decay-scheduler.thresholds</name>\n  <value>[26, 50, 100]</value>\n  <description>The client load threshold, as an integer percentage, for each\n    priority queue. Clients producing less load, as a percent of total\n    operations, than specified at position i will be given priority i. This\n    should be a comma-separated list of length equal to the number of priority\n    levels minus 1 (the last is implicitly 100).\n    Thresholds ascend by a factor of 2 (e.g., for 4 levels: 13,25,50).\n    This property applies to DecayRpcScheduler.\n  </description>\n</property>\n\n<property>\n  <name>file.blocksize</name>\n  <value>33554432</value>\n  <description>Block size</description>\n</property>\n\n<property>\n  <name>fs.adl.oauth2.msi.port</name>\n  <value>3000</value>\n    <description>\n      The localhost port for the MSI token service. This is the port specified\n      when creating the Azure VM. The default, if this setting is not specified,\n      is 50342.\n      Used by MSI token provider.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "proxy-read-timeout: 0\n\nlogger: zap\n\nenable-v2: true\n\nadvertise-client-urls: http://localhost:2379\n\nproxy-dial-timeout: 1000\n\nmax-snapshots: 10\n\nproxy-failure-wait: 10000\n\nquota-backend-bytes: 0\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "etcd", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.regionserver.hlog.reader.impl</name>\n  <value>org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader</value>\n    <description>The WAL file reader implementation.</description>\n</property>\n\n<property>\n  <name>hbase.hregion.majorcompaction.jitter</name>\n  <value>0.50</value>\n    <description>A multiplier applied to hbase.hregion.majorcompaction to cause compaction to occur\n      a given amount of time either side of hbase.hregion.majorcompaction. The smaller the number,\n      the closer the compactions will happen to the hbase.hregion.majorcompaction\n      interval.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.compaction.enabled</name>\n  <value>true</value>\n    <description>Enable/disable compactions on by setting true/false.\n      We can further switch compactions dynamically with the\n      compaction_switch shell command.</description>\n</property>\n\n<property>\n  <name>hbase.hstore.compaction.kv.max</name>\n  <value>1</value>\n    <description>The maximum number of KeyValues to read and then write in a batch when flushing or\n      compacting. Set this lower if you have big KeyValues and problems with Out Of Memory\n      Exceptions Set this higher if you have wide, small rows. </description>\n</property>\n\n<property>\n  <name>hbase.hstore.compaction.throughput.lower.bound</name>\n  <value>26214400</value>\n    <description>The target lower bound on aggregate compaction throughput, in bytes/sec. Allows\n    you to tune the minimum available compaction throughput when the\n    PressureAwareCompactionThroughputController throughput controller is active. (It is active by\n    default.)</description>\n</property>\n\n<property>\n  <name>hbase.ipc.server.fallback-to-simple-auth-allowed</name>\n  <value>false</value>\n    <description>When a server is configured to require secure connections, it will\n      reject connection attempts from clients using SASL SIMPLE (unsecure) authentication.\n      This setting allows secure servers to accept SASL SIMPLE connections from clients\n      when the client requests.  When false (the default), the server will not allow the fallback\n      to SIMPLE authentication, and will reject the connection.  WARNING: This setting should ONLY\n      be used as a temporary measure while converting clients over to secure authentication.  It\n      MUST BE DISABLED for secure operation.</description>\n</property>\n\n<property>\n  <name>hbase.thrift.maxQueuedRequests</name>\n  <value>500</value>\n    <description>The maximum number of pending Thrift connections waiting in the queue. If\n     there are no idle threads in the pool, the server queues requests. Only\n     when the queue overflows, new threads are added, up to\n     hbase.thrift.maxQueuedRequests threads.</description>\n</property>\n\n<property>\n  <name>hbase.snapshot.working.dir</name>\n  <value>/valid/dir1</value>\n    <description>Location where the snapshotting process will occur. The location of the\n      completed snapshots will not change, but the temporary directory where the snapshot\n      process occurs will be set to this location. This can be a separate filesystem than\n      the root directory, for performance increase purposes. See HBASE-21098 for more\n      information</description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.directory.search.timeout</name>\n  <value>20000</value>\n  <description>\n    The attribute applied to the LDAP SearchControl properties to set a\n    maximum time limit when searching and awaiting a result.\n    Set to 0 if infinite wait period is desired.\n    Default is 10 seconds. Units in milliseconds.\n  </description>\n</property>\n\n<property>\n  <name>fs.default.name</name>\n  <value>file:///</value>\n  <description>Deprecated. Use (fs.defaultFS) property\n  instead</description>\n</property>\n\n<property>\n  <name>fs.s3a.retry.throttle.limit</name>\n  <value>40</value>\n  <description>\n    Number of times to retry any throttled request.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.select.input.csv.quote.character</name>\n  <value>\"</value>\n  <description>In S3 Select queries over CSV files: quote character.\n    \\t is remapped to the TAB character, \\r to CR \\n to newline. \\\\ to \\\n    and \\\" to \"\n  </description>\n</property>\n\n<property>\n  <name>ipc.[port_number].scheduler.priority.levels</name>\n  <value>8</value>\n  <description>How many priority levels to use within the scheduler and call\n    queue. This property applies to RpcScheduler and CallQueue.\n  </description>\n</property>\n\n<property>\n  <name>file.client-write-packet-size</name>\n  <value>65536</value>\n  <description>Packet size for clients to write</description>\n</property>\n\n<property>\n  <name>fs.permissions.umask-mode</name>\n  <value>hbase</value>\n  <description>\n    The umask used when creating files and directories.\n    Can be in octal or in symbolic. Examples are:\n    \"022\" (octal for u=rwx,g=r-x,o=r-x in symbolic),\n    or \"u=rwx,g=rwx,o=\" (symbolic for 007 in octal).\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.key.default.bitlength</name>\n  <value>256</value>\n  <description>\n    The length (bits) of keys we want the KeyProvider to produce. Key length\n    defines the upper-bound on an algorithm's security, ideally, it would\n    coincide with the lower-bound on an algorithm's security.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "minSessionTimeout=0\n\nmaxSessionTimeout=0\n\nsyncLimit=1\n\nmaxClientCnxns=120\n\nsslQuorum=false\n\ninitLimit=1\n\npeerType=participant\n\nsecureClientPort=3000\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.instrumentation.requires.admin</name>\n  <value>true</value>\n  <description>\n    Indicates if administrator ACLs are required to access\n    instrumentation servlets (JMX, METRICS, CONF, STACKS).\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.num.attempts.before.failover</name>\n  <value>6</value>\n  <description>\n    This property is the number of attempts to be made for LDAP operations\n    using a single LDAP instance. If multiple LDAP servers are configured\n    and this number of failed operations is reached, we will switch to the\n    next LDAP server. The configuration for the overall number of attempts\n    will still be respected, failover will thus be performed only if this\n    property is less than hadoop.security.group.mapping.ldap.num.attempts.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.fast.upload.buffer</name>\n  <value>disk</value>\n  <description>\n    The buffering mechanism to for data being written.\n    Values: disk, array, bytebuffer.\n\n    \"disk\" will use the directories listed in fs.s3a.buffer.dir as\n    the location(s) to save data prior to being uploaded.\n\n    \"array\" uses arrays in the JVM heap\n\n    \"bytebuffer\" uses off-heap memory within the JVM.\n\n    Both \"array\" and \"bytebuffer\" will consume memory in a single stream up to the number\n    of blocks set by:\n\n        fs.s3a.multipart.size * fs.s3a.fast.upload.active.blocks.\n\n    If using either of these mechanisms, keep this value low\n\n    The total number of threads performing work across all threads is set by\n    fs.s3a.threads.max, with fs.s3a.max.total.tasks values setting the number of queued\n    work items.\n  </description>\n</property>\n\n<property>\n  <name>fs.azure.authorization.caching.enable</name>\n  <value>true</value>\n  <description>\n    Config flag to enable caching of authorization results and saskeys in WASB.\n    This flag is relevant only when fs.azure.authorization is enabled.\n  </description>\n</property>\n\n<property>\n  <name>io.mapfile.bloom.size</name>\n  <value>524288</value>\n  <description>The size of BloomFilter-s used in BloomMapFile. Each time this many\n  keys is appended the next BloomFilter will be created (inside a DynamicBloomFilter).\n  Larger values minimize the number of filters, which slightly increases the performance,\n  but may waste too much space if the total number of keys is usually much smaller\n  than this number.\n  </description>\n</property>\n\n<property>\n  <name>file.blocksize</name>\n  <value>134217728</value>\n  <description>Block size</description>\n</property>\n\n<property>\n  <name>hadoop.http.authentication.token.validity</name>\n  <value>18000</value>\n  <description>\n    Indicates how long (in seconds) an authentication token is valid before it has\n    to be renewed.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.kms.client.encrypted.key.cache.size</name>\n  <value>250</value>\n  <description>\n    Size of the EncryptedKeyVersion cache Queue for each key\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "clientPort=file://\n\nquorum.auth.enableSasl=false\n\npeerType=participant\n\nclientPortAddress=0.0.0.0:3001\n\nlocalSessionsEnabled=false\n\nquorum.auth.serverRequireSasl=true\n\nminSessionTimeout=0\n\nlocalSessionsUpgradingEnabled=true\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": true}}, {"input": "bind-source-addr=999.-1.0.0\n\nreplica-announce-ip=5.5.5.5\n\nlazyfree-lazy-user-del=no\n\naof-rewrite-incremental-fsync=yes\n\nproc-title-template=\"{title} {listen-addr} {server-mode}\"\n\nunixsocketperm=1400\n\ncluster-enabled=yes\n\nhash-max-listpack-value=64\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "maintenance_work_mem=1MB\n\nunix_socket_group=''\n\nparallel_leader_participation=on\n\nenable_partition_pruning=on\n\ncpu_operator_cost=0.00125\n\nenable_indexonlyscan=on\n\ndefault_toast_compression='pglz'\n\nrecovery_target_action='pause'\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "postgresql", "is_synthetic": true}}, {"input": "archive_mode=off\n\nevent_source='PostgreSQL'\n\ncheckpoint_timeout=5min\n\nlog_recovery_conflict_waits=off\n\nfrom_collapse_limit=16\n\nvacuum_freeze_min_age=100000000\n\nsuperuser_reserved_connections=6\n\ndebug_pretty_print=on\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "postgresql", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.master.port</name>\n  <value>16000</value>\n    <description>The port the HBase Master should bind to.</description>\n</property>\n\n<property>\n  <name>hbase.master.logcleaner.plugins</name>\n  <value>org.apache.hadoop.hbase.master.cleaner.TimeToLiveProcedureWALCleaner</value>\n    <description>A comma-separated list of BaseLogCleanerDelegate invoked by\n    the LogsCleaner service. These WAL cleaners are called in order,\n    so put the cleaner that prunes the most files in front. To\n    implement your own BaseLogCleanerDelegate, just put it in HBase's classpath\n    and add the fully qualified class name here. Always add the above\n    default log cleaners in the list.</description>\n</property>\n\n<property>\n  <name>hbase.client.retries.number</name>\n  <value>30</value>\n    <description>Maximum retries.  Used as maximum for all retryable\n    operations such as the getting of a cell's value, starting a row update,\n    etc.  Retry interval is a rough function based on hbase.client.pause.  At\n    first we retry at this interval but then with backoff, we pretty quickly reach\n    retrying every ten seconds.  See HConstants#RETRY_BACKOFF for how the backup\n    ramps up.  Change this setting and hbase.client.pause to suit your workload.</description>\n</property>\n\n<property>\n  <name>hfile.index.block.max.size</name>\n  <value>131072</value>\n      <description>When the size of a leaf-level, intermediate-level, or root-level\n          index block in a multi-level block index grows to this size, the\n          block is written out and a new block is started.</description>\n</property>\n\n<property>\n  <name>hbase.auth.key.update.interval</name>\n  <value>172800000</value>\n    <description>The update interval for master key for authentication tokens\n    in servers in milliseconds.  Only used when HBase security is enabled.</description>\n</property>\n\n<property>\n  <name>hbase.auth.token.max.lifetime</name>\n  <value>1209600000</value>\n    <description>The maximum lifetime in milliseconds after which an\n    authentication token expires.  Only used when HBase security is enabled.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.storefile.refresh.period</name>\n  <value>-1</value>\n    <description>\n      The period (in milliseconds) for refreshing the store files for the secondary regions. 0\n      means this feature is disabled. Secondary regions sees new files (from flushes and\n      compactions) from primary once the secondary region refreshes the list of files in the\n      region (there is no notification mechanism). But too frequent refreshes might cause\n      extra Namenode pressure. If the files cannot be refreshed for longer than HFile TTL\n      (hbase.master.hfilecleaner.ttl) the requests are rejected. Configuring HFile TTL to a larger\n      value is also recommended with this setting.\n    </description>\n</property>\n\n<property>\n  <name>hbase.hstore.engine.class</name>\n  <value>hhh</value>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": false}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.master.logcleaner.ttl</name>\n  <value>600000</value>\n    <description>How long a WAL remain in the archive ({hbase.rootdir}/oldWALs) directory,\n    after which it will be cleaned by a Master thread. The value is in milliseconds.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.regionSplitLimit</name>\n  <value>500</value>\n    <description>\n      Limit for the number of regions after which no more region splitting\n      should take place. This is not hard limit for the number of regions\n      but acts as a guideline for the regionserver to stop splitting after\n      a certain limit. Default is set to 1000.\n    </description>\n</property>\n\n<property>\n  <name>hbase.zookeeper.peerport</name>\n  <value>1444</value>\n    <description>Port used by ZooKeeper peers to talk to each other.\n    See https://zookeeper.apache.org/doc/r3.3.3/zookeeperStarted.html#sc_RunningReplicatedZooKeeper\n    for more information.</description>\n</property>\n\n<property>\n  <name>hbase.normalizer.period</name>\n  <value>300000</value>\n    <description>Period at which the region normalizer runs in the Master.</description>\n</property>\n\n<property>\n  <name>hbase.hstore.compaction.min.size</name>\n  <value>134217728</value>\n    <description>A StoreFile (or a selection of StoreFiles, when using ExploringCompactionPolicy)\n      smaller than this size will always be eligible for minor compaction.\n      HFiles this size or larger are evaluated by hbase.hstore.compaction.ratio to determine if\n      they are eligible. Because this limit represents the \"automatic include\" limit for all\n      StoreFiles smaller than this value, this value may need to be reduced in write-heavy\n      environments where many StoreFiles in the 1-2 MB range are being flushed, because every\n      StoreFile will be targeted for compaction and the resulting StoreFiles may still be under the\n      minimum size and require further compaction. If this parameter is lowered, the ratio check is\n      triggered more quickly. This addressed some issues seen in earlier versions of HBase but\n      changing this parameter is no longer necessary in most situations. Default: 128 MB expressed\n      in bytes.</description>\n</property>\n\n<property>\n  <name>hbase.master.normalizer.class</name>\n  <value>org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer</value>\n    <description>\n      Class used to execute the region normalization when the period occurs.\n      See the class comment for more on how it works\n      http://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/master/normalizer/SimpleRegionNormalizer.html\n    </description>\n</property>\n\n<property>\n  <name>hbase.regionserver.storefile.refresh.period</name>\n  <value>1</value>\n    <description>\n      The period (in milliseconds) for refreshing the store files for the secondary regions. 0\n      means this feature is disabled. Secondary regions sees new files (from flushes and\n      compactions) from primary once the secondary region refreshes the list of files in the\n      region (there is no notification mechanism). But too frequent refreshes might cause\n      extra Namenode pressure. If the files cannot be refreshed for longer than HFile TTL\n      (hbase.master.hfilecleaner.ttl) the requests are rejected. Configuring HFile TTL to a larger\n      value is also recommended with this setting.\n    </description>\n</property>\n\n<property>\n  <name>hbase.mob.compaction.threads.max</name>\n  <value>1</value>\n    <description>\n      The max number of threads used in MobCompactor.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.tmp.dir</name>\n  <value>${java.io.tmpdir}/hbase-${user.name}</value>\n    <description>Temporary directory on the local filesystem.\n    Change this setting to point to a location more permanent\n    than '/tmp', the usual resolve for java.io.tmpdir, as the\n    '/tmp' directory is cleared on machine restart.</description>\n</property>\n\n<property>\n  <name>hbase.zookeeper.quorum</name>\n  <value>localhost</value>\n    <description>Comma separated list of servers in the ZooKeeper ensemble\n    (This config. should have been named hbase.zookeeper.ensemble).\n    For example, \"host1.mydomain.com,host2.mydomain.com,host3.mydomain.com\".\n    By default this is set to localhost for local and pseudo-distributed modes\n    of operation. For a fully-distributed setup, this should be set to a full\n    list of ZooKeeper ensemble servers. If HBASE_MANAGES_ZK is set in hbase-env.sh\n    this is the list of servers which hbase will start/stop ZooKeeper on as\n    part of cluster start/stop.  Client-side, we will take this list of\n    ensemble members and put it together with the hbase.zookeeper.property.clientPort\n    config. and pass it into zookeeper constructor as the connectString\n    parameter.</description>\n</property>\n\n<property>\n  <name>hbase.zookeeper.property.initLimit</name>\n  <value>10</value>\n    <description>Property from ZooKeeper's config zoo.cfg.\n    The number of ticks that the initial synchronization phase can take.</description>\n</property>\n\n<property>\n  <name>hbase.ipc.server.fallback-to-simple-auth-allowed</name>\n  <value>true</value>\n    <description>When a server is configured to require secure connections, it will\n      reject connection attempts from clients using SASL SIMPLE (unsecure) authentication.\n      This setting allows secure servers to accept SASL SIMPLE connections from clients\n      when the client requests.  When false (the default), the server will not allow the fallback\n      to SIMPLE authentication, and will reject the connection.  WARNING: This setting should ONLY\n      be used as a temporary measure while converting clients over to secure authentication.  It\n      MUST BE DISABLED for secure operation.</description>\n</property>\n\n<property>\n  <name>hbase.coprocessor.enabled</name>\n  <value>false</value>\n    <description>Enables or disables coprocessor loading. If 'false'\n    (disabled), any other coprocessor related configuration will be ignored.\n    </description>\n</property>\n\n<property>\n  <name>hbase.coprocessor.user.enabled</name>\n  <value>false</value>\n    <description>Enables or disables user (aka. table) coprocessor loading.\n    If 'false' (disabled), any table coprocessor attributes in table\n    descriptors will be ignored. If \"hbase.coprocessor.enabled\" is 'false'\n    this setting has no effect.\n    </description>\n</property>\n\n<property>\n  <name>hbase.table.lock.enable</name>\n  <value>false</value>\n    <description>Set to true to enable locking the table in zookeeper for schema change operations.\n    Table locking from master prevents concurrent schema modifications to corrupt table\n    state.</description>\n</property>\n\n<property>\n  <name>hbase.region.replica.replication.enabled</name>\n  <value>false</value>\n    <description>\n      Whether asynchronous WAL replication to the secondary region replicas is enabled or not.\n      If this is enabled, a replication peer named \"region_replica_replication\" will be created\n      which will tail the logs and replicate the mutations to region replicas for tables that\n      have region replication > 1. If this is enabled once, disabling this replication also\n      requires disabling the replication peer using shell or Admin java class.\n      Replication to secondary region replicas works over standard inter-cluster replication.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.master.logcleaner.ttl</name>\n  <value>300000</value>\n    <description>How long a WAL remain in the archive ({hbase.rootdir}/oldWALs) directory,\n    after which it will be cleaned by a Master thread. The value is in milliseconds.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.handler.count</name>\n  <value>15</value>\n    <description>Count of RPC Listener instances spun up on RegionServers.\n      Same property is used by the Master for count of master handlers.\n      Too many handlers can be counter-productive. Make it a multiple of\n      CPU count. If mostly read-only, handlers count close to cpu count\n      does well. Start with twice the CPU count and tune from there.</description>\n</property>\n\n<property>\n  <name>hbase.ipc.server.callqueue.handler.factor</name>\n  <value>0.1</value>\n    <description>Factor to determine the number of call queues.\n      A value of 0 means a single queue shared between all the handlers.\n      A value of 1 means that each handler has its own queue.</description>\n</property>\n\n<property>\n  <name>hbase.hregion.percolumnfamilyflush.size.lower.bound.min</name>\n  <value>8388608</value>\n    <description>\n    If FlushLargeStoresPolicy is used and there are multiple column families,\n    then every time that we hit the total memstore limit, we find out all the\n    column families whose memstores exceed a \"lower bound\" and only flush them\n    while retaining the others in memory. The \"lower bound\" will be\n    \"hbase.hregion.memstore.flush.size / column_family_number\" by default\n    unless value of this property is larger than that. If none of the families\n    have their memstore size more than lower bound, all the memstores will be\n    flushed (just as usual).\n    </description>\n</property>\n\n<property>\n  <name>hbase.coprocessor.enabled</name>\n  <value>true</value>\n    <description>Enables or disables coprocessor loading. If 'false'\n    (disabled), any other coprocessor related configuration will be ignored.\n    </description>\n</property>\n\n<property>\n  <name>hbase.rest.threads.min</name>\n  <value>1</value>\n    <description>The minimum number of threads of the REST server thread pool.\n        The thread pool always has at least these number of threads so\n        the REST server is ready to serve incoming requests.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.thrift.framed</name>\n  <value>false</value>\n    <description>Use Thrift TFramedTransport on the server side.\n      This is the recommended transport for thrift servers and requires a similar setting\n      on the client side. Changing this to false will select the default transport,\n      vulnerable to DoS when malformed requests are issued due to THRIFT-601.\n    </description>\n</property>\n\n<property>\n  <name>hbase.master.loadbalancer.class</name>\n  <value>org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer</value>\n    <description>\n      Class used to execute the regions balancing when the period occurs.\n      See the class comment for more on how it works\n      http://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.html\n      It replaces the DefaultLoadBalancer as the default (since renamed\n      as the SimpleLoadBalancer).\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "cluster-announce-port=-100.22\n\nhash-max-listpack-value=32\n\ntcp-keepalive=600\n\ndir=./\n\naof-use-rdb-preamble=yes\n\nenable-protected-configs=no\n\nloglevel=notice\n\nproc-title-template=\"{title} {listen-addr} {server-mode}\"\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.zookeeper.property.clientPort</name>\n  <value>65536</value>\n    <description>Property from ZooKeeper's config zoo.cfg.\n    The port at which the clients will connect.</description>\n</property>\n\n<property>\n  <name>hbase.client.pause</name>\n  <value>50</value>\n    <description>General client pause value.  Used mostly as value to wait\n    before running a retry of a failed get, region lookup, etc.\n    See hbase.client.retries.number for description of how we backoff from\n    this initial pause amount and how this pause works w/ retries.</description>\n</property>\n\n<property>\n  <name>hbase.hregion.memstore.mslab.enabled</name>\n  <value>false</value>\n    <description>\n      Enables the MemStore-Local Allocation Buffer,\n      a feature which works to prevent heap fragmentation under\n      heavy write loads. This can reduce the frequency of stop-the-world\n      GC pauses on large heaps.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.compaction.enabled</name>\n  <value>true</value>\n    <description>Enable/disable compactions on by setting true/false.\n      We can further switch compactions dynamically with the\n      compaction_switch shell command.</description>\n</property>\n\n<property>\n  <name>hbase.hstore.compaction.max.size</name>\n  <value>4611686018427387903</value>\n    <description>A StoreFile (or a selection of StoreFiles, when using ExploringCompactionPolicy)\n      larger than this size will be excluded from compaction. The effect of\n      raising hbase.hstore.compaction.max.size is fewer, larger StoreFiles that do not get\n      compacted often. If you feel that compaction is happening too often without much benefit, you\n      can try raising this value. Default: the value of LONG.MAX_VALUE, expressed in bytes.</description>\n</property>\n\n<property>\n  <name>hbase.rest.threads.min</name>\n  <value>2</value>\n    <description>The minimum number of threads of the REST server thread pool.\n        The thread pool always has at least these number of threads so\n        the REST server is ready to serve incoming requests.</description>\n</property>\n\n<property>\n  <name>hbase.wal.dir.perms</name>\n  <value>350</value>\n    <description>FS Permissions for the root WAL directory in a secure(kerberos) setup.\n      When master starts, it creates the WAL dir with this permissions or sets the permissions\n      if it does not match.</description>\n</property>\n\n<property>\n  <name>hbase.mob.cache.evict.remain.ratio</name>\n  <value>0.25</value>\n    <description>\n      The ratio (between 0.0 and 1.0) of files that remains cached after an eviction\n      is triggered when the number of cached mob files exceeds the hbase.mob.file.cache.size.\n      The default value is 0.5f.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.namenode.max.objects</name>\n  <value>-1</value>\n  <description>The maximum number of files, directories and blocks\n  dfs supports. A value of zero indicates no limit to the number\n  of objects that dfs supports.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.decommission.max.concurrent.tracked.nodes</name>\n  <value>200</value>\n  <description>\n    The maximum number of decommission-in-progress or\n    entering-maintenance datanodes nodes that will be tracked at one time by\n    the namenode. Tracking these datanode consumes additional NN memory\n    proportional to the number of blocks on the datnode. Having a conservative\n    limit reduces the potential impact of decommissioning or maintenance of\n    a large number of nodes at once.\n      \n    A value of 0 means no limit will be enforced.\n  </description>\n</property>\n\n<property>\n  <name>dfs.nameservices</name>\n  <value>ns1</value>\n  <description>\n    Comma-separated list of nameservices.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.deadnode.detection.suspectnode.queue.max</name>\n  <value>2000</value>\n    <description>\n      The max queue size of probing suspect node.\n    </description>\n</property>\n\n<property>\n  <name>dfs.client.deadnode.detection.rpc.threads</name>\n  <value>40</value>\n    <description>\n      The maximum number of threads to use for calling RPC call to recheck the liveness of dead node.\n    </description>\n</property>\n\n<property>\n  <name>dfs.namenode.ec.userdefined.policy.allowed</name>\n  <value>false</value>\n  <description>If set to false, doesn't allow addition of user defined\n    erasure coding policies.\n  </description>\n</property>\n\n<property>\n  <name>dfs.http.client.retry.policy.spec</name>\n  <value>[5000, 3, 30000, 5]</value>\n  <description>\n    Specify a policy of multiple linear random retry for WebHDFS client,\n    e.g. given pairs of number of retries and sleep time (n0, t0), (n1, t1),\n    ..., the first n0 retries sleep t0 milliseconds on average,\n    the following n1 retries sleep t1 milliseconds on average, and so on.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.block.write.locateFollowingBlock.retries</name>\n  <value>5</value>\n  <description>\n    Number of retries to use when finding the next block during HDFS writes.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "track_counts=on\n\ndeadlock_timeout=10s\n\nenable_incremental_sort=on\n\nwal_retrieve_retry_interval=10s\n\nplan_cache_mode=auto\n\nrecovery_prefetch=try\n\ntimezone_abbreviations='Default'\n\nscram_iterations=8192\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "postgresql", "is_synthetic": true}}, {"input": "vacuum_multixact_freeze_table_age=75000000\n\ndebug_print_plan=off\n\nautovacuum_vacuum_cost_delay=2ms\n\nevent_triggers=on\n\nwork_mem=1MB\n\nkrb_server_keyfile='FILE:${sysconfdir}/krb5.keytab'\n\nsyslog_facility='LOCAL0'\n\nenable_hashagg=on\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "postgresql", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.namenode.metrics.logger.period.seconds</name>\n  <value>300</value>\n  <description>\n    This setting controls how frequently the NameNode logs its metrics. The\n    logging configuration must also define one or more appenders for\n    NameNodeMetricsLog for the metrics to be logged.\n    NameNode metrics logging is disabled if this value is set to zero or\n    less than zero.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.reencrypt.throttle.limit.handler.ratio</name>\n  <value>1.0</value>\n  <description>Throttling ratio for the re-encryption, indicating what fraction\n    of time should the re-encrypt handler thread work under NN read lock.\n    Larger than 1.0 values are interpreted as 1.0. Negative value or 0 are\n    invalid values and will fail NN startup.\n  </description>\n</property>\n\n<property>\n  <name>dfs.balancer.keytab.file</name>\n  <value>/valid/file2</value>\n  <description>\n    The keytab file used by the Balancer to login as its\n    service principal. The principal name is configured with\n    dfs.balancer.kerberos.principal. Keytab based login can be\n    enabled with dfs.balancer.keytab.enabled.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.read.striped.threadpool.size</name>\n  <value>9</value>\n  <description>\n    The maximum number of threads used for parallel reading\n    in striped layout.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.replica.cache.expiry.time</name>\n  <value>1m</value>\n  <description>\n    Living time of replica cached files in milliseconds.\n  </description>\n</property>\n\n<property>\n  <name>dfs.mover.keytab.file</name>\n  <value>/valid/file2</value>\n  <description>\n    The keytab file used by the Mover to login as its\n    service principal. The principal name is configured with\n    dfs.mover.kerberos.principal. Keytab based login can be\n    enabled with dfs.mover.keytab.enabled.\n  </description>\n</property>\n\n<property>\n  <name>dfs.storage.policy.satisfier.recheck.timeout.millis</name>\n  <value>60000</value>\n  <description>\n    Blocks storage movements monitor re-check interval in milliseconds.\n    This check will verify whether any blocks storage movement results arrived from DN\n    and also verify if any of file blocks movements not at all reported to DN\n    since dfs.storage.policy.satisfier.self.retry.timeout.\n    The default value is 1 * 60 * 1000 (1 mins)\n  </description>\n</property>\n\n<property>\n  <name>dfs.hosts</name>\n  <value>host1 ro:host2 rw</value>\n  <description>Names a file that contains a list of hosts that are\n  permitted to connect to the namenode. The full pathname of the file\n  must be specified.  If the value is empty, all hosts are\n  permitted.</description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": false}}, {"input": "alluxio.security.authentication.type=NOSASL\n\nalluxio.security.login.username=ciri\n\nalluxio.master.embedded.journal.catchup.retry.wait=10s\n\nalluxio.underfs.s3.upload.threads.max=10\n\nalluxio.underfs.s3.streaming.upload.partition.size=64MB\n\nalluxio.master.embedded.journal.flush.size.max=320MB\n\nalluxio.master.backup.transport.timeout=30sec\n\nalluxio.security.stale.channel.purge.interval=1day\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    \n<property>\n  <name>yarn.nodemanager.container-monitor.enabled</name>\n  <value>false</value>\n    <description>Enable container monitor</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.container-metrics.period-ms</name>\n  <value>-1</value>\n    <description>\n    Container metrics flush period in ms.  Set to -1 for flush on completion.\n    </description>\n</property>\n\n<property>\n  <name>yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size</name>\n  <value>20000</value>\n    <description>Size of cache for recently written entity start times for leveldb timeline store in number of entities.</description>\n</property>\n\n<property>\n  <name>yarn.timeline-service.recovery.enabled</name>\n  <value>false</value>\n    <description>Enable timeline server to recover state after starting. If\n    true, then yarn.timeline-service.state-store-class must be specified.\n    </description>\n</property>\n\n<property>\n  <name>yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs</name>\n  <value>1800</value>\n    <description>\n    Timeout in seconds for YARN node graceful decommission.\n    This is the maximal time to wait for running containers and applications to complete\n    before transition a DECOMMISSIONING node into DECOMMISSIONED.\n    </description>\n</property>\n\n<property>\n  <name>yarn.federation.subcluster-resolver.class</name>\n  <value>org.apache.hadoop.yarn.server.federation.resolver.DefaultSubClusterResolverImpl</value>\n    <description>\n      Class name for SubClusterResolver\n    </description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.pluggable-device-framework.enabled</name>\n  <value>false</value>\n    <description>\n      This setting controls if pluggable device framework is enabled.\n      Disabled by default\n    </description>\n</property>\n\n<property>\n  <name>yarn.resourcemanager.submission-preprocessor.file-refresh-interval-ms</name>\n  <value>60000</value>\n    <description>Submission processor refresh interval</description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "yarn", "is_synthetic": true}}, {"input": "DATE_FORMAT='N j, Y'\n\nFILE_UPLOAD_PERMISSIONS=0o644\n\nSESSION_SERIALIZER='django.contrib.sessions.serializers.JSONSerializer'\n\nEMAIL_HOST='localhost'\n\nFORMAT_MODULE_PATH=None\n\nDEBUG=True\n\nCSRF_FAILURE_VIEW='django.views.csrf.csrf_failure'\n\nUSE_DEPRECATED_PYTZ=False\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "django", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>fs.ftp.transfer.mode</name>\n  <value>BLOCK_TRANSFER_MODE</value>\n  <description>\n    Set FTP's transfer mode based on configuration. Valid values are\n    STREAM_TRANSFER_MODE, BLOCK_TRANSFER_MODE and COMPRESSED_TRANSFER_MODE.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.s3guard.ddb.throttle.retry.interval</name>\n  <value>200ms</value>\n    <description>\n      Initial interval to retry after a request is throttled events;\n      the back-off policy is exponential until the number of retries of\n      fs.s3a.s3guard.ddb.max.retries is reached.\n    </description>\n</property>\n\n<property>\n  <name>fs.AbstractFileSystem.abfss.impl</name>\n  <value>org.apache.hadoop.fs.azurebfs.Abfss</value>\n  <description>AbstractFileSystem implementation class of abfss://</description>\n</property>\n\n<property>\n  <name>ipc.[port_number].backoff.enable</name>\n  <value>false</value>\n  <description>Whether or not to enable client backoff when a queue is full.\n  </description>\n</property>\n\n<property>\n  <name>ipc.[port_number].scheduler.impl</name>\n  <value>org.apache.hadoop.ipc.DefaultRpcScheduler</value>\n  <description>The fully qualified name of a class to use as the\n    implementation of the scheduler. The default implementation is\n    org.apache.hadoop.ipc.DefaultRpcScheduler (no-op scheduler) when not using\n    FairCallQueue. If using FairCallQueue, defaults to\n    org.apache.hadoop.ipc.DecayRpcScheduler. Use\n    org.apache.hadoop.ipc.DecayRpcScheduler in conjunction with the Fair Call\n    Queue.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.zk.num-retries</name>\n  <value>1000</value>\n    <description>Number of tries to connect to ZooKeeper.</description>\n</property>\n\n<property>\n  <name>hadoop.domainname.resolver.impl</name>\n  <value>org.apache.hadoop.net.DNSDomainNameResolver</value>\n    <description>The implementation of DomainNameResolver used for service (NameNodes,\n      RBF Routers etc) discovery. The default implementation\n      org.apache.hadoop.net.DNSDomainNameResolver returns all IP addresses associated\n      with the input domain name of the services by querying the underlying DNS.\n    </description>\n</property>\n\n<property>\n  <name>hadoop.http.sni.host.check.enabled</name>\n  <value>true</value>\n    <description>\n      Enable Server Name Indication (SNI) host check for HTTPS enabled server.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "tcp_keepalives_idle=true\n\ncpu_operator_cost=0.0025\n\ntimezone_abbreviations='Default'\n\nmax_parallel_workers_per_gather=1\n\nsynchronize_seqscans=on\n\nrecovery_target_action='pause'\n\ngeqo_threshold=6\n\ntrack_activity_query_size=2048\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "postgresql", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.https.server.keystore.resource</name>\n  <value>ssl-server.xml</value>\n  <description>Resource file from which ssl server keystore\n  information will be extracted\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.block.write.replace-datanode-on-failure.best-effort</name>\n  <value>1.5</value>\n  <description>\n    This property is used only if the value of\n    dfs.client.block.write.replace-datanode-on-failure.enable is true.\n\n    Best effort means that the client will try to replace a failed datanode\n    in write pipeline (provided that the policy is satisfied), however, it \n    continues the write operation in case that the datanode replacement also\n    fails.\n\n    Suppose the datanode replacement fails.\n    false: An exception should be thrown so that the write will fail.\n    true : The write should be resumed with the remaining datandoes.\n  \n    Note that setting this property to true allows writing to a pipeline\n    with a smaller number of datanodes.  As a result, it increases the\n    probability of data loss.\n  </description>\n</property>\n\n<property>\n  <name>nfs.dump.dir</name>\n  <value>/valid/file2</value>\n  <description>\n    This directory is used to temporarily save out-of-order writes before\n    writing to HDFS. For each file, the out-of-order writes are dumped after\n    they are accumulated to exceed certain threshold (e.g., 1MB) in memory. \n    One needs to make sure the directory has enough space.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.path.based.cache.refresh.interval.ms</name>\n  <value>60000</value>\n  <description>\n    The amount of milliseconds between subsequent path cache rescans.  Path\n    cache rescans are when we calculate which blocks should be cached, and on\n    what datanodes.\n\n    By default, this parameter is set to 30 seconds.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.fsdatasetcache.max.threads.per.volume</name>\n  <value>4</value>\n  <description>\n    The maximum number of threads per volume to use for caching new data\n    on the datanode. These threads consume both I/O and CPU. This can affect\n    normal datanode operations.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.delegation.token.always-use</name>\n  <value>false</value>\n  <description>\n    For testing.  Setting to true always allows the DT secret manager\n    to be used, even if security is disabled.\n  </description>\n</property>\n\n<property>\n  <name>dfs.qjournal.finalize-segment.timeout.ms</name>\n  <value>120000</value>\n  <description>\n    Quorum timeout in milliseconds during finalizing for a specific\n    segment.\n  </description>\n</property>\n\n<property>\n  <name>dfs.disk.balancer.max.disk.throughputInMBperSec</name>\n  <value>10</value>\n    <description>Maximum disk bandwidth used by diskbalancer\n      during read from a source disk. The unit is MB/sec.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.tmp.dir</name>\n  <value>/valid/file1</value>\n    <description>Temporary directory on the local filesystem.\n    Change this setting to point to a location more permanent\n    than '/tmp', the usual resolve for java.io.tmpdir, as the\n    '/tmp' directory is cleared on machine restart.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.region.split.policy</name>\n  <value>org.apache.hadoop.hbase.regionserver.SteppingSplitPolicy</value>\n    <description>\n      A split policy determines when a region should be split. The various\n      other split policies that are available currently are BusyRegionSplitPolicy,\n      ConstantSizeRegionSplitPolicy, DisabledRegionSplitPolicy,\n      DelimitedKeyPrefixRegionSplitPolicy, KeyPrefixRegionSplitPolicy, and\n      SteppingSplitPolicy. DisabledRegionSplitPolicy blocks manual region splitting.\n    </description>\n</property>\n\n<property>\n  <name>hbase.client.localityCheck.threadPoolSize</name>\n  <value>2</value>\n</property>\n\n<property>\n  <name>hbase.hregion.preclose.flush.size</name>\n  <value>2621440</value>\n    <description>\n      If the memstores in a region are this size or larger when we go\n      to close, run a \"pre-flush\" to clear out memstores before we put up\n      the region closed flag and take the region offline.  On close,\n      a flush is run under the close flag to empty memory.  During\n      this time the region is offline and we are not taking on any writes.\n      If the memstore content is large, this flush could take a long time to\n      complete.  The preflush is meant to clean out the bulk of the memstore\n      before putting up the close flag and taking the region offline so the\n      flush that runs under the close flag has little to do.</description>\n</property>\n\n<property>\n  <name>hbase.hregion.memstore.mslab.enabled</name>\n  <value>false</value>\n    <description>\n      Enables the MemStore-Local Allocation Buffer,\n      a feature which works to prevent heap fragmentation under\n      heavy write loads. This can reduce the frequency of stop-the-world\n      GC pauses on large heaps.</description>\n</property>\n\n<property>\n  <name>hbase.ipc.server.fallback-to-simple-auth-allowed</name>\n  <value>false</value>\n    <description>When a server is configured to require secure connections, it will\n      reject connection attempts from clients using SASL SIMPLE (unsecure) authentication.\n      This setting allows secure servers to accept SASL SIMPLE connections from clients\n      when the client requests.  When false (the default), the server will not allow the fallback\n      to SIMPLE authentication, and will reject the connection.  WARNING: This setting should ONLY\n      be used as a temporary measure while converting clients over to secure authentication.  It\n      MUST BE DISABLED for secure operation.</description>\n</property>\n\n<property>\n  <name>hbase.rest-csrf.browser-useragents-regex</name>\n  <value>^Mozilla.*,^Opera.*</value>\n  <description>\n    A comma-separated list of regular expressions used to match against an HTTP\n    request's User-Agent header when protection against cross-site request\n    forgery (CSRF) is enabled for REST server by setting\n    hbase.rest.csrf.enabled to true.  If the incoming User-Agent matches\n    any of these regular expressions, then the request is considered to be sent\n    by a browser, and therefore CSRF prevention is enforced.  If the request's\n    User-Agent does not match any of these regular expressions, then the request\n    is considered to be sent by something other than a browser, such as scripted\n    automation.  In this case, CSRF is not a potential attack vector, so\n    the prevention is not enforced.  This helps achieve backwards-compatibility\n    with existing automation that has not been updated to send the CSRF\n    prevention header.\n  </description>\n</property>\n\n<property>\n  <name>hbase.mob.delfile.max.count</name>\n  <value>3</value>\n    <description>\n      The max number of del files that is allowed in the mob compaction.\n      In the mob compaction, when the number of existing del files is larger than\n      this value, they are merged until number of del files is not larger this value.\n      The default value is 3.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.connection.timeout.ms</name>\n  <value>120000</value>\n  <description>\n    This property is the connection timeout (in milliseconds) for LDAP\n    operations. If the LDAP provider doesn't establish a connection within the\n    specified period, it will abort the connect attempt. Non-positive value\n    means no LDAP connection timeout is specified in which case it waits for the\n    connection to establish until the underlying network times out.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.ssl</name>\n  <value>true</value>\n  <description>\n    Whether or not to use SSL when connecting to the LDAP server.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.s3guard.ddb.max.retries</name>\n  <value>1</value>\n    <description>\n      Max retries on throttled/incompleted DynamoDB operations\n      before giving up and throwing an IOException.\n      Each retry is delayed with an exponential\n      backoff timer which starts at 100 milliseconds and approximately\n      doubles each time.  The minimum wait before throwing an exception is\n      sum(100, 200, 400, 800, .. 100*2^N-1 ) == 100 * ((2^N)-1)\n    </description>\n</property>\n\n<property>\n  <name>fs.wasbs.impl</name>\n  <value>org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure</value>\n  <description>The implementation class of the Secure Native Azure Filesystem</description>\n</property>\n\n<property>\n  <name>ipc.client.connect.retry.interval</name>\n  <value>500</value>\n  <description>Indicates the number of milliseconds a client will wait for\n    before retrying to establish a server connection.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.rpc.socket.factory.class.default</name>\n  <value>org.apache.hadoop.net.StandardSocketFactory</value>\n  <description> Default SocketFactory to use. This parameter is expected to be\n    formatted as \"package.FactoryClassName\".\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.crypto.buffer.size</name>\n  <value>16384</value>\n  <description>\n    The buffer size used by CryptoInputStream and CryptoOutputStream.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.registry.zk.retry.times</name>\n  <value>1</value>\n    <description>\n      Zookeeper connection retry count before failing\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "alluxio.fuse.user.group.translation.enabled=10000\n\nalluxio.job.master.client.threads=1024\n\nalluxio.worker.web.bind.host=127.0.0.1\n\nalluxio.user.client.cache.store.type=LOCAL\n\nalluxio.worker.network.max.inbound.message.size=8MB\n\nalluxio.worker.container.hostname=127.0.0.1\n\nalluxio.proxy.s3.writetype=CACHE_THROUGH\n\nalluxio.worker.reviewer.probabilistic.hardlimit.bytes=1MB\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "auto-compaction-mode: NOEXIST_MODE\n\nproxy-dial-timeout: 2000\n\nheartbeat-interval: 200\n\ninitial-advertise-peer-urls: http://localhost:2380\n\nstrict-reconfig-check: true\n\nmax-wals: 10\n\nelection-timeout: 1000\n\nproxy-refresh-interval: 60000\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "etcd", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.local.dir</name>\n  <value>${hbase.tmp.dir}/local/</value>\n    <description>Directory on the local filesystem to be used\n    as a local storage.</description>\n</property>\n\n<property>\n  <name>hbase.master.logcleaner.plugins</name>\n  <value>org.apache.hadoop.hbase.master.cleaner.TimeToLiveProcedureWALCleaner</value>\n    <description>A comma-separated list of BaseLogCleanerDelegate invoked by\n    the LogsCleaner service. These WAL cleaners are called in order,\n    so put the cleaner that prunes the most files in front. To\n    implement your own BaseLogCleanerDelegate, just put it in HBase's classpath\n    and add the fully qualified class name here. Always add the above\n    default log cleaners in the list.</description>\n</property>\n\n<property>\n  <name>hbase.wal.dir.perms</name>\n  <value>1400</value>\n    <description>FS Permissions for the root WAL directory in a secure(kerberos) setup.\n      When master starts, it creates the WAL dir with this permissions or sets the permissions\n      if it does not match.</description>\n</property>\n\n<property>\n  <name>hbase.snapshot.restore.failsafe.name</name>\n  <value>hbase-failsafe-{snapshot.name}-{restore.timestamp}</value>\n    <description>Name of the failsafe snapshot taken by the restore operation.\n      You can use the {snapshot.name}, {table.name} and {restore.timestamp} variables\n      to create a name based on what you are restoring.</description>\n</property>\n\n<property>\n  <name>hbase.master.loadbalancer.class</name>\n  <value>org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer</value>\n    <description>\n      Class used to execute the regions balancing when the period occurs.\n      See the class comment for more on how it works\n      http://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/master/balancer/StochasticLoadBalancer.html\n      It replaces the DefaultLoadBalancer as the default (since renamed\n      as the SimpleLoadBalancer).\n    </description>\n</property>\n\n<property>\n  <name>hbase.mob.compaction.mergeable.threshold</name>\n  <value>1342177280</value>\n    <description>\n      If the size of a mob file is less than this value, it's regarded as a small\n      file and needs to be merged in mob compaction. The default value is 1280MB.\n    </description>\n</property>\n\n<property>\n  <name>hbase.mob.compaction.chore.period</name>\n  <value>604800</value>\n    <description>\n      The period that MobCompactionChore runs. The unit is second.\n      The default value is one week.\n    </description>\n</property>\n\n<property>\n  <name>hbase.master.wait.on.service.seconds</name>\n  <value>30</value>\n    <description>Default is 5 minutes. Make it 30 seconds for tests. See\n    HBASE-19794 for some context.</description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "proxy-read-timeout: 0\n\nlog-outputs: [stderr]\n\nenable-pprof: true\n\nforce-new-cluster: true\n\nproxy-write-timeout: 2500\n\nname: 'default'\n\nheartbeat-interval: 50\n\nmax-wals: 10\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "etcd", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>fs.s3a.multiobjectdelete.enable</name>\n  <value>true</value>\n  <description>When enabled, multiple single-object delete requests are replaced by\n    a single 'delete multiple objects'-request, reducing the number of requests.\n    Beware: legacy S3-compatible object stores might not support this request.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.select.input.csv.header</name>\n  <value>none</value>\n  <description>In S3 Select queries over CSV files: what is the role of the header? One of \"none\", \"ignore\" and \"use\"</description>\n</property>\n\n<property>\n  <name>fs.s3a.select.output.csv.quote.character</name>\n  <value>\"</value>\n  <description>\n    In S3 Select queries: the quote character for generated CSV Files.\n  </description>\n</property>\n\n<property>\n  <name>ipc.ping.interval</name>\n  <value>30000</value>\n  <description>Timeout on waiting response from server, in milliseconds.\n  The client will send ping when the interval is passed without receiving bytes,\n  if ipc.client.ping is set to true.\n  </description>\n</property>\n\n<property>\n  <name>ipc.[port_number].weighted-cost.lockshared</name>\n  <value>20</value>\n  <description>The weight multiplier to apply to the time spent in the\n    processing phase which holds a shared (read) lock.\n    This property applies to WeightedTimeCostProvider.\n  </description>\n</property>\n\n<property>\n  <name>ftp.blocksize</name>\n  <value>33554432</value>\n  <description>Block size</description>\n</property>\n\n<property>\n  <name>ha.health-monitor.check-interval.ms</name>\n  <value>2000</value>\n  <description>\n    How often to check the service.\n  </description>\n</property>\n\n<property>\n  <name>ha.health-monitor.sleep-after-disconnect.ms</name>\n  <value>1000</value>\n  <description>\n    How long to sleep after an unexpected RPC error.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "geqo_seed=1.0\n\nstats_fetch_consistency=cache\n\nvacuum_multixact_freeze_table_age=75000000\n\nparallel_tuple_cost=0.1\n\nautovacuum_work_mem=-1\n\njit_provider='llvmjit'\n\neffective_io_concurrency=2\n\nunix_socket_permissions=388\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "postgresql", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.datanode.du.reserved.pct</name>\n  <value>0</value>\n  <description>Reserved space in percentage. Read dfs.datanode.du.reserved.calculator to see\n    when this takes effect. The actual number of bytes reserved will be calculated by using the\n    total capacity of the data directory in question. Specific storage type based reservation\n    is also supported. The property can be followed with corresponding storage types\n    ([ssd]/[disk]/[archive]/[ram_disk]) for cluster with heterogeneous storage.\n    For example, reserved percentage space for RAM_DISK storage can be configured using property\n    'dfs.datanode.du.reserved.pct.ram_disk'. If specific storage type reservation is not configured\n    then dfs.datanode.du.reserved.pct will be used.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.data.dir</name>\n  <value>file//</value>\n  <description>Determines where on the local filesystem an DFS data node\n  should store its blocks.  If this is a comma-delimited\n  list of directories, then data will be stored in all named\n  directories, typically on different devices. The directories should be tagged\n  with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS\n  storage policies. The default storage type will be DISK if the directory does\n  not have a storage type tagged explicitly. Directories that do not exist will\n  be created if local filesystem permission allows.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.checkpoint.dir</name>\n  <value>/valid/dir2</value>\n  <description>Determines where on the local filesystem the DFS secondary\n      name node should store the temporary images to merge.\n      If this is a comma-delimited list of directories then the image is\n      replicated in all of the directories for redundancy.\n  </description>\n</property>\n\n<property>\n  <name>dfs.edit.log.transfer.timeout</name>\n  <value>60000</value>\n  <description>\n    Socket timeout for edit log transfer in milliseconds. This timeout\n    should be configured such that normal edit log transfer for journal\n    node syncing can complete successfully.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.avoid.write.stale.datanode</name>\n  <value>false</value>\n  <description>\n    Indicate whether or not to avoid writing to &quot;stale&quot; datanodes whose \n    heartbeat messages have not been received by the namenode \n    for more than a specified time interval. Writes will avoid using \n    stale datanodes unless more than a configured ratio \n    (dfs.namenode.write.stale.datanode.ratio) of datanodes are marked as \n    stale. See dfs.namenode.avoid.read.stale.datanode for a similar setting\n    for reads.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold</name>\n  <value>10737418240</value>\n  <description>\n    Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to\n    org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy.\n    This setting controls how much DN volumes are allowed to differ in terms of\n    bytes of free disk space before they are considered imbalanced. If the free\n    space of all the volumes are within this range of each other, the volumes\n    will be considered balanced and block assignments will be done on a pure\n    round robin basis. Support multiple size unit suffix(case insensitive), as\n    described in dfs.blocksize.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.slow.io.warning.threshold.ms</name>\n  <value>30000</value>\n  <description>The threshold in milliseconds at which we will log a slow\n    io warning in a dfsclient. By default, this parameter is set to 30000\n    milliseconds (30 seconds).\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.missing.checkpoint.periods.before.shutdown</name>\n  <value>6</value>\n  <description>\n    The number of checkpoint period windows (as defined by the property\n    dfs.namenode.checkpoint.period) allowed by the Namenode to perform\n    saving the namespace before shutdown.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "quorum.cnxn.threads.size=1\n\nautopurge.snapRetainCount=6\n\nquorum.auth.learner.saslLoginContext=QuorumLearner\n\nminSessionTimeout=-1\n\nsslQuorumReloadCertFiles=true\n\nelectionAlg=6\n\nlocalSessionsEnabled=true\n\nclientPortAddress=0.0.0.0:3001\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": true}}, {"input": "alluxio.job.master.job.capacity=-5.5\n\nalluxio.worker.tieredstore.level0.dirs.quota=${alluxio.worker.ramdisk.size}\n\nalluxio.underfs.gcs.directory.suffix=/valid/file1\n\nalluxio.master.metastore.dir=/valid/file2\n\nalluxio.network.host.resolution.timeout=5sec\n\nalluxio.job.master.web.bind.host=127.0.0.1\n\nalluxio.master.backup.transport.timeout=1sec\n\nalluxio.master.file.access.time.update.precision=1d\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>fs.defaultFS</name>\n  <value>ciri</value>\n  <description>The name of the default file system.  A URI whose\n  scheme and authority determine the FileSystem implementation.  The\n  uri's scheme determines the config property (fs.SCHEME.impl) naming\n  the FileSystem implementation class.  The uri's authority is used to\n  determine the host, port, etc. for a filesystem.</description>\n</property>\n\n<property>\n  <name>fs.wasbs.impl</name>\n  <value>org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure</value>\n  <description>The implementation class of the Secure Native Azure Filesystem</description>\n</property>\n\n<property>\n  <name>fs.azure.sas.expiry.period</name>\n  <value>180d</value>\n  <description>\n    The default value to be used for expiration period for SAS keys generated.\n    Can use the following suffix (case insensitive):\n    ms(millis), s(sec), m(min), h(hour), d(day)\n    to specify the time (such as 2s, 2m, 1h, etc.).\n  </description>\n</property>\n\n<property>\n  <name>ipc.server.log.slow.rpc</name>\n  <value>false</value>\n    <description>This setting is useful to troubleshoot performance issues for\n     various services. If this value is set to true then we log requests that\n     fall into 99th percentile as well as increment RpcSlowCalls counter.\n    </description>\n</property>\n\n<property>\n  <name>ftp.bytes-per-checksum</name>\n  <value>256</value>\n  <description>The number of bytes per checksum.  Must not be larger than\n  ftp.stream-buffer-size</description>\n</property>\n\n<property>\n  <name>hadoop.http.cross-origin.enabled</name>\n  <value>true</value>\n  <description>Enable/disable the cross-origin (CORS) filter.</description>\n</property>\n\n<property>\n  <name>hadoop.http.cross-origin.allowed-origins</name>\n  <value>*</value>\n  <description>Comma separated list of origins that are allowed for web services\n    needing cross-origin (CORS) support. If a value in the list contains an\n    asterix (*), a regex pattern, escaping any dots ('.' -> '\\.') and replacing\n    the asterix such that it captures any characters ('*' -> '.*'), is generated.\n    Values prefixed with 'regex:' are interpreted directly as regular expressions,\n    e.g. use the expression 'regex:https?:\\/\\/foo\\.bar:([0-9]+)?' to allow any\n    origin using the 'http' or 'https' protocol in the domain 'foo.bar' on any\n    port. The use of simple wildcards ('*') is discouraged, and only available for\n    backward compatibility.</description>\n</property>\n\n<property>\n  <name>hadoop.caller.context.enabled</name>\n  <value>false</value>\n    <description>When the feature is enabled, additional fields are written into\n      name-node audit log records for auditing coarse granularity operations.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "cluster-config-file=nodes-6379.conf\n\noom-score-adj=no\n\ntimeout=1\n\naof-timestamp-enabled=no\n\nrdbchecksum=yes\n\njemalloc-bg-thread=yes\n\nrepl-diskless-sync=yes\n\ndynamic-hz=yes\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "hash_mem_multiplier=-5.5\n\nenable_presorted_aggregate=on\n\ndata_sync_retry=off\n\nautovacuum_work_mem=-2\n\nmaintenance_work_mem=128MB\n\nlog_autovacuum_min_duration=10min\n\nvacuum_failsafe_age=1600000000\n\nwal_sender_timeout=30s\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "postgresql", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.bind.password.file</name>\n  <value>/valid/file1</value>\n  <description>\n    The path to a file containing the password of the bind user. If\n    the password is not configured in credential providers and the property\n    hadoop.security.group.mapping.ldap.bind.password is not set,\n    LDAPGroupsMapping reads password from the file.\n\n    IMPORTANT: This file should be readable only by the Unix user running\n    the daemons and should be a local file.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.s3guard.ddb.table.create</name>\n  <value>false</value>\n  <description>\n    If true, the S3A client will create the table if it does not already exist.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.select.input.csv.record.delimiter</name>\n  <value>\\n</value>\n  <description>In S3 Select queries over CSV files: the record delimiter.\n    \\t is remapped to the TAB character, \\r to CR \\n to newline. \\\\ to \\\n    and \\\" to \"\n  </description>\n</property>\n\n<property>\n  <name>ipc.client.connect.timeout</name>\n  <value>10000</value>\n  <description>Indicates the number of milliseconds a client will wait for the\n               socket to establish a server connection.\n  </description>\n</property>\n\n<property>\n  <name>ipc.[port_number].scheduler.impl</name>\n  <value>org.apache.hadoop.ipc.DefaultRpcScheduler</value>\n  <description>The fully qualified name of a class to use as the\n    implementation of the scheduler. The default implementation is\n    org.apache.hadoop.ipc.DefaultRpcScheduler (no-op scheduler) when not using\n    FairCallQueue. If using FairCallQueue, defaults to\n    org.apache.hadoop.ipc.DecayRpcScheduler. Use\n    org.apache.hadoop.ipc.DecayRpcScheduler in conjunction with the Fair Call\n    Queue.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.http.cross-origin.max-age</name>\n  <value>1800</value>\n  <description>The number of seconds a pre-flighted request can be cached\n    for web services needing cross-origin (CORS) support.</description>\n</property>\n\n<property>\n  <name>ha.zookeeper.parent-znode</name>\n  <value>/hadoop-ha</value>\n  <description>\n    The ZooKeeper znode under which the ZK failover controller stores\n    its information. Note that the nameservice ID is automatically\n    appended to this znode, so it is not normally necessary to\n    configure this, even in a federated environment.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.registry.zk.quorum</name>\n  <value>localhost:2181</value>\n    <description>\n      List of hostname:port pairs defining the\n      zookeeper quorum binding for the registry\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "clientPortAddress=1.1.1.1.1.1\n\nquorum.auth.server.saslLoginContext=QuorumServer\n\nquorum.auth.enableSasl=true\n\nautopurge.purgeInterval=1\n\nreconfigEnabled=false\n\nportUnification=false\n\nelectionAlg=3\n\nsecureClientPortAddress=0.0.0.0:3000\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.connection.timeout.ms</name>\n  <value>60000</value>\n  <description>\n    This property is the connection timeout (in milliseconds) for LDAP\n    operations. If the LDAP provider doesn't establish a connection within the\n    specified period, it will abort the connect attempt. Non-positive value\n    means no LDAP connection timeout is specified in which case it waits for the\n    connection to establish until the underlying network times out.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.uid.cache.secs</name>\n  <value>7200</value>\n    <description>\n        This is the config controlling the validity of the entries in the cache\n        containing the userId to userName and groupId to groupName used by\n        NativeIO getFstat().\n    </description>\n</property>\n\n<property>\n  <name>fs.default.name</name>\n  <value>file:///</value>\n  <description>Deprecated. Use (fs.defaultFS) property\n  instead</description>\n</property>\n\n<property>\n  <name>fs.s3a.change.detection.version.required</name>\n  <value>true</value>\n  <description>\n    Determines if S3 object version attribute defined by\n    fs.s3a.change.detection.source should be treated as required.  If true and the\n    referred attribute is unavailable in an S3 GetObject response,\n    NoVersionAttributeException is thrown.  Setting to 'true' is encouraged to\n    avoid potential for inconsistent reads with third-party S3 implementations or\n    against S3 buckets that have object versioning disabled.\n  </description>\n</property>\n\n<property>\n  <name>ipc.client.connect.max.retries</name>\n  <value>1</value>\n  <description>Indicates the number of retries a client will make to establish\n               a server connection.\n  </description>\n</property>\n\n<property>\n  <name>ipc.client.connect.timeout</name>\n  <value>40000</value>\n  <description>Indicates the number of milliseconds a client will wait for the\n               socket to establish a server connection.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.http.authentication.simple.anonymous.allowed</name>\n  <value>true</value>\n  <description>\n    Indicates if anonymous requests are allowed when using 'simple' authentication.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.http.staticuser.user</name>\n  <value>xdsuper</value>\n  <description>\n    The user name to filter as, on static web filters\n    while rendering content. An example use is the HDFS\n    web UI (user to be used for browsing files).\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>fs.AbstractFileSystem.hdfs.impl</name>\n  <value>org.apache.hadoop.fs.Hdfs</value>\n  <description>The FileSystem for hdfs: uris.</description>\n</property>\n\n<property>\n  <name>fs.s3a.multipart.threshold</name>\n  <value>128M</value>\n  <description>How big (in bytes) to split upload or copy operations up into.\n    This also controls the partition size in renamed files, as rename() involves\n    copying the source file(s).\n    A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.\n  </description>\n</property>\n\n<property>\n  <name>file.stream-buffer-size</name>\n  <value>8192</value>\n  <description>The size of buffer to stream files.\n  The size of this buffer should probably be a multiple of hardware\n  page size (4096 on Intel x86), and it determines how much data is\n  buffered during read and write operations.</description>\n</property>\n\n<property>\n  <name>ftp.stream-buffer-size</name>\n  <value>4096</value>\n  <description>The size of buffer to stream files.\n  The size of this buffer should probably be a multiple of hardware\n  page size (4096 on Intel x86), and it determines how much data is\n  buffered during read and write operations.</description>\n</property>\n\n<property>\n  <name>hadoop.http.authentication.token.validity</name>\n  <value>18000</value>\n  <description>\n    Indicates how long (in seconds) an authentication token is valid before it has\n    to be renewed.\n  </description>\n</property>\n\n<property>\n  <name>fs.permissions.umask-mode</name>\n  <value>002</value>\n  <description>\n    The umask used when creating files and directories.\n    Can be in octal or in symbolic. Examples are:\n    \"022\" (octal for u=rwx,g=r-x,o=r-x in symbolic),\n    or \"u=rwx,g=rwx,o=\" (symbolic for 007 in octal).\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.key.default.bitlength</name>\n  <value>256</value>\n  <description>\n    The length (bits) of keys we want the KeyProvider to produce. Key length\n    defines the upper-bound on an algorithm's security, ideally, it would\n    coincide with the lower-bound on an algorithm's security.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.domainname.resolver.impl</name>\n  <value>org.apache.hadoop.net.DNSDomainNameResolver</value>\n    <description>The implementation of DomainNameResolver used for service (NameNodes,\n      RBF Routers etc) discovery. The default implementation\n      org.apache.hadoop.net.DNSDomainNameResolver returns all IP addresses associated\n      with the input domain name of the services by querying the underlying DNS.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "fs.cos.connection.timeout=1sec\n\nalluxio.worker.management.backoff.strategy=ANY\n\nalluxio.security.group.mapping.class=alluxio.security.group.provider.ShellBasedUnixGroupsMapping\n\nalluxio.master.log.config.report.heartbeat.interval=2h\n\nalluxio.master.metrics.service.threads=1\n\nalluxio.worker.tieredstore.level1.dirs.mediumtype=${alluxio.worker.tieredstore.level1.alias}\n\nalluxio.worker.network.netty.boss.threads=0\n\nalluxio.master.rpc.port=3001\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.https.server.keystore.resource</name>\n  <value>ssl-server.xml</value>\n  <description>Resource file from which ssl server keystore\n  information will be extracted\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.replication.min</name>\n  <value>1</value>\n  <description>Minimal block replication.\n  </description>\n</property>\n\n<property>\n  <name>dfs.blockreport.intervalMsec</name>\n  <value>10800000</value>\n  <description>Determines block reporting interval in milliseconds.</description>\n</property>\n\n<property>\n  <name>dfs.client.mmap.cache.timeout.ms</name>\n  <value>3600000</value>\n  <description>\n    The minimum length of time that we will keep an mmap entry in the cache\n    between uses.  If an entry is in the cache longer than this, and nobody\n    uses it, it will be removed by a background thread.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.directoryscan.interval</name>\n  <value>0</value>\n  <description>Interval in seconds for Datanode to scan data directories and\n  reconcile the difference between blocks in memory and on the disk.\n  Support multiple time unit suffix(case insensitive), as described\n  in dfs.heartbeat.interval.If no time unit is specified then seconds\n  is assumed.\n  </description>\n</property>>\n\n<property>\n  <name>dfs.datanode.data.write.bandwidthPerSec</name>\n  <value>-1</value>\n    <description>\n      Specifies the maximum amount of bandwidth that the data transfering can utilize for writing block or pipeline\n      recovery when\n      BlockConstructionStage is PIPELINE_SETUP_APPEND_RECOVERY or PIPELINE_SETUP_STREAMING_RECOVERY.\n      When the bandwidth value is zero, there is no limit.\n    </description>\n</property>\n\n<property>\n  <name>dfs.qjournal.accept-recovery.timeout.ms</name>\n  <value>120000</value>\n  <description>\n    Quorum timeout in milliseconds during accept phase of\n    recovery/synchronization for a specific segment.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.lease-hard-limit-sec</name>\n  <value>600</value>\n    <description>\n      Determines the namenode automatic lease recovery interval in seconds.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": false}}, {"input": "replica-lazy-flush=no\n\nlist-compress-depth=1\n\nport=3189\n\naclfile=/etc/redis/users.acl\n\nzset-max-listpack-value=32\n\nreplica-announce-ip=5.5.5.5\n\nrdb-del-sync-files=no\n\nzset-max-listpack-entries=256\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "clientPort=3001\n\nstandaloneEnabled=false\n\ndataLogDir=/valid/dir1\n\nquorum.auth.enableSasl=true\n\nquorum.auth.serverRequireSasl=false\n\nautopurge.purgeInterval=-1\n\nlocalSessionsEnabled=false\n\nmaxClientCnxns=30\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.master.info.port</name>\n  <value>3000</value>\n    <description>The port for the HBase Master web UI.\n    Set to -1 if you do not want a UI instance run.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.handler.count</name>\n  <value>true</value>\n    <description>Count of RPC Listener instances spun up on RegionServers.\n      Same property is used by the Master for count of master handlers.\n      Too many handlers can be counter-productive. Make it a multiple of\n      CPU count. If mostly read-only, handlers count close to cpu count\n      does well. Start with twice the CPU count and tune from there.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.region.split.policy</name>\n  <value>org.apache.hadoop.hbase.regionserver.SteppingSplitPolicy</value>\n    <description>\n      A split policy determines when a region should be split. The various\n      other split policies that are available currently are BusyRegionSplitPolicy,\n      ConstantSizeRegionSplitPolicy, DisabledRegionSplitPolicy,\n      DelimitedKeyPrefixRegionSplitPolicy, KeyPrefixRegionSplitPolicy, and\n      SteppingSplitPolicy. DisabledRegionSplitPolicy blocks manual region splitting.\n    </description>\n</property>\n\n<property>\n  <name>hbase.client.scanner.timeout.period</name>\n  <value>120000</value>\n    <description>Client scanner lease period in milliseconds.</description>\n</property>\n\n<property>\n  <name>hbase.auth.token.max.lifetime</name>\n  <value>1209600000</value>\n    <description>The maximum lifetime in milliseconds after which an\n    authentication token expires.  Only used when HBase security is enabled.</description>\n</property>\n\n<property>\n  <name>hbase.rest.port</name>\n  <value>8080</value>\n    <description>The port for the HBase REST server.</description>\n</property>\n\n<property>\n  <name>hbase.data.umask</name>\n  <value>002</value>\n    <description>File permissions that should be used to write data\n      files when hbase.data.umask.enable is true</description>\n</property>\n\n<property>\n  <name>hbase.rest.filter.classes</name>\n  <value>org.apache.hadoop.hbase.rest.filter.GzipFilter</value>\n    <description>\n      Servlet filters for REST service.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "quorum.auth.enableSasl=false\n\nportUnification=true\n\nquorum.auth.learner.saslLoginContext=QuorumLearner\n\nquorum.auth.kerberos.servicePrincipal=zkquorum/localhost\n\nquorum.cnxn.threads.size=10\n\nelectionAlg=4\n\nclientPort=3000\n\nreconfigEnabled=false\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": false}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.hstore.flusher.count</name>\n  <value>2</value>\n    <description> The number of flush threads. With fewer threads, the MemStore flushes will be\n      queued. With more threads, the flushes will be executed in parallel, increasing the load on\n      HDFS, and potentially causing more compactions. </description>\n</property>\n\n<property>\n  <name>hbase.rest.readonly</name>\n  <value>false</value>\n    <description>Defines the mode the REST server will be started in. Possible values are:\n    false: All HTTP methods are permitted - GET/PUT/POST/DELETE.\n    true: Only the GET method is permitted.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.thrift.framed.max_frame_size_in_mb</name>\n  <value>2</value>\n    <description>Default frame size when using framed transport, in MB</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.checksum.verify</name>\n  <value>true</value>\n    <description>\n        If set to true (the default), HBase verifies the checksums for hfile\n        blocks. HBase writes checksums inline with the data when it writes out\n        hfiles. HDFS (as of this writing) writes checksums to a separate file\n        than the data file necessitating extra seeks.  Setting this flag saves\n        some on i/o.  Checksum verification by HDFS will be internally disabled\n        on hfile streams when this flag is set.  If the hbase-checksum verification\n        fails, we will switch back to using HDFS checksums (so do not disable HDFS\n        checksums!  And besides this feature applies to hfiles only, not to WALs).\n        If this parameter is set to false, then hbase will not verify any checksums,\n        instead it will depend on checksum verification being done in the HDFS client.\n    </description>\n</property>\n\n<property>\n  <name>hbase.server.scanner.max.result.size</name>\n  <value>104857600</value>\n    <description>Maximum number of bytes returned when calling a scanner's next method.\n    Note that when a single row is larger than this limit the row is still returned completely.\n    The default value is 100MB.\n    This is a safety setting to protect the server from OOM situations.\n    </description>\n</property>\n\n<property>\n  <name>hbase.rest.csrf.enabled</name>\n  <value>true</value>\n  <description>\n    Set to true to enable protection against cross-site request forgery (CSRF)\n\t</description>\n</property>\n\n<property>\n  <name>hbase.rpc.rows.warning.threshold</name>\n  <value>10000</value>\n    <description>\n      Number of rows in a batch operation above which a warning will be logged.\n    </description>\n</property>\n\n<property>\n  <name>hbase.regionserver.slowlog.ringbuffer.size</name>\n  <value>256</value>\n    <description>\n      Default size of ringbuffer to be maintained by each RegionServer in order\n      to store online slowlog responses. This is an in-memory ring buffer of\n      requests that were judged to be too slow in addition to the responseTooSlow\n      logging. The in-memory representation would be complete.\n      For more details, please look into Doc Section:\n      Get Slow Response Log from shell\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.namenode.fs-limits.min-block-size</name>\n  <value>524288</value>\n  <description>Minimum block size in bytes, enforced by the Namenode at create\n      time. This prevents the accidental creation of files with tiny block\n      sizes (and thus many blocks), which can degrade performance. Support multiple\n      size unit suffix(case insensitive), as described in dfs.blocksize.\n  </description>\n</property>\n\n<property>\n  <name>dfs.ha.tail-edits.period.backoff-max</name>\n  <value>1</value>\n  <description>\n    The maximum time the tailer should wait between checking for new edit log\n    entries. Exponential backoff will be applied when an edit log tail is\n    performed but no edits are available to be read. Values less than or\n    equal to zero disable backoff entirely; this is the default behavior.\n    Supports multiple time unit suffix (case insensitive), as described\n    in dfs.heartbeat.interval.\n  </description>\n</property>\n\n<property>\n  <name>dfs.short.circuit.shared.memory.watcher.interrupt.check.ms</name>\n  <value>120000</value>\n  <description>\n    The length of time in milliseconds that the short-circuit shared memory\n    watcher will go between checking for java interruptions sent from other\n    threads.  This is provided mainly for unit tests.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.lock-reporting-threshold-ms</name>\n  <value>300</value>\n  <description>When thread waits to obtain a lock, or a thread holds a lock for\n    more than the threshold, a log message will be written. Note that\n    dfs.lock.suppress.warning.interval ensures a single log message is\n    emitted per interval for waiting threads and a single message for holding\n    threads to avoid excessive logging.\n  </description>\n</property>\n\n<property>\n  <name>dfs.webhdfs.rest-csrf.browser-useragents-regex</name>\n  <value>^Mozilla.*,^Opera.*</value>\n  <description>\n    A comma-separated list of regular expressions used to match against an HTTP\n    request's User-Agent header when protection against cross-site request\n    forgery (CSRF) is enabled for WebHDFS by setting\n    dfs.webhdfs.reset-csrf.enabled to true.  If the incoming User-Agent matches\n    any of these regular expressions, then the request is considered to be sent\n    by a browser, and therefore CSRF prevention is enforced.  If the request's\n    User-Agent does not match any of these regular expressions, then the request\n    is considered to be sent by something other than a browser, such as scripted\n    automation.  In this case, CSRF is not a potential attack vector, so\n    the prevention is not enforced.  This helps achieve backwards-compatibility\n    with existing automation that has not been updated to send the CSRF\n    prevention header.\n  </description>\n</property>\n\n<property>\n  <name>dfs.xframe.value</name>\n  <value>NOEXIST_TRANSFER_MODE</value>\n    <description>\n      This configration value allows user to specify the value for the\n      X-FRAME-OPTIONS. The possible values for this field are\n      DENY, SAMEORIGIN and ALLOW-FROM. Any other value will throw an\n      exception when namenode and datanodes are starting up.\n    </description>\n</property>\n\n<property>\n  <name>dfs.balancer.keytab.file</name>\n  <value>/valid/file1</value>\n  <description>\n    The keytab file used by the Balancer to login as its\n    service principal. The principal name is configured with\n    dfs.balancer.kerberos.principal. Keytab based login can be\n    enabled with dfs.balancer.keytab.enabled.\n  </description>\n</property>\n\n<property>\n  <name>dfs.block.invalidate.limit</name>\n  <value>2000</value>\n  <description>\n    The maximum number of invalidate blocks sent by namenode to a datanode\n    per heartbeat deletion command. This property works with\n    \"dfs.namenode.invalidate.work.pct.per.iteration\" to throttle block\n    deletions.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "alluxio.underfs.eventual.consistency.retry.max.num=0\n\nalluxio.master.ufs.active.sync.event.rate.interval=60sec\n\nalluxio.master.lock.pool.high.watermark=1000000\n\nalluxio.user.hostname=127.0.0.1\n\nalluxio.underfs.cleanup.enabled=true\n\nalluxio.underfs.s3.upload.threads.max=40\n\nalluxio.user.file.buffer.bytes=8MB\n\nalluxio.security.login.impersonation.username=_HDFS_USER_\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "alluxio.user.client.cache.enabled=false\n\nalluxio.user.client.cache.size=1024MB\n\nalluxio.user.client.cache.evictor.class=alluxio.client.file.cache.evictor.LRUCacheEvictor\n\nalluxio.master.mount.table.root.readonly=false\n\nalluxio.user.network.max.inbound.message.size=0.1\n\nalluxio.worker.ufs.block.open.timeout=1min\n\nalluxio.master.backup.connect.interval.max=1sec\n\nalluxio.user.block.read.retry.max.duration=5min\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "cluster-announce-tls-port=1.1\n\nmaxmemory-clients=1g\n\nauto-aof-rewrite-percentage=100\n\ntimeout=2\n\nrepl-backlog-size=1mb\n\ntls-protocols=\"TLSv1.2 TLSv1.3\"\n\nlatency-monitor-threshold=0\n\nlazyfree-lazy-expire=no\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>fs.s3a.proxy.port</name>\n  <value>65536</value>\n  <description>Proxy server port. If this property is not set\n    but fs.s3a.proxy.host is, port 80 or 443 is assumed (consistent with\n    the value of fs.s3a.connection.ssl.enabled).</description>\n</property>\n\n<property>\n  <name>fs.azure.secure.mode</name>\n  <value>true</value>\n  <description>\n    Config flag to identify the mode in which fs.azure.NativeAzureFileSystem needs\n    to run under. Setting it \"true\" would make fs.azure.NativeAzureFileSystem use\n    SAS keys to communicate with Azure storage.\n  </description>\n</property>\n\n<property>\n  <name>ipc.[port_number].scheduler.priority.levels</name>\n  <value>1</value>\n  <description>How many priority levels to use within the scheduler and call\n    queue. This property applies to RpcScheduler and CallQueue.\n  </description>\n</property>\n\n<property>\n  <name>net.topology.node.switch.mapping.impl</name>\n  <value>org.apache.hadoop.net.ScriptBasedMapping</value>\n  <description> The default implementation of the DNSToSwitchMapping. It\n    invokes a script specified in net.topology.script.file.name to resolve\n    node names. If the value for net.topology.script.file.name is not set, the\n    default value of DEFAULT_RACK is returned for all node names.\n  </description>\n</property>\n\n<property>\n  <name>file.blocksize</name>\n  <value>134217728</value>\n  <description>Block size</description>\n</property>\n\n<property>\n  <name>hadoop.http.authentication.token.validity</name>\n  <value>72000</value>\n  <description>\n    Indicates how long (in seconds) an authentication token is valid before it has\n    to be renewed.\n  </description>\n</property>\n\n<property>\n  <name>adl.feature.ownerandgroup.enableupn</name>\n  <value>true</value>\n    <description>\n      When true : User and Group in FileStatus/AclStatus response is\n      represented as user friendly name as per Azure AD profile.\n\n      When false (default) : User and Group in FileStatus/AclStatus\n      response is represented by the unique identifier from Azure AD\n      profile (Object ID as GUID).\n\n      For optimal performance, false is recommended.\n    </description>\n</property>\n\n<property>\n  <name>hadoop.zk.retry-interval-ms</name>\n  <value>2000</value>\n    <description>Retry interval in milliseconds when connecting to ZooKeeper.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.namenode.kerberos.principal.pattern</name>\n  <value>*</value>\n  <description>\n    A client-side RegEx that can be configured to control\n    allowed realms to authenticate with (useful in cross-realm env.)\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.deadnode.detection.deadnode.queue.max</name>\n  <value>100</value>\n    <description>\n      The max queue size of probing dead node.\n    </description>\n</property>\n\n<property>\n  <name>dfs.namenode.quota.init-threads</name>\n  <value>4</value>\n  <description>\n    The number of concurrent threads to be used in quota initialization. The\n    speed of quota initialization also affects the namenode fail-over latency.\n    If the size of name space is big, try increasing this.\n  </description>\n</property>\n\n<property>\n  <name>dfs.xframe.enabled</name>\n  <value>false</value>\n    <description>\n      If true, then enables protection against clickjacking by returning\n      X_FRAME_OPTIONS header value set to SAMEORIGIN.\n      Clickjacking protection prevents an attacker from using transparent or\n      opaque layers to trick a user into clicking on a button\n      or link on another page.\n    </description>\n</property>\n\n<property>\n  <name>dfs.balancer.getBlocks.min-block-size</name>\n  <value>5242880</value>\n  <description>\n    Minimum block threshold size in bytes to ignore when fetching a source's\n    block list.\n  </description>\n</property>\n\n<property>\n  <name>dfs.mover.retry.max.attempts</name>\n  <value>-100</value>\n  <description>\n    The maximum number of retries before the mover consider the\n    move failed.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.edits.asynclogging</name>\n  <value>true</value>\n  <description>\n    If set to true, enables asynchronous edit logs in the Namenode.  If set\n    to false, the Namenode uses the traditional synchronous edit logs.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.gc.time.monitor.sleep.interval.ms</name>\n  <value>5s</value>\n    <description>\n      Determines the sleep interval in the window. The GcTimeMonitor wakes up in\n      the sleep interval periodically to compute the gc time proportion. The\n      shorter the interval the preciser the GcTimePercentage. The sleep interval\n      must be shorter than the window size.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "secureClientPortAddress=0.0.0\n\nautopurge.purgeInterval=-1\n\nsslQuorum=false\n\nstandaloneEnabled=false\n\ntickTime=3000\n\nsyncLimit=10\n\nreconfigEnabled=false\n\nlocalSessionsUpgradingEnabled=true\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": true}}, {"input": "FILE_UPLOAD_DIRECTORY_PERMISSIONS=None\n\nEMAIL_SSL_KEYFILE=None\n\nPASSWORD_HASHERS=['django.contrib.auth.hashers.PBKDF2PasswordHasher','django.contrib.auth.hashers.PBKDF2SHA1PasswordHasher','django.contrib.auth.hashers.Argon2PasswordHasher','django.contrib.auth.hashers.BCryptSHA256PasswordHasher','django.contrib.auth.hashers.ScryptPasswordHasher']\n\nLANGUAGE_COOKIE_HTTPONLY=False\n\nABSOLUTE_URL_OVERRIDES={}\n\nAUTHENTICATION_BACKENDS=['django.contrib.auth.backends.ModelBackend']\n\nSESSION_COOKIE_PATH='/'\n\nEMAIL_USE_SSL=False\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "django", "is_synthetic": true}}, {"input": "alluxio.fuse.shared.caching.reader.enabled=1.5\n\nalluxio.job.master.web.bind.host=127.0.0.1\n\nalluxio.master.audit.logging.queue.capacity=5000\n\nalluxio.worker.network.reader.max.chunk.size.bytes=1MB\n\nalluxio.worker.file.buffer.size=10MB\n\nalluxio.table.transform.manager.job.history.retention.time=300sec\n\nalluxio.web.file.info.enabled=false\n\nalluxio.underfs.eventual.consistency.retry.max.num=0\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.groups.shell.command.timeout</name>\n  <value>1s</value>\n  <description>\n    Used by the ShellBasedUnixGroupsMapping class, this property controls how\n    long to wait for the underlying shell command that is run to fetch groups.\n    Expressed in seconds (e.g. 10s, 1m, etc.), if the running command takes\n    longer than the value configured, the command is aborted and the groups\n    resolver would return a result of no groups found. A value of 0s (default)\n    would mean an infinite wait (i.e. wait until the command exits on its own).\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.group.mapping.ldap.posix.attr.uid.name</name>\n  <value>uidNumber</value>\n  <description>\n    The attribute of posixAccount to use when groups for membership.\n    Mostly useful for schemas wherein groups have memberUids that use an\n    attribute other than uidNumber.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.multiobjectdelete.enable</name>\n  <value>false</value>\n  <description>When enabled, multiple single-object delete requests are replaced by\n    a single 'delete multiple objects'-request, reducing the number of requests.\n    Beware: legacy S3-compatible object stores might not support this request.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.block.size</name>\n  <value>32M</value>\n  <description>Block size to use when reading files using s3a: file system.\n    A suffix from the set {K,M,G,T,P} may be used to scale the numeric value.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.list.version</name>\n  <value>4</value>\n  <description>\n    Select which version of the S3 SDK's List Objects API to use.  Currently\n    support 2 (default) and 1 (older API).\n  </description>\n</property>\n\n<property>\n  <name>hadoop.http.authentication.token.validity</name>\n  <value>18000</value>\n  <description>\n    Indicates how long (in seconds) an authentication token is valid before it has\n    to be renewed.\n  </description>\n</property>\n\n<property>\n  <name>ha.zookeeper.session-timeout.ms</name>\n  <value>5000</value>\n  <description>\n    The session timeout to use when the ZKFC connects to ZooKeeper.\n    Setting this value to a lower value implies that server crashes\n    will be detected more quickly, but risks triggering failover too\n    aggressively in the case of a transient error or network blip.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.ssl.client.conf</name>\n  <value>/valid/file1</value>\n  <description>\n    Resource file from which ssl client keystore information will be extracted\n    This file is looked up in the classpath, typically it should be in Hadoop\n    conf/ directory.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.local.dir</name>\n  <value>/valid/file1</value>\n    <description>Directory on the local filesystem to be used\n    as a local storage.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.region.split.policy</name>\n  <value>org.apache.hadoop.hbase.regionserver.SteppingSplitPolicy</value>\n    <description>\n      A split policy determines when a region should be split. The various\n      other split policies that are available currently are BusyRegionSplitPolicy,\n      ConstantSizeRegionSplitPolicy, DisabledRegionSplitPolicy,\n      DelimitedKeyPrefixRegionSplitPolicy, KeyPrefixRegionSplitPolicy, and\n      SteppingSplitPolicy. DisabledRegionSplitPolicy blocks manual region splitting.\n    </description>\n</property>\n\n<property>\n  <name>hbase.regions.slop</name>\n  <value>0.002</value>\n    <description>Rebalance if any regionserver has average + (average * slop) regions.\n      The default value of this parameter is 0.001 in StochasticLoadBalancer (the default load balancer),\n      while the default is 0.2 in other load balancers (i.e., SimpleLoadBalancer).</description>\n</property>\n\n<property>\n  <name>hbase.storescanner.parallel.seek.enable</name>\n  <value>true</value>\n    <description>\n      Enables StoreFileScanner parallel-seeking in StoreScanner,\n      a feature which can reduce response latency under special conditions.</description>\n</property>\n\n<property>\n  <name>hfile.block.cache.size</name>\n  <value>0.8</value>\n    <description>Percentage of maximum heap (-Xmx setting) to allocate to block cache\n        used by a StoreFile. Default of 0.4 means allocate 40%.\n        Set to 0 to disable but it's not recommended; you need at least\n        enough cache to hold the storefile indices.</description>\n</property>\n\n<property>\n  <name>hfile.block.index.cacheonwrite</name>\n  <value>false</value>\n      <description>This allows to put non-root multi-level index blocks into the block\n          cache at the time the index is being written.</description>\n</property>\n\n<property>\n  <name>hbase.auth.token.max.lifetime</name>\n  <value>1209600000</value>\n    <description>The maximum lifetime in milliseconds after which an\n    authentication token expires.  Only used when HBase security is enabled.</description>\n</property>\n\n<property>\n  <name>hbase.rootdir.perms</name>\n  <value>700</value>\n    <description>FS Permissions for the root data subdirectory in a secure (kerberos) setup.\n    When master starts, it creates the rootdir with this permissions or sets the permissions\n    if it does not match.</description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "alluxio.master.shell.backup.state.lock.grace.mode=FORCE\n\nalluxio.debug=true\n\nalluxio.master.lock.pool.low.watermark=500000\n\nalluxio.master.rpc.port=3001\n\nalluxio.user.file.master.client.pool.gc.interval=120sec\n\nalluxio.user.conf.cluster.default.enabled=true\n\nalluxio.user.network.streaming.netty.worker.threads=-1\n\nfs.cos.connection.timeout=100sec\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>fs.ftp.data.connection.mode</name>\n  <value>ACTIVE_LOCAL_DATA_CONNECTION_MODE</value>\n  <description>Set the FTPClient's data connection mode based on configuration.\n    Valid values are ACTIVE_LOCAL_DATA_CONNECTION_MODE,\n    PASSIVE_LOCAL_DATA_CONNECTION_MODE and PASSIVE_REMOTE_DATA_CONNECTION_MODE.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.path.style.access</name>\n  <value>false</value>\n  <description>Enable S3 path style access ie disabling the default virtual hosting behaviour.\n    Useful for S3A-compliant storage providers as it removes the need to set up DNS for virtual hosting.\n  </description>\n</property>\n\n<property>\n  <name>ipc.[port_number].decay-scheduler.decay-factor</name>\n  <value>0.25</value>\n  <description>When decaying the operation counts of users, the multiplicative\n    decay factor to apply. Higher values will weight older operations more\n    strongly, essentially giving the scheduler a longer memory, and penalizing\n    heavy clients for a longer period of time.\n    This property applies to DecayRpcScheduler.\n  </description>\n</property>\n\n<property>\n  <name>tfile.fs.output.buffer.size</name>\n  <value>262144</value>\n  <description>\n    Buffer size used for FSDataOutputStream in bytes.\n  </description>\n</property>\n\n<property>\n  <name>tfile.fs.input.buffer.size</name>\n  <value>524288</value>\n  <description>\n    Buffer size used for FSDataInputStream in bytes.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.ssl.enabled</name>\n  <value>false</value>\n  <description>\n    Deprecated. Use dfs.http.policy and yarn.http.policy instead.\n  </description>\n</property>\n\n<property>\n  <name>nfs.exports.allowed.hosts</name>\n  <value>* rw</value>\n  <description>\n    By default, the export can be mounted by any client. The value string\n    contains machine name and access privilege, separated by whitespace\n    characters. The machine name format can be a single host, a Java regular\n    expression, or an IPv4 address. The access privilege uses rw or ro to\n    specify read/write or read-only access of the machines to exports. If the\n    access privilege is not provided, the default is read-only. Entries are separated by \";\".\n    For example: \"192.168.0.0/22 rw ; host.*\\.example\\.com ; host1.test.org ro;\".\n    Only the NFS gateway needs to restart after this property is updated.\n  </description>\n</property>\n\n<property>\n  <name>fs.adl.impl</name>\n  <value>org.apache.hadoop.fs.adl.AdlFileSystem</value>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.common.configuration.version</name>\n  <value>3.0.0</value>\n  <description>version of this configuration file</description>\n</property>\n\n<property>\n  <name>hadoop.security.groups.shell.command.timeout</name>\n  <value>0s</value>\n  <description>\n    Used by the ShellBasedUnixGroupsMapping class, this property controls how\n    long to wait for the underlying shell command that is run to fetch groups.\n    Expressed in seconds (e.g. 10s, 1m, etc.), if the running command takes\n    longer than the value configured, the command is aborted and the groups\n    resolver would return a result of no groups found. A value of 0s (default)\n    would mean an infinite wait (i.e. wait until the command exits on its own).\n  </description>\n</property>\n\n<property>\n  <name>io.file.buffer.size</name>\n  <value>4096</value>\n  <description>The size of buffer for use in sequence files.\n  The size of this buffer should probably be a multiple of hardware\n  page size (4096 on Intel x86), and it determines how much data is\n  buffered during read and write operations.</description>\n</property>\n\n<property>\n  <name>io.bytes.per.checksum</name>\n  <value>8192</value>\n  <description>The number of bytes per checksum.  Must not be larger than\n  io.file.buffer.size.</description>\n</property>\n\n<property>\n  <name>io.erasurecode.codec.xor.rawcoders</name>\n  <value>xor_native</value>\n  <description>\n    Comma separated raw coder implementations for the xor codec. The earlier\n    factory is prior to followings in case of failure of creating raw coders.\n  </description>\n</property>\n\n<property>\n  <name>fs.azure.authorization</name>\n  <value>false</value>\n  <description>\n    Config flag to enable authorization support in WASB. Setting it to \"true\" enables\n    authorization support to WASB. Currently WASB authorization requires a remote service\n    to provide authorization that needs to be specified via fs.azure.authorization.remote.service.url\n    configuration\n  </description>\n</property>\n\n<property>\n  <name>hadoop.ssl.require.client.cert</name>\n  <value>false</value>\n  <description>Whether client certificates are required</description>\n</property>\n\n<property>\n  <name>hadoop.shell.missing.defaultFs.warning</name>\n  <value>false</value>\n    <description>\n      Enable hdfs shell commands to display warnings if (fs.defaultFS) property\n      is not set.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hbase.regionserver.info.port</name>\n  <value>16030</value>\n    <description>The port for the HBase RegionServer web UI\n    Set to -1 if you do not want the RegionServer UI to run.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.dns.interface</name>\n  <value>eth1</value>\n    <description>The name of the Network Interface from which a region server\n      should report its IP address.</description>\n</property>\n\n<property>\n  <name>hbase.hstore.compaction.kv.max</name>\n  <value>20</value>\n    <description>The maximum number of KeyValues to read and then write in a batch when flushing or\n      compacting. Set this lower if you have big KeyValues and problems with Out Of Memory\n      Exceptions Set this higher if you have wide, small rows. </description>\n</property>\n\n<property>\n  <name>hbase.storescanner.parallel.seek.threads</name>\n  <value>20</value>\n    <description>\n      The default thread pool size if parallel-seeking feature enabled.</description>\n</property>\n\n<property>\n  <name>hbase.master.keytab.file</name>\n  <value>/valid/file1</value>\n    <description>Full path to the kerberos keytab file to use for logging in\n    the configured HMaster server principal.</description>\n</property>\n\n<property>\n  <name>hbase.rest.threads.max</name>\n  <value>50</value>\n    <description>The maximum number of threads of the REST server thread pool.\n        Threads in the pool are reused to process REST requests. This\n        controls the maximum number of requests processed concurrently.\n        It may help to control the memory used by the REST server to\n        avoid OOM issues. If the thread pool is full, incoming requests\n        will be queued up and wait for some free threads.</description>\n</property>\n\n<property>\n  <name>hbase.snapshot.enabled</name>\n  <value>true</value>\n    <description>Set to true to allow snapshots to be taken / restored / cloned.</description>\n</property>\n\n<property>\n  <name>hbase.mob.compactor.class</name>\n  <value>org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactor</value>\n    <description>\n      Implementation of mob compactor, the default one is PartitionedMobCompactor.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.client.https.keystore.resource</name>\n  <value>ssl-client.xml</value>\n  <description>Resource file from which ssl client keystore\n  information will be extracted\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.handler.count</name>\n  <value>5</value>\n  <description>The number of server threads for the datanode.</description>\n</property>\n\n<property>\n  <name>dfs.namenode.write-lock-reporting-threshold-ms</name>\n  <value>5000</value>\n  <description>When a write lock is held on the namenode for a long time,\n    this will be logged as the lock is released. This sets how long the\n    lock must be held for logging to occur.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.posix.acl.inheritance.enabled</name>\n  <value>true</value>\n    <description>\n      Set to true to enable POSIX style ACL inheritance. When it is enabled\n      and the create request comes from a compatible client, the NameNode\n      will apply default ACLs from the parent directory to the create mode\n      and ignore the client umask. If no default ACL found, it will apply the\n      client umask.\n    </description>\n</property>\n\n<property>\n  <name>dfs.client.socketcache.capacity</name>\n  <value>8</value>\n  <description>\n    Socket cache capacity (in entries) for short-circuit reads.\n    If this value is set to 0, the client socket cache is disabled.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.fs-limits.min-block-size</name>\n  <value>1048576</value>\n  <description>Minimum block size in bytes, enforced by the Namenode at create\n      time. This prevents the accidental creation of files with tiny block\n      sizes (and thus many blocks), which can degrade performance. Support multiple\n      size unit suffix(case insensitive), as described in dfs.blocksize.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.failed.volumes.tolerated</name>\n  <value>-2</value>\n  <description>The number of volumes that are allowed to\n  fail before a datanode stops offering service. By default\n  any volume failure will cause a datanode to shutdown.\n  The value should be greater than or equal to -1 , -1 represents minimum\n  1 valid volume.\n  </description>\n</property>\n\n<property>\n  <name>dfs.webhdfs.oauth2.enabled</name>\n  <value>true</value>\n  <description>\n    If true, enables OAuth2 in WebHDFS\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": false}}, {"input": "clientPortAddress=256.256.256.256\n\ndataDir=/valid/dir1\n\nquorum.auth.learner.saslLoginContext=QuorumLearner\n\nquorum.auth.serverRequireSasl=false\n\nmaxSessionTimeout=0\n\nautopurge.snapRetainCount=1\n\nreconfigEnabled=true\n\nminSessionTimeout=-2\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    \n<property>\n  <name>yarn.nodemanager.runtime.linux.docker.default-container-network</name>\n  <value>host</value>\n    <description>The network used when launching containers using the\n      DockerContainerRuntime when no network is specified in the request\n      . This network must be one of the (configurable) set of allowed container\n      networks.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.recovery.dir</name>\n  <value>/valid/file1</value>\n    <description>The local filesystem directory in which the node manager will\n    store state when recovery is enabled.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.disk-health-checker.enable</name>\n  <value>false</value>\n    <description>\n    Flag to enable NodeManager disk health checker\n    </description>\n</property>\n\n<property>\n  <name>yarn.timeline-service.webapp.https.address</name>\n  <value>${yarn.timeline-service.hostname}:8190</value>\n    <description>The https address of the timeline service web application.</description>\n</property>\n\n<property>\n  <name>yarn.timeline-service.client.internal-timers-ttl-secs</name>\n  <value>210</value>\n    <description>\n      How long the internal Timer Tasks can be alive in writer. If there is no\n      write operation for this configured time, the internal timer tasks will\n      be close.\n    </description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.node-attributes.provider.fetch-interval-ms</name>\n  <value>300000</value>\n    <description>\n      Time interval that determines how long NM fetches node attributes\n      from a given provider. If -1 is configured then node labels are\n      retrieved from provider only during initialization. Defaults to 10 mins.\n    </description>\n</property>\n\n<property>\n  <name>yarn.resourcemanager.nm-container-queuing.max-queue-wait-time-ms</name>\n  <value>200</value>\n    <description>\n    Max queue wait time for a container queue at a NodeManager.\n    </description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.container.stderr.pattern</name>\n  <value>{*stderr*,*STDERR*}</value>\n    <description>\n    Error filename pattern, to identify the file in the container's\n    Log directory which contain the container's error log. As error file\n    redirection is done by client/AM and yarn will not be aware of the error\n    file name. YARN uses this pattern to identify the error file and tail\n    the error log as diagnostics when the container execution returns non zero\n    value. Filename patterns are case sensitive and should match the\n    specifications of FileSystem.globStatus(Path) api. If multiple filenames\n    matches the pattern, first file matching the pattern will be picked.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "yarn", "is_synthetic": true}}, {"input": "alluxio.user.network.rpc.max.connections=2\n\nalluxio.job.worker.web.port=3000\n\nalluxio.master.embedded.journal.addresses=127.0.0.1\n\nalluxio.user.file.sequential.pread.threshold=1MB\n\nalluxio.user.local.writer.chunk.size.bytes=64KB\n\nalluxio.user.block.read.retry.sleep.max=4sec\n\nalluxio.user.network.streaming.keepalive.time=18446744073709551614\n\nalluxio.underfs.s3.disable.dns.buckets=false\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "alluxio.job.master.client.threads=9.9\n\nalluxio.master.daily.backup.state.lock.grace.mode=TIMEOUT\n\nalluxio.user.streaming.reader.buffer.size.messages=16\n\nalluxio.user.file.persist.on.rename=true\n\nalluxio.worker.remote.io.slow.threshold=1s\n\nalluxio.underfs.s3.streaming.upload.enabled=false\n\nalluxio.jvm.monitor.sleep.interval=2sec\n\nalluxio.master.journal.folder=${alluxio.work.dir}/journal\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "dataLogDir=tmp////staging\n\nstandaloneEnabled=true\n\nquorum.auth.kerberos.servicePrincipal=zkquorum/localhost\n\nquorum.auth.serverRequireSasl=true\n\nmaxSessionTimeout=0\n\nquorum.auth.enableSasl=true\n\nautopurge.purgeInterval=-1\n\nquorumListenOnAllIPs=false\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "zookeeper", "is_synthetic": true}}, {"input": "repl-diskless-load=NONE\n\nprotected-mode=yes\n\nenable-protected-configs=no\n\nacllog-max-len=256\n\nslowlog-log-slower-than=5000\n\nreplica-serve-stale-data=yes\n\nappenddirname=\"appendonlydir\"\n\nreplica-announce-port=1234\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>hadoop.security.groups.cache.background.reload.threads</name>\n  <value>1</value>\n  <description>\n    Only relevant if hadoop.security.groups.cache.background.reload is true.\n    Controls the number of concurrent background user->group cache entry\n    refreshes. Pending refresh requests beyond this value are queued and\n    processed when a thread is free.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.security.credential.provider.path</name>\n  <value>/valid/file1</value>\n  <description>\n    A comma-separated list of URLs that indicates the type and\n    location of a list of providers that should be consulted.\n  </description>\n</property>\n\n<property>\n  <name>fs.swift.impl</name>\n  <value>org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem</value>\n  <description>The implementation class of the OpenStack Swift Filesystem</description>\n</property>\n\n<property>\n  <name>fs.s3a.socket.send.buffer</name>\n  <value>16384</value>\n  <description>Socket send buffer hint to amazon connector. Represented in bytes.</description>\n</property>\n\n<property>\n  <name>fs.s3a.committer.staging.abort.pending.uploads</name>\n  <value>true</value>\n  <description>\n    Should the staging committers abort all pending uploads to the destination\n    directory?\n\n    Changing this if more than one partitioned committer is\n    writing to the same destination tree simultaneously; otherwise\n    the first job to complete will cancel all outstanding uploads from the\n    others. However, it may lead to leaked outstanding uploads from failed\n    tasks. If disabled, configure the bucket lifecycle to remove uploads\n    after a time period, and/or set up a workflow to explicitly delete\n    entries. Otherwise there is a risk that uncommitted uploads may run up\n    bills.\n  </description>\n</property>\n\n<property>\n  <name>fs.s3a.ssl.channel.mode</name>\n  <value>default_jsse</value>\n  <description>\n    If secure connections to S3 are enabled, configures the SSL\n    implementation used to encrypt connections to S3. Supported values are:\n    \"default_jsse\", \"default_jsse_with_gcm\", \"default\", and \"openssl\".\n    \"default_jsse\" uses the Java Secure Socket Extension package (JSSE).\n    However, when running on Java 8, the GCM cipher is removed from the list\n    of enabled ciphers. This is due to performance issues with GCM in Java 8.\n    \"default_jsse_with_gcm\" uses the JSSE with the default list of cipher\n    suites. \"default_jsse_with_gcm\" is equivalent to the behavior prior to\n    this feature being introduced. \"default\" attempts to use OpenSSL rather\n    than the JSSE for SSL encryption, if OpenSSL libraries cannot be loaded,\n    it falls back to the \"default_jsse\" behavior. \"openssl\" attempts to use\n    OpenSSL as well, but fails if OpenSSL libraries cannot be loaded.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.ssl.enabled</name>\n  <value>true</value>\n  <description>\n    Deprecated. Use dfs.http.policy and yarn.http.policy instead.\n  </description>\n</property>\n\n<property>\n  <name>hadoop.registry.zk.retry.interval.ms</name>\n  <value>500</value>\n    <description>\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hcommon", "is_synthetic": true}}, {"input": "tls-protocols=NOEXIST_TLS\n\naclfile=/valid/file2.acl\n\ncluster-announce-port=2\n\nrepl-disable-tcp-nodelay=no\n\nport=6379\n\nrepl-diskless-load=disabled\n\nzset-max-listpack-entries=128\n\nrdb-save-incremental-fsync=yes\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "redis", "is_synthetic": true}}, {"input": "alluxio.master.journal.flush.batch.time=500ms\n\nalluxio.master.journal.flush.timeout=400ms\n\nalluxio.user.short.circuit.enabled=true\n\nalluxio.master.filesystem.liststatus.result.message.length=10000\n\nalluxio.underfs.s3.proxy.port=3000\n\nalluxio.worker.keytab.file=/valid/file2\n\nalluxio.worker.ufs.block.open.timeout=1min\n\nalluxio.network.connection.shutdown.graceful.timeout=90sec\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}, {"input": "log_error_verbosity=default\n\nmax_connections=50\n\nlog_connections=off\n\ngeqo_effort=10\n\nlog_line_prefix='%m [%p] '\n\nvacuum_defer_cleanup_age=0\n\nhot_standby=on\n\nstats_fetch_consistency=cache\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "postgresql", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.datanode.dns.nameserver</name>\n  <value>default</value>\n  <description>\n    The host name or IP address of the name server (DNS) which a DataNode\n    should use to determine its own host name.\n\n    Prefer using hadoop.security.dns.nameserver over\n    dfs.datanode.dns.nameserver.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.read.considerLoad</name>\n  <value>true</value>\n  <description>\n    Decide if sort block locations considers the target's load or not when read.\n    Turn off by default.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.du.reserved.calculator</name>\n  <value>org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$ReservedSpaceCalculatorAbsolute</value>\n  <description>Determines the class of ReservedSpaceCalculator to be used for\n    calculating disk space reservedfor non-HDFS data. The default calculator is\n    ReservedSpaceCalculatorAbsolute which will use dfs.datanode.du.reserved\n    for a static reserved number of bytes. ReservedSpaceCalculatorPercentage\n    will use dfs.datanode.du.reserved.pct to calculate the reserved number\n    of bytes based on the size of the storage. ReservedSpaceCalculatorConservative and\n    ReservedSpaceCalculatorAggressive will use their combination, Conservative will use\n    maximum, Aggressive minimum. For more details see ReservedSpaceCalculator.\n  </description>\n</property>\n\n<property>\n  <name>dfs.client.write.exclude.nodes.cache.expiry.interval.millis</name>\n  <value>1200000</value>\n  <description>The maximum period to keep a DN in the excluded nodes list\n  at a client. After this period, in milliseconds, the previously excluded node(s) will\n  be removed automatically from the cache and will be considered good for block allocations\n  again. Useful to lower or raise in situations where you keep a file open for very long\n  periods (such as a Write-Ahead-Log (WAL) file) to make the writer tolerant to cluster maintenance\n  restarts. Defaults to 10 minutes.</description>\n</property>\n\n<property>\n  <name>dfs.datanode.data.dir</name>\n  <value>/p1,/p2,/p3</value>\n  <description>Determines where on the local filesystem an DFS data node\n  should store its blocks.  If this is a comma-delimited\n  list of directories, then data will be stored in all named\n  directories, typically on different devices. The directories should be tagged\n  with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS\n  storage policies. The default storage type will be DISK if the directory does\n  not have a storage type tagged explicitly. Directories that do not exist will\n  be created if local filesystem permission allows.\n  </description>\n</property>\n\n<property>\n  <name>dfs.ha.tail-edits.period</name>\n  <value>60s</value>\n  <description>\n    How often, the StandbyNode and ObserverNode should check if there are new\n    edit log entries ready to be consumed. This is the minimum period between\n    checking; exponential backoff will be applied if no edits are found and\n    dfs.ha.tail-edits.period.backoff-max is configured. By default, no\n    backoff is applied.\n    Supports multiple time unit suffix (case insensitive), as described\n    in dfs.heartbeat.interval.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.top.enabled</name>\n  <value>true</value>\n  <description>Enable nntop: reporting top users on namenode\n  </description>\n</property>\n\n<property>\n  <name>dfs.storage.policy.satisfier.keytab.file</name>\n  <value>/valid/file2</value>\n  <description>\n    The keytab file used by external StoragePolicySatisfier to login as its\n    service principal. The principal name is configured with\n    dfs.storage.policy.satisfier.kerberos.principal. Keytab based login\n    is required when dfs.storage.policy.satisfier.mode is external.\n  </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": false}}, {"input": "unix_socket_directories=dev/urandom///\n\nexit_on_error=off\n\narray_nulls=on\n\nvacuum_multixact_freeze_min_age=5000000\n\nsuperuser_reserved_connections=1\n\narchive_mode=off\n\nmax_wal_size=2GB\n\nlog_recovery_conflict_waits=off\n\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "postgresql", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n  <property>\n    <name>hbase.regionserver.keytab.file</name>\n    <value>/path/to/valid/       </value>\n    <description>Full path to the kerberos keytab file to use for logging in\n    the configured HRegionServer server principal.</description>\n  </property>\n\n<property>\n  <name>hbase.master.fileSplitTimeout</name>\n  <value>1200000</value>\n    <description>Splitting a region, how long to wait on the file-splitting\n      step before aborting the attempt. Default: 600000. This setting used\n      to be known as hbase.regionserver.fileSplitTimeout in hbase-1.x.\n      Split is now run master-side hence the rename (If a\n      'hbase.master.fileSplitTimeout' setting found, will use it to\n      prime the current 'hbase.master.fileSplitTimeout'\n      Configuration.</description>\n</property>\n\n<property>\n  <name>hbase.regionserver.logroll.period</name>\n  <value>1800000</value>\n    <description>Period at which we will roll the commit log regardless\n    of how many edits it has.</description>\n</property>\n\n<property>\n  <name>hbase.client.max.perregion.tasks</name>\n  <value>2</value>\n    <description>The maximum number of concurrent mutation tasks the client will\n    maintain to a single Region. That is, if there is already\n    hbase.client.max.perregion.tasks writes in progress for this region, new puts\n    won't be sent to this region until some writes finishes.</description>\n</property>\n\n<property>\n  <name>hfile.block.cache.size</name>\n  <value>0.8</value>\n    <description>Percentage of maximum heap (-Xmx setting) to allocate to block cache\n        used by a StoreFile. Default of 0.4 means allocate 40%.\n        Set to 0 to disable but it's not recommended; you need at least\n        enough cache to hold the storefile indices.</description>\n</property>\n\n<property>\n  <name>hbase.ipc.client.fallback-to-simple-auth-allowed</name>\n  <value>true</value>\n    <description>When a client is configured to attempt a secure connection, but attempts to\n      connect to an insecure server, that server may instruct the client to\n      switch to SASL SIMPLE (unsecure) authentication. This setting controls\n      whether or not the client will accept this instruction from the server.\n      When false (the default), the client will not allow the fallback to SIMPLE\n      authentication, and will abort the connection.</description>\n</property>\n\n<property>\n  <name>hbase.defaults.for.version.skip</name>\n  <value>false</value>\n    <description>Set to true to skip the 'hbase.defaults.for.version' check.\n    Setting this to true can be useful in contexts other than\n    the other side of a maven generation; i.e. running in an\n    IDE.  You'll want to set this boolean to true to avoid\n    seeing the RuntimeException complaint: \"hbase-default.xml file\n    seems to be for and old version of HBase (\\${hbase.version}), this\n    version is X.X.X-SNAPSHOT\"</description>\n</property>\n\n<property>\n  <name>hbase.master.loadbalance.bytable</name>\n  <value>false</value>\n    <description>Factor Table name when the balancer runs.\n      Default: false.\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "hbase", "is_synthetic": false}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n\n<property>\n  <name>dfs.namenode.redundancy.considerLoad</name>\n  <value>false</value>\n  <description>\n    Decide if chooseTarget considers the target's load or not when write.\n    Turn on by default.\n  </description>\n</property>\n\n<property>\n  <name>dfs.namenode.redundancy.considerLoad.factor</name>\n  <value>4.0</value>\n    <description>The factor by which a node's load can exceed the average\n      before being rejected for writes, only if considerLoad is true.\n    </description>\n</property>\n\n<property>\n  <name>dfs.namenode.service.handler.count</name>\n  <value>1</value>\n  <description>The number of Namenode RPC server threads that listen to\n  requests from DataNodes and from all other non-client nodes.\n  dfs.namenode.service.handler.count will be valid only if\n  dfs.namenode.servicerpc-address is configured.\n  </description>\n</property>\n\n<property>\n  <name>dfs.datanode.block.id.layout.upgrade.threads</name>\n  <value>24</value>\n  <description>The number of threads to use when creating hard links from\n    current to previous blocks during upgrade of a DataNode to block ID-based\n    block layout (see HDFS-6482 for details on the layout).</description>\n</property>\n\n<property>\n  <name>dfs.namenode.edekcacheloader.interval.ms</name>\n  <value>500</value>\n  <description>When KeyProvider is configured, the interval time of warming\n    up edek cache on NN starts up / becomes active. All edeks will be loaded\n    from KMS into provider cache. The edek cache loader will try to warm up the\n    cache until succeed or NN leaves active state.\n  </description>\n</property>\n\n<property>\n  <name>dfs.checksum.combine.mode</name>\n  <value>MD5MD5CRC</value>\n  <description>\n    Defines how lower-level chunk/block checksums are combined into file-level\n    checksums; the original MD5MD5CRC mode is not comparable between files\n    with different block layouts, while modes like COMPOSITE_CRC are\n    comparable independently of block layout.\n  </description>\n</property>\n\n<property>\n  <name>dfs.storage.policy.satisfier.datanode.cache.refresh.interval.ms</name>\n  <value>150000</value>\n  <description>\n    How often to refresh the datanode storages cache in milliseconds. This cache\n    keeps live datanode storage reports fetched from namenode. After elapsed time,\n    it will again fetch latest datanodes from namenode.\n    By default, this parameter is set to 5 minutes.\n  </description>\n</property>\n\n<property>\n  <name>dfs.provided.aliasmap.inmemory.leveldb.dir</name>\n  <value>/tmp</value>\n    <description>\n      The directory where the leveldb files will be kept\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "hdfs", "is_synthetic": true}}, {"input": "<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n\n<configuration>\n    \n<property>\n  <name>yarn.scheduler.maximum-allocation-mb</name>\n  <value>4096</value>\n    <description>The maximum allocation for every container request at the RM\n    in MBs. Memory requests higher than this will throw an\n    InvalidResourceRequestException.</description>\n</property>\n\n<property>\n  <name>yarn.resourcemanager.zk-retry-interval-ms</name>\n  <value>1000</value>\n    <description>Retry interval in milliseconds when connecting to ZooKeeper.\n      When HA is enabled, the value here is NOT used. It is generated\n      automatically from yarn.resourcemanager.zk-timeout-ms and\n      yarn.resourcemanager.zk-num-retries.\n    </description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.linux-container-executor.cgroups.delete-delay-ms</name>\n  <value>40</value>\n    <description>Delay in ms between attempts to remove linux cgroup</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.runtime.linux.docker.default-container-network</name>\n  <value>host</value>\n    <description>The network used when launching containers using the\n      DockerContainerRuntime when no network is specified in the request\n      . This network must be one of the (configurable) set of allowed container\n      networks.</description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.recovery.enabled</name>\n  <value>false</value>\n    <description>Enable the node manager to recover after starting</description>\n</property>\n\n<property>\n  <name>yarn.resourcemanager.node-labels.provider.fetch-interval-ms</name>\n  <value>3600000</value>\n    <description>\n    When \"yarn.node-labels.configuration-type\" is configured with\n    \"delegated-centralized\", then periodically node labels are retrieved\n    from the node labels provider. This configuration is to define the\n    interval. If -1 is configured then node labels are retrieved from\n    provider only once for each node after it registers. Defaults to 30 mins.\n    </description>\n</property>\n\n<property>\n  <name>yarn.nodemanager.log-aggregation.num-log-files-per-app</name>\n  <value>60</value>\n    <description>Define how many aggregated log files per application per NM\n    we can have in remote file system. By default, the total number of\n    aggregated log files per application per NM is 30.\n    </description>\n</property>\n\n<property>\n  <name>yarn.node-attribute.fs-store.impl.class</name>\n  <value>org.apache.hadoop.yarn.server.resourcemanager.nodelabels.FileSystemNodeAttributeStore</value>\n    <description>\n      Choose different implementation of node attribute's storage\n    </description>\n</property>\n\n</configuration>\n", "expected_output": "invalid", "reason": "", "expected_retrieval": [], "meta": {"category": "yarn", "is_synthetic": true}}, {"input": "alluxio.master.shell.backup.state.lock.try.duration=1s\n\nalluxio.worker.management.tier.align.range=200\n\nalluxio.user.client.cache.local.store.file.buckets=2000\n\nalluxio.master.metastore.inode.enumerator.buffer.count=10000\n\nalluxio.job.master.worker.timeout=1sec\n\nalluxio.user.network.streaming.keepalive.timeout=30sec\n\nalluxio.user.file.create.ttl=0\n\nalluxio.underfs.gcs.default.mode=350\n\n", "expected_output": "valid", "reason": "", "expected_retrieval": [], "meta": {"category": "alluxio", "is_synthetic": true}}]}