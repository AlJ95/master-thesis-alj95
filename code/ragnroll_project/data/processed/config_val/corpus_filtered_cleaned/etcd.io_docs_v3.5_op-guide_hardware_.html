<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Hardware recommendations | etcd</title>
<meta name=description content="Hardware guidelines for administering etcd clusters"><meta property="og:url" content="https://etcd.io/docs/v3.5/op-guide/hardware/"><meta property="og:site_name" content="etcd"><meta property="og:title" content="Hardware recommendations"><meta property="og:description" content="Hardware guidelines for administering etcd clusters"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2024-12-05T01:25:33+00:00"><meta itemprop=name content="Hardware recommendations"><meta itemprop=description content="Hardware guidelines for administering etcd clusters"><meta itemprop=dateModified content="2024-12-05T01:25:33+00:00"><meta itemprop=wordCount content="942"><meta name=twitter:card content="summary"><meta name=twitter:title content="Hardware recommendations"><meta name=twitter:description content="Hardware guidelines for administering etcd clusters"></head><body class=td-page><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><h1>Hardware recommendations</h1><div class=lead>Hardware guidelines for administering etcd clusters</div><p>etcd usually runs well with limited resources for development or testing purposes; it’s common to develop with etcd on a laptop or a cheap cloud machine. However, when running etcd clusters in production, some hardware guidelines are useful for proper administration. These suggestions are not hard rules; they serve as a good starting point for a robust production deployment. As always, deployments should be tested with simulated workloads before running in production.</p><h2 id=cpus>CPUs</h2><p>Few etcd deployments require a lot of CPU capacity. Typical clusters need two to four cores to run smoothly.
Heavily loaded etcd deployments, serving thousands of clients or tens of thousands of requests per second, tend to be CPU bound since etcd can serve requests from memory. Such heavy deployments usually need eight to sixteen dedicated cores.</p><h2 id=memory>Memory</h2><p>etcd has a relatively small memory footprint but its performance still depends on having enough memory. An etcd server will aggressively cache key-value data and spends most of the rest of its memory tracking watchers. Typically 8GB is enough. For heavy deployments with thousands of watchers and millions of keys, allocate 16GB to 64GB memory accordingly.</p><h2 id=disks>Disks</h2><p>Fast disks are the most critical factor for etcd deployment performance and stability.</p><p>A slow disk will increase etcd request latency and potentially hurt cluster stability. Since etcd’s consensus protocol depends on persistently storing metadata to a log, a majority of etcd cluster members must write every request down to disk. Additionally, etcd will also incrementally checkpoint its state to disk so it can truncate this log. If these writes take too long, heartbeats may time out and trigger an election, undermining the stability of the cluster. In general, to tell whether a disk is fast enough for etcd, a benchmarking tool such as <a href=https://github.com/axboe/fio target=_blank rel=noopener>fio</a> can be used. Read <a href=https://web.archive.org/web/20240726111518/https://prog.world/is-storage-speed-suitable-for-etcd-ask-fio/ target=_blank rel=noopener>here</a> for an example.</p><p>etcd is very sensitive to disk write latency. Typically 50 sequential IOPS (e.g., a 7200 RPM disk) is required. For heavily loaded clusters, 500 sequential IOPS (e.g., a typical local SSD or a high performance virtualized block device) is recommended. Note that most cloud providers publish concurrent IOPS rather than sequential IOPS; the published concurrent IOPS can be 10x greater than the sequential IOPS. To measure actual sequential IOPS, we suggest using a disk benchmarking tool such as <a href=https://github.com/ongardie/diskbenchmark target=_blank rel=noopener>diskbench</a> or <a href=https://github.com/axboe/fio target=_blank rel=noopener>fio</a>.</p><p>etcd requires only modest disk bandwidth but more disk bandwidth buys faster recovery times when a failed member has to catch up with the cluster. Typically 10MB/s will recover 100MB data within 15 seconds. For large clusters, 100MB/s or higher is suggested for recovering 1GB data within 15 seconds.</p><p>When possible, back etcd’s storage with a SSD. A SSD usually provides lower write latencies and with less variance than a spinning disk, thus improving the stability and reliability of etcd. If using spinning disk, get the fastest disks possible (15,000 RPM). Using RAID 0 is also an effective way to increase disk speed, for both spinning disks and SSD. With at least three cluster members, mirroring and/or parity variants of RAID are unnecessary; etcd&rsquo;s consistent replication already gets high availability.</p><h2 id=network>Network</h2><p>Multi-member etcd deployments benefit from a fast and reliable network. In order for etcd to be both consistent and partition tolerant, an unreliable network with partitioning outages will lead to poor availability. Low latency ensures etcd members can communicate fast. High bandwidth can reduce the time to recover a failed etcd member. 1GbE is sufficient for common etcd deployments. For large etcd clusters, a 10GbE network will reduce mean time to recovery.</p><p>Deploy etcd members within a single data center when possible to avoid latency overheads and lessen the possibility of partitioning events. If a failure domain in another data center is required, choose a data center closer to the existing one. Please also read the <a href=../../tuning/>tuning</a> documentation for more information on cross data center deployment.</p><h2 id=example-hardware-configurations>Example hardware configurations</h2><p>Here are a few example hardware setups on AWS and GCE environments. As mentioned before, but must be stressed regardless, administrators should test an etcd deployment with a simulated workload before putting it into production.</p><p>Note that these configurations assume these machines are totally dedicated to etcd. Running other applications along with etcd on these machines may cause resource contentions and lead to cluster instability.</p><h3 id=small-cluster>Small cluster</h3><p>A small cluster serves fewer than 100 clients, fewer than 200 of requests per second, and stores no more than 100MB of data.</p><p>Example application workload: A 50-node Kubernetes cluster</p><table><thead><tr><th>Provider</th><th>Type</th><th>vCPUs</th><th>Memory (GB)</th><th>Max concurrent IOPS</th><th>Disk bandwidth (MB/s)</th></tr></thead><tbody><tr><td>AWS</td><td>m4.large</td><td>2</td><td>8</td><td>3600</td><td>56.25</td></tr><tr><td>GCE</td><td>n1-standard-2 + 50GB PD SSD</td><td>2</td><td>7.5</td><td>1500</td><td>25</td></tr></tbody></table><h3 id=medium-cluster>Medium cluster</h3><p>A medium cluster serves fewer than 500 clients, fewer than 1,000 of requests per second, and stores no more than 500MB of data.</p><p>Example application workload: A 250-node Kubernetes cluster</p><table><thead><tr><th>Provider</th><th>Type</th><th>vCPUs</th><th>Memory (GB)</th><th>Max concurrent IOPS</th><th>Disk bandwidth (MB/s)</th></tr></thead><tbody><tr><td>AWS</td><td>m4.xlarge</td><td>4</td><td>16</td><td>6000</td><td>93.75</td></tr><tr><td>GCE</td><td>n1-standard-4 + 150GB PD SSD</td><td>4</td><td>15</td><td>4500</td><td>75</td></tr></tbody></table><h3 id=large-cluster>Large cluster</h3><p>A large cluster serves fewer than 1,500 clients, fewer than 10,000 of requests per second, and stores no more than 1GB of data.</p><p>Example application workload: A 1,000-node Kubernetes cluster</p><table><thead><tr><th>Provider</th><th>Type</th><th>vCPUs</th><th>Memory (GB)</th><th>Max concurrent IOPS</th><th>Disk bandwidth (MB/s)</th></tr></thead><tbody><tr><td>AWS</td><td>m4.2xlarge</td><td>8</td><td>32</td><td>8000</td><td>125</td></tr><tr><td>GCE</td><td>n1-standard-8 + 250GB PD SSD</td><td>8</td><td>30</td><td>7500</td><td>125</td></tr></tbody></table><h3 id=xlarge-cluster>xLarge cluster</h3><p>An xLarge cluster serves more than 1,500 clients, more than 10,000 of requests per second, and stores more than 1GB data.</p><p>Example application workload: A 3,000 node Kubernetes cluster</p><table><thead><tr><th>Provider</th><th>Type</th><th>vCPUs</th><th>Memory (GB)</th><th>Max concurrent IOPS</th><th>Disk bandwidth (MB/s)</th></tr></thead><tbody><tr><td>AWS</td><td>m4.4xlarge</td><td>16</td><td>64</td><td>16,000</td><td>250</td></tr><tr><td>GCE</td><td>n1-standard-16 + 500GB PD SSD</td><td>16</td><td>60</td><td>15,000</td><td>250</td></tr></tbody></table><div class=d-print-none><h2 class=feedback--title>Feedback</h2><p class=feedback--question>Was this page helpful?</p><button class="btn btn-primary mb-4 feedback--answer feedback--answer-yes">Yes</button>
<button class="btn btn-primary mb-4 feedback--answer feedback--answer-no">No</button><p class="feedback--response feedback--response-yes">Glad to hear it! Please <a href=https://github.com/etcd-io/website/issues/new>tell us how we can improve</a>.</p><p class="feedback--response feedback--response-no">Sorry to hear that. Please <a href=https://github.com/etcd-io/website/issues/new>tell us how we can improve</a>.</p></div><br><div class=td-page-meta__lastmod>Last modified December 5, 2024: <a href=https://github.com/etcd-io/website/commit/d12fe2fd2af9e33e4d921bcfe28c76829911ee88>fix fio blogpost link (d12fe2f)</a></div></div></main></div></div></div></body></html>