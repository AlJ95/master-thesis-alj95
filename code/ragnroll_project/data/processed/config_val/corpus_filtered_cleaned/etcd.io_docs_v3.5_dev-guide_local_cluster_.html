<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Set up a local cluster | etcd</title>
<meta name=description content="Configuring local clusters for testing and development"><meta property="og:url" content="https://etcd.io/docs/v3.5/dev-guide/local_cluster/"><meta property="og:site_name" content="etcd"><meta property="og:title" content="Set up a local cluster"><meta property="og:description" content="Configuring local clusters for testing and development"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2023-04-18T08:35:41+12:00"><meta itemprop=name content="Set up a local cluster"><meta itemprop=description content="Configuring local clusters for testing and development"><meta itemprop=dateModified content="2023-04-18T08:35:41+12:00"><meta itemprop=wordCount content="509"><meta name=twitter:card content="summary"><meta name=twitter:title content="Set up a local cluster"><meta name=twitter:description content="Configuring local clusters for testing and development"></head><body class=td-page><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><h1>Set up a local cluster</h1><div class=lead>Configuring local clusters for testing and development</div><p>For testing and development deployments, the quickest and easiest way is to configure a local cluster. For a production deployment, refer to the <a href=../../op-guide/clustering/>clustering</a> section.</p><h2 id=local-standalone-cluster>Local standalone cluster</h2><h3 id=starting-a-cluster>Starting a cluster</h3><p>Run the following to deploy an etcd cluster as a standalone cluster:</p><pre tabindex=0><code>$ ./etcd
...
</code></pre><p>If the <code>etcd</code> binary is not present in the current working directory, it might be located either at <code>$GOPATH/bin/etcd</code> or at <code>/usr/local/bin/etcd</code>. Run the command appropriately.</p><p>The running etcd member listens on <code>localhost:2379</code> for client requests.</p><h3 id=interacting-with-the-cluster>Interacting with the cluster</h3><p>Use <code>etcdctl</code> to interact with the running cluster:</p><ol><li><p>Store an example key-value pair in the cluster:</p><pre tabindex=0><code>  $ ./etcdctl put foo bar
  OK
</code></pre><p>If OK is printed, storing key-value pair is successful.</p></li><li><p>Retrieve the value of <code>foo</code>:</p><pre tabindex=0><code>$ ./etcdctl get foo
bar
</code></pre><p>If <code>bar</code> is returned, interaction with the etcd cluster is working as expected.</p></li></ol><h2 id=local-multi-member-cluster>Local multi-member cluster</h2><h3 id=starting-a-cluster-1>Starting a cluster</h3><p>A <code>Procfile</code> at the base of the etcd git repository is provided to easily configure a local multi-member cluster. To start a multi-member cluster, navigate to the root of the etcd source tree and perform the following:</p><ol><li><p>Install <code>goreman</code> to control Procfile-based applications:</p><pre tabindex=0><code>$ go install github.com/mattn/goreman@latest
</code></pre></li><li><p>Start a cluster with <code>goreman</code> using etcd&rsquo;s stock Procfile:</p><pre tabindex=0><code>$ goreman -f Procfile start
</code></pre><p>The members start running. They listen on <code>localhost:2379</code>, <code>localhost:22379</code>, and <code>localhost:32379</code> respectively for client requests.</p></li></ol><h3 id=interacting-with-the-cluster-1>Interacting with the cluster</h3><p>Use <code>etcdctl</code> to interact with the running cluster:</p><ol><li><p>Print the list of members:</p><pre tabindex=0><code>$ etcdctl --write-out=table --endpoints=localhost:2379 member list
</code></pre><p>The list of etcd members are displayed as follows:</p><pre tabindex=0><code>+------------------+---------+--------+------------------------+------------------------+
|        ID        | STATUS  |  NAME  |       PEER ADDRS       |      CLIENT ADDRS      |
+------------------+---------+--------+------------------------+------------------------+
| 8211f1d0f64f3269 | started | infra1 | http://127.0.0.1:2380  | http://127.0.0.1:2379  |
| 91bc3c398fb3c146 | started | infra2 | http://127.0.0.1:22380 | http://127.0.0.1:22379 |
| fd422379fda50e48 | started | infra3 | http://127.0.0.1:32380 | http://127.0.0.1:32379 |
+------------------+---------+--------+------------------------+------------------------+
</code></pre></li><li><p>Store an example key-value pair in the cluster:</p><pre tabindex=0><code>$ etcdctl put foo bar
OK
</code></pre><p>If OK is printed, storing key-value pair is successful.</p></li></ol><h3 id=testing-fault-tolerance>Testing fault tolerance</h3><p>To exercise etcd&rsquo;s fault tolerance, kill a member and attempt to retrieve the key.</p><ol><li><p>Identify the process name of the member to be stopped.</p><p>The <code>Procfile</code> lists the properties of the multi-member cluster. For example, consider the member with the process name, <code>etcd2</code>.</p></li><li><p>Stop the member:</p><pre tabindex=0><code># kill etcd2
$ goreman run stop etcd2
</code></pre></li><li><p>Store a key:</p><pre tabindex=0><code>$ etcdctl put key hello
OK
</code></pre></li><li><p>Retrieve the key that is stored in the previous step:</p><pre tabindex=0><code>$ etcdctl get key
hello
</code></pre></li><li><p>Retrieve a key from the stopped member:</p><pre tabindex=0><code>$ etcdctl --endpoints=localhost:22379 get key
</code></pre><p>The command should display an error caused by connection failure:</p><pre tabindex=0><code>2017/06/18 23:07:35 grpc: Conn.resetTransport failed to create client transport: connection error: desc = &#34;transport: dial tcp 127.0.0.1:22379: getsockopt: connection refused&#34;; Reconnecting to &#34;localhost:22379&#34;
Error:  grpc: timed out trying to connect
</code></pre></li><li><p>Restart the stopped member:</p><pre tabindex=0><code>$ goreman run restart etcd2
</code></pre></li><li><p>Get the key from the restarted member:</p><pre tabindex=0><code>$ etcdctl --endpoints=localhost:22379 get key
hello
</code></pre><p>Restarting the member re-establish the connection. <code>etcdctl</code> will now be able to retrieve the key successfully. To learn more about interacting with etcd, read <a href=../interacting_v3/>interacting with etcd section</a>.</p></li></ol><div class=d-print-none><h2 class=feedback--title>Feedback</h2><p class=feedback--question>Was this page helpful?</p><button class="btn btn-primary mb-4 feedback--answer feedback--answer-yes">Yes</button>
<button class="btn btn-primary mb-4 feedback--answer feedback--answer-no">No</button><p class="feedback--response feedback--response-yes">Glad to hear it! Please <a href=https://github.com/etcd-io/website/issues/new>tell us how we can improve</a>.</p><p class="feedback--response feedback--response-no">Sorry to hear that. Please <a href=https://github.com/etcd-io/website/issues/new>tell us how we can improve</a>.</p></div><br><div class=td-page-meta__lastmod>Last modified April 18, 2023: <a href=https://github.com/etcd-io/website/commit/964f7abe746ede8f5e44833b72aba55bbbabac66>Fix instructions for installing goreman. (964f7ab)</a></div></div></main></div></div></div></body></html>