<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Disaster recovery | etcd</title>
<meta name=description content="etcd v3 snapshot & restore facilities"><meta property="og:url" content="https://etcd.io/docs/v3.5/op-guide/recovery/"><meta property="og:site_name" content="etcd"><meta property="og:title" content="Disaster recovery"><meta property="og:description" content="etcd v3 snapshot & restore facilities"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="docs"><meta property="article:modified_time" content="2024-04-11T12:06:51+09:00"><meta itemprop=name content="Disaster recovery"><meta itemprop=description content="etcd v3 snapshot & restore facilities"><meta itemprop=dateModified content="2024-04-11T12:06:51+09:00"><meta itemprop=wordCount content="1013"><meta name=twitter:card content="summary"><meta name=twitter:title content="Disaster recovery"><meta name=twitter:description content="etcd v3 snapshot & restore facilities"></head><body class=td-page><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><h1>Disaster recovery</h1><div class=lead>etcd v3 snapshot & restore facilities</div><p>etcd is designed to withstand machine failures. An etcd cluster automatically recovers from temporary failures (e.g., machine reboots) and tolerates up to <em>(N-1)/2</em> permanent failures for a cluster of N members. When a member permanently fails, whether due to hardware failure or disk corruption, it loses access to the cluster. If the cluster permanently loses more than <em>(N-1)/2</em> members then it disastrously fails, irrevocably losing quorum. Once quorum is lost, the cluster cannot reach consensus and therefore cannot continue accepting updates.</p><p>To recover from disastrous failure, etcd v3 provides snapshot and restore facilities to recreate the cluster without v3 key data loss. To recover v2 keys, refer to the <a href=/docs/v2.3/admin_guide#disaster-recovery>v2 admin guide</a>.</p><h2 id=snapshotting-the-keyspace>Snapshotting the keyspace</h2><p>Recovering a cluster first needs a snapshot of the keyspace from an etcd member. A snapshot may either be taken from a live member with the <code>etcdctl snapshot save</code> command or by copying the <code>member/snap/db</code> file from an etcd data directory. For example, the following command snapshots the keyspace served by <code>$ENDPOINT</code> to the file <code>snapshot.db</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ <span style=color:#000>ETCDCTL_API</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>3</span> etcdctl --endpoints <span style=color:#000>$ENDPOINT</span> snapshot save snapshot.db
</span></span></code></pre></div><p>Note that taking the snapshot from the <code>member/snap/db</code> file might lose data that has not been written yet, but is included in the wal (write-ahead-log) folder.</p><h2 id=status-of-a-snapshot>Status of a snapshot</h2><p>To understand which revision and hash a given snapshot contains, you can use the <code>etcdutl snapshot status</code> command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ etcdutl snapshot status snapshot.db -w table
</span></span><span style=display:flex><span>+---------+----------+------------+------------+
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>|</span>  HASH   <span style=color:#000;font-weight:700>|</span> REVISION <span style=color:#000;font-weight:700>|</span> TOTAL KEYS <span style=color:#000;font-weight:700>|</span> TOTAL SIZE <span style=color:#000;font-weight:700>|</span>
</span></span><span style=display:flex><span>+---------+----------+------------+------------+
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>|</span> 7ef846e <span style=color:#000;font-weight:700>|</span>   <span style=color:#0000cf;font-weight:700>485261</span> <span style=color:#000;font-weight:700>|</span>      <span style=color:#0000cf;font-weight:700>11642</span> <span style=color:#000;font-weight:700>|</span>      <span style=color:#0000cf;font-weight:700>94</span> MB <span style=color:#000;font-weight:700>|</span>
</span></span><span style=display:flex><span>+---------+----------+------------+------------+
</span></span></code></pre></div><h2 id=restoring-a-cluster>Restoring a cluster</h2><h3 id=revision-difference>Revision Difference</h3><p>When you are restoring a cluster, existing clients may perceive the revision going back by many hundreds or thousands. This is due to the fact that a given snapshot only contains the data lineage up until the point of when it was taken, whereas the current state might already be further ahead.</p><p>This is particularly a problem when running Kubernetes using etcd, where controllers and operators may use so called <code>informers</code> which act as local caches and get notified on updates using watches. Restoring to an older revision may not correctly refresh the caches, causing unpredictable and inconsistent behavior in the controllers.</p><p>When restoring from a snapshot in the context of either: known consumers of the watch API, local cached copies of etcd data or when using Kubernetes in general - it is highly recommended to restore using &ldquo;revision bumps&rdquo; below.</p><h3 id=restoring-from-snapshot>Restoring from snapshot</h3><p>To restore a cluster, all that is needed is a single snapshot &ldquo;db&rdquo; file. A cluster restore with <code>etcdutl snapshot restore</code> creates new etcd data directories; all members should restore using the same snapshot. Restoring overwrites some snapshot metadata (specifically, the member ID and cluster ID); the member loses its former identity. This metadata overwrite prevents the new member from inadvertently joining an existing cluster. Therefore in order to start a cluster from a snapshot, the restore must start a new logical cluster.</p><p>A simple restore can be excuted like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ etcdutl snapshot restore snapshot.db --data-dir output-dir
</span></span></code></pre></div><h3 id=integrity-checks>Integrity Checks</h3><p>Snapshot integrity may be optionally verified at restore time. If the snapshot is taken with <code>etcdctl snapshot save</code>, it will have an integrity hash that is checked by <code>etcdutl snapshot restore</code>. If the snapshot is copied from the data directory, there is no integrity hash and it will only restore by using <code>--skip-hash-check</code>.</p><h3 id=restoring-with-revision-bump>Restoring with revision bump</h3><p>In order to ensure the revisions are never decreasing after a restore, you can supply the <code>--bump-revision</code> option. This option takes a 64 bit integer, which denotes how many revisions to add to the current revision of the snapshot. Since each write to etcd increases the revision by one, you may cover a week old snapshot with bumping by 1'000'000'000 assuming that etcd runs with less than 1500 writes per second.</p><p>In the context of Kubernetes controllers, it is important to also mark all the revisions, including the bump, as compacted using <code>--mark-compacted</code>. This ensures that all watches are terminated and etcd does not respond to requests about revisions that happened after taking the snapshot - effectively invalidating its informer caches.</p><p>A full invocation may look like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ etcdutl snapshot restore snapshot.db --bump-revision <span style=color:#0000cf;font-weight:700>1000000000</span> --mark-compacted --data-dir output-dir
</span></span></code></pre></div><h3 id=restoring-with-updated-membership>Restoring with updated membership</h3><p>The members of an etcd cluster are stored in etcd itself and maintained through the raft consensus algorithm. When quorum is lost entirely, you may want to reconsider where and how the new cluster is formed, for example, on an entirely new set of members.</p><p>When restoring from a snapshot, you can directly supply the new membership into the datastore as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ etcdutl snapshot restore snapshot.db <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --name m1 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --initial-cluster <span style=color:#000>m1</span><span style=color:#ce5c00;font-weight:700>=</span>http://host1:2380,m2<span style=color:#ce5c00;font-weight:700>=</span>http://host2:2380,m3<span style=color:#ce5c00;font-weight:700>=</span>http://host3:2380 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --initial-cluster-token etcd-cluster-1 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --initial-advertise-peer-urls http://host1:2380
</span></span></code></pre></div><p>This ensures that the newly constructed cluster only connects to the other restored members with the given token and not older members that might still be alive and try to connect.</p><p>Alternatively, when starting up etcd, you can supply <code>--force-new-cluster</code> to overwrite cluster membership while keeping existing application data. Note that this is strongly discouraged because it will panic if other members from previous cluster are still alive. Make sure to save snapshots periodically.</p><h3 id=end-2-end-example>End-2-End Example</h3><p>Grab a snapshot from a live cluster using:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ etcdctl snapshot save snapshot.db
</span></span></code></pre></div><p>Continuing from the previous example, the following creates new etcd data directories (<code>m1.etcd</code>, <code>m2.etcd</code>, <code>m3.etcd</code>) for a three member cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ etcdutl snapshot restore snapshot.db <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --name m1 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --initial-cluster <span style=color:#000>m1</span><span style=color:#ce5c00;font-weight:700>=</span>http://host1:2380,m2<span style=color:#ce5c00;font-weight:700>=</span>http://host2:2380,m3<span style=color:#ce5c00;font-weight:700>=</span>http://host3:2380 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --initial-cluster-token etcd-cluster-1 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --initial-advertise-peer-urls http://host1:2380
</span></span><span style=display:flex><span>$ etcdutl snapshot restore snapshot.db <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --name m2 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --initial-cluster <span style=color:#000>m1</span><span style=color:#ce5c00;font-weight:700>=</span>http://host1:2380,m2<span style=color:#ce5c00;font-weight:700>=</span>http://host2:2380,m3<span style=color:#ce5c00;font-weight:700>=</span>http://host3:2380 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --initial-cluster-token etcd-cluster-1 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --initial-advertise-peer-urls http://host2:2380
</span></span><span style=display:flex><span>$ etcdutl snapshot restore snapshot.db <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --name m3 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --initial-cluster <span style=color:#000>m1</span><span style=color:#ce5c00;font-weight:700>=</span>http://host1:2380,m2<span style=color:#ce5c00;font-weight:700>=</span>http://host2:2380,m3<span style=color:#ce5c00;font-weight:700>=</span>http://host3:2380 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --initial-cluster-token etcd-cluster-1 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --initial-advertise-peer-urls http://host3:2380
</span></span></code></pre></div><p>Next, start <code>etcd</code> with the new data directories:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ etcd <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --name m1 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --listen-client-urls http://host1:2379 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --advertise-client-urls http://host1:2379 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --listen-peer-urls http://host1:2380 <span style=color:#000;font-weight:700>&amp;</span>
</span></span><span style=display:flex><span>$ etcd <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --name m2 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --listen-client-urls http://host2:2379 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --advertise-client-urls http://host2:2379 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --listen-peer-urls http://host2:2380 <span style=color:#000;font-weight:700>&amp;</span>
</span></span><span style=display:flex><span>$ etcd <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --name m3 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --listen-client-urls http://host3:2379 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --advertise-client-urls http://host3:2379 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>  --listen-peer-urls http://host3:2380 <span style=color:#000;font-weight:700>&amp;</span>
</span></span></code></pre></div><p>Now the restored etcd cluster should be available and serving the keyspace from the snapshot.</p><div class=d-print-none><h2 class=feedback--title>Feedback</h2><p class=feedback--question>Was this page helpful?</p><button class="btn btn-primary mb-4 feedback--answer feedback--answer-yes">Yes</button>
<button class="btn btn-primary mb-4 feedback--answer feedback--answer-no">No</button><p class="feedback--response feedback--response-yes">Glad to hear it! Please <a href=https://github.com/etcd-io/website/issues/new>tell us how we can improve</a>.</p><p class="feedback--response feedback--response-no">Sorry to hear that. Please <a href=https://github.com/etcd-io/website/issues/new>tell us how we can improve</a>.</p></div><br><div class=td-page-meta__lastmod>Last modified April 11, 2024: <a href=https://github.com/etcd-io/website/commit/639b1316db7738ac747122bc325bfd3693225c45>Fix command line example status of snapshot section in op-guide/recovery (639b131)</a></div></div></main></div></div></div></body></html>