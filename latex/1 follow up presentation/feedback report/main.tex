

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  
\usepackage[hyperindex]{hyperref}
\usepackage{url}

\IEEEoverridecommandlockouts                              
\overrideIEEEmargins


\title{\LARGE \bf
Developing a Benchmarking Framework for RAG Systems \newline Feedback Bericht
}


\author{von Jan Albrecht}


\begin{document}



\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\section{Zusammenfassung der Präsentation}
Ich habe in der Präsentation das Framework für RAG-Systeme vorgestellt und bin auf die Besonderheiten eingegangen, die bei der Umsetzung relevant sind, insbesondere die modulare Architektur eines RAG-Systems und die verschiedenen Aspekte der Evaluation. 
Ersteres fokussiert sich darauf, dass möglichst viele RAG-Varianten und IRA-Methoden (Iterativ, Rekursiv, Adaptiv) mit dem Framework abgebildet werden können.
Die Evaluation kann in End-to-End-Evaluation und Komponenten-Evaluation unterschieden werden. 
Während End-to-End-Evaluation für den Endnutzer relevanter ist, kann Komponentenevaluation zur Fehleranalyse und Hyperparameter Tuning verwendet werden.\\


%%%%%%%%%%%%%%%%%%%%%%%% INTRODUCTION - DONE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Zusammenfassung der Feedbacks}
% Gehe hier auf alle Feedbacks im Allgemeinen ein und motiviere sie. 
% Gehe in den nachfolgenden Sektionen auf die einzelnen Feedbacks ein.
% - Mehr Fokus auf Implementierung der Re-Konfigurierbarkeit
%   - Mehr Metriken (System / HW / Andere? -> siehe AISE)
% - Weniger Fokus auf das eigentliche Benchmarken
Das Feedback war positiv und der Plan zur Umsetzung stimmig. 
Es wurde eingeworfen, den Fokus mehr auf das Framework zu richten.
Es kam die Befürchtung auf, dass mit zunehmender Arbeit für die Benchmarks, die eigentliche Umsetzung des Frameworks zu kurz kommt. Besonderer Fokus sollte auf die Rekonfigurierbarkeit gelegt werden.
Neben der potenziellen Evaluation durch typische Performance-Metriken wie Precision, Recall und F1-Score, sollten Hardware Metriken wie RAM, Inferenzzeit und CPU-Zeit verwendet werden, da diese in der Praxis eine wichtige Rolle spielen.
Zudem wurde empfohlen, das Paper der SWS-Abteilung "A Methodology for Evaluating RAG Systems: A Case Study On Configuration Dependency Validation" zu lesen und zu analysieren. Der entwickelte Blueprint soll dabei auf seine Anwendbarkeit in der praktischen Umsetzung überprüft werden.
Außerdem wurde darauf hingewiesen, dass SLURM für die Experimente eingesetzt werden kann.\\
%%%%%%%%%%%%%%%%% .CREATING A .TEX FILE - DONE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Angepasste Umsetzung}
Für die Anpassung der Umsetzung muss die Implementierung des Frameworks und das Evaluieren durch einen Software Engineering Task angepasst werden. \\

\subsection{Angepasste Umsetzung des Frameworks}
% Mehr Fokus auf Implementierung der Re-Konfigurierbarkeit
% Es müssen mehr Metriken zur Verfügung stehen -> Wie zum Beispiel die Inferenzzeit für eine Anfrage.
% -> Daraus resultiert allerdings auch, dass Prozesse parallelisiert gestartet werden müssen. 
%   -> Welche könenn denn parallel gestartet werden?
%   -> Wie müssen die Prozesse experimentell ablaufen, sodass keine anderen Prozesse diese beeinflussen (interne Validität)
%   -> Daraus folgen Hardware Herausforderungen, welche ich ohne Sebastian Simon nicht schaffen kann, da ich die Univerisitäts Infrastruktur nicht kenne
Das Framework, basierend auf dem von der SWS-Abteilung veröffentlichten Blueprint, sollte folgende Voraussetzungen erfüllen:\\[6pt]

\begin{itemize}
   \item Einfache Integration der benötigten Kontextressourcen
   \item Hohe Abdeckung möglicher RAG-Varianten
   \item Hohe Abdeckung möglicher Evaluatonsmetriken
   \item Eine LLM- oder Naive-RAG-Baseline
   \item Aufteilung der Daten in Validation \& Testdatensatz (Fehleranalyse/Hyperparameter-Tuning \& Bestimmung des Generalisierungsfehler)\\
\end{itemize}

Weiterhin wurden die Anforderungen gestellt, dass das Framework einfach zu rekonfigurieren sein soll, sodass die Evaluationen nach Fehleranalyse einfach zu replizieren sind. 
Replizierbarkeit und Interne Gültigkeit der Experimente sind Voraussetzungen für das Framework. 

Das Framework wird neben Task-spezifischen Metriken, Hardware-Metriken nutzen. Darunter zählen zum Beispiel Inferenzzeit, RAM und CPU-Zeit. 
Daher ist es wichtig, dass die Evaluierungen nicht parallel gestartet werden, sondern isoliert von anderen Prozessen und sequenziell auf einer dedizierten Hardware. \\

\subsubsection{Analyse bestehender Frameworks und Bibliotheken}

Für die Neuausrichtung des Fokus, werde ich noch sicherstellen, dass ein solches Framework noch nicht existiert.

\paragraph{AutoRAG \cite{AutoRAG}}
AutoRAG ist ein Tool für Hyperparameter-Tuning für RAG-Systeme. Der Nutzer erstellt eine Konfigurationsdatei, welche 
die sogenannten Nodes (Retriever, Generator, Reranker) definiert und die Hyperparameter (Top-K, Models, Prompts) angibt. 
Danach läuft die Analyse und es wird die Konfiguration des besten Systems angegeben. 

Das Tool nutzt nur End-to-End-Evaluation und ist schwierig anpassbar. Laut den Entwicklern im Discord sind fortgeschrittene Methoden (IRA) derzeit nicht möglich. 
Es bietet keine Möglichkeit ein Baseline-LLM zu evaluieren. 

\paragraph{Llama-Index \cite{Liu_LlamaIndex_2022} \& Langchain \cite{Chase_LangChain_2022}}
Llama-Index und Langchain sind umfangreiche Bibliotheken für die Entwicklung von RAG-Pipelines. 
Sie bieten jedoch keine zeiteffiziente Möglichkeit zum Benchmarken vieler RAG-Systeme.

\paragraph{RaLLe \cite{ralle}}
RaLLe ist eine Bibliothek zur zeiteffizienten Entwicklung von RAG-Systemen. Der letzte Commit ist über ein Jahr alt und die Bibliothek scheint sehr restriktv. Unter anderem ist es nur möglich .tsv-Dateien mit einer vorgegebenen Struktur zu nutzen. Fortgeschrittene Methoden sind nicht möglich.

\paragraph{FlashRAG \cite{FlashRAG}}

FlashRAG ist ein Python Toolkit, dass eine ähnliche Motivation hat, wie ich sie hier versuche umzusetzen. 
Es bietet eine Konfigurationsdatei als Start, es hat eine Auswahl von Task-spezifischen Metriken und eine Vielzahl an RAG-Komponenten. Es lassen sich viele RAG-Varianten und einige Spezialfälle damit umsetzen (IRA, Self-RAG \cite{Asai.10172023}, FLARE \cite{Jiang.5112023}, IRCoT \cite{Trivedi.12202022}).

\paragraph{RAGLAB\cite{zhang-etal-2024-raglab}}
RAGLAB ist das neueste Framework und bietet ähnliche Funktionalitäten wie dessen Vorgänger. Es führt "Fair Comparison" ein, einen faireren Vergleich für LLM's und Retriever zum Beispiel das Entfernen von Special Tokens im Training oder dem Seedsetzen in Retrieval und Generator Pipelines.


\paragraph{Haystack\cite{Pietsch_Haystack_the_end-to-end_2019} \& FastRAG \cite{Izsak_fastRAG_Efficient_Retrieval_2023}}
Haystack ist ähnlich wie Llama-Index und LangChain eine umfangreiche Bibliothek für die Entwicklung von RAG-Pipelines. Sie bietet Pipeline Definition mittels YAML oder JSON Dateien und ist stark modular aufgebaut. Es lassen sich IRA Methoden damit entwickeln. 
FastRAG ist kompatibel mit Haystack und hat eine Vielzahl an RAG-Komponenten entwickelt, mit denen es möglich ist Spezialfälle aus Papers zu entwickeln. 
Darunter zählen zum Beispiel REPLUG\cite{shi2023replugretrievalaugmentedblackboxlanguage}, PLAID \cite{santhanam2022plaidefficientenginelate}, ColBERT\cite{santhanam2022colbertv2effectiveefficientretrieval} oder quantized LLM's.\\

\subsubsection{Einordnung bestehender Frameworks und Bibliotheken}

Bei jeder Analyse wurden zwei Punkte betrachtet:\\
\begin{itemize}
   \item Bietet das untersuchte Frameworks bereits die Lösung für das in der Masterarbeit untersuchte Thema?
   \item Kann ich das Framework als Fundament für mein Framework nutzen?\\
\end{itemize}

Keines der Frameworks bietet einen vollständige Pipeline für das Entwickeln von RAG-Varianten durch iteratives Rekonfigurieren und systematischer replizierbarer Evaluierung mit Holdout-Testset. 
Eine LLM-Baseline zur Überprüfung, ob ein RAG-System eine Verbesserung darstellt, ist in keinem der Frameworks vorhanden.
Weiterhin misst keines der Frameworks den Einfluss der Komplexität auf Metriken wie Inferenzzeit, RAM-Usage oder CPU-Time. 
Damit ist der erste Punkt beantwortet. Für den zweiten Punkt können einige Frameworks herausgefiltert werden.
RaLLe und AutoRAG sind sehr restriktiv und es lassen sich nur eine begrenzte Anzahl von RAG-Varianten umsetzen. Llama-Index und LangChain sind ebenfalls restriktiv und nicht zeiteffizient genug, um viele verschiedene RAG-Varianten zu testen.
RAGLAB bietet keine Pipeline-Definition durch Konfigurationsdateien wie YAML oder JSON an. Dies könnte die Entwicklungsgeschwindigkeit stark reduzieren.

FlashRAG und Haystack+FastRAG wären mögliche Kandidaten als Basis für das geplante Framework. 
Ich werde Haystack und FastRAG nutzen, da Haystack stark modular aufgebaut ist und in zukunft eine UI bieten wird um RAG-Systeme per Drag\&Drop zu erstellen. 
Die daraus resultierenden RAG-Varianten können in YAML's exportiert werden und für mein Framework benutzt werden.\\

Zusammenfassend wird sich die Umsetzung des Frameworks auf die Erweiterung von Haystack und FastRAG beschränken, sodass ein Benchmarking für RAG-Systeme nach dem Blueprint des durch die SWS-Abteilung veröffentlichten Paper durchgeführt werden kann.

Mit Haystack als Fundament werden einige Funktionalitäten bereits erfüllt sein. Die Vielfalt an Rag-Varianten durch Angabe einer Konfigurationsdatei ist bereits möglich.
Weiterhin bietet Haystack sowohl eigene Metriken für Komponenten- und End-to-End-Evaluation an. 
Das Framework mit DeepEval, UpTrain und RAGAS integrierbar. Somit lassen sich die meisten Performance-Metriken nur mit zusätzlichen Aufwand berechnen, da sie nicht durch die Konfigurationsdatei definiert werden können.

Abschließend lässt sich zusammenfassen, dass Kontextressourcen und RAG-Architekturen vollständig durch Haystack abgedeckt sind. 
Die Evaluierung wird in der Umsetzung dieses Frameworks a) nutzerfreundlicher gemacht, in dem sie in der Konfigurationsdatei angegeben werden kann und b) durch weitere relevante Metriken wie Hardware-Metriken erweitert.
Das Framework wird per default für ein LLM und ein Naive-RAG als Baseline ausführen. Nachfolgend wird die Konfiguration der RAG-Architekur ausgelesen und die Evaluation durchgeführt.
Die Konfigurationsdatei sollte Matrix-Schreibweisen zulassen, sodass verschiedene Hyperparameter getestet werden können. 
Die Testdaten sollten automatisch in Validation \& Holdout-Testset aufgeteilt werden.

Offen bleibt die Frage, wie die Fehleranalyse einfacher gestaltet werden kann.

\subsection{Angepasste Umsetzung der Benchmarks}
% Replikation von bereits durchgeführten Experimenten
Der Fokus auf die Umsetzung des Frameworks bedeutet, dass der Umfang der Benchmark-Experiment reduziert werden muss. Anstelle eigener Experimente werde ich mich auf bereits existierende Experimente konzentrieren.
Mögliche Experimente sind Configuration Validation mit dem CTest Datensatz und Vulnerability Detection mit den Datensätzen SySeVR, diversful oder bifi.


%% This section was initially prepared using BibTeX.  The .bbl file was
%% placed here later
\bibliography{refs}
\bibliographystyle{plain}
%% The file named.bst is a bibliography style file for BibTeX 0.99c



\end{document}

% \begin{itemize}

% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as Ò3.5-inch disk driveÓ.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ÒWb/m2Ó or Òwebers per square meterÓ, not Òwebers/m2Ó.  Spell out units when they appear in text: Ò. . . a few henriesÓ, not Ò. . . a few HÓ.
% \item Use a zero before decimal points: Ò0.25Ó, not Ò.25Ó. Use Òcm3Ó, not ÒccÓ. (bullet list)

% \end{itemize}




% \subsection{Equations}

% The equations are an exception to the prescribed specifications of this template. You will need to determine whether or not your equation should be typed using either the Times New Roman or the Symbol font (please no other font). To create multileveled equations, it may be necessary to treat the equation as a graphic and insert it into the text after your paper is styled. Number equations consecutively. Equation numbers, within parentheses, are to position flush right, as in (1), using a right tab stop. To make your equations more compact, you may use the solidus ( / ), the exp function, or appropriate exponents. Italicize Roman symbols for quantities and variables, but not Greek symbols. Use a long dash rather than a hyphen for a minus sign. Punctuate equations with commas or periods when they are part of a sentence, as in

% $$
% \alpha + \beta = \chi \eqno{(1)}
% $$

% Note that the equation is centered using a center tab stop. Be sure that the symbols in your equation have been defined before or immediately following the equation. Use Ò(1)Ó, not ÒEq. (1)Ó or Òequation (1)Ó, except at the beginning of a sentence: ÒEquation (1) is . . .





\section{USING THE TEMPLATE}

Use this sample document as your LaTeX source file to create your document. Save this file as {\bf root.tex}. You have to make sure to use the cls file that came with this distribution. If you use a different style file, you cannot expect to get required margins. Note also that when you are creating your out PDF file, the source file is only part of the equation. {\it Your \TeX\ $\rightarrow$ PDF filter determines the output file size. Even if you make all the specifications to output a letter file in the source - if you filter is set to produce A4, you will only get A4 output. }

It is impossible to account for all possible situation, one would encounter using \TeX. If you are using multiple \TeX\ files you must make sure that the ``MAIN`` source file is called root.tex - this is particularly important if your conference is using PaperPlaza's built in \TeX\ to PDF conversion tool.

\subsection{Headings, etc}

Text heads organize the topics on a relational, hierarchical basis. For example, the paper title is the primary text head because all subsequent material relates and elaborates on this one topic. If there are two or more sub-topics, the next level head (uppercase Roman numerals) should be used and, conversely, if there are not at least two sub-topics, then no subheads should be introduced. Styles named ÒHeading 1Ó, ÒHeading 2Ó, ÒHeading 3Ó, and ÒHeading 4Ó are prescribed.

\subsection{Figures and Tables}

Positioning Figures and Tables: Place figures and tables at the top and bottom of columns. Avoid placing them in the middle of columns. Large figures and tables may span across both columns. Figure captions should be below the figures; table heads should appear above the tables. Insert figures and tables after they are cited in the text. Use the abbreviation ÒFig. 1Ó, even at the beginning of a sentence.

\begin{table}[h]
\caption{An Example of a Table}
\label{table_example}
\begin{center}
\begin{tabular}{|c||c|}
\hline
One & Two\\
\hline
Three & Four\\
\hline
\end{tabular}
\end{center}
\end{table}


   \begin{figure}[thpb]
      \centering
      \framebox{\parbox{3in}{We suggest that you use a text box to insert a graphic (which is ideally a 300 dpi TIFF or EPS file, with all fonts embedded) because, in an document, this method is somewhat more stable than directly inserting a picture.
}}
      %\includegraphics[scale=1.0]{figurefile}
      \caption{Inductance of oscillation winding on amorphous
       magnetic core versus DC bias magnetic field}
      \label{figurelabel}
   \end{figure}
   