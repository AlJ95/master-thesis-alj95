The year 2017 can be stated as the beginning of the interesting journey of artificial intelligent language models. With the publication of "Attention is all you need" from \citet{vaswani2023attentionneed} the rapid development of language models and later large language models (LLMs) took of. 

Today we have a variety of real world products that are using this technology, such as content generators like ChatGPT (\citet{OpenAI_2022}) or Claude (\citet{Anthropic_2023}), translators like DeepL (\citet{DeepL_SE}) and Coding Assistants like Github Copilot (\citet{Friedman_2022}). The list can be expanded with technologies like sentiment analysis, question answering systems, market research or education systems. Open-source models are available for each of these technologies, providing strong alternatives to proprietary services.

The remarkably capacity of large language models has led to wide acceptance in society. However LLMs have fundamental problems that can not be solved with more training or larger models. Training models frequently is expensive, so daily training isn't feasible. Therefore every new information such as elections, weather or sport results which occurred between last training and user prompt are unknown to the model. On top of that, models can only be trained on available data. Private informations of users that might be relevant for the prompt are not considered in the generation process. LLMs struggle also with long-tail information, which occures rarely in the training data (\citet{Kandpal.15.11.2022}).

When information is missing or underrepresented, outputs will deviate from user inputs, repeat previous outputs or may be made up by the LLM. (\citet{Zhang.03.09.2023}). This technology is already used many sectors with billions of customers such as marketing, retail and eCommerce, education, healthcare, finance and law and media. It is crucial to develop systems that are as correct as possible.

The solution to potential missing information in training is to provide all necessary information to the LLM beforehand within the prompt, so that the generator just have to construct a coherent text for the user. This can be achieved with so call Retrieval-Augmented Generation Systems (RAG-Systems), where the raw user prompt is used to retrieve relevant data from an database that gets summarised and inserted into the prompt for the generator. This method overcomes many of the challenges LLMs face. Data can be accessed from private and up-to-date sources. The frequency of information occurrences no longer matters as long as the database includes it for retrieval. For having recent information, it is not longer required to train the underlying model.
% cite original RAG Paper
% cite paper that shows the advantages of RAG

The price for RAG-Systems is high. The system requires additional steps between the prompt request and output. Most of those steps can not be parallelized. That results in longer inference times and also leads to the more resource intensive system. Next to the increasing infrastructure costs, developing and maintaining a RAG-System is more time consuming than developing a LLM, because the LLM is a part of the larger system. RAG-Systems are not by default significantly better systems than pure LLMs as \citet{Simon.10112024} showed. These complex systems are highly sensitive to small configuration changes.

Therefore there is an important question to be asked: 
\begin{quotation}
    "Is an advanced RAG system necessary for your use case, or is a standalone LLM sufficient?"
\end{quotation}
\begin{quotation}
    "Is your RAG-System the best one for your specific use-case?"
\end{quotation}

The answers to this questions are hard to find, because you have to implement a RAG-System, to test it on your problem and your data. The scientific landscape of Retrieval-Augmented Generation Systems is a vast community with rapid development. Staying up-to-date with that research topic is time consuming for companies and research departments. 

There are companies and research groups that successfully solved parts of this problem with developing tools, frameworks and libraries such as AutoRAG (\citeyear{AutoRAG}), Llama-Index (\citeyear{Liu_LlamaIndex_2022}), LangChain (\citeyear{Chase_LangChain_2022}), RaLLe (\citeyear{ralle}), FlashRAG (\citeyear{FlashRAG}), RAGLAB (\citeyear{zhang-etal-2024-raglab}), Haystack (\citeyear{Pietsch_Haystack_the_end-to-end_2019}) and FastRAG (\citeyear{Izsak_fastRAG_Efficient_Retrieval_2023}). While an in-depth analysis will follow later in this thesis, it can be stated, that all of those tools and frameworks are focussed on developing RAG-Variants, make them production-ready or evaluating them for performance, ignoring the fact, that RAG-Systems must be measured for hardware metrics like latency, inference time and CPU usage do determine if the benefits in performance compensate the disadvantages. Additional to this, \citet{Simon.10112024} showed there is lack of external validity in the development of RAG-Systems, because the iterative reconfiguration of those systems that leads to the best performance is an hyperparameter tuning process that overfits the model to the tested data and therefore needs like traditional model training an validation dataset and a holdout test dataset, which is only used to estimate the generalization error.

With that master thesis I will make two contributions to the scientific landscape of RAG-Systems: (i) A novel benchmarking framework following the systematical blueprint showed by \citet{Simon.10112024} and evaluating hardware metrics next to performance, (ii) A RAG-System for the software engineering task configuration validation that is evaluated with the here presented benchmarking framework. \\[24pt]


\large Here comes Chapter Outline ...
% I will use the abbreviation LLM for large language models and RAG-System for Retrieval-Augmented Generation System predominantly in this master thesis.
% LLMs haben ein fundamentelles Problem ... 