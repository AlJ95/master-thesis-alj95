
There are several types of text retrieval for RAG systems. \cite{Zhao.29.02.2024} classified four different types of retrieval techniques regarding how the retrieved information are passed to the generation. In this thesis I will only focus on query-based retrievals as they are the most common and widely used retrieval techniques. The other three types are latent-representative-based retrieval, logit-based and speculative retrieval.

Another categorization of text retrieval can be done by the type of retrieval. There are three main types of retrieval: sparse retrieval, dense retrieval and hybrid retrieval. Sparse retrieval is based on traditional information retrieval techniques like TF-IDF or BM25. Dense retrieval is based on transformers models and hybrid retrieval combines both sparse and dense retrieval techniques. Following I  will describe those techniques in more detail.
\subsection{Sparse Retrieval}
\label{sec:sparse_retrieval}

\paragraph{TF-IDF}
\label{sec:tfidf}

TF as shown in \cite{Manning.2009} refers to the term frequency of a term $t$ in a document $d$. The inverse document frequency (IDF) is calculated as the logarithm of the total number of documents $N$ divided by the number of documents containing the term $t$. Therefore the IDF factor is low for terms that occur in all documents and high for distinguishing terms that occur in frequently in a low number of documents. The position of the words is ignored.

\begin{equation}
    \text{TF-IDF}(t, d, D) = \text{TF}(t, d) \cdot \text{IDF}(t, D) = \text{TF}(t, d) \cdot \log\left(\frac{N}{\text{DF}(t, D)}\right)
\end{equation}


\paragraph{BM25}
\label{sec:bm25}

\cite{Manning.2009} showed several versions of it. One simpler one is defined as following.

\begin{equation}
    \text{BM25}(t, d, D) = \text{IDF}(t, D) \cdot \frac{(k_1 + 1) \cdot TF_{t, d}}{k_1((1-b)+b \cdot \frac{L_d}{L_{ave}}) + TF_{t, d}}
\end{equation}

The BM25 score is an advanced version of the TF-IDF score with two free parameters $k_1$ and $b$. The parameter $k_1$ is a scaling factor to determine how relevant term frequency is. The parameter $b$ is for document length scaling, reducing scores of long documents.

\subsection{Dense Retrieval}
\label{sec:dense_retrieval}

Dense Passage Retrieval as shown in \cite{karpukhin2020densepassageretrievalopendomain} utilizes Bidirectional Encoder Representation from Transformers (BERT, \cite{devlin2019bertpretrainingdeepbidirectional}). BERT can be seen as the encoders part of an sequence-to-sequence transformers architecture. Therefore it can be used to encode the text passages used as contexts at the classification token $\[CLS\]$. The text is thus mapped into an d-dimensional real-valued space with the assumption that semnatic similar passages are close to each other in this space. The query is also encoded into this space and the similarity between the mapped query vector and each passage vector is calculated. The similarity is calculated by the dot product or other similarity functions. The top-k similar passages are then selected and used for the generation.

\subsection{Hybrid Retrieval}
\label{sec:hybrid_retrieval}

Both sparse retrieval techniques TF-IDF and BM25 are good for keyword specific searches, but perform poorly on a semantic comparison of documents and query. Encoders like BERT were trained for understanding languages and are thus good for a semantic understanding. Often it is not trivial to dertermine if a text retrieval needs a keyword-relevant sparse retrieval or dense retrieval considering semantics. 

Hybrid models calculate both dense retrieval and sparse retrieval scores and multiply it then with factor $\alpha$ and $1-\alpha$ respectively. An factor $\alpha=1$ results in using only dense retrieval. There is no universal performant value for $\alpha$. Therefore this is a hyperparamter tuning parameter that needs to get optimizied.