\section{Introduction}
Modern software systems involve intricate changes that extend far beyond source code modifications. A significant portion of system behavior is governed by configuration files, encompassing deployment scripts, service settings, infrastructure definitions, and more. Ensuring the correctness of these configurations is critical, yet challenging. Software vendors often provide extensive manuals to guide administrators, but the length and complexity of these documents can be prohibitive, frequently leading practitioners to guess when configuring systems \cite{Xiang.2020}. This challenge is compounded by the rapid pace of technological evolution, which necessitates constant updates to configuration schemes.

The consequences of misconfiguration can be severe, significantly contributing to production bugs and system failures, as highlighted by large-scale studies \cite{Tang.2015}. Traditionally, validating configurations has relied on methods such as static analysis, integration testing, and manual review \cite{Lian.2024}. While valuable, these approaches often struggle to keep pace with the complexity and dynamism of modern software environments. Furthermore, the scale at which validation is required can be immense; Facebook, for example, reported executing trillions of configuration checks daily \cite{Tang.2015}. This necessitates solutions that are not only accurate but also highly performant and resource-efficient, implying that evaluation solely based on metrics like precision or recall is insufficient. Consequently, there is a pressing need for more automated, reliable, and scalable validation techniques.

Retrieval-Augmented Generation (RAG) systems offer a promising approach to address this gap. By combining the knowledge retrieval capabilities of search systems with the reasoning power of Large Language Models (LLMs), RAG can potentially interpret complex technical documentation (like configuration manuals) and apply this understanding to validate specific configurations against best practices or requirements.

This chapter aims to demonstrate the practical application of the RAGBench evaluation framework, detailed in Chapter 4, to the specific problem of configuration validation. We will leverage this framework to systematically evaluate the performance of various RAG configurations against established baselines using an extended version of the CTest dataset \cite{Lian.2024}, which includes synthetically generated examples. While acknowledging that configuration validation is a multifaceted problem involving aspects like inter-configuration dependencies, this experiment focuses specifically on validating individual configuration settings based on documented guidelines.

The remainder of this chapter is structured as follows: Section \ref{sec:related_works_exp} discusses related work in applying RAG to software engineering tasks, particularly configuration validation. Section \ref{sec:exp_design_exec} details the experiment design, including the dataset, baselines, RAG configurations, evaluation metrics, and tools used, adhering to the methodology outlined in Chapter 4. Section \ref{sec:exp_results} presents and analyzes the results obtained on the validation and test sets, including end-to-end performance, component analysis, and failure analysis. Section \ref{sec:exp_discussion} discusses the overall findings and their implications. Finally, Section \ref{sec:exp_conclusion} summarizes the key outcomes of the experiment.
% Note: Added \label tags for section references - adjust as needed if you rename sections.

\section{Related Works} \label{sec:related_works_exp}
Several approaches have been developed for validating software configurations.

Specification-based frameworks, such as ConfValley \cite{Huang.2015}, function by having engineers define validation rules explicitly, often in a declarative manner. Configuration Testing validates configuration values by executing the software components that use these values and observing the resulting program behavior \cite{XudongSun.2020}. This method is used to detect the dynamic effects of configuration settings and identify errors.

Lian et al. \cite{Lian.2024} introduced Ciri, a method that uses Large Language Models for configuration validation. Ciri employs prompt engineering and few-shot learning, providing the LLM with examples of both valid and invalid configurations alongside relevant documentation snippets to identify misconfigurations. This work applies Retrieval-Augmented Generation to the configuration validation task presented by Lian et al. \cite{Lian.2024}. We utilize the RAGBench evaluation framework chapter \ref{chap:design} to systematically assess and reconfigure different RAG systems for this specific task, aiming to optimize their performance through iterative refinement.

\section{Experiment Design and Execution} \label{sec:exp_design_exec}
The following sections detail the design and execution of our configuration validation experiment, adhering strictly to the methodology established by the RAGBench framework presented in Chapter \ref{chap:design}. The core principle involves splitting the evaluation dataset into validation and test sets, performing all system configuration, evaluation, and iterative refinement exclusively on the validation set, and finally assessing generalization on the held-out test set. Specifically, we employ a 70/30 validation-test split for the extended CTest dataset to ensure a sufficiently large test set for estimating generalization error (Section \ref{sec:valtestsplit}).

Our experimental workflow begins by establishing performance baselines using a standalone LLM and a naive RAG system (Section \ref{sec:framework-baselines}). We then evaluate an initial RAG configuration. Subsequent steps involve analyzing performance bottlenecks and failures using the framework's end-to-end and component-level metrics, alongside detailed trace analysis facilitated by MLflow and Langfuse (Section \ref{sec:evaluation-techniques}). Insights from this analysis guide iterative reconfiguration cycles aimed at improving performance on the validation set.

A key difference in our evaluation approach compared to the original Ciri experiment \cite{Lian.2024} lies in handling output formatting. While Ciri reran queries until a correctly formatted answer was obtained, our framework employs a strict format check; if the generator's output does not conform to the expected format (e.g., providing a clear "valid" or "invalid" classification alongside reasoning, as per Section \ref{sec:generator-evaluation}), the response is marked as incorrect for the purpose of end-to-end metric calculation. This reflects a scenario where automated post-processing requires predictable output.

% Differences Ciri to Me:
% They run it as long as it needs to get the correctly formatted answer
% I check with format_checker if the format is correct, otherwise its false

% Start with baselines and an RAG with trivial prompt
% Check for bottlenecks via MLFlow and Langfuse
% <Reiterate here a bit out of the typical experiment workflow showed in 40_DEsign_of_Valid_RAG_Evaluation.tex>

% We will use a 70/30 Validation Test Split for having enough on the test set to estimate correctly


\subsection{Experimental Setup} \label{sec:exp_setup}
The experiments were conducted using the computational resources specified below. The primary environment for running the evaluation framework, including data processing, baseline evaluations, and RAG pipeline executions involving API-based LLMs or CPU-based components, was a dedicated server hosted by Hetzner.

\begin{itemize}
    \item \textbf{Hetzner Server:} [TODO: Insert Hetzner Server Specifications - e.g., Model, CPU, RAM, Disk]
\end{itemize}

For computationally intensive tasks requiring local GPU acceleration, specifically for hosting and running certain large language models, we utilized cloud-based GPU instances provisioned through runpod.io. The vLLM library \cite{Kwon.12.09.2023} was employed for efficient LLM inference on these instances.

\begin{itemize}
    \item \textbf{Runpod.io Instance:} NVIDIA H100 GPU [TODO: Add vRAM amount, e.g., 80GB]
    \item \textbf{vLLM Parameters:} [TODO: Insert key vLLM parameters if non-default, e.g., tensor parallelism, max tokens]
\end{itemize}

The core software components underpinning the experimental setup are provided by the RAGBench framework itself:
\begin{itemize}
    \item \textbf{Haystack:} Used for defining, configuring (via YAML), and executing the RAG pipelines \cite{Pietsch_Haystack_the_end-to-end_2019}.
    \item \textbf{MLflow:} Employed for logging experiment parameters, configurations (including YAML metadata), and evaluation metrics (end-to-end and component-level) for visualization and comparison \cite{MLflow}.
    \item \textbf{Langfuse:} Utilized for capturing detailed execution traces of the RAG pipelines, enabling in-depth failure analysis \cite{Langfuse}.
    \item \textbf{Vector Database:} We use two different vector databases: ChromaDB and In-Memory for estimating the effect on a external database as used in practice.
\end{itemize}


\subsection{Reconfiguration Phases} \label{sec:exp_results} % Renamed section slightly to better reflect content
% Present and discuss the results obtained using ONLY the validation dataset. 


\paragraph{Dataset} % Added subsection for Dataset description
% Describe the dataset used for evaluation.
% Source, size, characteristics.
% Mention the validation-test split ratio (e.g., 80/20) and justification (Sec 4.1). - Already mentioned split ratio earlier.
[TODO: Describe the extended CTest dataset - source, size, format, types of configurations covered, how it was extended (synthetic data?)]

\paragraph{Initial Configuration and Baselines} \label{sec:exp_initial_config}
This section details the performance of the baseline systems and the initial RAG configuration evaluated on the validation set. 

\subparagraph{Baselines}
Two baselines were established as per the framework's methodology:
\begin{itemize}
    \item \textbf{Standalone LLM:} We want to use three different models for baselines: OpenAI's gpt-4o-mini\cite{OpenAI_2022}, Google's gemma-24b \cite{Gemma3.25.03.2025} and Alibaba's QwQ-32B\cite{qwq32b,qwen2.5}
    \item \textbf{Naive RAG:} A simple retrieve-read pipeline using BM25 retrieval over the documentation corpus and the same LLM as the standalone baseline (using \textit{gpt-4o-mini} and a Top-K of 10)
\end{itemize}


\subparagraph{Initial RAG Configuration} % Describe the specific RAG configuration tested first.
The first RAG system evaluated employed a straightforward configuration:


\paragraph{Reconfiguration I} \label{sec:exp_reconfig_1}
Based on the analysis of the initial configuration, the first reconfiguration phase focused on ...


\paragraph{Reconfiguration II} \label{sec:exp_reconfig_2}


\paragraph{Reconfiguration III} \label{sec:exp_reconfig_3}

\subsection{Generalization Test} \label{sec:exp_generalization}
% Present the results of the final selected configuration(s) on the held-out test set (Sec 4.1, Sec 4.6).
% Compare test set performance against validation set performance for these configurations.
% Use tables/figures.
% Discuss any significant differences and potential signs of overfitting.

\subsection{Discussion} \label{sec:exp_discussion}
% Interpret the overall results of the experiment.
% Which configuration performed best overall?
% Did the RAG systems outperform the baselines?
% Was the added complexity of advanced RAG justified?
% Discuss the implications of the findings for the specific task (configuration validation).
% Acknowledge limitations of the experiment (e.g., dataset size, scope of configurations tested).

\subsection{Conclusion} \label{sec:exp_conclusion}
% Summarize the main findings and contributions of this specific experiment.
% Reiterate the performance of the best RAG system found.
