The year 2017 can be stated as the beginning of the interesting journey of artificial intelligent language models. With the publication of "Attention is all you need" from \citet{vaswani2023attentionneed} the rapid development of language models and later large language models (LLMs) took of. 

Today there is a variety of real world products that are using this technology, such as content generators like ChatGPT (\citet{OpenAI_2022}) or Claude (\citet{Anthropic_2023}), translators like DeepL (\citet{DeepL_SE}) and Coding Assistants like Github Copilot (\citet{Friedman_2022}). The list can be expanded with technologies like sentiment analysis, question answering systems, market research or education systems. Open-source models are available for each of these technologies, providing strong alternatives to proprietary services.

The remarkably capacity of large language models has led to wide acceptance in society. However LLMs have fundamental problems that can not be solved with more training or larger models. Training models frequently is expensive, so daily training isn't feasible. Therefore every new information such as elections, weather or sport results which occurred between last training and user prompt are unknown to the model. On top of that, models can only be trained on available data. Private informations of users that might be relevant for the prompt are not considered in the generation process. LLMs struggle also with long-tail information, which occures rarely in the training data (\citet{Kandpal.15.11.2022}).

When information is missing or underrepresented, outputs will deviate from user inputs, repeat previous outputs or may be made up by the LLM. (\citet{Zhang.03.09.2023}). This technology is already used many sectors with billions of customers such as marketing, retail and eCommerce, education, healthcare, finance and law and media. It is crucial to develop systems that are as correct as possible.

The solution to potential missing information in training is to provide all necessary information to the LLM beforehand within the prompt, so that the generator just have to construct a coherent text for the user. This can be achieved with so call Retrieval-Augmented Generation Systems (RAGs), where the raw user prompt is used to retrieve relevant data from an database that gets summarised and inserted into the prompt for the generator. This method overcomes many of the challenges LLMs face. Data can be accessed from private and up-to-date sources. The frequency of information occurrences no longer matters as long as the database includes it for retrieval. For having recent information, it is not longer required to train the underlying model.
% cite original RAG Paper
% cite paper that shows the advantages of RAG

The price for RAGs is high. The system requires additional steps between the prompt request and output. Most of those steps can not be parallelized. That results in longer inference times and also leads to the more resource intensive system. Next to the increasing infrastructure costs, developing and maintaining a RAG is more time consuming than developing a LLM, because the LLM is a part of the larger system. RAGs are not by default significantly better systems than pure LLMs as \citet{Simon.10112024} showed. These complex systems are highly sensitive to small configuration changes.

Therefore there is an important question to be asked: 
\begin{quotation}
    "Is an advanced RAG system necessary for your use case, or is a standalone LLM sufficient?"
\end{quotation}
\begin{quotation}
    "Is your RAG the best one for your specific use-case?"
\end{quotation}

The answers to this questions are hard to find, because you have to implement a RAG, to test it on your problem and your data. The scientific landscape of Retrieval-Augmented Generation Systems is a vast community with rapid development. Staying up-to-date with that research topic is time consuming for companies and research departments. 

The implementation of the RAG is just one part of the decision process. It is difficult to evaluate LLMs and all systems that are based on that technology, because outputs are not deterministic and such models are like a black-box. It is not obvious why the model responds with a certain output. Another problem is that there is a whole set of potential correct answers, because text-based outputs can have many forms. Lets consider following example:\\

Question: \textit{Is the evaluation process of LLMs an easy task?}\\
Answer 1: \textit{The evaluation process of LLMs is not an easy task.}\\
Answer 2: \textit{No, the evaluation process is a difficult task.}\\[6pt]

Both answers are correct, but which metric must be used to measure this outcome? 

The Evaluation of RAGs is even harder, because it has the same problems next to its own ones. RAGs are complex and consists of many parts. Each part can lead to errors and therefore all components of the systems need to be evaluated next to an end-to-end evaluation of the whole system as \citet{Salemi.2024} and \citet{Yu.2024} showed.

There are companies and research groups that successfully solved parts of this problem with developing tools, frameworks and libraries such as AutoRAG (\citeyear{AutoRAG}), Llama-Index (\citeyear{Liu_LlamaIndex_2022}), LangChain (\citeyear{Chase_LangChain_2022}), RaLLe (\citeyear{ralle}), FlashRAG (\citeyear{FlashRAG}), RAGLAB (\citeyear{zhang-etal-2024-raglab}), Haystack (\citeyear{Pietsch_Haystack_the_end-to-end_2019}) and FastRAG (\citeyear{Izsak_fastRAG_Efficient_Retrieval_2023}). While an in-depth analysis will follow later in this thesis, it can be stated, that all of those tools and frameworks are focussed on developing RAG-Variants, make them production-ready or evaluating them for performance, ignoring the fact, that RAGs must be measured for hardware metrics like latency, inference time and CPU usage to determine if the benefits in performance compensate the disadvantages. Additional to this, \citet{Simon.10112024} showed there is lack of external validity in the development of RAGs, because the iterative reconfiguration of those systems that leads to the best performance is an hyperparameter tuning process that overfits the model to the tested data and therefore requires an dataset split with an validation dataset and a holdout test dataset, which is only used to estimate the generalization error.

With that master thesis I will make two contributions to the scientific landscape of RAGs: (i) A novel benchmarking framework following the systematical blueprint showed by \citet{Simon.10112024} and evaluating hardware metrics next to SOTA performance evaluations, (ii) A RAG for the software engineering task configuration validation that is evaluated with the here presented benchmarking framework. For (i), the framework will extend Haystack \citeyear{Pietsch_Haystack_the_end-to-end_2019} next to the FastRAG library \citeyear{Izsak_fastRAG_Efficient_Retrieval_2023}. Haystack is an open-source framework for LLMs, RAGs and SOTA search systems. FastRAG build upon Haystack and adds special RAG architectures. \\


\large Here comes most important results \\
\large Here comes main conclusions \\
\large Here comes Chapter Outline ...\\
% I will use the abbreviation LLM for large language models and RAG for Retrieval-Augmented Generation System predominantly in this master thesis.
% LLMs haben ein fundamentelles Problem ... 