The year 2017 can be stated as the beginning of the interesting journey of artificial intelligence language models. With the publication of "Attention is all you need" from Vaswani et al. \cite{vaswani2023attentionneed}, the rapid development of language models and later large language models (LLMs) took off.

Today there is a variety of real world products that are using this technology, such as content generators like ChatGPT \cite{OpenAI_2022} or Claude \cite{Anthropic_2023}, translators like DeepL \cite{DeepL_SE} and Coding Assistants like Github Copilot \cite{Friedman_2022}. The list can be expanded with technologies like sentiment analysis, question answering systems, market research or education systems. Open-source models are available for each of these technologies, providing strong alternatives to proprietary services.

The remarkable capacity of large language models has led to wide acceptance in society. However, LLMs have fundamental problems that cannot be solved with more training or larger models. Training models frequently is expensive, so daily training isn't feasible. Therefore, every new piece of information, such as election outcomes, weather updates, or sports results, that has occurred between the last training and the user prompt are unknown to the model. On top of that, models can only be trained on available data. Private information of users that might be relevant for the prompt are not considered in the generation process. LLMs also struggle with long-tail information, which occurs rarely in the training data \cite{Kandpal.15.11.2022}.

When information is missing or underrepresented, outputs may deviate from user inputs, repeat previous outputs, or be fabricated by the LLM \cite{Zhang.03.09.2023}. This technology is already used in many sectors with billions of customers, such as in marketing, retail and eCommerce, education, healthcare, finance, law, and media. It is crucial to develop systems that are as correct as possible.

The solution to potential missing information in training is to provide all necessary information to the LLM beforehand within the prompt, so that the generator just has to construct a coherent text for the user. This can be achieved with so-called Retrieval-Augmented Generation Systems (RAGs), where the raw user prompt is used to retrieve relevant data from a database that is summarized and inserted into the prompt for the generator. This method overcomes many of the challenges LLMs face. Data can be accessed from private and up-to-date sources. The frequency of information occurrences no longer matters as long as the database includes it for retrieval. Having recent information no longer requires training the underlying model.
% cite original RAG Paper
% cite paper that shows the advantages of RAG

However, the adoption of RAG systems introduces significant overhead and complexity. The system requires additional steps between the prompt request and output. Most of those steps cannot be parallelized. This results in longer inference times and also leads to a more resource-intensive system. Next to the increasing infrastructure costs, developing and maintaining a RAG is more time consuming than developing a LLM, because the LLM is a part of the larger system. RAGs are not by default significantly better systems than pure LLMs as Simon \cite{Simon.10112024} showed. These complex systems are highly sensitive to small configuration changes.

Therefore, important questions arise:
\begin{quotation}
    "Is an advanced RAG system necessary for your use case, or is a standalone LLM sufficient?"
\end{quotation}
\begin{quotation}
    "Is your RAG system the optimal one for your specific use case?"
\end{quotation}

The answers to these questions are hard to find, because one has to implement a RAG to test it on a specific problem and dataset. The scientific landscape of Retrieval-Augmented Generation Systems is a vast community with rapid development. Staying up-to-date with that research topic is time-consuming for companies and research departments. 

The implementation of the RAG is just one part of the decision process. It is difficult to evaluate LLMs and all systems that are based on this technology, because outputs are not deterministic and such models are like a black box. It is not obvious why the model responds with a certain output. Another problem is that there is a whole set of potential correct answers, because text-based outputs can have many forms. Let's consider following example:\\

Question: \textit{Is the evaluation process of LLMs an easy task?}\\
Answer 1: \textit{The evaluation process of LLMs is not an easy task.}\\
Answer 2: \textit{No, the evaluation process is a difficult task.}\\[6pt]

Both answers are correct, but which metric must be used to measure this outcome? 

The evaluation of RAGs is even harder because it has the same problems in addition to its own. RAGs are complex and are composed of many parts. Each part can lead to errors, and therefore all components of the system need to be evaluated in addition to an end-to-end evaluation of the whole system, as Salemi \cite{Salemi.2024} and Yu \cite{Yu.2024} showed.

There are companies and research groups that successfully solved parts of this problem with developing tools, frameworks, and libraries such as AutoRAG \cite{AutoRAG}, Llama-Index \cite{Liu_LlamaIndex_2022}, LangChain \cite{Chase_LangChain_2022}, RaLLe \cite{ralle}, FlashRAG \cite{FlashRAG}, RAGLAB \cite{zhang-etal-2024-raglab}, Haystack \cite{Pietsch_Haystack_the_end-to-end_2019} and FastRAG \cite{Izsak_fastRAG_Efficient_Retrieval_2023}. While an in-depth analysis will follow later in this thesis, it can be stated that all of these tools and frameworks are focused on developing RAG variants, making them production-ready, or evaluating them for performance, ignoring the fact that RAGs must be measured for hardware metrics such as latency, inference time, and CPU usage to determine if the benefits in performance compensate for the disadvantages. Additionally, Simon \cite{Simon.10112024} showed there is a lack of external validity in the development of RAGs, because the iterative reconfiguration of these systems that leads to the best performance is a hyperparameter tuning process that overfits the model to the tested data and therefore requires a dataset split with a validation dataset and a holdout test dataset, the latter of which is only used to estimate the generalization error.

In this master's thesis, we will make two contributions to the scientific landscape of RAGs: (i) A novel benchmarking framework, RAGnRoll, following the systematic blueprint shown by Simon \cite{Simon.10112024} and evaluating hardware metrics alongside state-of-the-art (SOTA) performance evaluations, (ii) A practical demonstration of RAGnRoll through an experiment on the software engineering task of configuration validation. For (i), the framework will extend Haystack \cite{Pietsch_Haystack_the_end-to-end_2019} and leverage the FastRAG library \cite{Izsak_fastRAG_Efficient_Retrieval_2023}. Haystack is an open-source framework for LLMs, RAGs, and SOTA search systems. FastRAG builds upon Haystack and adds special RAG architectures. 

The primary results of this work indicate that while RAG systems offer a powerful approach to overcoming LLM limitations, their effective implementation is non-trivial. The development of RAGnRoll highlighted the critical necessity of incorporating rigorous machine learning practices, such as a strict validation-test split, into RAG evaluation to mitigate overfitting. Furthermore, the application of RAGnRoll to software configuration validation revealed that retrieval quality is often the predominant bottleneck, and complex RAG architectures do not automatically surpass well-prompted standalone LLMs without careful, task-specific optimization of the retrieval components. Our experiments achieved a competitive F1-score of 0.776 for configuration validation using a standalone model with a Ciri-like few-shot setup, underscoring the importance of robust baselines.

This thesis concludes that a systematic and holistic benchmarking approach, as facilitated by RAGnRoll, is indispensable for advancing RAG technology. It enables data-driven decisions, helping to determine if a RAG system is truly necessary and how to best configure it for a specific use case, moving beyond ad-hoc tuning towards more principled and reliable RAG solutions.

This thesis is structured as follows:
Chapter \ref{chap:background} provides an overview of the foundational concepts, including Large Language Models, text retrieval techniques, and the architecture of Retrieval-Augmented Generation systems.
Chapter \ref{chap:relwork} discusses existing research and tools related to RAG evaluation and benchmarking.
Chapter \ref{chap:design} details the design and methodology of the RAGnRoll framework, emphasizing its core principles for achieving generalizable and reproducible RAG experiments.
Chapter \ref{chap:Experiment} presents a practical application of RAGnRoll, detailing an experiment focused on using RAG for software configuration validation, and analyzes the obtained results.
Finally, Chapter \ref{chap:Conclusion} summarizes the contributions of this thesis and discusses the implications of RAGnRoll for future research and development in the field of retrieval-augmented generation.
