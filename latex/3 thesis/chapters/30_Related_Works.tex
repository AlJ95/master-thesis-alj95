\section{RAG Evaluation}

\paragraph{Difficulties in RAG/LLM Evaluation}
Yu, Gan et al.\cite{Yu.2024} note the temporal complexity of information and the need for holistic evaluation methodologies in RAG systems. Ru, Qiu et al. Chang, Wang et al.\cite{Chang.06.07.2023} advocate for specialized evaluations over uniform benchmarks and highlight challenges such as dynamic out-of-distribution evaluation and data contamination. Zhao, Zhang et al.\cite{Zhao.29.02.2024} identify key limitations in RAG systems, including knowledge updates and data leakage, proposing research directions to address these issues. Gao, Xiaong, et al.\cite{Gao.18.12.2023} stress the need for refined evaluation methodologies to keep pace with RAG's rapid evolution and expansion into multimodal domains. 

\paragraph{Component-Specific Challenges and Failures in RAG Systems}
Li et al.\cite{Li.13.01.2025} investigate various design choices in RAG systems, highlighting the superior performance of approaches like Contrastive In-Context Learning RAG and Focus Mode RAG. They emphasize the critical importance of retrieved context quality and prompt formulation over simply increasing the knowledge base size. Liu et al.\cite{JintaoLiu.2024} note the difficulty in locating problems within the RAG pipeline, while Barnett et al.\cite{Barnett.2024} identify seven key failure points in RAG systems, stressing the need for effective validation during operation. Huang et al.\cite{Huang.2023} provide a comprehensive taxonomy of hallucinations in LLMs, discussing their causes and mitigation strategies and highlight the limitations of current retrieval-augmented systems. Zhao et al.\cite{Zhao.29.02.2024} explore retrieval quality issues, suggesting that excessive retrieval can degrade results. Liu et al.\cite{Liu.06.07.2023} demonstrate that LLMs perform best when relevant information is positioned at the beginning or end of the context, underperforming when it is in the middle. Together, these studies underscore the multifaceted challenges at the component level and illustrate that both design choices and operational failures contribute significantly to the overall limitations of RAG systems.


\paragraph{Evaluation Methodologies and Transparency}
Simon et al.\cite{Simon.10112024} propose a blueprint for reusable empirical study designs for evaluating RAG systems. They emphasize the need for open data for reproducibility while ensuring it remains closed to LLM training to prevent data leakage. The authors also stress the importance of standardized methodologies and transparent reporting to ensure validity and replicability. Wagner et al.\cite{Wagner.12.11.2024} underscore the challenges in reproducing LLM results due to unknown hyperparameters and training data, advocating for transparent yet encoded datasets to enhance reproducibility. Pimentel et al.\cite{M.A.Pimentel.2024} highlight the impact of implementation details on model performance and the need for transparent and standardized reporting of metric calculation procedures to foster reproducibility and comparability across studies. They emphasize that LLM performance is highly dependent on the evaluation methodologies and implementation details used. Overall, these works call for a shift towards greater transparency and standardization in evaluation practices, which is essential for robust performance assessments in RAG systems.

\paragraph{End-to-End Evaluation}

Traditionally, RAG evaluation has primarily relied on end-to-end assessment by comparing the generated output with one or more ground truth references \cite{Salemi.2024}. Although this approach is crucial for gauging overall performance, it suffers from significant limitations when evaluating retrieval models. In particular, end-to-end evaluation lacks transparency regarding which retrieved document contributed to the final output, thereby hindering the interpretability of the system's behavior. 

While end-to-end evaluation provides a holistic view of RAG system performance, understanding the contributions of individual components to the overall system behavior is equally critical. In their comprehensive study, Li et al. \cite{Li.13.01.2025} systematically investigate the intricate relationships between various RAG components and configurations, addressing nine key research questions to unravel the operational mechanisms of RAG systems

\paragraph{Component Evaluation}
The evaluation of generation tasks, especially those involving creativity or open-ended responses, is inherently challenging due to the subjective nature of defining "correct" or "high-quality" outputs \cite{Yu.2024}. To address this, researchers have proposed advanced evaluation methods. For instance, an "Oracle Prompt," which includes all relevant documents, serves as an upper bound for evaluating generation by simulating perfect retrieval conditions \cite{Krishna.19.09.2024}. However, this method can overemphasize retrieval quality by assuming ideal conditions. Li et al. emphasize the widespread adoption of LLMs as judges, noting their ability to align with human judgments while highlighting significant biases, such as preference leakage towards related models \cite{Li.13.01.2025}.

Evaluating the retrieval component of RAG systems presents unique challenges. Ashkan Alinejad et al.\cite{Alinejad.2024} highlight the complexity of assessing retrieval effectiveness by comparing human judgments alongside four evaluation types: Exact Match, Token-Based, Embedding-Based, and LLM-Based. Alireza Salemi and Hamed Zamani\cite{Salemi.2024} introduce eRAG, a novel evaluation framework that leverages downstream task performance to assess retrieval quality more effectively. They demonstrate that traditional relevance labels have a weak correlation with RAG performance, emphasizing the need for task-oriented evaluation methods. Jin et al.\cite{Jin.19.09.2024} reveal that increasing the number of retrieved passages does not consistently improve end-to-end performance, often leading to performance degradation due to hard negatives. To mitigate this, they propose methods like retrieval reordering and fine-tuning techniques to enhance context utilization. Furthermore, Mallen et al. \cite{Mallen.2023} find that retrieving contexts may be unnecessary and even detrimental when dealing with common knowledge, but it benefits questions about rare knowledge, underscoring the importance of contextual relevance in retrieval strategies.

\section{Frameworks}
Recent advancements in frameworks for RAG development and evaluation have significantly enhanced the systematic assessment of Retrieval-Augmented Generation (RAG) systems. RAGAS\cite{Es.2023} introduces reference-free metrics to evaluate faithfulness, answer relevance, and context relevance, while INSPECTORRAGET\cite{Fadnis.26.04.2024} provides an interactive platform for comprehensive analysis of RAG systems' performance and annotator quality. RaLLe\cite{Hoshi.8212023b} focuses on developing and optimizing R-LLMs through transparent prompt engineering, whereas RAGGED\cite{JenniferHsia.2024} offers insights into model behaviors under varying context configurations. FlashRAG\cite{Jin.5222024} and BERGEN\cite{Rau.01.07.2024} are modular toolkits supporting diverse RAG components, datasets, and metrics, with FlashRAG emphasizing efficiency and BERGEN promoting reproducibility. AutoRAG\cite{Kim.10282024} automates pipeline optimization, RAGCHECKER\cite{Ru.15.08.2024} employs claim-level metrics for fine-grained evaluation, and ARES\cite{SaadFalcon.16.11.2023} leverages synthetic data and lightweight judges for scalable assessment. R-Eval\cite{Tu.2024} and RAGLAB\cite{Zhang.8212024} provide unified, customizable frameworks for evaluating domain knowledge and comparing RAG algorithms, respectively. Although these frameworks significantly advance RAG research, they do not explicitly address the challenge of overfitting on the validation data set, which can arise from the iterative reconfiguration of RAG systems. This problem is comparable to that when training large language models, if there is no training split into train, validation, and holdout-test datasets. In the following chapter, we will introduce an evaluation design that enhances the generalization of such systems in a transparent and reproducible manner.
