@misc{PLACEHOLDER, journal={PLACEHOLDER JOURNAL}, author={PLACEHOLDER | Needs to be cited fact}, year={????}, title={ToDo | CITE | ae oe ue ?}, } 

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017, revised in 2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{Wolf.09.10.2019,
 abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.},
 author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and {Le Scao}, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
 date = {09.10.2019},
 title = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
 url = {http://arxiv.org/pdf/1910.03771},
 file = {Wolf, Debut et al 09102019 - HuggingFace's Transformers:Attachments/Wolf, Debut et al 09102019 - HuggingFace's Transformers.pdf:application/pdf;Wolf, Debut et al. 09.10.2019 - HuggingFace's Transformers:Attachments/Wolf, Debut et al. 09.10.2019 - HuggingFace's Transformers.pdf:application/pdf}
}

% This file was created with Citavi 6.18.0.1

@misc{Hou.8212023,
 abstract = {Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We select and analyze 395 research papers from January 2017 to January 2024 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and flagging promising areas for future study. Our artifacts are publicly available at https://github.com/xinyi-hou/LLM4SE{\_}SLR.},
 author = {Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and {Lo David} and Grundy, John and Wang, Haoyu},
 date = {8/21/2023},
 title = {Large Language Models for Software Engineering: A Systematic Literature Review},
 url = {http://arxiv.org/pdf/2308.10620v6},
 keywords = {Generation Task;LLM4SE},
 file = {2308.10620v6:Attachments/2308.10620v6.pdf:application/pdf}
}


@inproceedings{Yin.2024,
 author = {Yin, Junqi and Bose, Avishek and Cong, Guojing and Lyngaas, Isaac and Anthony, Quentin},
 title = {Comparative Study of Large Language Model Architectures on Frontier},
 pages = {556--569},
 publisher = {IEEE},
 isbn = {979-8-3503-8711-7},
 editor = {Chard, Kyle and Li, Zhuozhao},
 booktitle = {2024 IEEE International Parallel and Distributed Processing Symposium},
 year = {2024},
 address = {Piscataway, NJ},
 doi = {10.1109/IPDPS57955.2024.00056},
 file = {Yin, Bose et al. 5 27 2024 - 5 31 2024 - Comparative Study of Large Language:Attachments/Yin, Bose et al. 5 27 2024 - 5 31 2024 - Comparative Study of Large Language.pdf:application/pdf}
}

@article{ACKLEY.1985,
 author = {ACKLEY, D. and HINTON, G. and SEJNOWSKI, T.},
 year = {1985},
 title = {A learning algorithm for boltzmann machines},
 pages = {147--169},
 volume = {9},
 number = {1},
 issn = {03640213},
 journal = {Cognitive Science},
 doi = {10.1016/S0364-0213(85)80012-4},
 file = {ACKLEY, HINTON et al. 1985 - A learning algorithm for boltzmann:Attachments/ACKLEY, HINTON et al. 1985 - A learning algorithm for boltzmann.pdf:application/pdf}
}

@misc{Fan.13.05.2018,
 abstract = {We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.},
 author = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
 date = {13.05.2018},
 title = {Hierarchical Neural Story Generation},
 url = {http://arxiv.org/pdf/1805.04833},
 file = {Fan, Lewis et al. 13.05.2018 - Hierarchical Neural Story Generation:Attachments/Fan, Lewis et al. 13.05.2018 - Hierarchical Neural Story Generation.pdf:application/pdf}
}

@misc{Holtzman.22.04.2019,
 abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive.  In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
 author = {Holtzman, Ari and Buys, Jan and {Du Li} and Forbes, Maxwell and Choi, Yejin},
 date = {22.04.2019},
 title = {The Curious Case of Neural Text Degeneration},
 url = {http://arxiv.org/pdf/1904.09751},
 file = {Holtzman, Buys et al. 22.04.2019 - The Curious Case of Neural:Attachments/Holtzman, Buys et al. 22.04.2019 - The Curious Case of Neural.pdf:application/pdf}
}

@misc{OpenAI_2022, url={https://openai.com/index/chatgpt}, journal={Introducing chatgpt}, author={OpenAI}, year={2022}, month={Nov}} 

@misc{Anthropic_2023, title={Introducing claude}, url={https://www.anthropic.com/news/introducing-claude}, journal={Anthropic}, author={Anthropic}, year={2023}, month={Mar}} 

@misc{DeepL_SE, title={Deepl übersetzer: Der Präziseste übersetzer der welt}, url={https://www.deepl.com/de/}, journal={DeepL Übersetzer: Der präziseste Übersetzer der Welt}, author={DeepL}, year={2017}} 

@misc{Friedman_2022, title={Introducing github copilot: Your AI pair programmer}, url={https://github.blog/news-insights/product-news/introducing-github-copilot-ai-pair-programmer/}, journal={The GitHub Blog}, author={Friedman, Nat}, year={2022}, month={Feb}} 

% This file was created with Citavi 6.19.2.1

@misc{Zhang.03.09.2023,
 abstract = {While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.},
 author = {Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and Wang, Longyue and Luu, Anh Tuan and Bi, Wei and Shi, Freda and Shi, Shuming},
 date = {03.09.2023},
 year = {2023},
 title = {Siren's Song in the AI Ocean: A Survey on Hallucination in Large  Language Models},
 url = {http://arxiv.org/pdf/2309.01219},
 keywords = {Hallucinations;LLMs},
 file = {Zhang, Li et al. 03.09.2023 - Siren's Song in the AI:Attachments/Zhang, Li et al. 03.09.2023 - Siren's Song in the AI.pdf:application/pdf}
}

@misc{Simon.10112024,
 abstract = {Retrieval-augmented generation (RAG) is an umbrella of different components, design decisions, and domain-specific adaptations to enhance the capabilities of large language models and counter their limitations regarding hallucination and outdated and missing knowledge. Since it is unclear which design decisions lead to a satisfactory performance, developing RAG systems is often experimental and needs to follow a systematic and sound methodology to gain sound and reliable results. However, there is currently no generally accepted methodology for RAG evaluation despite a growing interest in this technology. In this paper, we propose a first blueprint of a methodology for a sound and reliable evaluation of RAG systems and demonstrate its applicability on a real-world software engineering research task: the validation of configuration dependencies across software technologies. In summary, we make two novel contributions: (i) A novel, reusable methodological design for evaluating RAG systems, including a demonstration that represents a guideline, and (ii) a RAG system, which has been developed following this methodology, that achieves the highest accuracy in the field of dependency validation. For the blueprint's demonstration, the key insights are the crucial role of choosing appropriate baselines and metrics, the necessity for systematic RAG refinements derived from qualitative failure analysis, as well as the reporting practices of key design decision to foster replication and evaluation.},
 author = {Simon, Sebastian and Mailach, Alina and Dorn, Johannes and Siegmund, Norbert},
 date = {10/11/2024},
 year = {2024},
 title = {A Methodology for Evaluating RAG Systems: A Case Study On Configuration Dependency Validation},
 url = {http://arxiv.org/pdf/2410.08801v1},
 keywords = {Benchmarking;evaluation;Replicable;Validity},
 file = {2410.08801v1:Attachments/2410.08801v1.pdf:application/pdf}
}

% This file was created with Citavi 6.19.2.1

@book{Yu.2024,
 abstract = {Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.},
 author = {Yu, Hao and Gan, Aoran and Zhang, Kai and Tong, Shiwei and Liu, Qi and Liu, Zhaofeng},
 year = {2024},
 title = {Evaluation of Retrieval-Augmented Generation: A Survey},
 keywords = {Auepora;Benchmarking;evaluation;Metrics;Retrieval-Augmented Generation},
 file = {Yu, Gan et al. 2024 - Evaluation of Retrieval-Augmented Generation:Attachments/Yu, Gan et al. 2024 - Evaluation of Retrieval-Augmented Generation.pdf:application/pdf}
}

% This file was created with Citavi 6.19.2.1

@inproceedings{Salemi.2024,
 author = {Salemi, Alireza and Zamani, Hamed},
 title = {Evaluating Retrieval Quality in Retrieval-Augmented Generation},
 keywords = {Document-level Retrieval Evaluation;eRAG;Retrieval Evaluation;Retrieval Quality;Retrieval-Augmented Generation},
 pages = {2395--2400},
 publisher = {ACM},
 booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
 year = {2024},
 address = {New York, NY, USA},
 doi = {10.1145/3626772.3657957},
 file = {Salemi, Zamani 2024 - Evaluating Retrieval Quality in Retrieval-Augmented:Attachments/Salemi, Zamani 2024 - Evaluating Retrieval Quality in Retrieval-Augmented.pdf:application/pdf}
}







@misc{Kandpal.15.11.2022,
 abstract = {The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.},
 author = {Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
 date = {15.11.2022},
 year = {2022},
 title = {Large Language Models Struggle to Learn Long-Tail Knowledge},
 url = {http://arxiv.org/pdf/2211.08411},
 keywords = {Hallucinations;LLMs;Long-Tail Information},
 file = {Kandpal, Deng et al. 15.11.2022 - Large Language Models Struggle:Attachments/Kandpal, Deng et al. 15.11.2022 - Large Language Models Struggle.pdf:application/pdf}
}


@inproceedings{Rashkin.,
 author = {Rashkin, Hannah and Reitter, David and Tomar, Gaurav Singh and Das, Dipanjan},
 title = {Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features},
 keywords = {Hallucinations;LLMs},
 pages = {704--718},
 publisher = {{Association for Computational Linguistics}},
 editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 address = {Stroudsburg, PA, USA},
 doi = {10.18653/v1/2021.acl-long.58},
 year = {2021},
 file = {Rashkin, Reitter et al. - Increasing Faithfulness in Knowledge-Grounded Dialogue:Attachments/Rashkin, Reitter et al. - Increasing Faithfulness in Knowledge-Grounded Dialogue.pdf:application/pdf}
}


% RAG Frameworks
@misc{Liu_LlamaIndex_2022,
author = {Liu, Jerry},
doi = {10.5281/zenodo.1234},
month = {11},
title = {{LlamaIndex}},
howpublished = {\url{https://github.com/jerryjliu/llama_index}},
url = {https://github.com/jerryjliu/llama_index},
year = {2022}
}

@misc{Chase_LangChain_2022,
author = {Chase, Harrison},
month = oct,
title = {{LangChain}},
url = {https://github.com/langchain-ai/langchain},
howpublished = {\url{https://github.com/langchain-ai/langchain}},
year = {2022}
}


@misc{AutoRAG,
author = {Kim, Jeffrey and Kim, Bobb},
month = feb,
title = {{AutoRAG}},
url = {https://github.com/Marker-Inc-Korea/AutoRAG},
howpublished = {\url{https://github.com/Marker-Inc-Korea/AutoRAG}},
year = {2024}
}


@inproceedings{zhang-etal-2024-raglab,
    title = "{RAGLAB}: A Modular and Research-Oriented Unified Framework for Retrieval-Augmented Generation",
    author = "Zhang, Xuanwang and
      Song, Yunze and
      Wang, Yidong and
      Tang, Shuyun and
      others",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = dec,
    year = "2024",
    publisher = "Association for Computational Linguistics",
}

@software{Izsak_fastRAG_Efficient_Retrieval_2023,
author = {Izsak, Peter and Berchansky, Moshe and Fleischer, Daniel and Laperdon, Ronen},
license = {Apache-2.0},
month = feb,
title = {{fastRAG: Efficient Retrieval Augmentation and Generation Framework}},
howpublished = {\url{https://github.com/IntelLabs/fastrag}},
url = {https://github.com/IntelLabs/fastrag},
version = {1.0},
year = {2023}
}

@software{Pietsch_Haystack_the_end-to-end_2019,
author = {Pietsch, Malte and Möller, Timo and Kostic, Bogdan and Risch, Julian and Pippi, Massimiliano and Jobanputra, Mayank and Zanzottera, Sara and Cerza, Silvano and Blagojevic, Vladimir and Stadelmann, Thomas and Soni, Tanay and Lee, Sebastian},
month = nov,
title = {{Haystack: the end-to-end NLP framework for pragmatic builders}},
howpublished = {\url{https://github.com/deepset-ai/haystack}},
url = {https://github.com/deepset-ai/haystack},
year = {2019}
}

@misc{ralle,
      title={RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models}, 
      author={Yasuto Hoshi and Daisuke Miyashita and Youyang Ng and Kento Tatsuno and Yasuhiro Morioka and Osamu Torii and Jun Deguchi},
      url={https://arxiv.org/abs/2308.10633},
      year={2023},
      eprint={2308.10633},
      howpublished = {\url{https://arxiv.org/abs/2308.10633}},
      publisher={arXiv}
}

@article{FlashRAG,
    author={Jiajie Jin and
            Yutao Zhu and
            Xinyu Yang and
            Chenghao Zhang and
            Zhicheng Dou},
    title={FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research},
    journal={CoRR},
    volume={abs/2405.13576},
    year={2024},
    url={https://arxiv.org/abs/2405.13576},
    howpublished = {\url{https://arxiv.org/abs/2405.13576}},
    eprinttype={arXiv},
    eprint={2405.13576}
}

% Text Retrieval
% This file was created with Citavi 6.19.2.1

@misc{Zhao.29.02.2024,
 abstract = {Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.},
 author = {Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Jiang, Jie and Cui, Bin},
 date = {29.02.2024},
 title = {Retrieval-Augmented Generation for AI-Generated Content: A Survey},
 url = {http://arxiv.org/pdf/2402.19473},
 file = {Zhao, Zhang et al. 29.02.2024 - Retrieval-Augmented Generation for AI-Generated Content:Attachments/Zhao, Zhang et al. 29.02.2024 - Retrieval-Augmented Generation for AI-Generated Content.pdf:application/pdf}
}


