@misc{PLACEHOLDER, journal={PLACEHOLDER JOURNAL}, author={PLACEHOLDER | Needs to be cited fact}, year={????}, title={ToDo | CITE | ae oe ue ?}, } 

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017, revised in 2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{Wolf.09.10.2019,
 abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \url{https://github.com/huggingface/transformers}.},
 author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and {Le Scao}, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
 date = {09.10.2019},
 year = {2019},
 title = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
 url = {http://arxiv.org/pdf/1910.03771},
 file = {Wolf, Debut et al 09102019 - HuggingFace's Transformers:Attachments/Wolf, Debut et al 09102019 - 
 HuggingFace's Transformers.pdf:application/pdf;Wolf, Debut et al. 09.10.2019 - HuggingFace's Transformers:Attachments/Wolf, Debut et al. 09.10.2019 - HuggingFace's Transformers.pdf:application/pdf}
}

% This file was created with Citavi 6.18.0.1

@misc{Hou.8212023,
 abstract = {Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We select and analyze 395 research papers from January 2017 to January 2024 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and flagging promising areas for future study. Our artifacts are publicly available at https://github.com/xinyi-hou/LLM4SE{\_}SLR.},
 author = {Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and {Lo David} and Grundy, John and Wang, Haoyu},
 date = {8/21/2023},
 year = {2023},
 title = {Large Language Models for Software Engineering: A Systematic Literature Review},
 url = {http://arxiv.org/pdf/2308.10620v6},
 keywords = {Generation Task;LLM4SE},
 file = {2308.10620v6:Attachments/2308.10620v6.pdf:application/pdf}
}


@inproceedings{Yin.2024,
 author = {Yin, Junqi and Bose, Avishek and Cong, Guojing and Lyngaas, Isaac and Anthony, Quentin},
 title = {Comparative Study of Large Language Model Architectures on Frontier},
 pages = {556--569},
 publisher = {IEEE},
 isbn = {979-8-3503-8711-7},
 editor = {Chard, Kyle and Li, Zhuozhao},
 booktitle = {2024 IEEE International Parallel and Distributed Processing Symposium},
 year = {2024},
 address = {Piscataway, NJ},
 doi = {10.1109/IPDPS57955.2024.00056},
 file = {Yin, Bose et al. 5 27 2024 - 5 31 2024 - Comparative Study of Large Language:Attachments/Yin, Bose et al. 5 27 2024 - 5 31 2024 - Comparative Study of Large Language.pdf:application/pdf}
}

@article{ACKLEY.1985,
 author = {Ackley, D. and Hinton, G. and Sejnowski, T.},
 year = {1985},
 title = {A learning algorithm for boltzmann machines},
 pages = {147--169},
 volume = {9},
 number = {1},
 issn = {03640213},
 journal = {Cognitive Science},
 doi = {10.1016/S0364-0213(85)80012-4},
 file = {ACKLEY, HINTON et al. 1985 - A learning algorithm for boltzmann:Attachments/ACKLEY, HINTON et al. 1985 - A learning algorithm for boltzmann.pdf:application/pdf}
}

@misc{Fan.13.05.2018,
 abstract = {We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.},
 author = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
 date = {13.05.2018},
 year = {2018},
 title = {Hierarchical Neural Story Generation},
 url = {http://arxiv.org/pdf/1805.04833},
 file = {Fan, Lewis et al. 13.05.2018 - Hierarchical Neural Story Generation:Attachments/Fan, Lewis et al. 13.05.2018 - Hierarchical Neural Story Generation.pdf:application/pdf}
}

@misc{Holtzman.22.04.2019,
 abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive.  In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
 author = {Holtzman, Ari and Buys, Jan and {Du Li} and Forbes, Maxwell and Choi, Yejin},
 date = {22.04.2019},
 year = {2019},
 title = {The Curious Case of Neural Text Degeneration},
 url = {http://arxiv.org/pdf/1904.09751},
 file = {Holtzman, Buys et al. 22.04.2019 - The Curious Case of Neural:Attachments/Holtzman, Buys et al. 22.04.2019 - The Curious Case of Neural.pdf:application/pdf}
}

@misc{OpenAI_2022, url={https://openai.com/index/chatgpt}, journal={Introducing chatgpt}, author={OpenAI}, year={2022}, month={Nov}} 

@misc{Anthropic_2023, title={Introducing claude}, url={https://www.anthropic.com/news/introducing-claude}, journal={Anthropic}, author={Anthropic}, year={2023}, month={Mar}} 

@misc{DeepL_SE, title={Deepl übersetzer: Der Präziseste übersetzer der welt}, url={https://www.deepl.com/de/}, journal={DeepL Übersetzer: Der präziseste Übersetzer der Welt}, author={DeepL}, year={2017}} 

@misc{Friedman_2022, title={Introducing github copilot: Your AI pair programmer}, url={https://github.blog/news-insights/product-news/introducing-github-copilot-ai-pair-programmer/}, journal={The GitHub Blog}, author={Friedman, Nat}, year={2022}, month={Feb}} 

% This file was created with Citavi 6.19.2.1

@misc{Zhang.03.09.2023,
 abstract = {While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.},
 author = {Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and Wang, Longyue and Luu, Anh Tuan and Bi, Wei and Shi, Freda and Shi, Shuming},
 date = {03.09.2023},
 year = {2023},
 title = {Siren's Song in the AI Ocean: A Survey on Hallucination in Large  Language Models},
 url = {http://arxiv.org/pdf/2309.01219},
 keywords = {Hallucinations;LLMs},
 file = {Zhang, Li et al. 03.09.2023 - Siren's Song in the AI:Attachments/Zhang, Li et al. 03.09.2023 - Siren's Song in the AI.pdf:application/pdf}
}

@misc{Simon.10112024,
 abstract = {Retrieval-augmented generation (RAG) is an umbrella of different components, design decisions, and domain-specific adaptations to enhance the capabilities of large language models and counter their limitations regarding hallucination and outdated and missing knowledge. Since it is unclear which design decisions lead to a satisfactory performance, developing RAG systems is often experimental and needs to follow a systematic and sound methodology to gain sound and reliable results. However, there is currently no generally accepted methodology for RAG evaluation despite a growing interest in this technology. In this paper, we propose a first blueprint of a methodology for a sound and reliable evaluation of RAG systems and demonstrate its applicability on a real-world software engineering research task: the validation of configuration dependencies across software technologies. In summary, we make two novel contributions: (i) A novel, reusable methodological design for evaluating RAG systems, including a demonstration that represents a guideline, and (ii) a RAG system, which has been developed following this methodology, that achieves the highest accuracy in the field of dependency validation. For the blueprint's demonstration, the key insights are the crucial role of choosing appropriate baselines and metrics, the necessity for systematic RAG refinements derived from qualitative failure analysis, as well as the reporting practices of key design decision to foster replication and evaluation.},
 author = {Simon, Sebastian and Mailach, Alina and Dorn, Johannes and Siegmund, Norbert},
 date = {10/11/2024},
 year = {2024},
 title = {A Methodology for Evaluating RAG Systems: A Case Study On Configuration Dependency Validation},
 url = {http://arxiv.org/pdf/2410.08801v1},
 keywords = {Benchmarking;evaluation;Replicable;Validity},
 file = {2410.08801v1:Attachments/2410.08801v1.pdf:application/pdf}
}

@misc{Yu.2024,
 abstract = {Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.},
 author = {Yu, Hao and Gan, Aoran and Zhang, Kai and Tong, Shiwei and Liu, Qi and Liu, Zhaofeng},
 year = {2024},
 title = {Evaluation of Retrieval-Augmented Generation: A Survey},
 keywords = {Auepora;Benchmarking;evaluation;Metrics;Retrieval-Augmented Generation},
 file = {Yu, Gan et al. 2024 - Evaluation of Retrieval-Augmented Generation:Attachments/Yu, Gan et al. 2024 - Evaluation of Retrieval-Augmented Generation.pdf:application/pdf}
}

% This file was created with Citavi 6.19.2.1

@inproceedings{Salemi.2024,
 author = {Salemi, Alireza and Zamani, Hamed},
 title = {Evaluating Retrieval Quality in Retrieval-Augmented Generation},
 keywords = {Document-level Retrieval Evaluation;eRAG;Retrieval Evaluation;Retrieval Quality;Retrieval-Augmented Generation},
 pages = {2395--2400},
 publisher = {ACM},
 booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
 year = {2024},
 address = {New York, NY, USA},
 doi = {10.1145/3626772.3657957},
 file = {Salemi, Zamani 2024 - Evaluating Retrieval Quality in Retrieval-Augmented:Attachments/Salemi, Zamani 2024 - Evaluating Retrieval Quality in Retrieval-Augmented.pdf:application/pdf}
}







@misc{Kandpal.15.11.2022,
 abstract = {The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.},
 author = {Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
 date = {15.11.2022},
 year = {2022},
 title = {Large Language Models Struggle to Learn Long-Tail Knowledge},
 url = {http://arxiv.org/pdf/2211.08411},
 keywords = {Hallucinations;LLMs;Long-Tail Information},
 file = {Kandpal, Deng et al. 15.11.2022 - Large Language Models Struggle:Attachments/Kandpal, Deng et al. 15.11.2022 - Large Language Models Struggle.pdf:application/pdf}
}


@inproceedings{Rashkin.,
 author = {Rashkin, Hannah and Reitter, David and Tomar, Gaurav Singh and Das, Dipanjan},
 title = {Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features},
 keywords = {Hallucinations;LLMs},
 pages = {704--718},
 publisher = {{Association for Computational Linguistics}},
 editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 address = {Stroudsburg, PA, USA},
 doi = {10.18653/v1/2021.acl-long.58},
 year = {2021},
 file = {Rashkin, Reitter et al. - Increasing Faithfulness in Knowledge-Grounded Dialogue:Attachments/Rashkin, Reitter et al. - Increasing Faithfulness in Knowledge-Grounded Dialogue.pdf:application/pdf}
}


% RAG Frameworks
@misc{Liu_LlamaIndex_2022,
author = {Liu, Jerry},
doi = {10.5281/zenodo.1234},
month = {11},
title = {{LlamaIndex}},
howpublished = {\url{https://github.com/jerryjliu/llama_index}},
url = {https://github.com/jerryjliu/llama_index},
year = {2022}
}

@misc{Chase_LangChain_2022,
author = {Chase, Harrison},
month = oct,
title = {{LangChain}},
url = {https://github.com/langchain-ai/langchain},
howpublished = {\url{https://github.com/langchain-ai/langchain}},
year = {2022}
}


@misc{AutoRAG,
author = {Kim, Jeffrey and Kim, Bobb},
month = feb,
title = {{AutoRAG}},
url = {https://github.com/Marker-Inc-Korea/AutoRAG},
howpublished = {\url{https://github.com/Marker-Inc-Korea/AutoRAG}},
year = {2024}
}


@inproceedings{zhang-etal-2024-raglab,
    title = "{RAGLAB}: A Modular and Research-Oriented Unified Framework for Retrieval-Augmented Generation",
    author = "Zhang, Xuanwang and
      Song, Yunze and
      Wang, Yidong and
      Tang, Shuyun and
      others",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = dec,
    year = "2024",
    publisher = "Association for Computational Linguistics",
}

@software{Izsak_fastRAG_Efficient_Retrieval_2023,
author = {Izsak, Peter and Berchansky, Moshe and Fleischer, Daniel and Laperdon, Ronen},
license = {Apache-2.0},
title = {{fastRAG: Efficient Retrieval Augmentation and Generation Framework}},
howpublished = {\url{https://github.com/IntelLabs/fastrag}},
url = {https://github.com/IntelLabs/fastrag},
version = {1.0},
year = {2023}
}

@software{Pietsch_Haystack_the_end-to-end_2019,
author = {Pietsch, Malte and Möller, Timo and Kostic, Bogdan and Risch, Julian and Pippi, Massimiliano and Jobanputra, Mayank and Zanzottera, Sara and Cerza, Silvano and Blagojevic, Vladimir and Stadelmann, Thomas and Soni, Tanay and Lee, Sebastian},
month = nov,
title = {{Haystack: the end-to-end NLP framework for pragmatic builders}},
howpublished = {\url{https://github.com/deepset-ai/haystack}},
url = {https://github.com/deepset-ai/haystack},
year = {2019}
}

@misc{ralle,
      title={RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models}, 
      author={Yasuto Hoshi and Daisuke Miyashita and Youyang Ng and Kento Tatsuno and Yasuhiro Morioka and Osamu Torii and Jun Deguchi},
      url={https://arxiv.org/abs/2308.10633},
      year={2023},
      eprint={2308.10633},
      howpublished = {\url{https://arxiv.org/abs/2308.10633}},
      publisher={arXiv}
}

@article{FlashRAG,
    author={Jiajie Jin and
            Yutao Zhu and
            Xinyu Yang and
            Chenghao Zhang and
            Zhicheng Dou},
    title={FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research},
    journal={CoRR},
    volume={abs/2405.13576},
    year={2024},
    url={https://arxiv.org/abs/2405.13576},
    howpublished = {\url{https://arxiv.org/abs/2405.13576}},
    eprinttype={arXiv},
    eprint={2405.13576}
}

% Text Retrieval
% This file was created with Citavi 6.19.2.1

@misc{Zhao.29.02.2024,
 abstract = {Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.},
 author = {Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Jiang, Jie and Cui, Bin},
 date = {29.02.2024},
 year = {2024},
 title = {Retrieval-Augmented Generation for AI-Generated Content: A Survey},
 url = {http://arxiv.org/pdf/2402.19473},
 file = {Zhao, Zhang et al. 29.02.2024 - Retrieval-Augmented Generation for AI-Generated Content:Attachments/Zhao, Zhang et al. 29.02.2024 - Retrieval-Augmented Generation for AI-Generated Content.pdf:application/pdf}
}


@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}


@misc{karpukhin2020densepassageretrievalopendomain,
      title={Dense Passage Retrieval for Open-Domain Question Answering}, 
      author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
      year={2020},
      eprint={2004.04906},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.04906}, 
}

% This file was created with Citavi 7.0.4.0

@book{Manning.2009,
 author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
 year = {2009},
 title = {Introduction to information retrieval},
 address = {Cambridge},
 edition = {Reprinted.},
 publisher = {{Cambridge Univ. Press}},
 isbn = {9780521865715},
 doi = {10.1017/CBO9780511809071},
 file = {Manning, Raghavan et al. 2009 - Introduction to information retrieval:Attachments/Manning, Raghavan et al. 2009 - Introduction to information retrieval.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@misc{Gao.18.12.2023,
 abstract = {Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
 author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
 date = {18.12.2023},
 year = {2023},
 title = {Retrieval-Augmented Generation for Large Language Models: A Survey},
 url = {http://arxiv.org/pdf/2312.10997},
 keywords = {Advanced RAG;answer faithfulness;augmentation;Context Relevance;evaluation;framework;Large Language Models;Modular RAG;Naive RAG;Retrieval-Augmented Generation},
 file = {Gao, Xiong et al. 18.12.2023 - Retrieval-Augmented Generation for Large Language:Attachments/Gao, Xiong et al. 18.12.2023 - Retrieval-Augmented Generation for Large Language.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@misc{Huang.2023,
 abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.},
 author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
 date = {2024},
 year = {2024},
 title = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
 url = {http://arxiv.org/pdf/2311.05232},
 doi = {10.1145/3703155},
 file = {Huang, Yu et al. 2023 - A Survey on Hallucination:Attachments/Huang, Yu et al. 2023 - A Survey on Hallucination.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@article{Chen.2024,
 abstract = {Retrieval-Augmented Generation (RAG) ist eine vielversprechende Methode, um Halluzinationen von gro{\ss}en Sprachmodellen (LLMs) zu minimieren. Allerdings fehlt es an umfassender Evaluierung, wie sich RAG auf unterschiedliche LLMs auswirkt, was es schwierig macht, Engp{\"a}sse in der Leistungsf{\"a}higkeit von RAG zu identifizieren. In dieser Arbeit untersuchen wir systematisch den Einfluss von RAG auf LLMs und analysieren deren Leistung in vier wesentlichen F{\"a}higkeiten: Noise Robustness (St{\"o}rungsrobustheit), Negative Rejection (Ablehnung falscher Informationen), Information Integration (Informationsintegration) und Counterfactual Robustness (Robustheit gegen{\"u}ber falschen Informationen). Hierf{\"u}r haben wir den Retrieval-Augmented Generation Benchmark (RGB) erstellt und sechs repr{\"a}sentative LLMs in Englisch und Chinesisch bewertet. Die Ergebnisse zeigen, dass, obwohl RAG die Genauigkeit von Antworten verbessert, die Modelle immer noch erhebliche Schw{\"a}chen in diesen Bereichen aufweisen.},
 author = {Chen, Jiawei and Lin, Hongyu and Han, Xianpei and {Le Sun}},
 year = {2024},
 title = {Benchmarking Large Language Models in Retrieval-Augmented Generation},
 keywords = {Benchmarking;Counterfactual Robustness;Information Integration;Large Language Models;Negative Rejection;Noise Robustness;Retrieval-Augmented Generation},
 pages = {17754--17762},
 volume = {38},
 number = {16},
 issn = {2159-5399},
 journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
 doi = {10.1609/aaai.v38i16.29728},
 file = {Chen, Lin et al. 2024 - Benchmarking Large Language Models:Attachments/Chen, Lin et al. 2024 - Benchmarking Large Language Models.pdf:application/pdf}
}

@inproceedings{Shuster.,
 author = {Shuster, Kurt and Poff, Spencer and Chen, Moya and Kiela, Douwe and Weston, Jason},
 title = {Retrieval Augmentation Reduces Hallucination in Conversation},
 pages = {3784--3803},
 publisher = {{Association for Computational Linguistics}},
 editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
 address = {Stroudsburg, PA, USA},
 year = {2021},
 doi = {10.18653/v1/2021.findings-emnlp.320},
 file = {Shuster, Poff et al. - Retrieval Augmentation Reduces Hallucination:Attachments/Shuster, Poff et al. - Retrieval Augmentation Reduces Hallucination.pdf:application/pdf}
}


@article{muennighoff2022mteb,
    doi = {10.48550/ARXIV.2210.07316},
    url = {https://arxiv.org/abs/2210.07316},
    author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
    title = {MTEB: Massive Text Embedding Benchmark},
    publisher = {arXiv},
    journal={arXiv preprint arXiv:2210.07316},  
    year = {2022}
}

% This file was created with Citavi 7.0.4.0

@misc{LeipzigWikipedia.2025,
 abstract = {Leipzig (, LYPE-sig, -⁠sikh; German: [ˈlaɪptsɪ{\c{c}}] ; Upper Saxon: Leibz'sch; Upper Sorbian: Lipsk) is the most populous city in the German state of Saxony. The city has a population of 628,718 inhabitants as of 2023. It is the eighth-largest city in Germany and is part of the Central German Metropolitan Region. The name of the city is usually interpreted as a Slavic term meaning place of linden trees, in line with many other Slavic placenames in the region.

Leipzig is located about 150 km (90 mi) southwest of Berlin, in the southernmost part of the North German Plain (the Leipzig Bay), at the confluence of the White Elster and its tributaries Plei{\ss}e and Parthe along which Leipzig Riverside Forest, Europe's largest intra-city riparian forest has developed. Leipzig is at the centre of Neuseenland (new lake district), consisting of several artificial lakes created from former lignite open-pit mines.

Leipzig has been a trade city since at least the time of the Holy Roman Empire. The city sits at the intersection of the Via Regia and the Via Imperii, two important medieval trade routes. Leipzig's trade fair dates back to 1190. Between 1764 and 1945, the city was a centre of publishing. After the Second World War and during the period of the German Democratic Republic (East Germany) Leipzig remained a major urban centre in East Germany, but its cultural and economic importance declined.

Events in Leipzig in 1989 played a significant role in precipitating the fall of communism in Central and Eastern Europe, mainly through demonstrations starting from St. Nicholas Church. The immediate effects of the reunification of Germany included the collapse of the local economy (which had come to depend on highly polluting heavy industry), severe unemployment, and urban blight. By the early 2000s the trend had reversed, and since then Leipzig has undergone some significant changes, including urban and economic rejuvenation, and modernisation of the transport infrastructure.

Leipzig is home to one of the oldest universities in Europe (Leipzig University). It is the main seat of the German National Library (the second is Frankfurt), the seat of the German Music Archive, as well as of the German Federal Administrative Court. Leipzig Zoo is one of the most modern zoos in Europe and as of 2018 ranks first in Germany and second in Europe.

Leipzig's late-19th-century Gr{\"u}nderzeit architecture consists of around 12,500 buildings. The city's central railway terminus Leipzig Hauptbahnhof is, at 83,460 square metres (898,400 sq ft), Europe's largest railway station measured by floor area. Since Leipzig City Tunnel came into operation in 2013, it has formed the centrepiece of the S-Bahn Mitteldeutschland (S-Bahn Central Germany) public transit system, Germany's largest S-Bahn network, with a system length of 802 km (498 mi).

Leipzig has long been a major centre for music, including classical and modern dark wave. The Thomanerchor (English: St. Thomas Choir of Leipzig), a boys' choir, was founded in 1212. The Leipzig Gewandhaus Orchestra, established in 1743, is one of the oldest symphony orchestras in the world. Several well-known composers lived and worked in Leipzig, including Johann Sebastian Bach (1723 to 1750) and Felix Mendelssohn (1835 to 1847). The University of Music and Theatre {\textquotedbl}Felix Mendelssohn Bartholdy{\textquotedbl} was founded in 1843. The Oper Leipzig, one of the most prominent opera houses in Germany, was founded in 1693. During a stay in Gohlis, which is now part of the city, Friedrich Schiller wrote his poem {\textquotedbl}Ode to Joy{\textquotedbl}.},
 editor = {Wikipedia},
 author = {Wikipedia},
 year = {2025},
 title = {Leipzig},
 url = {https://en.wikipedia.org/w/index.php?title=Leipzig&oldid=1269825057},
 urldate = {21.01.2025},
 file = {Wikipedia (Hg.) 2025 - Leipzig:Attachments/Wikipedia (Hg.) 2025 - Leipzig.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@book{Jing.2024,
 abstract = {This survey explores the synergistic potential of Large Language Models (LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving research area. With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues. VecDBs emerge as a compelling solution to these issues by offering an efficient means to store, retrieve, and manage the high-dimensional vector representations intrinsic to LLM operations. Through this nuanced review, we delineate the foundational principles of LLMs and VecDBs and critically analyze their integration's impact on enhancing LLM functionalities. This discourse extends into a discussion on the speculative future developments in this domain, aiming to catalyze further research into optimizing the confluence of LLMs and VecDBs for advanced data handling and knowledge extraction capabilities.},
 author = {Jing, Zhi and Su, Yongye and Han, Yikun},
 year = {2024},
 title = {When Large Language Models Meet Vector Databases: A Survey},
 file = {Jing, Su et al. 2024 - When Large Language Models Meet:Attachments/Jing, Su et al. 2024 - When Large Language Models Meet.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@inproceedings{Kukreja.2023,
 author = {Kukreja, Sanjay and Kumar, Tarun and Bharate, Vishal and Purohit, Amit and Dasgupta, Abhijit and Guha, Debashis},
 title = {Vector Databases and Vector Embeddings-Review},
 pages = {231--236},
 publisher = {IEEE},
 booktitle = {2023 International Workshop on Artificial Intelligence and Image Processing (IWAIIP)},
 year = {2023},
 doi = {10.1109/iwaiip58158.2023.10462847},
 file = {Vector{\_}Databases{\_}and{\_}Vector{\_}Embeddings-Review:Attachments/Vector{\_}Databases{\_}and{\_}Vector{\_}Embeddings-Review.pdf:application/pdf}
}


@article{Pan.2024,
 abstract = {There are now over 20 commercial vector database management systems (VDBMSs), all produced within the past five years. But embedding-based retrieval has been studied for over ten years, and similarity search a staggering half century and more. Driving this shift from algorithms to systems are new data intensive applications, notably large language models, that demand vast stores of unstructured data coupled with reliable, secure, fast, and scalable query processing capability. A variety of new data management techniques now exist for addressing these needs, however there is no comprehensive survey to thoroughly review these techniques and systems. We start by identifying five main obstacles to vector data management, namely the ambiguity of semantic similarity, large size of vectors, high cost of similarity comparison, lack of structural properties that can be used for indexing, and difficulty of efficiently answering ``hybrid'' queries that jointly search both attributes and vectors. Overcoming these obstacles has led to new approaches to query processing, storage and indexing, and query optimization and execution. For query processing, a variety of similarity scores and query types are now well understood; for storage and indexing, techniques include vector compression, namely quantization, and partitioning techniques based on randomization, learned partitioning, and ``navigable'' partitioning; for query optimization and execution, we describe new operators for hybrid queries, as well as techniques for plan enumeration, plan selection, distributed query processing, data manipulation queries, and hardware accelerated query execution. These techniques lead to a variety of VDBMSs across a spectrum of design and runtime characteristics, including ``native'' systems that are specialized for vectors and ``extended'' systems that incorporate vector capabilities into existing systems. We then discuss benchmarks, and finally outline research challenges and point the direction for future work.},
 author = {Pan, James Jie and Wang, Jianguo and Li, Guoliang},
 year = {2024},
 title = {Survey of vector database management systems},
 url = {https://link.springer.com/article/10.1007/s00778-024-00864-x},
 pages = {1591--1615},
 volume = {33},
 number = {5},
 issn = {0949-877X},
 journal = {The VLDB Journal},
 doi = {10.1007/s00778-024-00864-x},
 file = {Pan, Wang et al. 2024 - Survey of vector database management:Attachments/Pan, Wang et al. 2024 - Survey of vector database management.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@misc{Pinecone.22.01.2025,
 year = {22.01.2025},
 title = {Hierarchical Navigable Small Worlds (HNSW) | Pinecone},
 author = {Pinecone},
 url = {https://www.pinecone.io/learn/series/faiss/hnsw/},
 urldate = {22.01.2025}
}


% This file was created with Citavi 7.0.4.0

@article{Malkov.2014,
 author = {Malkov, Yury and Ponomarenko, Alexander and Logvinov, Andrey and Krylov, Vladimir},
 year = {2014},
 title = {Approximate nearest neighbor algorithm based on navigable small world graphs},
 pages = {61--68},
 volume = {45},
 issn = {03064379},
 journal = {Information Systems},
 doi = {10.1016/j.is.2013.10.006}
}

% This file was created with Citavi 7.0.4.0

@article{Pugh.1990,
 author = {Pugh, William},
 year = {1990},
 title = {Skip lists: a probabilistic alternative to balanced trees},
 pages = {668--676},
 volume = {33},
 number = {6},
 issn = {0001-0782},
 journal = {Communications of the ACM},
 doi = {10.1145/78973.78977}
}


% This file was created with Citavi 7.0.4.0

@misc{ErikBernhardsson.22.01.2025,
 abstract = {Benchmarks of approximate nearest neighbor libraries in Python - erikbern/ann-benchmarks},
 author = {{Erik Bernhardsson}},
 year = {22.01.2025},
 title = {erikbern/ann-benchmarks: Benchmarks of approximate nearest neighbor libraries in Python},
 url = {https://github.com/erikbern/ann-benchmarks?tab=readme-ov-file},
 urldate = {22.01.2025}
}

% This file was created with Citavi 7.0.4.0

@misc{Schulhoff.06.06.2024,
 abstract = {Generative Artificial Intelligence (GenAI) systems are increasingly being deployed across diverse industries and research domains. Developers and end-users interact with these systems through the use of prompting and prompt engineering. Although prompt engineering is a widely adopted and extensively researched area, it suffers from conflicting terminology and a fragmented ontological understanding of what constitutes an effective prompt due to its relatively recent emergence. We establish a structured understanding of prompt engineering by assembling a taxonomy of prompting techniques and analyzing their applications. We present a detailed vocabulary of 33 vocabulary terms, a taxonomy of 58 LLM prompting techniques, and 40 techniques for other modalities. Additionally, we provide best practices and guidelines for prompt engineering, including advice for prompting state-of-the-art (SOTA) LLMs such as ChatGPT. We further present a meta-analysis of the entire literature on natural language prefix-prompting. As a culmination of these efforts, this paper presents the most comprehensive survey on prompt engineering to date.},
 author = {Schulhoff, Sander and Ilie, Michael and Balepur, Nishant and Kahadze, Konstantine and Liu, Amanda and Si, Chenglei and Li, Yinheng and Gupta, Aayush and Han, HyoJung and Schulhoff, Sevien and Dulepet, Pranav Sandeep and Vidyadhara, Saurav and Ki, Dayeon and Agrawal, Sweta and Pham, Chau and Kroiz, Gerson and Li, Feileen and Tao, Hudson and Srivastava, Ashay and {Da Costa}, Hevander and Gupta, Saloni and Rogers, Megan L. and Goncearenco, Inna and Sarli, Giuseppe and Galynker, Igor and Peskoff, Denis and Carpuat, Marine and White, Jules and Anadkat, Shyamal and Hoyle, Alexander and Resnik, Philip},
 date = {06.06.2024},
 title = {The Prompt Report: A Systematic Survey of Prompting Techniques},
 url = {http://arxiv.org/pdf/2406.06608},
 file = {Schulhoff, Ilie et al. 06.06.2024 - The Prompt Report:Attachments/Schulhoff, Ilie et al. 06.06.2024 - The Prompt Report.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@misc{Brown.28.05.2020,
 abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
 author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 date = {28.05.2020},
 title = {Language Models are Few-Shot Learners},
 url = {http://arxiv.org/pdf/2005.14165},
 file = {Brown, Mann et al. 28.05.2020 - Language Models are Few-Shot Learners (2):Attachments/Brown, Mann et al. 28.05.2020 - Language Models are Few-Shot Learners (2).pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@misc{Wei.28.01.2022,
 abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
 author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and {Le Quoc} and Zhou, Denny},
 date = {28.01.2022},
 title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
 url = {http://arxiv.org/pdf/2201.11903},
 file = {Wei, Wang et al. 28.01.2022 - Chain-of-Thought Prompting Elicits Reasoning:Attachments/Wei, Wang et al. 28.01.2022 - Chain-of-Thought Prompting Elicits Reasoning.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@misc{Ru.15.08.2024,
 abstract = {Despite Retrieval-Augmented Generation (RAG) showing promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements. In this paper, we propose a fine-grained evaluation framework, RAGChecker, that incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation verifies that RAGChecker has significantly better correlations with human judgments than other evaluation metrics. Using RAGChecker, we evaluate 8 RAG systems and conduct an in-depth analysis of their performance, revealing insightful patterns and trade-offs in the design choices of RAG architectures. The metrics of RAGChecker can guide researchers and practitioners in developing more effective RAG systems. This work has been open sourced at https://github.com/amazon-science/RAGChecker.},
 author = {Ru, Dongyu and Qiu, Lin and Hu, Xiangkun and Zhang, Tianhang and Shi, Peng and Chang, Shuaichen and Jiayang, Cheng and Wang, Cunxiang and Sun, Shichao and Li, Huanyu and Zhang, Zizhao and Wang, Binjie and Jiang, Jiarong and He, Tong and Wang, Zhiguo and Liu, Pengfei and Zhang, Yue and Zhang, Zheng},
 date = {15.08.2024},
 title = {RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented  Generation},
 url = {http://arxiv.org/pdf/2408.08067},
 file = {Ru, Qiu et al 15082024 - RAGChecker:Attachments/Ru, Qiu et al 15082024 - RAGChecker.pdf:application/pdf;Ru, Qiu et al. 15.08.2024 - RAGChecker:Attachments/Ru, Qiu et al. 15.08.2024 - RAGChecker.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@misc{Chang.06.07.2023,
 abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.},
 author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
 date = {06.07.2023},
 title = {A Survey on Evaluation of Large Language Models},
 url = {http://arxiv.org/pdf/2307.03109},
 file = {Chang, Wang et al. 06.07.2023 - A Survey on Evaluation:Attachments/Chang, Wang et al. 06.07.2023 - A Survey on Evaluation.pdf:application/pdf}
}


% Frameworks

% This file was created with Citavi 7.0.4.0

@misc{Li.13.01.2025,
 abstract = {Retrieval-Augmented Generation (RAG) systems have recently shown remarkable advancements by integrating retrieval mechanisms into language models, enhancing their ability to produce more accurate and contextually relevant responses. However, the influence of various components and configurations within RAG systems remains underexplored. A comprehensive understanding of these elements is essential for tailoring RAG systems to complex retrieval tasks and ensuring optimal performance across diverse applications. In this paper, we develop several advanced RAG system designs that incorporate query expansion, various novel retrieval strategies, and a novel Contrastive In-Context Learning RAG. Our study systematically investigates key factors, including language model size, prompt design, document chunk size, knowledge base size, retrieval stride, query expansion techniques, Contrastive In-Context Learning knowledge bases, multilingual knowledge bases, and Focus Mode retrieving relevant context at sentence-level. Through extensive experimentation, we provide a detailed analysis of how these factors influence response quality. Our findings offer actionable insights for developing RAG systems, striking a balance between contextual richness and retrieval-generation efficiency, thereby paving the way for more adaptable and high-performing RAG frameworks in diverse real-world scenarios. Our code and implementation details are publicly available.},
 author = {Li, Siran and Stenzel, Linus and Eickhoff, Carsten and Bahrainian, Seyed Ali},
 date = {13.01.2025},
 title = {Enhancing Retrieval-Augmented Generation: A Study of Best Practices},
 url = {http://arxiv.org/pdf/2501.07391},
 file = {Li, Stenzel et al. 13.01.2025 - Enhancing Retrieval-Augmented Generation:Attachments/Li, Stenzel et al. 13.01.2025 - Enhancing Retrieval-Augmented Generation.pdf:application/pdf}
}



% This file was created with Citavi 7.0.4.0

@article{JintaoLiu.2024,
 abstract = {A Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline, including chunking, retrieval, reranking, and generation, and a holistic benchmark dataset tailored for diverse data scenarios covering a wide range of document formats and query types is released. Retrieval-Augmented Generation (RAG) aims to enhance large language models (LLMs) to generate more accurate and reliable answers with the help of the retrieved context from external knowledge sources, thereby reducing the incidence of hallucinations. Despite the advancements, evaluating these systems remains a crucial research area due to the following issues: (1) Limited data diversity: The insufficient diversity of knowledge sources and query types constrains the applicability of RAG systems; (2) Obscure problems location: Existing evaluation methods have difficulty in locating the stage of the RAG pipeline where problems occur; (3) Unstable retrieval evaluation: These methods often fail to effectively assess retrieval performance, particularly when the chunking strategy changes. To tackle these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline, including chunking, retrieval, reranking, and generation. To effectively evaluate the first three phases, we introduce multi-granularity keywords, including coarse-grained and fine-grained keywords, to assess the retrieved context instead of relying on the annotation of golden chunks. Moreover, we release a holistic benchmark dataset tailored for diverse data scenarios covering a wide range of document formats and query types. We demonstrate the utility of the CoFE-RAG framework by conducting experiments to evaluate each stage of RAG systems. Our evaluation method provides unique insights into the effectiveness of RAG systems in handling diverse data scenarios, offering a more nuanced understanding of their capabilities and limitations.},
 author = {{Jintao Liu} and {Ruixue Ding} and {Linhao Zhang} and {Pengjun Xie} and {Fie Huang}},
 year = {2024},
 journal = {arXiv.org},
 title = {CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity},
 url = {https://www.semanticscholar.org/paper/CoFE-RAG%3A-A-Comprehensive-Full-chain-Evaluation-for-Liu-Ding/779797f595acb8dc35690c1252a8b415455a3ded},
 file = {Jintao Liu, Ruixue Ding et al. 2024 - CoFE-RAG A Comprehensive Full-chain Evaluation:Attachments/Jintao Liu, Ruixue Ding et al. 2024 - CoFE-RAG A Comprehensive Full-chain Evaluation.pdf:application/pdf}
 
}

% This file was created with Citavi 7.0.4.0

@inproceedings{Barnett.2024,
 author = {Barnett, Scott and Kurniawan, Stefanus and Thudumu, Srikanth and Brannelly, Zach and Abdelrazek, Mohamed},
 title = {Seven Failure Points When Engineering a Retrieval Augmented Generation System},
 pages = {194--199},
 publisher = {ACM},
 isbn = {9798400705915},
 editor = {Bosch, Jan and Lewis, Grace and Cleland-Huang, Jane and Muccini, Henry},
 booktitle = {Proceedings of the IEEE/ACM 3rd International Conference on AI Engineering - Software Engineering for AI},
 year = {2024},
 address = {New York, NY, USA},
 doi = {10.1145/3644815.3644945},
 file = {3644815.3644945:Attachments/3644815.3644945.pdf:application/pdf}
}



% This file was created with Citavi 7.0.4.0

@misc{Liu.06.07.2023,
 abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.},
 author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
 date = {06.07.2023},
 title = {Lost in the Middle: How Language Models Use Long Contexts},
 url = {http://arxiv.org/pdf/2307.03172},
 file = {Liu, Lin et al. 06.07.2023 - Lost in the Middle:Attachments/Liu, Lin et al. 06.07.2023 - Lost in the Middle.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@misc{Wagner.12.11.2024,
 abstract = {In the short period since the release of ChatGPT in November 2022, large language models (LLMs) have changed the software engineering research landscape. While there are numerous opportunities to use LLMs for supporting research or software engineering tasks, solid science needs rigorous empirical evaluations. However, so far, there are no specific guidelines for conducting and assessing studies involving LLMs in software engineering research. Our focus is on empirical studies that either use LLMs as part of the research process (e.g., for data annotation) or studies that evaluate existing or new tools that are based on LLMs. This paper contributes the first set of guidelines for such studies. Our goal is to start a discussion in the software engineering research community to reach a common understanding of what our community standards are for high-quality empirical studies involving LLMs.},
 author = {Wagner, Stefan and Bar{\'o}n, Marvin Mu{\~n}oz and Falessi, Davide and Baltes, Sebastian},
 date = {12.11.2024},
 title = {Towards Evaluation Guidelines for Empirical Studies involving LLMs},
 url = {http://arxiv.org/pdf/2411.07668},
 file = {Wagner, Bar{\'o}n et al 12112024 - Towards Evaluation Guidelines for Empirical:Attachments/Wagner, Bar{\'o}n et al 12112024 - Towards Evaluation Guidelines for Empirical.pdf:application/pdf;Wagner, Bar{\'o}n et al. 12.11.2024 - Towards Evaluation Guidelines for Empirical:Attachments/Wagner, Bar{\'o}n et al. 12.11.2024 - Towards Evaluation Guidelines for Empirical.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@article{M.A.Pimentel.2024,
 abstract = {This paper provides an exploration and critical analysis of some of these evaluation methodologies, shedding light on their strengths, limitations, and impact on advancing the state-of-the-art in natural language processing. As large language models (LLMs) continue to evolve, the need for robust and standardized evaluation benchmarks becomes paramount. Evaluating the performance of these models is a complex challenge that requires careful consideration of various linguistic tasks, model architectures, and benchmarking methodologies. In recent years, various frameworks have emerged as noteworthy contributions to the field, offering comprehensive evaluation tests and benchmarks for assessing the capabilities of LLMs across diverse domains. This paper provides an exploration and critical analysis of some of these evaluation methodologies, shedding light on their strengths, limitations, and impact on advancing the state-of-the-art in natural language processing.},
 author = {{M. A. Pimentel} and {Cl'ement Christophe} and {Tathagata Raha} and {Prateek Munjal} and {Praveen K Kanithi} and {Shadab Khan}},
 year = {2024},
 title = {Beyond Metrics: A Critical Analysis of the Variability in Large Language Model Evaluation Frameworks},
 url = {https://www.semanticscholar.org/paper/Beyond-Metrics%3A-A-Critical-Analysis-of-the-in-Large-Pimentel-Christophe/c36873ca457af16e9d44be609e477d456706b2f5?utm_source=alert_email&utm_content=LibraryFolder&utm_campaign=AlertEmails_WEEKLY&utm_term=LibraryFolder+PaperCitation&email_index=0-6-6&utm_medium=41498835},
 journal = {arXiv.org},
 file = {M. A. Pimentel, Cl'ement Christophe et al. 2024 - Beyond Metrics:Attachments/M. A. Pimentel, Cl'ement Christophe et al. 2024 - Beyond Metrics.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@misc{Krishna.19.09.2024,
 abstract = {Large Language Models (LLMs) have demonstrated significant performance improvements across various cognitive tasks. An emerging application is using LLMs to enhance retrieval-augmented generation (RAG) capabilities. These systems require LLMs to understand user queries, retrieve relevant information, and synthesize coherent and accurate responses. Given the increasing real-world deployment of such systems, comprehensive evaluation becomes crucial. To this end, we propose FRAMES (Factuality, Retrieval, And reasoning MEasurement Set), a high-quality evaluation dataset designed to test LLMs' ability to provide factual responses, assess retrieval capabilities, and evaluate the reasoning required to generate final answers. While previous work has provided datasets and benchmarks to evaluate these abilities in isolation, FRAMES offers a unified framework that provides a clearer picture of LLM performance in end-to-end RAG scenarios. Our dataset comprises challenging multi-hop questions that require the integration of information from multiple sources. We present baseline results demonstrating that even state-of-the-art LLMs struggle with this task, achieving 0.40 accuracy with no retrieval. The accuracy is significantly improved with our proposed multi-step retrieval pipeline, achieving an accuracy of 0.66 ({\textgreater}50{\%} improvement). We hope our work will help bridge evaluation gaps and assist in developing more robust and capable RAG systems.},
 author = {Krishna, Satyapriya and Krishna, Kalpesh and Mohananey, Anhad and Schwarcz, Steven and Stambler, Adam and Upadhyay, Shyam and Faruqui, Manaal},
 date = {19.09.2024},
 title = {Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented  Generation},
 url = {http://arxiv.org/pdf/2409.12941},
 file = {Krishna, Krishna et al 19092024 - Fact, Fetch:Attachments/Krishna, Krishna et al 19092024 - Fact, Fetch.pdf:application/pdf;Krishna, Krishna et al. 19.09.2024 - Fact, Fetch:Attachments/Krishna, Krishna et al. 19.09.2024 - Fact, Fetch.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@book{Es.2023,
 abstract = {We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With RAGAs, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.},
 author = {Es, Shahul and James, Jithin and Espinosa-Anke, Luis and Schockaert, Steven},
 year = {2023},
 title = {RAGAS: Automated Evaluation of Retrieval Augmented Generation},
 keywords = {framework;RAGAS;Retrieval-Augmented Generation},
 file = {Es, James et al. 2023 - RAGAS Automated Evaluation of Retrieval:Attachments/Es, James et al. 2023 - RAGAS Automated Evaluation of Retrieval.pdf:application/pdf}
}


@misc{Fadnis.26.04.2024,
 abstract = {Large Language Models (LLM) have become a popular approach for implementing Retrieval Augmented Generation (RAG) systems, and a significant amount of effort has been spent on building good models and metrics. In spite of increased recognition of the need for rigorous evaluation of RAG systems, few tools exist that go beyond the creation of model output and automatic calculation. We present InspectorRAGet, an introspection platform for RAG evaluation. InspectorRAGet allows the user to analyze aggregate and instance-level performance of RAG systems, using both human and algorithmic metrics as well as annotator quality. InspectorRAGet is suitable for multiple use cases and is available publicly to the community. The demo video is available at https://youtu.be/MJhe8QIXcEc},
 author = {Fadnis, Kshitij and Patel, Siva Sankalp and Boni, Odellia and Katsis, Yannis and Rosenthal, Sara and Sznajder, Benjamin and Danilevsky, Marina},
 date = {26.04.2024},
 title = {InspectorRAGet: An Introspection Platform for RAG Evaluation},
 url = {http://arxiv.org/pdf/2404.17347},
 file = {Fadnis, Patel et al. 26.04.2024 - InspectorRAGet:Attachments/Fadnis, Patel et al. 26.04.2024 - InspectorRAGet.pdf:application/pdf}
}


@misc{Rau.01.07.2024,
 abstract = {Retrieval-Augmented Generation allows to enhance Large Language Models with external knowledge. In response to the recent popularity of generative LLMs, many RAG approaches have been proposed, which involve an intricate number of different configurations such as evaluation datasets, collections, metrics, retrievers, and LLMs. Inconsistent benchmarking poses a major challenge in comparing approaches and understanding the impact of each component in the pipeline. In this work, we study best practices that lay the groundwork for a systematic evaluation of RAG and present BERGEN, an end-to-end library for reproducible research standardizing RAG experiments. In an extensive study focusing on QA, we benchmark different state-of-the-art retrievers, rerankers, and LLMs. Additionally, we analyze existing RAG metrics and datasets. Our open-source library BERGEN is available under \url{https://github.com/naver/bergen}.},
 author = {Rau, David and D{\'e}jean, Herv{\'e} and Chirkova, Nadezhda and Formal, Thibault and Wang, Shuai and Nikoulina, Vassilina and Clinchant, St{\'e}phane},
 date = {01.07.2024},
 title = {BERGEN: A Benchmarking Library for Retrieval-Augmented Generation},
 url = {http://arxiv.org/pdf/2407.01102},
 keywords = {Benchmarking;framework;Retrieval-Augmented Generation},
 file = {Rau, D{\'e}jean et al. 01.07.2024 - BERGEN A Benchmarking Library:Attachments/Rau, D{\'e}jean et al. 01.07.2024 - BERGEN A Benchmarking Library.pdf:application/pdf}
}



@misc{SaadFalcon.16.11.2023,
 abstract = {Evaluating retrieval-augmented generation (RAG) systems traditionally relies on hand annotations for input queries, passages to retrieve, and responses to generate. We introduce ARES, an Automated RAG Evaluation System, for evaluating RAG systems along the dimensions of context relevance, answer faithfulness, and answer relevance. By creating its own synthetic training data, ARES finetunes lightweight LM judges to assess the quality of individual RAG components. To mitigate potential prediction errors, ARES utilizes a small set of human-annotated datapoints for prediction-powered inference (PPI). Across eight different knowledge-intensive tasks in KILT, SuperGLUE, and AIS, ARES accurately evaluates RAG systems while using only a few hundred human annotations during evaluation. Furthermore, ARES judges remain effective across domain shifts, proving accurate even after changing the type of queries and/or documents used in the evaluated RAG systems. We make our code and datasets publicly available on Github.},
 author = {Saad-Falcon, Jon and Khattab, Omar and Potts, Christopher and Zaharia, Matei},
 date = {16.11.2023},
 title = {ARES: An Automated Evaluation Framework for Retrieval-Augmented  Generation Systems},
 url = {http://arxiv.org/pdf/2311.09476},
 keywords = {answer faithfulness;answer relevance;ARES;Context Relevance;evaluation;prediction-powered inference;Retrieval-Augmented Generation;synthetic data generation},
 file = {Saad-Falcon, Khattab et al. 16.11.2023 - ARES An Automated Evaluation Framework:Attachments/Saad-Falcon, Khattab et al. 16.11.2023 - ARES An Automated Evaluation Framework.pdf:application/pdf}
}


@misc{Tu.2024,
 abstract = {Large language models have achieved remarkable success on general NLP tasks, but they may fall short for domain-specific problems. Recently, various Retrieval-Augmented Large Language Models (RALLMs) are proposed to address this shortcoming. However, existing evaluation tools only provide a few baselines and evaluate them on various domains without mining the depth of domain knowledge. In this paper, we address the challenges of evaluating RALLMs by introducing the R-Eval toolkit, a Python toolkit designed to streamline the evaluation of different RAG workflows in conjunction with LLMs. Our toolkit, which supports popular built-in RAG workflows and allows for the incorporation of customized testing data on the specific domain, is designed to be user-friendly, modular, and extensible. We conduct an evaluation of 21 RALLMs across three task levels and two representative domains, revealing significant variations in the effectiveness of RALLMs across different tasks and domains. Our analysis emphasizes the importance of considering both task and domain requirements when choosing a RAG workflow and LLM combination. We are committed to continuously maintaining our platform at this https URL to facilitate both the industry and the researchers.},
 author = {Tu, Shangqing and Wang, Yuanchun and Yu, Jifan and Xie, Yuyang and Shi, Yaran and Wang, Xiaozhi and Zhang, Jing and Hou, Lei and Li, Juanzi},
 date = {2024},
 title = {R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models},
 url = {http://arxiv.org/pdf/2406.11681},
 doi = {10.1145/3637528.3671564},
 file = {Tu, Wang et al. 2024 - R-Eval A Unified Toolkit:Attachments/Tu, Wang et al. 2024 - R-Eval A Unified Toolkit.pdf:application/pdf}
}


@misc{Wang.08.06.2023,
 abstract = {Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our results indicate that PandaLM-7B achieves 93.75{\%} of GPT-3.5's evaluation ability and 88.28{\%} of GPT-4's in terms of F1-score on our test dataset. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage. All resources of PandaLM are released at https://github.com/WeOpenML/PandaLM.},
 author = {Wang, Yidong and Yu, Zhuohao and Zeng, Zhengran and Yang, Linyi and Wang, Cunxiang and Chen, Hao and Jiang, Chaoya and Xie, Rui and Wang, Jindong and Xie, Xing and Ye, Wei and Zhang, Shikun and Zhang, Yue},
 date = {08.06.2023},
 title = {PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning  Optimization},
 url = {http://arxiv.org/pdf/2306.05087},
 file = {Wang, Yu et al. 08.06.2023 - PandaLM An Automatic Evaluation Benchmark:Attachments/Wang, Yu et al. 08.06.2023 - PandaLM An Automatic Evaluation Benchmark.pdf:application/pdf}
}


@misc{Hoshi.8212023b,
 abstract = {Retrieval-augmented large language models (R-LLMs) combine pre-trained large language models (LLMs) with information retrieval systems to improve the accuracy of factual question-answering. However, current libraries for building R-LLMs provide high-level abstractions without sufficient transparency for evaluating and optimizing prompts within specific inference processes such as retrieval and generation. To address this gap, we present RaLLe, an open-source framework designed to facilitate the development, evaluation, and optimization of R-LLMs for knowledge-intensive tasks. With RaLLe, developers can easily develop and evaluate R-LLMs, improving hand-crafted prompts, assessing individual inference processes, and objectively measuring overall system performance quantitatively. By leveraging these features, developers can enhance the performance and accuracy of their R-LLMs in knowledge-intensive generation tasks. We open-source our code at https://github.com/yhoshi3/RaLLe.},
 author = {Hoshi, Yasuto and Miyashita, Daisuke and Ng, Youyang and Tatsuno, Kento and Morioka, Yasuhiro and Torii, Osamu and Deguchi, Jun},
 date = {8/21/2023},
 title = {RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented  Large Language Models},
 url = {http://arxiv.org/pdf/2308.10633v2},
 file = {2308.10633v2:Attachments/2308.10633v2.pdf:application/pdf}
}


@article{JenniferHsia.2024,
 abstract = {RAGGED, a framework for analyzing RAG configurations across various DBQA tasks, discovers distinct LM behaviors in response to varying context quantities, context qualities, and retrievers and provides a deeper analysis of these differences. Retrieval-augmented generation (RAG) can significantly improve the performance of language models (LMs) by providing additional context for tasks such as document-based question answering (DBQA). However, the effectiveness of RAG is highly dependent on its configuration. To systematically find the optimal configuration, we introduce RAGGED, a framework for analyzing RAG configurations across various DBQA tasks. Using the framework, we discover distinct LM behaviors in response to varying context quantities, context qualities, and retrievers. For instance, while some models are robust to noisy contexts, monotonically performing better with more contexts, others are more noise-sensitive and can effectively use only a few contexts before declining in performance. This framework also provides a deeper analysis of these differences by evaluating the LMs{\&}{\#}39; sensitivity to signal and noise under specific context quality conditions. Using RAGGED, researchers and practitioners can derive actionable insights about how to optimally configure their RAG systems for their specific question-answering tasks.},
 author = {{Jennifer Hsia} and {Afreen Shaikh} and {Zhiruo Wang} and {Graham Neubig}},
 year = {2024},
 title = {RAGGED: Towards Informed Design of Retrieval Augmented Generation Systems},
 url = {https://www.semanticscholar.org/paper/RAGGED%3A-Towards-Informed-Design-of-Retrieval-Hsia-Shaikh/e1446fc9b11e73e9f1d867df9012fdc3d3df60d0},
 journal = {arXiv.org},
 file = {Jennifer Hsia, Afreen Shaikh et al. 2024 - RAGGED Towards Informed Design:Attachments/Jennifer Hsia, Afreen Shaikh et al. 2024 - RAGGED Towards Informed Design.pdf:application/pdf}
}


@misc{Jin.5222024,
 abstract = {With the advent of Large Language Models (LLMs), the potential of Retrieval Augmented Generation (RAG) techniques have garnered considerable research attention. Numerous novel algorithms and models have been introduced to enhance various aspects of RAG systems. However, the absence of a standardized framework for implementation, coupled with the inherently intricate RAG process, makes it challenging and time-consuming for researchers to compare and evaluate these approaches in a consistent environment. Existing RAG toolkits like LangChain and LlamaIndex, while available, are often heavy and unwieldy, failing to meet the personalized needs of researchers. In response to this challenge, we propose FlashRAG, an efficient and modular open-source toolkit designed to assist researchers in reproducing existing RAG methods and in developing their own RAG algorithms within a unified framework. Our toolkit implements 12 advanced RAG methods and has gathered and organized 32 benchmark datasets. Our toolkit has various features, including customizable modular framework, rich collection of pre-implemented RAG works, comprehensive datasets, efficient auxiliary pre-processing scripts, and extensive and standard evaluation metrics. Our toolkit and resources are available at https://github.com/RUC-NLPIR/FlashRAG.},
 author = {Jin, Jiajie and Zhu, Yutao and Yang, Xinyu and Zhang, Chenghao and Dou, Zhicheng},
 date = {5/22/2024},
 title = {FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation  Research},
 url = {http://arxiv.org/pdf/2405.13576v1},
 keywords = {Computer Science - Computation and Language;Computer Science - Information Retrieval},
 file = {2405.13576v1:Attachments/2405.13576v1.pdf:application/pdf}
}


@article{KaigeXie.2024,
 abstract = {A novel evaluation framework based on sub-question coverage, which measures how well a RAG system addresses different facets of a question, is introduced, and it is demonstrated that leveraging core sub-questions enhances both retrieval and answer generation in a RAG system, resulting in a 74{\%} win rate over the baseline that lacks sub-questions. Evaluating retrieval-augmented generation (RAG) systems remains challenging, particularly for open-ended questions that lack definitive answers and require coverage of multiple sub-topics. In this paper, we introduce a novel evaluation framework based on sub-question coverage, which measures how well a RAG system addresses different facets of a question. We propose decomposing questions into sub-questions and classifying them into three types -- core, background, and follow-up -- to reflect their roles and importance. Using this categorization, we introduce a fine-grained evaluation protocol that provides insights into the retrieval and generation characteristics of RAG systems, including three commercial generative answer engines: You.com, Perplexity AI, and Bing Chat. Interestingly, we find that while all answer engines cover core sub-questions more often than background or follow-up ones, they still miss around 50{\%} of core sub-questions, revealing clear opportunities for improvement. Further, sub-question coverage metrics prove effective for ranking responses, achieving 82{\%} accuracy compared to human preference annotations. Lastly, we also demonstrate that leveraging core sub-questions enhances both retrieval and answer generation in a RAG system, resulting in a 74{\%} win rate over the baseline that lacks sub-questions.},
 author = {{Kaige Xie} and {Philippe Laban} and {Prafulla Kumar Choubey} and {Caiming Xiong} and {Chien-Sheng Wu}},
 year = {2024},
 title = {Do RAG Systems Cover What Matters? Evaluating and Optimizing Responses with Sub-Question Coverage},
 url = {https://www.semanticscholar.org/paper/Do-RAG-Systems-Cover-What-Matters-Evaluating-and-Xie-Laban/48cd27b420972e5d1af4f071682cc67708d42d73},
 file = {Kaige Xie, Philippe Laban et al. 2024 - Do RAG Systems Cover What:Attachments/Kaige Xie, Philippe Laban et al. 2024 - Do RAG Systems Cover What.pdf:application/pdf}
}


@misc{Kim.10282024,
 abstract = {Using LLMs (Large Language Models) in conjunction with external documents has made RAG (Retrieval-Augmented Generation) an essential technology. Numerous techniques and modules for RAG are being researched, but their performance can vary across different datasets. Finding RAG modules that perform well on specific datasets is challenging. In this paper, we propose the AutoRAG framework, which automatically identifies suitable RAG modules for a given dataset. AutoRAG explores and approximates the optimal combination of RAG modules for the dataset. Additionally, we share the results of optimizing a dataset using AutoRAG. All experimental results and data are publicly available and can be accessed through our GitHub repository https://github.com/Marker-Inc-Korea/AutoRAG{\_}ARAGOG{\_}Paper .},
 author = {Kim, Dongkyu and Kim, Byoungwook and Han, Donggeon and Eibich, Matou{\v{s}}},
 date = {10/28/2024},
 title = {AutoRAG: Automated Framework for optimization of Retrieval Augmented  Generation Pipeline},
 url = {http://arxiv.org/pdf/2410.20878v1},
 file = {2410.20878v1:Attachments/2410.20878v1.pdf:application/pdf}
}



@misc{Zhang.8212024,
 abstract = {Large Language Models (LLMs) demonstrate human-level capabilities in dialogue, reasoning, and knowledge retention. However, even the most advanced LLMs face challenges such as hallucinations and real-time updating of their knowledge. Current research addresses this bottleneck by equipping LLMs with external knowledge, a technique known as Retrieval Augmented Generation (RAG). However, two key issues constrained the development of RAG. First, there is a growing lack of comprehensive and fair comparisons between novel RAG algorithms. Second, open-source tools such as LlamaIndex and LangChain employ high-level abstractions, which results in a lack of transparency and limits the ability to develop novel algorithms and evaluation metrics. To close this gap, we introduce RAGLAB, a modular and research-oriented open-source library. RAGLAB reproduces 6 existing algorithms and provides a comprehensive ecosystem for investigating RAG algorithms. Leveraging RAGLAB, we conduct a fair comparison of 6 RAG algorithms across 10 benchmarks. With RAGLAB, researchers can efficiently compare the performance of various algorithms and develop novel algorithms.},
 author = {Zhang, Xuanwang and Song, Yunze and Wang, Yidong and Tang, Shuyun and Li, Xinfeng and Zeng, Zhengran and Wu, Zhen and Ye, Wei and Xu, Wenyuan and Zhang, Yue and Dai, Xinyu and Zhang, Shikun and Wen, Qingsong},
 date = {8/21/2024},
 title = {RAGLAB: A Modular and Research-Oriented Unified Framework for  Retrieval-Augmented Generation},
 url = {http://arxiv.org/pdf/2408.11381v2},
 keywords = {framework;RAGLAB},
 file = {2408.11381v2:Attachments/2408.11381v2.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@misc{Zeng.28.03.2024,
 abstract = {In the evolving landscape of large language models (LLMs) tailored for software engineering, the need for benchmarks that accurately reflect real-world development scenarios is paramount. Current benchmarks are either too simplistic or fail to capture the multi-tasking nature of software development. To address this, we introduce CoderUJB, a new benchmark designed to evaluate LLMs across diverse Java programming tasks that are executable and reflective of actual development scenarios, acknowledging Java's prevalence in real-world software production. CoderUJB comprises 2,239 programming questions derived from 17 real open-source Java projects and spans five practical programming tasks. Our empirical study on this benchmark investigates the coding abilities of various open-source and closed-source LLMs, examining the effects of continued pre-training in specific programming languages code and instruction fine-tuning on their performance. The findings indicate that while LLMs exhibit strong potential, challenges remain, particularly in non-functional code generation (e.g., test generation and defect detection). Importantly, our results advise caution in the specific programming languages continued pre-training and instruction fine-tuning, as these techniques could hinder model performance on certain tasks, suggesting the need for more nuanced strategies. CoderUJB thus marks a significant step towards more realistic evaluations of programming capabilities in LLMs, and our study provides valuable insights for the future development of these models in software engineering.},
 author = {Zeng, Zhengran and Wang, Yidong and Xie, Rui and Ye, Wei and Zhang, Shikun},
 date = {28.03.2024},
 title = {CoderUJB: An Executable and Unified Java Benchmark for Practical  Programming Scenarios},
 url = {http://arxiv.org/pdf/2403.19287},
 file = {Zeng, Wang et al. 28.03.2024 - CoderUJB An Executable and Unified:Attachments/Zeng, Wang et al. 28.03.2024 - CoderUJB An Executable and Unified.pdf:application/pdf}
}

% This file was created with Citavi 7.0.4.0

@article{Gao.2023,
 abstract = {Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.},
 author = {Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie},
 year = {2023},
 title = {Precise Zero-Shot Dense Retrieval without Relevance Labels},
 url = {https://aclanthology.org/2023.acl-long.99/},
 pages = {1762--1777},
 journal = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2023.acl-long.99},
 file = {Gao, Ma et al. 2023 - Precise Zero-Shot Dense Retrieval:Attachments/Gao, Ma et al. 2023 - Precise Zero-Shot Dense Retrieval.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@misc{Mallen.20.12.2022,
 abstract = {Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only when necessary. Experimental results show that this significantly improves models' performance while reducing the inference costs.},
 author = {Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh},
 date = {20.12.2022},
 title = {When Not to Trust Language Models: Investigating Effectiveness of  Parametric and Non-Parametric Memories},
 url = {http://arxiv.org/pdf/2212.10511},
 file = {Mallen, Asai et al. 20.12.2022 - When Not to Trust Language:Attachments/Mallen, Asai et al. 20.12.2022 - When Not to Trust Language.pdf:application/pdf}
}

% This file was created with Citavi 7.0.4.0

@article{Ferrante.2021,
 author = {Ferrante, Marco and Ferro, Nicola and Fuhr, Norbert},
 year = {2021},
 title = {Towards Meaningful Statements in IR Evaluation: Mapping Evaluation Measures to Interval Scales},
 pages = {136182--136216},
 volume = {9},
 journal = {IEEE Access},
 doi = {10.1109/ACCESS.2021.3116857}
}

% This file was created with Citavi 7.0.4.0

@misc{EvidentlyAIInc..25.02.2025,
 author = {{Evidently AI, Inc.}},
 year = {25.02.2025},
 title = {Mean Average Precision (MAP) in ranking and recommendations},
 url = {https://www.evidentlyai.com/ranking-metrics/mean-average-precision-map},
 urldate = {26.02.2025}
}


% This file was created with Citavi 7.0.4.0

@misc{Li.03.02.2025,
 abstract = {Large Language Models (LLMs) as judges and LLM-based data synthesis have emerged as two fundamental LLM-driven data annotation methods in model development. While their combination significantly enhances the efficiency of model training and evaluation, little attention has been given to the potential contamination brought by this new model development paradigm. In this work, we expose preference leakage, a contamination problem in LLM-as-a-judge caused by the relatedness between the synthetic data generators and LLM-based evaluators. To study this issue, we first define three common relatednesses between data generator LLM and judge LLM: being the same model, having an inheritance relationship, and belonging to the same model family. Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive issue that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios. All of these findings imply that preference leakage is a widespread and challenging problem in the area of LLM-as-a-judge. We release all codes and data at: https://github.com/David-Li0406/Preference-Leakage.},
 author = {Li, Dawei and Sun, Renliang and Huang, Yue and Zhong, Ming and Jiang, Bohan and Han, Jiawei and Zhang, Xiangliang and Wang, Wei and Liu, Huan},
 date = {03.02.2025},
 title = {Preference Leakage: A Contamination Problem in LLM-as-a-judge},
 url = {http://arxiv.org/pdf/2502.01534},
 file = {Li, Sun et al. 03.02.2025 - Preference Leakage:Attachments/Li, Sun et al. 03.02.2025 - Preference Leakage.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@article{Chiang.2023,
 abstract = {Cheng-Han Chiang, Hung-yi Lee. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.},
 author = {Chiang, Cheng-Han and Lee, Hung-yi},
 year = {2023},
 title = {Can Large Language Models Be an Alternative to Human Evaluations?},
 url = {https://aclanthology.org/2023.acl-long.870/},
 pages = {15607--15631},
 journal = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2023.acl-long.870},
 file = {Chiang, Lee 2023 - Can Large Language Models Be:Attachments/Chiang, Lee 2023 - Can Large Language Models Be.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@misc{Lin.13.10.2020,
 abstract = {The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This survey provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has been responsible for a paradigm shift in natural language processing (NLP), information retrieval (IR), and beyond. In this survey, we provide a synthesis of existing work as a single point of entry for practitioners who wish to gain a better understanding of how to apply transformers to text ranking problems and researchers who wish to pursue work in this area. We cover a wide range of modern techniques, grouped into two high-level categories: transformer models that perform reranking in multi-stage architectures and dense retrieval techniques that perform ranking directly. There are two themes that pervade our survey: techniques for handling long documents, beyond typical sentence-by-sentence processing in NLP, and techniques for addressing the tradeoff between effectiveness (i.e., result quality) and efficiency (e.g., query latency, model and index size). Although transformer architectures and pretraining techniques are recent innovations, many aspects of how they are applied to text ranking are relatively well understood and represent mature techniques. However, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, this survey also attempts to prognosticate where the field is heading.},
 author = {Lin, Jimmy and Nogueira, Rodrigo and Yates, Andrew},
 date = {13.10.2020},
 title = {Pretrained Transformers for Text Ranking: BERT and Beyond},
 url = {http://arxiv.org/pdf/2010.06467},
 file = {Lin, Nogueira et al. 13.10.2020 - Pretrained Transformers for Text Ranking:Attachments/Lin, Nogueira et al. 13.10.2020 - Pretrained Transformers for Text Ranking.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@misc{Jin.08.10.2024,
 abstract = {Retrieval-augmented generation (RAG) empowers large language models (LLMs) to utilize external knowledge sources. The increasing capacity of LLMs to process longer input sequences opens up avenues for providing more retrieved information, to potentially enhance the quality of generated outputs. It is plausible to assume that a larger retrieval set would contain more relevant information (higher recall), that might result in improved performance. However, our empirical findings demonstrate that for many long-context LLMs, the quality of generated output initially improves first, but then subsequently declines as the number of retrieved passages increases. This paper investigates this phenomenon, identifying the detrimental impact of retrieved {\textquotedbl}hard negatives{\textquotedbl} as a key contributor. To mitigate this and enhance the robustness of long-context LLM-based RAG, we propose both training-free and training-based approaches. We first showcase the effectiveness of retrieval reordering as a simple yet powerful training-free optimization. Furthermore, we explore training-based methods, specifically RAG-specific implicit LLM fine-tuning and RAG-oriented fine-tuning with intermediate reasoning, demonstrating their capacity for substantial performance gains. Finally, we conduct a systematic analysis of design choices for these training-based methods, including data distribution, retriever selection, and training context length.},
 author = {Jin, Bowen and Yoon, Jinsung and Han, Jiawei and Arik, Sercan O.},
 date = {08.10.2024},
 title = {Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG},
 file = {Jin, Yoon et al 08102024 - Long-Context LLMs Meet RAG:Attachments/Jin, Yoon et al 08102024 - Long-Context LLMs Meet RAG.pdf:application/pdf}
}


% This file was created with Citavi 7.0.4.0

@misc{Xu.16.01.2025,
 abstract = {Language has long been conceived as an essential tool for human reasoning. The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks. Researchers have moved beyond simple autoregressive token generation by introducing the concept of {\textquotedbl}thought{\textquotedbl} -- a sequence of tokens representing intermediate steps in the reasoning process. This innovative paradigm enables LLMs' to mimic complex human reasoning processes, such as tree search and reflective thinking. Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes. This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs' reasoning capacity by providing substantially more training data. Furthermore, recent studies demonstrate that encouraging LLMs to {\textquotedbl}think{\textquotedbl} with more tokens during test-time inference can further significantly boost reasoning accuracy. Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model. The introduction of OpenAI's o1 series marks a significant milestone in this research direction. In this survey, we present a comprehensive review of recent progress in LLM reasoning. We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling. We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions.},
 author = {Xu, Fengli and Hao, Qianyue and Zong, Zefang and Wang, Jingwei and Zhang, Yunke and Wang, Jingyi and Lan, Xiaochong and Gong, Jiahui and Ouyang, Tianjian and Meng, Fanjin and Shao, Chenyang and Yan, Yuwei and Yang, Qinglong and Song, Yiwen and Ren, Sijian and Hu, Xinyuan and Li, Yu and Feng, Jie and Gao, Chen and Li, Yong},
 date = {16.01.2025},
 title = {Towards Large Reasoning Models: A Survey of Reinforced Reasoning with  Large Language Models},
 url = {http://arxiv.org/pdf/2501.09686},
 file = {Xu, Hao et al. 16.01.2025 - Towards Large Reasoning Models:Attachments/Xu, Hao et al. 16.01.2025 - Towards Large Reasoning Models.pdf:application/pdf}
}


